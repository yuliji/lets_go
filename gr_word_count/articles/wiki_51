<doc id="6677" url="https://en.wikipedia.org/wiki?curid=6677" title="Classical liberalism">
Classical liberalism

Classical liberalism is a political ideology and a branch of liberalism which advocates civil liberties under the rule of law with an emphasis on economic freedom. Closely related to economic liberalism, it developed in the early 19th century, building on ideas from the previous century as a response to urbanisation and to the Industrial Revolution in Europe and the United States. Notable individuals whose ideas contributed to classical liberalism include John Locke, Jean-Baptiste Say, Thomas Robert Malthus and David Ricardo. It drew on the classical economic ideas espoused by Adam Smith in Book One of "The Wealth of Nations" and on a belief in natural law, utilitarianism and progress. The term classical liberalism has often been applied in retrospect to distinguish earlier 19th-century liberalism from social liberalism.

Core beliefs of classical liberals included new ideas—which departed from both the older conservative idea of society as a family and from the later sociological concept of society as complex set of social networks. Classical liberals believe that individuals are "egoistic, coldly calculating, essentially inert and atomistic" and that society is no more than the sum of its individual members.

Classical liberals agreed with Thomas Hobbes that government had been created by individuals to protect themselves from each other and that the purpose of government should be to minimize conflict between individuals that would otherwise arise in a state of nature. These beliefs were complemented by a belief that laborers could be best motivated by financial incentive. This belief led to the passage of the Poor Law Amendment Act 1834, which limited the provision of social assistance, based on the idea that markets are the mechanism that most efficiently leads to wealth. Adopting Thomas Robert Malthus's population theory, they saw poor urban conditions as inevitable, believed population growth would outstrip food production and thus regarded that consequence desirable because starvation would help limit population growth. They opposed any income or wealth redistribution, believing it would be dissipated by the lowest orders.

Drawing on ideas of Adam Smith, classical liberals believed that it is in the common interest that all individuals be able to secure their own economic self-interest. They were critical of what would come to be the idea of the welfare state as interfering in a free market. Despite Smith’s resolute recognition of the importance and value of labor and of laborers, classical liberals selectively criticized labour's group rights being pursued at the expense of individual rights while accepting corporations' rights, which led to inequality of bargaining power. Classical liberals argued that individuals should be free to obtain work from the highest-paying employers while the profit motive would ensure that products that people desired were produced at prices they would pay. In a free market, both labor and capital would receive the greatest possible reward while production would be organized efficiently to meet consumer demand. Classical liberals argued for what they called a minimal state, limited to the following functions:

Classical liberals asserted that rights are of a negative nature and therefore stipulate that other individuals and governments are to refrain from interfering with the free market, opposing social liberals who assert that individuals have positive rights, such as the right to vote, the right to an education, the right to health care and the right to a living wage. For society to guarantee positive rights, it requires taxation over and above the minimum needed to enforce negative rights.

Core beliefs of classical liberals did not necessarily include democracy or government by a majority vote by citizens because "there is nothing in the bare idea of majority rule to show that majorities will always respect the rights of property or maintain rule of law". For example, James Madison argued for a constitutional republic with protections for individual liberty over a pure democracy, reasoning that in a pure democracy a "common passion or interest will, in almost every case, be felt by a majority of the whole [...] and there is nothing to check the inducements to sacrifice the weaker party".

In the late 19th century, classical liberalism developed into neo-classical liberalism, which argued for government to be as small as possible to allow the exercise of individual freedom. In its most extreme form, neo-classical liberalism advocated social Darwinism. Right-libertarianism is a modern form of neo-classical liberalism.

Friedrich Hayek identified two different traditions within classical liberalism, namely the British tradition and the French tradition. Hayek saw the British philosophers Bernard Mandeville, David Hume, Adam Smith, Adam Ferguson, Josiah Tucker and William Paley as representative of a tradition that articulated beliefs in empiricism, the common law and in traditions and institutions which had spontaneously evolved but were imperfectly understood. The French tradition included Jean-Jacques Rousseau, Marquis de Condorcet, the Encyclopedists and the Physiocrats. This tradition believed in rationalism and sometimes showed hostility to tradition and religion. Hayek conceded that the national labels did not exactly correspond to those belonging to each tradition since he saw the Frenchmen Montesquieu, Benjamin Constant and Alexis de Tocqueville as belonging to the British tradition and the British Thomas Hobbes, Joseph Priestley, Richard Price and Thomas Paine as belonging to the French tradition. Hayek also rejected the label "laissez-faire" as originating from the French tradition and alien to the beliefs of Hume and Smith.

Guido De Ruggiero also identified differences between "Montesquieu and Rousseau, the English and the democratic types of liberalism" and argued that there was a "profound contrast between the two Liberal systems". He claimed that the spirit of "authentic English Liberalism" had "built up its work piece by piece without ever destroying what had once been built, but basing upon it every new departure". This liberalism had "insensibly adapted ancient institutions to modern needs" and "instinctively recoiled from all abstract proclamations of principles and rights". Ruggiero claimed that this liberalism was challenged by what he called the "new Liberalism of France" that was characterised by egalitarianism and a "rationalistic consciousness".

In 1848, Francis Lieber distinguished between what he called "Anglican and Gallican Liberty". Lieber asserted that "independence in the highest degree, compatible with safety and broad national guarantees of liberty, is the great aim of Anglican liberty, and self-reliance is the chief source from which it draws its strength". On the other hand, Gallican liberty "is sought in government [...]. [T]he French look for the highest degree of political civilization in organizational, that is, in the highest degree of interference by public power".

Classical liberalism in Britain developed from Whiggery and radicalism, was also heavily influenced by French physiocracy and represented a new political ideology. Whiggery had become a dominant ideology following the Glorious Revolution of 1688 and was associated with the defence of the British Parliament, upholding the rule of law and defending landed property. The origins of rights were seen as being in an ancient constitution, which had existed from time immemorial. These rights, which some Whigs considered to include freedom of the press and freedom of speech, were justified by custom rather than by natural rights. They believed that the power of the executive had to be constrained. While they supported limited suffrage, they saw voting as a privilege rather than as a right. However, there was no consistency in Whig ideology and diverse writers including John Locke, David Hume, Adam Smith and Edmund Burke were all influential among Whigs, although none of them was universally accepted.

From the 1790s to the 1820s, British radicals concentrated on parliamentary and electoral reform, emphasising natural rights and popular sovereignty. Richard Price and Joseph Priestley adapted the language of Locke to the ideology of radicalism. The radicals saw parliamentary reform as a first step toward dealing with their many grievances, including the treatment of Protestant Dissenters, the slave trade, high prices and high taxes.

There was greater unity to classical liberalism ideology than there had been with Whiggery. Classical liberals were committed to individualism, liberty and equal rights. They believed that required a free economy with minimal government interference. Writers such as John Bright and Richard Cobden opposed both aristocratic privilege and property, which they saw as an impediment to the development of a class of yeoman farmers. Some elements of Whiggery opposed this new thinking and were uncomfortable with the commercial nature of classical liberalism. These elements became associated with conservatism.
Classical liberalism was the dominant political theory in Britain from the early 19th century until the First World War. Its notable victories were the Catholic Emancipation Act of 1829, the Reform Act of 1832 and the repeal of the Corn Laws in 1846. The Anti-Corn Law League brought together a coalition of liberal and radical groups in support of free trade under the leadership of Richard Cobden and John Bright, who opposed militarism and public expenditure. Their policies of low public expenditure and low taxation were adopted by William Ewart Gladstone when he became Chancellor of the Exchequer and later Prime Minister. Classical liberalism was often associated with religious dissent and nonconformism.

Although classical liberals aspired to a minimum of state activity, they accepted the principle of government intervention in the economy from the early 19th century with passage of the Factory Acts. From around 1840 to 1860, "laissez-faire" advocates of the Manchester School and writers in "The Economist" were confident that their early victories would lead to a period of expanding economic and personal liberty and world peace, but would face reversals as government intervention and activity continued to expand from the 1850s. Jeremy Bentham and James Mill, although advocates of "laissez-faire", non-intervention in foreign affairs and individual liberty, believed that social institutions could be rationally redesigned through the principles of utilitarianism. The Conservative Prime Minister Benjamin Disraeli rejected classical liberalism altogether and advocated Tory democracy. By the 1870s, Herbert Spencer and other classical liberals concluded that historical development was turning against them. By the First World War, the Liberal Party had largely abandoned classical liberal principles.

The changing economic and social conditions of the 19th century led to a division between neo-classical and social (or welfare) liberals, who while agreeing on the importance of individual liberty differed on the role of the state. Neo-classical liberals, who called themselves "true liberals", saw Locke's "Second Treatise" as the best guide and emphasised "limited government" while social liberals supported government regulation and the welfare state. Herbert Spencer in Britain and William Graham Sumner were the leading neo-classical liberal theorists of the 19th century. Neo-classical liberalism has continued into the contemporary era, with writers such as John Rawls. The evolution from classical to social/welfare liberalism is for example reflected in Britain in the evolution of the thought of John Maynard Keynes.

In the United States, liberalism took a strong root because it had little opposition to its ideals, whereas in Europe liberalism was opposed by many reactionary or feudal interests such as the nobility, the aristocracy, the landed gentry, the established church and the aristocratic army officers.

Thomas Jefferson adopted many of the ideals of liberalism, but in the Declaration of Independence changed Locke's "life, liberty and property" to the more socially liberal "Life, Liberty and the pursuit of Happiness". As the United States grew, industry became a larger and larger part of American life; and during the term of its first populist President, Andrew Jackson, economic questions came to the forefront. The economic ideas of the Jacksonian era were almost universally the ideas of classical liberalism. Freedom, according to classical liberals, was maximised when the government took a "hands off" attitude toward the economy. 

Historian Kathleen G. Donohue argues:

Leading magazine "The Nation" espoused liberalism every week starting in 1865 under the influential editor Edwin Lawrence Godkin (1831–1902).

The ideas of classical liberalism remained essentially unchallenged until a series of depressions, thought to be impossible according to the tenets of classical economics, led to economic hardship from which the voters demanded relief. In the words of William Jennings Bryan, "You shall not crucify the American farmer on a cross of gold". Classical liberalism remained the orthodox belief among American businessmen until the Great Depression.

The Great Depression of the 1930s saw a sea change in liberalism, with priority shifting from the producers to consumers. Franklin D. Roosevelt's New Deal represented the dominance of modern liberalism in politics for decades. In the words of Arthur Schlesinger Jr.: 

Alan Wolfe summarizes the viewpoint that there is a continuous liberal understanding that includes both Adam Smith and John Maynard Keynes: 

The view that modern liberalism is a continuation of classical liberalism is not universally shared. James Kurth, Robert E. Lerner, John Micklethwait, Adrian Wooldridge and several other political scholars have argued that classical liberalism still exists today, but in the form of American conservatism. According to Deepak Lal, only in the United States does classical liberalism—through American conservatives—continue to be a significant political force.

Central to classical liberal ideology was their interpretation of John Locke's "Second Treatise of Government" and "A Letter Concerning Toleration", which had been written as a defence of the Glorious Revolution of 1688. Although these writings were considered too radical at the time for Britain's new rulers, they later came to be cited by Whigs, radicals and supporters of the American Revolution. However, much of later liberal thought was absent in Locke's writings or scarcely mentioned and his writings have been subject to various interpretations. For example, there is little mention of constitutionalism, the separation of powers and limited government.

James L. Richardson identified five central themes in Locke's writing: individualism, consent, the concepts of the rule of law and government as trustee, the significance of property and religious toleration. Although Locke did not develop a theory of natural rights, he envisioned individuals in the state of nature as being free and equal. The individual, rather than the community or institutions, was the point of reference. Locke believed that individuals had given consent to government and therefore authority derived from the people rather than from above. This belief would influence later revolutionary movements.

As a trustee, government was expected to serve the interests of the people, not the rulers; and rulers were expected to follow the laws enacted by legislatures. Locke also held that the main purpose of men uniting into commonwealths and governments was for the preservation of their property. Despite the ambiguity of Locke's definition of property, which limited property to "as much land as a man tills, plants, improves, cultivates, and can use the product of", this principle held great appeal to individuals possessed of great wealth.

Locke held that the individual had the right to follow his own religious beliefs and that the state should not impose a religion against Dissenters, but there were limitations. No tolerance should be shown for atheists, who were seen as amoral, or to Catholics, who were seen as owing allegiance to the Pope over their own national government.

Adam Smith's "The Wealth of Nations", published in 1776, was to provide most of the ideas of economics, at least until the publication of John Stuart Mill's "Principles of Political Economy" in 1848. Smith addressed the motivation for economic activity, the causes of prices and the distribution of wealth and the policies the state should follow to maximise wealth.

Smith wrote that as long as supply, demand, prices and competition were left free of government regulation, the pursuit of material self-interest, rather than altruism, would maximise the wealth of a society through profit-driven production of goods and services. An "invisible hand" directed individuals and firms to work toward the public good as an unintended consequence of efforts to maximise their own gain. This provided a moral justification for the accumulation of wealth, which had previously been viewed by some as sinful.

He assumed that workers could be paid wages as low as was necessary for their survival, which was later transformed by David Ricardo and Thomas Robert Malthus into the "iron law of wages". His main emphasis was on the benefit of free internal and international trade, which he thought could increase wealth through specialisation in production. He also opposed restrictive trade preferences, state grants of monopolies and employers' organisations and trade unions. Government should be limited to defence, public works and the administration of justice, financed by taxes based on income.

Smith's economics was carried into practice in the nineteenth century with the lowering of tariffs in the 1820s, the repeal of the Poor Relief Act that had restricted the mobility of labour in 1834 and the end of the rule of the East India Company over India in 1858.

In addition to Smith's legacy, Say's law, Thomas Robert Malthus' theories of population and David Ricardo's iron law of wages became central doctrines of classical economics. The pessimistic nature of these theories provided a basis for criticism of capitalism by its opponents and helped perpetuate the tradition of calling economics the "dismal science".

Jean-Baptiste Say was a French economist who introduced Smith's economic theories into France and whose commentaries on Smith were read in both France and Britain. Say challenged Smith's labour theory of value, believing that prices were determined by utility and also emphasised the critical role of the entrepreneur in the economy. However, neither of those observations became accepted by British economists at the time. His most important contribution to economic thinking was Say's law, which was interpreted by classical economists that there could be no overproduction in a market and that there would always be a balance between supply and demand. This general belief influenced government policies until the 1930s. Following this law, since the economic cycle was seen as self-correcting, government did not intervene during periods of economic hardship because it was seen as futile.

Malthus wrote two books, "An Essay on the Principle of Population" (published in 1798) and "Principles of Political Economy" (published in 1820). The second book which was a rebuttal of Say's law had little influence on contemporary economists. However, his first book became a major influence on classical liberalism. In that book, Malthus claimed that population growth would outstrip food production because population grew geometrically while food production grew arithmetically. As people were provided with food, they would reproduce until their growth outstripped the food supply. Nature would then provide a check to growth in the forms of vice and misery. No gains in income could prevent this and any welfare for the poor would be self-defeating. The poor were in fact responsible for their own problems which could have been avoided through self-restraint.

Ricardo, who was an admirer of Smith, covered many of the same topics, but while Smith drew conclusions from broadly empirical observations he used deduction, drawing conclusions by reasoning from basic assumptions While Ricardo accepted Smith's labour theory of value, he acknowledged that utility could influence the price of some rare items. Rents on agricultural land were seen as the production that was surplus to the subsistence required by the tenants. Wages were seen as the amount required for workers' subsistence and to maintain current population levels. According to his iron law of wages, wages could never rise beyond subsistence levels. Ricardo explained profits as a return on capital, which itself was the product of labour, but a conclusion many drew from his theory was that profit was a surplus appropriated by capitalists to which they were not entitled.

Utilitarianism provided the political justification for implementation of economic liberalism by British governments, which was to dominate economic policy from the 1830s. Although utilitarianism prompted legislative and administrative reform and John Stuart Mill's later writings on the subject foreshadowed the welfare state, it was mainly used as a justification for "laissez-faire".

The central concept of utilitarianism, which was developed by Jeremy Bentham, was that public policy should seek to provide "the greatest happiness of the greatest number". While this could be interpreted as a justification for state action to reduce poverty, it was used by classical liberals to justify inaction with the argument that the net benefit to all individuals would be higher.

Classical liberals saw utility as the foundation for public policies. This broke both with conservative "tradition" and Lockean "natural rights", which were seen as irrational. Utility, which emphasises the happiness of individuals, became the central ethical value of all liberalism. Although utilitarianism inspired wide-ranging reforms, it became primarily a justification for "laissez-faire" economics. However, classical liberals rejected Smith's belief that the "invisible hand" would lead to general benefits and embraced Malthus' view that population expansion would prevent any general benefit and Ricardo's view of the inevitability of class conflict. "Laissez-faire" was seen as the only possible economic approach and any government intervention was seen as useless and harmful. The Poor Law Amendment Act 1834 was defended on "scientific or economic principles" while the authors of the Elizabethan Poor Law of 1601 were seen as not having had the benefit of reading Malthus.

However, commitment to "laissez-faire" was not uniform and some economists advocated state support of public works and education. Classical liberals were also divided on free trade as Ricardo expressed doubt that the removal of grain tariffs advocated by Richard Cobden and the Anti-Corn Law League would have any general benefits. Most classical liberals also supported legislation to regulate the number of hours that children were allowed to work and usually did not oppose factory reform legislation.

Despite the pragmatism of classical economists, their views were expressed in dogmatic terms by such popular writers as Jane Marcet and Harriet Martineau. The strongest defender of "laissez-faire" was "The Economist" founded by James Wilson in 1843. "The Economist" criticised Ricardo for his lack of support for free trade and expressed hostility to welfare, believing that the lower orders were responsible for their economic circumstances. "The Economist" took the position that regulation of factory hours was harmful to workers and also strongly opposed state support for education, health, the provision of water and granting of patents and copyrights.

"The Economist" also campaigned against the Corn Laws that protected landlords in the United Kingdom of Great Britain and Ireland against competition from less expensive foreign imports of cereal products. A rigid belief in "laissez-faire" guided the government response in 1846–1849 to the Great Famine in Ireland, during which an estimated 1.5 million people died. The minister responsible for economic and financial affairs, Charles Wood, expected that private enterprise and free trade, rather than government intervention, would alleviate the famine. The Corn Laws were finally repealed in 1846 by the removal of tariffs on grain which kept the price of bread artificially high, but it came too late to stop the Irish famine, partly because it was done in stages over three years.

Several liberals, including Smith and Cobden, argued that the free exchange of goods between nations could lead to world peace. Erik Gartzke states: "Scholars like Montesquieu, Adam Smith, Richard Cobden, Norman Angell, and Richard Rosecrance have long speculated that free markets have the potential to free states from the looming prospect of recurrent warfare". American political scientists John R. Oneal and Bruce M. Russett, well known for their work on the democratic peace theory, state: 

In "The Wealth of Nations", Smith argued that as societies progressed from hunter gatherers to industrial societies the spoils of war would rise, but that the costs of war would rise further and thus making war difficult and costly for industrialised nations: 
Cobden believed that military expenditures worsened the welfare of the state and benefited a small, but concentrated elite minority, summing up British imperialism, which he believed was the result of the economic restrictions of mercantilist policies. To Cobden and many classical liberals, those who advocated peace must also advocate free markets. The belief that free trade would promote peace was widely shared by English liberals of the 19th and early 20th century, leading the economist John Maynard Keynes (1883–1946), who was a classical liberal in his early life, to say that this was a doctrine on which he was "brought up" and which he held unquestioned only until the 1920s. In his review of a book on Keynes, Michael S. Lawlor argues that it may be in large part due to Keynes' contributions in economics and politics, as in the implementation of the Marshall Plan and the way economies have been managed since his work, "that we have the luxury of not facing his unpalatable choice between free trade and full employment". A related manifestation of this idea was the argument of Norman Angell (1872–1967), most famously before World War I in "The Great Illusion" (1909), that the interdependence of the economies of the major powers was now so great that war between them was futile and irrational; and therefore unlikely.




</doc>
<doc id="6678" url="https://en.wikipedia.org/wiki?curid=6678" title="Cat">
Cat

The cat ("Felis catus") is a small carnivorous mammal. It is the only domesticated species in the family Felidae and often referred to as the domestic cat to distinguish it from wild members of the family. The cat is either a house cat, kept as a pet, or a feral cat, freely ranging and avoiding human contact.
A house cat is valued by humans for companionship and for its ability to hunt rodents. About 60 cat breeds are recognized by various cat registries.

Cats are similar in anatomy to the other felid species, with a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. They are predators who are most active at dawn and dusk (crepuscular). Cats can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small animals. Compared to humans, they see better in the dark (they see in near total darkness) and have a better sense of smell, but poorer color vision. Cats, despite being solitary hunters, are a social species. Cat communication includes the use of vocalizations including mewing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. Cats also communicate by secreting and perceiving pheromones.

Female domestic cats can have kittens from spring to late autumn, with litter sizes ranging from two to five kittens. 
Domestic cats can be bred and shown as registered pedigreed cats, a hobby known as cat fancy. Failure to control the breeding of pet cats by spaying and neutering, as well as abandonment of pets, has resulted in large numbers of feral cats worldwide, contributing to the extinction of entire bird species, and evoking population control.

It was long thought that cat domestication was initiated in Egypt, because cats in ancient Egypt were venerated since around 3100 BC.
However, the earliest indication for the taming of an African wildcat ("F. lybica") was found in Cyprus, where a cat skeleton was excavated close by a human Neolithic grave dating to around 7500 BC. African wildcats were probably first domesticated in the Near East. The leopard cat ("Prionailurus bengalensis") was tamed independently in China around 5500 BC, though this line of partially domesticated cats leaves no trace in the domestic cat populations of today.
As of 2017, the domestic cat was the second-most popular pet in the U.S. by number of pets owned, after freshwater fish, with 95 million cats owned. As of 2017, it was ranked the third-most popular pet in the UK, after fish and dogs, with around 8 million being owned. The number of cats in the United Kingdom has nearly doubled since 1965, when the cat population was 4.1 million.

The origin of the English word "cat" (Old English ) and its counterparts in other Germanic languages (such as German ), descended from Proto-Germanic "*kattōn-", is controversial. It has traditionally thought to be a borrowing from Late Latin , 'domestic cat', from (used around 75 AD by Martial), compare also Byzantine Greek , Portuguese and Spanish , French , Maltese , Lithuanian , and Old Church Slavonic (), among others.

The Late Latin word is generally thought to originate from an Afro-Asiatic language, but every proposed source word has presented problems. Many references refer to "Berber" (Kabyle) , 'wildcat', and Nubian as possible sources or cognates, but M. Lionel Bender suggests the Nubian term is a loan from Arabic . Jean-Paul Savignac suggests the Latin word is from an Ancient Egyptian precursor of Coptic , 'tomcat', or its feminine form suffixed with , but John Huehnergard says "the source [...] was clearly not Egyptian itself, where no analogous form is attested." Huehnergard opines it is "equally likely that the forms might derive from an ancient Germanic word, imported into Latin and thence to Greek and to Syriac and Arabic". Guus Kroonen also considers the word to be native to Germanic (due to morphological alternations) and Northern Europe, and suggests that it might ultimately be borrowed from Uralic, cf. Northern Sami , 'female stoat', and Hungarian , 'stoat'; from Proto-Uralic "*käďwä", 'female (of a furred animal)'. In any case, "cat" is a classic example of a word that has spread as a loanword among numerous languages and cultures: a .

An alternative word is English "puss" (extended as "pussy" and "pussycat"). Attested only from the 16th century, it may have been introduced from Dutch or from Low German , related to Swedish , or Norwegian , . Similar forms exist in Lithuanian and Irish or . The etymology of this word is unknown, but it may have simply arisen from a sound used to attract a cat.


The scientific name "Felis catus" for the domestic cat was proposed by Carl Linnaeus in the 10th edition of Systema Naturae published in 1758.
"Felis catus domesticus" was a scientific name proposed by the German naturalist Erxleben in 1777.
"Felis daemon" proposed by Satunin in 1904 was a black cat specimen from the Transcaucasus, later identified as a domestic cat.

In 2003, the International Commission on Zoological Nomenclature (ICZN) fixed the scientific name for the wildcat as "F. silvestris". The same commission ruled that the domestic cat is a distinct taxon "Felis catus".
Following results of phylogenetic research, the domestic cat was considered a wildcat subspecies "F. silvestris catus" in 2007.

In 2017, the IUCN Cat Classification Taskforce followed the recommendation of the ICZN in regarding the domestic cat as a distinct species.

The domestic cat is a member of the Felidae, a family that had a common ancestor about 10–15 million years ago.
The genus "Felis" diverged from the Felidae around 6–7 million years ago.
Members of this genus include the jungle cat ("F. chaus"), European wildcat ("F. silvestris"), African wildcat ("F. lybica"), Chinese mountain cat ("F. bieti"), sand cat ("F. margarita") and black-footed cat ("F. nigripes").
Results of phylogenetic research confirm that these wild "Felis" species evolved through sympatric or parapatric speciation, whereas the domestic cat evolved through artificial selection.

The earliest known indication for a tamed African wildcat was excavated close by a human grave in Shillourokambos, southern Cyprus, dating to about 9,200 to 9,500 years before present. As there is no evidence of native mammalian fauna on Cyprus, the inhabitants of this Neolithic village most likely brought the cat and other wild mammals to the island from the continent. Scientists therefore assume that African wildcats were attracted to early human settlements in the Fertile Crescent by rodents, in particular the house mouse ("Mus musculus"), and were tamed by Neolithic farmers. This commensal relationship between early farmers and tamed cats lasted thousands of years. As agricultural practices spread, so did tame and domesticated cats. Wildcats of Egypt contributed to the maternal gene pool of the domestic cat at a later time.
The earliest known evidence for the occurrence of the domestic cat in Greece dates to around 1200 BC. Greek, Phoenician, Carthaginian and Etruscan traders introduced domestic cats to southern Europe.
By the 5th century BC, it was a familiar animal around settlements in Magna Graecia and Etruria.
Domesticated cats were introduced to Corsica and Sardinia during the Roman Empire before the beginning of the 1st millennium.
The Egyptian domestic cat lineage is evidenced in a Baltic Sea port in northern Germany by the end of the Roman Empire in the 5th century.
During domestication, cats have undergone only minor changes in anatomy and behavior, and they are still capable of surviving in the wild. House cats often interbreed with feral cats, producing hybrids such as the Kellas cat in Scotland. Hybridisation between domestic and other small wild cat species is also possible.

Several natural behaviors and characteristics of wildcats may have preadapted them for domestication as pets. These traits include their small size, social nature, obvious body language, love of play and relatively high intelligence. Captive "Leopardus" cats may also display affectionate behavior toward humans, but have not been domesticated.

Domestic cats are generally smaller than wildcats in both skull and limb measurements.
Adult domestic cats typically weigh between although many breeds have a wide range of sizes, with male American Shorthairs, a common breed, ranging from 3 and 7 kg (7 to 15 lb) Some breeds, such as the Maine Coon, can occasionally exceed . Very small cats, less than , have been reported. The world record for the largest cat is . The smallest adult cat ever officially recorded weighed around . Feral cats tend to be lighter, as they have more limited access to food than house cats. The average feral adult male weighs , and the average adult female . Cats average about in height and in head/body length (males being larger than females), with tails averaging in length.

Cats have seven cervical vertebrae (as do most mammals); 13 thoracic vertebrae (humans have 12); seven lumbar vertebrae (humans have five); three sacral vertebrae (as do most mammals, but humans have five); and a variable number of caudal vertebrae in the tail (humans have only vestigial caudal vertebrae, fused into an internal coccyx). The extra lumbar and thoracic vertebrae account for the cat's spinal mobility and flexibility. Attached to the spine are 13 ribs, the shoulder, and the pelvis. Unlike human arms, cat forelimbs are attached to the shoulder by free-floating clavicle bones which allow them to pass their body through any space into which they can fit their head.

The cat skull is unusual among mammals in having very large eye sockets and a powerful specialized jaw. Within the jaw, cats have teeth adapted for killing prey and tearing meat. When it overpowers its prey, a cat delivers a lethal neck bite with its two long canine teeth, inserting them between two of the prey's vertebrae and severing its spinal cord, causing irreversible paralysis and death. Compared to other felines, domestic cats have narrowly spaced canine teeth relative to the size of their jaw, which is an adaptation to their preferred prey of small rodents, which have small vertebrae. The premolar and first molar together compose the carnassial pair on each side of the mouth, which efficiently shears meat into small pieces, like a pair of scissors. These are vital in feeding, since cats' small molars cannot chew food effectively, and cats are largely incapable of mastication. Although cats tend to have better teeth than most humans, with decay generally less likely because of a thicker protective layer of enamel, a less damaging saliva, less retention of food particles between teeth, and a diet mostly devoid of sugar, they are nonetheless subject to occasional tooth loss and infection.

Cats, like dogs, are digitigrades. They walk directly on their toes, with the bones of their feet making up the lower part of the visible leg. Cats are capable of walking very precisely because, like all felines, they directly register; that is, they place each hind paw (almost) directly in the print of the corresponding fore paw, minimizing noise and visible tracks. This also provides sure footing for their hind paws when they navigate rough terrain. Unlike most mammals, when cats walk, they use a "pacing" gait; that is, they move the two legs on one side of the body before the legs on the other side. This trait is shared with camels and giraffes. As a walk speeds up into a trot, a cat's gait changes to be a "diagonal" gait, similar to that of most other mammals (and many other land animals, such as lizards): the diagonally opposite hind and fore legs move simultaneously.

Like almost all members of the Felidae, cats have protractable and retractable claws. In their normal, relaxed position, the claws are sheathed with the skin and fur around the paw's toe pads. This keeps the claws sharp by preventing wear from contact with the ground and allows the silent stalking of prey. The claws on the fore feet are typically sharper than those on the hind feet. Cats can voluntarily extend their claws on one or more paws. They may extend their claws in hunting or self-defense, climbing, kneading, or for extra traction on soft surfaces. Most cats have five claws on their front paws, and four on their rear paws. The fifth front claw (the dewclaw) is proximal to the other claws. More proximally is a protrusion which appears to be a sixth "finger". This special feature of the front paws, on the inside of the wrists, is the carpal pad, also found on the paws of big cats and dogs. It has no function in normal walking, but is thought to be an antiskidding device used while jumping. Some breeds of cats are prone to polydactyly (extra toes and claws). These are particularly common along the northeast coast of North America.

Cats have excellent night vision and can see at only one-sixth the light level required for human vision. This is partly the result of cat eyes having a tapetum lucidum, which reflects any light that passes through the retina back into the eye, thereby increasing the eye's sensitivity to dim light. Another adaptation to dim light is the large pupils of cats' eyes. Unlike some big cats, such as tigers, domestic cats have slit pupils. These slit pupils can focus bright light without chromatic aberration, and are needed since the domestic cat's pupils are much larger, relative to their eyes, than the pupils of the big cats. At low light levels, a cat's pupils will expand to cover most of the exposed surface of its eyes. However, domestic cats have rather poor color vision and (like most nonprimate mammals) have only two types of cones, optimized for sensitivity to blue and yellowish green; they have limited ability to distinguish between red and green. A 1993 paper reported a response to middle wavelengths from a system other than the rods which might be due to a third type of cone. However, this appears to be an adaptation to low light levels rather than representing true trichromatic vision.

Cats have excellent hearing and can detect an extremely broad range of frequencies. They can hear higher-pitched sounds than either dogs or humans, detecting frequencies from 55 Hz to 79,000 Hz, a range of 10.5 octaves, while humans and dogs both have ranges of about 9 octaves. Cats can hear ultrasound, which is important in hunting because many species of rodents make ultrasonic calls. However, they do not communicate using ultrasound like rodents do. Cats' hearing is also sensitive and among the best of any mammal, being most acute in the range of 500 Hz to 32 kHz. This sensitivity is further enhanced by the cat's large movable outer ears (their pinnae), which both amplify sounds and help detect the direction of a noise.

Cats have an acute sense of smell, due in part to their well-developed olfactory bulb and a large surface of olfactory mucosa, about in area, which is about twice that of humans.

Cats and many other animals have a Jacobson's organ in their mouths that is used in the behavioral process of flehmening. It allows them to sense certain aromas in a way that humans cannot. Cats are sensitive to pheromones such as 3-mercapto-3-methylbutan-1-ol, which they use to communicate through urine spraying and marking with scent glands. Many cats also respond strongly to plants that contain nepetalactone, especially catnip, as they can detect that substance at less than one part per billion. About 70–80% of cats are affected by nepetalactone. This response is also produced by other plants, such as silver vine ("Actinidia polygama") and the herb valerian; it may be caused by the smell of these plants mimicking a pheromone and stimulating cats' social or sexual behaviors.

Cats have relatively few taste buds compared to humans (470 or so versus more than 9,000 on the human tongue). Domestic and wild cats share a gene mutation that keeps their sweet taste buds from binding to sugary molecules, leaving them with no ability to taste sweetness. Their taste buds instead respond to acids, amino acids like protein, and bitter tastes. Cats also have a distinct temperature preference for their food, preferring food with a temperature around which is similar to that of a fresh kill and routinely rejecting food presented cold or refrigerated (which would signal to the cat that the "prey" item is long dead and therefore possibly toxic or decomposing).
To aid with navigation and sensation, cats have dozens of movable whiskers (vibrissae) over their body, especially their faces. These provide information on the width of gaps and on the location of objects in the dark, both by touching objects directly and by sensing air currents; they also trigger protective blink reflexes to protect the eyes from damage.

Most breeds of cat have a noted fondness for settling in high places, or perching. In the wild, a higher place may serve as a concealed site from which to hunt; domestic cats may strike prey by pouncing from a perch such as a tree branch, as does a leopard. Another possible explanation is that height gives the cat a better observation point, allowing it to survey its territory. A cat falling from heights of up to 3 meters can right itself and land on its paws.

During a fall from a high place, a cat reflexively twists its body and rights itself to land on its feet using its acute sense of balance and flexibility. This reflex is known as the cat righting reflex.
An individual cat always rights itself in the same way during a fall, provided it has sufficient time to do so. The height required for this to occur is around .
Cats without a tail also have this reflex.
Several explanations have been proposed for this phenomenon since the late 19th century:

Cats are familiar and easily kept animals, and their physiology has been particularly well studied; it generally resembles those of other carnivorous mammals, but displays several unusual features probably attributable to cats' descent from desert-dwelling species.

Cats are able to tolerate quite high temperatures: Humans generally start to feel uncomfortable when their skin temperature passes about , but cats show no discomfort until their skin reaches around , and can tolerate temperatures of up to if they have access to water.

Cats conserve heat by reducing the flow of blood to their skin and lose heat by evaporation through their mouths. Cats have minimal ability to sweat, with glands located primarily in their paw pads, and pant for heat relief only at very high temperatures (but may also pant when stressed). A cat's body temperature does not vary throughout the day; this is part of cats' general lack of circadian rhythms and may reflect their tendency to be active both during the day and at night.

Cats' feces are comparatively dry and their urine is highly concentrated, both of which are adaptations to allow cats to retain as much water as possible. Their kidneys are so efficient, they can survive on a diet consisting only of meat, with no additional water, and can even rehydrate by drinking seawater.

While domestic cats are able to swim, they are generally reluctant to enter water as it quickly leads to exhaustion.

Cats are obligate carnivores: their physiology has evolved to efficiently process meat, and they have difficulty digesting plant matter. In contrast to omnivores such as rats, which only require about 4% protein in their diet, about 20% of a cat's diet must be protein. A cat's gastrointestinal tract is adapted to meat eating, being much shorter than that of omnivores and having low levels of several of the digestive enzymes needed to digest carbohydrates. These traits severely limit the cat's ability to digest and use plant-derived nutrients, as well as certain fatty acids. Despite the cat's meat-oriented physiology, several vegetarian or vegan cat foods have been marketed that are supplemented with chemically synthesized taurine and other nutrients, in attempts to produce a complete diet. Some of these products still fail to provide all the nutrients cats require, and diets containing no animal products pose the risk of causing severe nutritional deficiencies.

Cats do eat grass occasionally. A proposed explanation is that cats use grass as a source of folic acid. Another is that it is used to supply dietary fiber, helping the cat defecate more easily and expel parasites and other harmful material through feces and vomit.

Cats are unusually dependent on a constant supply of the amino acid arginine, and a diet lacking arginine causes marked weight loss and can be rapidly fatal. Arginine is an essential additive in cat food because cats have low levels of the enzymes aminotransferase and pyrroline-5-carboxylate which are responsible for the synthesis of ornithine and citrulline in the small intestine. Citrulline would typically go on to the kidneys to make arginine, but because cats have a deficiency in the enzymes that make it, citrulline is not produced in adequate quantities to make arginine. Arginine is essential in the urea cycle in order to convert the toxic component ammonia into urea that can then be excreted in the urine. Because of its essential role, deficiency in arginine results in a buildup of toxic ammonia and leads to hyperammonemia.
The symptoms of hyperammonemia include lethargy, vomiting, ataxia, hyperesthesia and can be serious enough to induce death and coma in a matter of days if a cat is being fed an arginine-free diet. The quick onset of these symptoms is due to the fact that diets devoid in arginine will typically still contain all of the other amino acids, which will continue to be catabolized by the body, producing mass amounts of ammonia that very quickly build up with no way of being excreted.

Another unusual feature is that the cat cannot produce taurine, with a deficiency in this nutrient causing macular degeneration, wherein the cat's retina slowly breaks down, causing irreversible blindness.
This is due to the hepatic activity of cystinesulfinic acid decarboxylase being low in cats. This limits the ability of cats to biosynthesize the taurine they need from its precursor, the amino acid cysteine, which ultimately results in inadequate taurine production needed for normal function. Deficiencies in taurine result in compensated function of feline cardiovascular and reproductive systems. These abnormalities can also be accompanied by developmental issues in the central nervous system along with degeneration of the retina.

Niacin is an essential vitamin for the cat; dietary deficiency can lead to anorexia, weight loss and an increase in body temperature. Biosynthesis of niacin occurs by metabolism of tryptophan via the kynurenine pathway to quinolinic acid, the niacin precursor. However, cats have a high activity of picolinic acid carboxylase, which converts one of the intermediates to picolinic acid instead of quinolinic acid. As a result, niacin can become deficient and require supplementation.

Preformed vitamin A is required in the cat for retinal and reproductive health. Vitamin A is considered to be a fat-soluble vitamin and is seen as essential in a cat's diet. Normally, the conversion of beta-carotenes into vitamin A occurs in the intestine (more specifically the mucosal layer) of species, however cats lack the ability to undergo this process. Both the kidney and liver are contributors to the use of vitamin A in the body of the majority of species while the cats liver does not produce the enzyme Beta-carotene 15,15'-monooxygenase which converts the beta-carotene into retinol (vitamin A). To summarize: cats do not have high levels of this enzyme leading to the cleavage and oxidation of carotenoids not taking place.

Vitamin D3 is a dietary requirement for cats as they lack the ability to synthesize vitamin D3 from sunlight. Cats obtain high levels of the enzyme 7-dehydrocholestrol delta 7 reductase which causes immediate conversion of vitamin D3 from sunlight to 7-dehydrocholesterol. This fat soluble vitamin is required in cats for bone formation through the promotion of calcium retention, along with nerve and muscle control through absorption of calcium and phosphorus.

Cats, like all mammals, need to get linoleic acid, an essential fatty acid, from their diet. Most mammals can convert linoleic acid to arachidonic acid, as well as the omega 3 fatty acids (eicosapentaenoic acid and docosahexaenoic acid) through the activity of enzymes, but this process is very limited in cats. The Δ6-desaturase enzyme eventually converts linoleic acid, which is in its salt form linoleate, to arachidonate (salt form of arachidonic acid) in the liver, but this enzyme has very little activity in cats. This means that arachidonic acid is an essential fatty acid for cats as they lack the ability to create required amounts of linoleic acid. Deficiency of arachidonic acid in cats is related to problems in growth, can cause injury and inflammation to skin (e.g. around the mouth) decreased platelet aggregation, fatty liver, increase in birth defects of kittens whose queens were deficient during pregnancy, and reproductive failure in queens. Arachidonic acid can also be metabolized to eicosanoids that create inflammatory responses which are needed to stimulate proper growth and repair mechanisms in the cat.

Outdoor cats are active both day and night, although they tend to be slightly more active at night. The timing of cats' activity is quite flexible and varied, which means house cats may be more active in the morning and evening, as a response to greater human activity at these times. Although they spend the majority of their time in the vicinity of their home, housecats can range many hundreds of meters from this central point, and are known to establish territories that vary considerably in size, in one study ranging from .

Cats conserve energy by sleeping more than most animals, especially as they grow older. The daily duration of sleep varies, usually between 12 and 16 hours, with 13 and 14 being the average. Some cats can sleep as much as 20 hours. The term "cat nap" for a short rest refers to the cat's tendency to fall asleep (lightly) for a brief period. While asleep, cats experience short periods of rapid eye movement sleep often accompanied by muscle twitches, which suggests they are dreaming.

Although wildcats are solitary, the social behavior of domestic cats is much more variable and ranges from widely dispersed individuals to feral cat colonies that gather around a food source, based on groups of co-operating females. Within such groups, one cat is usually dominant over the others. Each cat in a colony holds a distinct territory, with sexually active males having the largest territories, which are about 10 times larger than those of female cats and may overlap with several females' territories. These territories are marked by urine spraying, by rubbing objects at head height with secretions from facial glands, and by defecation. Between these territories are neutral areas where cats watch and greet one another without territorial conflicts. Outside these neutral areas, territory holders usually chase away stranger cats, at first by staring, hissing, and growling and, if that does not work, by short but noisy and violent attacks. Despite some cats cohabiting in colonies, they do not have a social survival strategy, or a pack mentality and always hunt alone.

However, some pet cats are poorly socialized. In particular, older cats may show aggressiveness towards newly arrived kittens, which may include biting and scratching; this type of behavior is known as feline asocial aggression.

Though cats and dogs are often characterized as natural enemies, they can live together if correctly socialized.

Life in proximity to humans and other domestic animals has led to a symbiotic social adaptation in cats, and cats may express great affection toward humans or other animals. Ethologically, the human keeper of a cat may function as a sort of surrogate for the cat's mother, and adult housecats live their lives in a kind of extended kittenhood, a form of behavioral neoteny. The high-pitched sounds housecats make to solicit food may mimic the cries of a hungry human infant, making them particularly difficult for humans to ignore.

Domestic cats' scent rubbing behavior towards humans or other cats is thought to be a feline means for social bonding.

Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling/snarling, grunting, and several different forms of meowing. By contrast, feral cats are generally silent. Their types of body language, including position of ears and tail, relaxation of the whole body, and kneading of the paws, are all indicators of mood. The tail and ears are particularly important social signal mechanisms in cats; for example, a raised tail acts as a friendly greeting, and flattened ears indicates hostility. Tail-raising also indicates the cat's position in the group's social hierarchy, with dominant individuals raising their tails less often than subordinate animals. Nose-to-nose touching is also a common greeting and may be followed by social grooming, which is solicited by one of the cats raising and tilting its head.

Purring may have developed as an evolutionary advantage as a signalling mechanism of reassurance between mother cats and nursing kittens. Post-nursing cats often purr as a sign of contentment: when being petted, becoming relaxed, or eating. The mechanism by which cats purr is elusive. The cat has no unique anatomical feature that is clearly responsible for the sound. It was until recent times, believed that only the cats of the "Felis" genus could purr. However, felids of the genus "Panthera" (tiger, lion, jaguar, and leopard) also produce non-continuous sounds, called "chuffs", similar to purring, but only when exhaling.

Cats are known for spending considerable amounts of time licking their coats to keep them clean. The cat's tongue has backwards-facing spines about 500 μm long, which are called papillae. These contain keratin which makes them rigid so the papillae act like a hairbrush. Some cats, particularly longhaired cats, occasionally regurgitate hairballs of fur that have collected in their stomachs from grooming. These clumps of fur are usually sausage-shaped and about long. Hairballs can be prevented with remedies that ease elimination of the hair through the gut, as well as regular grooming of the coat with a comb or stiff brush.

Among domestic cats, males are more likely to fight than females. Among feral cats, the most common reason for cat fighting is competition between two males to mate with a female. In such cases, most fights are won by the heavier male. Another common reason for fighting in domestic cats is the difficulty of establishing territories within a small home. Female cats also fight over territory or to defend their kittens. Neutering will decrease or eliminate this behavior in many cases, suggesting that the behavior is linked to sex hormones.

When cats become aggressive, they try to make themselves appear larger and more threatening by raising their fur, arching their backs, turning sideways and hissing or spitting. Often, the ears are pointed down and back to avoid damage to the inner ear and potentially listen for any changes behind them while focused forward. They may also vocalize loudly and bare their teeth in an effort to further intimidate their opponent. Fights usually consist of grappling and delivering powerful slaps to the face and body with the forepaws as well as bites. Cats also throw themselves to the ground in a defensive posture to rake their opponent's belly with their powerful hind legs.

Serious damage is rare, as the fights are usually short in duration, with the loser running away with little more than a few scratches to the face and ears. However, fights for mating rights are typically more severe and injuries may include deep puncture wounds and lacerations. Normally, serious injuries from fighting are limited to infections of scratches and bites, though these can occasionally kill cats if untreated. In addition, bites are probably the main route of transmission of feline immunodeficiency virus. Sexually active males are usually involved in many fights during their lives, and often have decidedly battered faces with obvious scars and cuts to their ears and nose.

Cats hunt small prey, primarily birds and rodents, and are often used as a form of pest control. Domestic cats are a major predator of wildlife in the United States, killing an estimated 1.4 to 3.7 billion birds and 6.9 to 20.7 billion mammals annually. The bulk of predation in the United States is done by 80 million feral and stray cats. Effective measures to reduce this population are elusive, meeting opposition from cat enthusiasts. In the case of free-ranging pets, equipping cats with bells and not letting them out at night will reduce wildlife predation.

Free-fed feral cats and house cats tend to consume many small meals in a single day, although the frequency and size of meals varies between individuals. Cats use two hunting strategies, either stalking prey actively, or waiting in ambush until an animal comes close enough to be captured. Although it is not certain, the strategy used may depend on the prey species in the area, with cats waiting in ambush outside burrows, but tending to actively stalk birds.

Perhaps the best known element of cats' hunting behavior, which is commonly misunderstood and often appalls cat owners because it looks like torture, is that cats often appear to "play" with prey by releasing it after capture. This behavior is due to an instinctive imperative to ensure that the prey is weak enough to be killed without endangering the cat. This behavior is referred to in the idiom "cat-and-mouse game" or simply "cat and mouse".

Another poorly understood element of cat hunting behavior is the presentation of prey to human guardians. Ethologist Paul Leyhausen proposed that cats adopt humans into their social group and share excess kill with others in the group according to the dominance hierarchy, in which humans are reacted to as if they are at, or near, the top. Anthropologist and zoologist Desmond Morris, in his 1986 book "Catwatching", suggests, when cats bring home mice or birds, they are attempting to teach their human to hunt, or trying to help their human as if feeding "an elderly cat, or an inept kitten". Morris's hypothesis is inconsistent with the fact that male cats also bring home prey, despite males having negligible involvement with raising kittens.

Domestic cats select food based on its temperature, smell and texture; they dislike chilled foods and respond most strongly to moist foods rich in amino acids, which are similar to meat. Cats may reject novel flavors (a response termed neophobia) and learn quickly to avoid foods that have tasted unpleasant in the past. They may also avoid sugary foods and milk. Most adult cats are lactose intolerant; the sugars in milk are not easily digested and may cause soft stools or diarrhea. They can also develop odd eating habits. Some cats like to eat or chew on other things, most commonly wool, but also plastic, cables, paper, string, aluminum foil, or even coal. This condition, pica, can threaten their health, depending on the amount and toxicity of the items eaten. See also Animal psychopathology § Pica.

Though cats usually prey on animals less than half their size, a feral cat in Australia has been photographed killing an adult pademelon of around the cat's weight at .

Since cats lack sufficient lips to create suction, they use a lapping method with the tongue to draw liquid upwards into their mouths. Lapping at a rate of four times a second, the cat touches the smooth tip of its tongue to the surface of the water, and quickly retracts it like a corkscrew, drawing water upwards.

Domestic cats, especially young kittens, are known for their love of play. This behavior mimics hunting and is important in helping kittens learn to stalk, capture, and kill prey. Cats also engage in play fighting, with each other and with humans. This behavior may be a way for cats to practice the skills needed for real combat, and might also reduce any fear they associate with launching attacks on other animals.

Owing to the close similarity between play and hunting, cats prefer to play with objects that resemble prey, such as small furry toys that move rapidly, but rapidly lose interest (they become habituated) in a toy they have played with before. Cats also tend to play with toys more when they are hungry. String is often used as a toy, but if it is eaten, it can become caught at the base of the cat's tongue and then move into the intestines, a medical emergency which can cause serious illness, even death. Owing to the risks posed by cats eating string, it is sometimes replaced with a laser pointer's dot, which cats may chase.

Female cats are seasonally polyestrous, which means they may have many periods of heat over the course of a year, the season beginning in spring and ending in late autumn. Heat periods occur about every two weeks and last about 4 to 7 days. Multiple males will be attracted to a female in heat. The males will fight over her, and the victor wins the right to mate. At first, the female rejects the male, but eventually the female allows the male to mate. The female utters a loud yowl as the male pulls out of her because a male cat's penis has a band of about 120–150 backwards-pointing penile spines, which are about 1 mm long; upon withdrawal of the penis, the spines rake the walls of the female's vagina, which acts to induce ovulation. This act also occurs to clear the vagina of other sperm in the context of a second (or more) mating, thus giving the later males a larger chance of conception.

After mating, the female cleans her vulva thoroughly. If a male attempts to mate with her at this point, the female attacks him. After about 20 to 30 minutes, once the female is finished grooming, the cycle will repeat.

Because ovulation is not always triggered by a single mating, females may not be impregnated by the first male with which they mate. Furthermore, cats are superfecund; that is, a female may mate with more than one male when she is in heat, with the result that different kittens in a litter may have different fathers.

The morula forms 124 hours after conception. At 148 hours, early blastocysts form. At 10–12 days, implantation occurs.

The gestation period of queens is between 64 and 67 days, with an average of 66 days.
Data on reproductive capacity of more than 2,300 free-ranging queens were collected during a study between May 1998 and October 2000. They had one to six kittens per litter, with an average of three kittens. They produced a mean of 1.4 litters per year, but a maximum of three litters in a year. Of 169 kittens, 127 died before they were six months old due to a trauma caused in most cases by dog attacks and road accidents.
The first litter is usually smaller than subsequent litters. Kittens are weaned between six and seven weeks of age. Queens normally reach sexual maturity at 5–10 months, and males at 5–7 months. This varies depending on breed.

Cats are ready to go to new homes at about 12 weeks of age, when they are ready to leave their mother. They can be surgically sterilized (spayed or castrated) as early as 7 weeks to limit unwanted reproduction. This surgery also prevents undesirable sex-related behavior, such as aggression, territory marking (spraying urine) in males and yowling (calling) in females. Traditionally, this surgery was performed at around six to nine months of age, but it is increasingly being performed before puberty, at about three to six months. In the US, about 80% of household cats are neutered.

The average lifespan of pet cats has risen in recent decades. In the early 1980s, it was about seven years, rising to 9.4 years in 1995 and 15.1 years in 2018. Some cats have been reported as surviving into their 30s, with the oldest known cat, Creme Puff, dying at a verified age of 38.

Spaying or neutering increases life expectancy: one study found neutered male cats live twice as long as intact males, while spayed female cats live 62% longer than intact females. Having a cat neutered confers health benefits, because castrated males cannot develop testicular cancer, spayed females cannot develop uterine or ovarian cancer, and both have a reduced risk of mammary cancer.

Despite widespread concern about the welfare of free-roaming cats, the lifespans of neutered feral cats in managed colonies compare favorably with those of pet cats.

The domestic cat is a cosmopolitan species and occurs across much of the world. It can live on the highest mountains and in the hottest deserts. It is adaptable and now present on all continents except Antarctica, and on 118 of the 131 main groups of islands—even on isolated islands such as the Kerguelen Islands.

The domestic cat's ability to thrive in almost any terrestrial habitat has led to its designation as one of the world's most invasive species.
As it is little altered from the wildcat, it can readily interbreed with the wildcat. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary and possibly also the Iberian Peninsula.

Feral cats can live in forests, grasslands, tundra, coastal areas, agricultural land, scrublands, urban areas, and wetlands. Their habitats include small islands with no human inhabitants. The close relatives of the domestic cat, the African wildcat ("Felis lybica") and the sand cat ("F. margarita") both inhabit desert environments. Domestic cats still show similar adaptations and behaviors.

Feral cats are domestic cats that were born in or have reverted to a wild state. They are unfamiliar with and wary of humans and roam freely in urban and rural areas. The numbers of feral cats is not known, but estimates of the US feral population range from 25 to 60 million. Feral cats may live alone, but most are found in large colonies, which occupy a specific territory and are usually associated with a source of food. Famous feral cat colonies are found in Rome around the Colosseum and Forum Romanum, with cats at some of these sites being fed and given medical attention by volunteers.

Public attitudes towards feral cats vary widely, ranging from seeing them as free-ranging pets, to regarding them as vermin. One common approach to reducing the feral cat population is termed 'trap-neuter-return', where the cats are trapped, neutered, immunized against diseases such as rabies and the feline Panleukopenia and Leukemia viruses, and then released. Before releasing them back into their feral colonies, the attending veterinarian often nips the tip off one ear to mark it as neutered and inoculated, since these cats may be trapped again. Volunteers continue to feed and give care to these cats throughout their lives. Given this support, their lifespans are increased, and behavior and nuisance problems caused by competition for food are reduced.

To date, little scientific data is available to assess the impact of cat predation on prey populations outside of agricultural situations. Even well-fed domestic cats may hunt and kill, mainly catching small mammals, but also birds, amphibians, reptiles, fish, and invertebrates. Hunting by domestic cats may be contributing to the decline in the numbers of birds in urban areas, although the importance of this effect remains controversial. In the wild, the introduction of feral cats during human settlement can threaten native species with extinction. In many cases, controlling or eliminating the populations of non-native cats can produce a rapid recovery in native animals. However, the ecological role of introduced cats can be more complicated. For example, cats can control the numbers of rats, which also prey on birds' eggs and young, so a cat population can protect an endangered bird species by suppressing mesopredators.

In isolated landmasses, such as Australasia, there are often no other native, medium-sized quadrupedal predators (including other feline species); this tends to exacerbate the impact of feral cats on small native animals. Native species such as the New Zealand kakapo and the Australian bettong, for example, tend to be more ecologically vulnerable and behaviorally "naive", when faced with predation by cats. Feral cats have had a major impact on these native species and have played a leading role in the endangerment and extinction of many animals.

Even in places with ancient and numerous cat populations, such as Western Europe, cats appear to be growing in number and independently of their environments' carrying capacity (such as the numbers of prey available). This may be explained, at least in part, by an abundance of food, from sources including feeding by pet owners and scavenging. For instance, research in Britain suggests that a high proportion of cats hunt only "recreationally", and in South Sweden, where research in 1982 found that the population density of cats was as high as .

In agricultural settings, cats can be effective at keeping mouse and rat populations low, but only if rodent harborage locations (such as tall grass) are kept under control. While cats are effective at preventing rodent population explosions, they are not effective for eliminating pre-existing severe infestations.

The domestic cat is a significant predator of birds. UK assessments indicate they may be accountable for an estimated 64.8 million bird deaths each year. A 2012 study suggests feral cats may kill several billion birds each year in the United States. Certain species appear more susceptible than others; for example, 30% of house sparrow mortality is linked to the domestic cat. In the recovery of ringed robins ("Erithacus rubecula") and dunnocks ("Prunella modularis"), 31% of deaths were a result of cat predation. In parts of North America, the presence of larger carnivores such as coyotes which prey on cats and other small predators reduces the effect of predation by cats and other small predators such as opossums and raccoons on bird numbers and variety. The proposal that cat populations will increase when the numbers of these top predators decline is called the mesopredator release hypothesis.

On islands, birds can contribute as much as 60% of a cat's diet. In nearly all cases, however, the cat cannot be identified as the sole cause for reducing the numbers of island birds, and in some instances, eradication of cats has caused a 'mesopredator release' effect; where the suppression of top carnivores creates an abundance of smaller predators that cause a severe decline in their shared prey. Domestic cats are, however, known to be a contributing factor to the decline of many species, a factor that has ultimately led, in some cases, to extinction. The South Island piopio, Chatham rail, and the New Zealand merganser are a few from a long list, with the most extreme case being the flightless Lyall's wren, which was driven to extinction only a few years after its discovery.

Some of the same factors that have promoted adaptive radiation of island avifauna over evolutionary time appear to promote vulnerability to non-native species in modern time. The susceptibility of many island birds is undoubtedly due to evolution in the absence of mainland predators, competitors, diseases, and parasites, in addition to lower reproductive rates and extended incubation periods. The loss of flight, or reduced flying ability is also characteristic of many island endemics. These biological aspects have increased vulnerability to extinction in the presence of introduced species, such as the domestic cat. Equally, behavioral traits exhibited by island species, such as "predatory naivety" and ground-nesting, have also contributed to their susceptibility.

Cats are common pets throughout the world, and their worldwide population exceeds 500 million as of 2007. Although cat guardianship has commonly been associated with women, a 2007 Gallup poll reported that men and women in the United States of America were equally likely to own a cat.

As well as being kept as pets, cats are also used in the international fur and leather industries for making coats, hats, blankets, and stuffed toys; and shoes, gloves, and musical instruments respectively (about 24 cats are needed to make a cat-fur coat). This use has been outlawed in the United States, Australia, and the European Union. Cat pelts have been used for superstitious purposes as part of the practise of witchcraft, and are still made into blankets in Switzerland as folk remedies believed to help rheumatism. In the Western intellectual tradition, the idea of cats as everyday objects have served to illustrate problems of quantum mechanics in the Schrödinger's cat thought experiment.

A few attempts to build a cat census have been made over the years, both through associations or national and international organizations (such as the Canadian Federation of Humane Societies's one) and over the Internet, but such a task does not seem simple to achieve. General estimates for the global population of domestic cats range widely from anywhere between 200 million to 600 million.

Cats are popular as a subject of art and photography, Walter Chandoha made his career photographing cats after his 1949 images of "Loco", an especially charming stray taken in, were published around the world. He is reported to have photographed 90,000 cats during his career and maintained an archive of 225,000 images that he drew from for publications during his lifetime.

A cat show is a judged event in which the owners of cats compete to win titles in various cat registering organizations by entering their cats to be judged after a breed standard. Both pedigreed and companion (or moggy) cats are admissible, although the rules differ from organization to organization. Cats are compared to a breed standard, and the owners of those judged to be closest to it are awarded a prize. Moggies are judged based on their temperament. Often, at the end of the year, all of the points accrued at various shows are added up and more national and regional titles are awarded.

A cat café is a theme café whose attraction is cats that can be watched and played with.

Ailurophobia (Greek αἴλουρος (ailouros), 'cat' and φόβος (phóbos), 'fear') is a fear of cats. The term may also be used to mean a hatred of cats.

Cats may bite humans when provoked, during play or when aggressive. Complications from cat bites can develop. A cat bite differs from the bites of other pets. This is because the teeth of a cat are sharp and pointed causing deep punctures. Skin usually closes rapidly over the bite and traps microorganisms that cause infection.

Cats can be infected or infested with viruses, bacteria, fungus, protozoans, arthropods or worms that can transmit diseases to humans. In some cases, the cat exhibits no symptoms of the disease, However, the same disease can then become evident in a human. The likelihood that a person will become diseased depends on the age and immune status of the person. Humans who have cats living in their home or in close association are more likely to become infected, however, those who do not keep cats as pets might also acquire infections from cat feces and parasites exiting the cat's body. Some of the infections of most concern include salmonella, cat-scratch disease and toxoplasmosis.

In ancient Egypt, cats were sacred animals, with the goddess Bast often depicted in cat form, sometimes taking on the war-like aspect of a lioness. Killing a cat was absolutely forbidden and the Greek historian Herodotus reports that, whenever a household cat died, the entire family would mourn and shave their eyebrows. Families took their dead cats to the sacred city of Bubastis, where they were embalmed and buried in sacred repositories. 
The earliest unmistakable evidence of the Greeks having domestic cats comes from two coins from Magna Graecia dating to the mid-fifth century BC showing Iokastos and Phalanthos, the legendary founders of Rhegion and Taras respectively, playing with their pet cats.

House cats seem to have been extremely rare among the ancient Greeks and Romans; Herodotus expressed astonishment at the domestic cats in Egypt, because he had only ever seen wildcats. Even during later times, weasels were far more commonly kept as pets and weasels, not cats, were seen as the ideal rodent-killers. The usual ancient Greek word for "cat" was "ailouros", meaning "thing with the waving tail", but this word could also be applied to any of the "various long-tailed carnivores kept for catching mice". Cats are rarely mentioned in ancient Greek literature, but Aristotle does remark in his "History of Animals" that "female cats are naturally lecherous." The Greeks later syncretized their own goddess Artemis with the Egyptian goddess Bast, adopting Bastet's associations with cats and ascribing them to Artemis. In Ovid's "Metamorphoses", when the deities flee to Egypt and take animal forms, the goddess Diana (the Roman equivalent of Artemis) turns into a cat. Cats eventually displaced ferrets as the pest control of choice because they were more pleasant to have around the house and were more enthusiastic hunters of mice. During the Middle Ages, many of Artemis's associations with cats were grafted onto the Virgin Mary. Cats are often shown in icons of Annunciation and of the Holy Family and, according to Italian folklore, on the same night that Mary gave birth to Jesus, a virgin cat in Bethlehem gave birth to a kitten. Domestic cats were spread throughout much of the rest of the world during the Age of Discovery, as ships' cats were carried on sailing ships to control shipboard rodents and as good-luck charms.

Several ancient religions believed cats are exalted souls, companions or guides for humans, that are all-knowing but mute so they cannot influence decisions made by humans. In Japan, the cat is a symbol of good fortune. In Norse mythology, Freyja, the goddess of love, beauty, and fertility, is depicted as riding a chariot drawn by cats. In Jewish legend, the first cat was living in the house of the first man Adam as a pet that got rid of mice. The cat was once partnering with the first dog before the latter broke an oath they had made which resulted in enmity between the descendants of these two animals. It is also written that neither cats nor foxes are represented in the water, while every other animal has an incarnation species in the water. Although no species are sacred in Islam, cats are revered by Muslims. Some Western writers have stated Muhammad had a favorite cat, Muezza. He is reported to have loved cats so much, "he would do without his cloak rather than disturb one that was sleeping on it". The story has no origin in early Muslim writers, and seems to confuse a story of a later Sufi saint, Ahmed ar-Rifa'i, centuries after Muhammad. One of the companions of Muhammad was known as "Abu Hurayrah" (Father of the Kitten), in reference to his documented affection to cats.

Many cultures have negative superstitions about cats. An example would be the belief that a black cat "crossing one's path" leads to bad luck, or that cats are witches' familiars used to augment a witch's powers and skills. The killing of cats in Medieval Ypres, Belgium, is commemorated in the innocuous present-day Kattenstoet (cat parade). In medieval France, cats would be burnt alive as a form of entertainment. According to Norman Davies, the assembled people "shrieked with laughter as the animals, howling with pain, were singed, roasted, and finally carbonized".

"It was the custom to burn a basket, barrel, or sack full of live cats, which was hung from a tall mast in the midst of the bonfire; sometimes a fox was burned. The people collected the embers and ashes of the fire and took them home, believing that they brought good luck. The French kings often witnessed these spectacles and even lit the bonfire with their own hands. In 1648 Louis XIV, crowned with a wreath of roses and carrying a bunch of roses in his hand, kindled the fire, danced at it and partook of the banquet afterwards in the town hall. But this was the last occasion when a monarch presided at the midsummer bonfire in Paris. At Metz midsummer fires were lighted with great pomp on the esplanade, and a dozen cats, enclosed in wicker cages, were burned alive in them, to the amusement of the people. Similarly at Gap, in the department of the Hautes-Alpes, cats used to be roasted over the midsummer bonfire."

According to a myth in many cultures, cats have multiple lives. In many countries, they are believed to have nine lives, but in Italy, Germany, Greece, Brazil and some Spanish-speaking regions, they are said to have seven lives, while in Turkish and Arabic traditions, the number of lives is six. The myth is attributed to the natural suppleness and swiftness cats exhibit to escape life-threatening situations. Also lending credence to this myth is the fact that falling cats often land on their feet, using an instinctive righting reflex to twist their bodies around. Nonetheless, cats can still be injured or killed by a high fall.

The domesticated cat and its closest wild ancestor are both diploid organisms that possess 38 chromosomes and roughly 20,000 genes.

Diseases which affect cats include acute infections, parasitic infestations, injuries; and chronic diseases such as kidney disease, thyroid disease, and arthritis. Vaccinations are available for many infectious diseases, as are treatments to eliminate parasites such as worms and fleas.

About 250 heritable genetic disorders have been identified in cats, many similar to human inborn errors. The high level of similarity among the metabolism of mammals allows many of these feline diseases to be diagnosed using genetic tests that were originally developed for use in humans, as well as the use of cats as animal models in the study of the human diseases.




</doc>
<doc id="6681" url="https://en.wikipedia.org/wiki?curid=6681" title="Crank">
Crank

Crank may refer to:








</doc>
<doc id="6682" url="https://en.wikipedia.org/wiki?curid=6682" title="Clade">
Clade

A clade (from , "klados", "branch"), also known as monophyletic group, is a group of organisms that consists of a common ancestor and all its lineal descendants, and represents a single "branch" on the "tree of life".

The common ancestor may be an individual, a population, a species (extinct or extant), and so on right up to a kingdom and further. Clades are nested, one in another, as each branch in turn splits into smaller branches. These splits reflect evolutionary history as populations diverged and evolved independently. Clades are termed monophyletic (Greek: "one clan") groups.

Over the last few decades, the cladistic approach has revolutionized biological classification and revealed surprising evolutionary relationships among organisms. Increasingly, taxonomists try to avoid naming taxa that are not clades; that is, taxa that are not monophyletic. Some of the relationships between organisms that the molecular biology arm of cladistics has revealed are that fungi are closer relatives to animals than they are to plants, archaea are now considered different from bacteria, and multicellular organisms may have evolved from archaea.

The term "clade" was coined in 1957 by the biologist Julian Huxley to refer to the result of cladogenesis, a concept Huxley borrowed from Bernhard Rensch.

Many commonly named groups, rodents and insects for example, are clades because, in each case, the group consists of a common ancestor with all its descendant branches. Rodents, for example, are a branch of mammals that split off after the end of the period when the clade Dinosauria stopped being the dominant terrestrial vertebrates 66 million years ago. The original population and all its descendants are a clade. The rodent clade corresponds to the order Rodentia, and insects to the class Insecta. These clades include smaller clades, such as chipmunk or ant, each of which consists of even smaller clades. The clade "rodent" is in turn included in the mammal, vertebrate and animal clades.

The idea of a clade did not exist in pre-Darwinian Linnaean taxonomy, which was based by necessity only on internal or external morphological similarities between organisms – although as it happens, many of the better known animal groups in Linnaeus' original Systema Naturae (notably among the vertebrate groups) do represent clades. The phenomenon of convergent evolution is, however, responsible for many cases where there are misleading similarities in the morphology of groups that evolved from different lineages.

With the increasing realization in the first half of the 19th century that species had changed and split through the ages, classification increasingly came to be seen as branches on the evolutionary tree of life. The publication of Darwin's theory of evolution in 1859 gave this view increasing weight. Thomas Henry Huxley, an early advocate of evolutionary theory, proposed a revised taxonomy based on clades.<ref name="original text w/ figures">Huxley, T.H. (1876): Lectures on Evolution. "New York Tribune". Extra. no 36. In Collected Essays IV: pp 46-138 original text w/ figures</ref> For example, he grouped birds with reptiles, based on fossil evidence.

German biologist Emil Hans Willi Hennig (1913 – 1976) is considered to be the founder of cladistics.
He proposed a classification system that represented repeated branchings of the family tree, as opposed to the previous systems, which put organisms on a "ladder", with supposedly more "advanced" organisms at the top.

Taxonomists have increasingly worked to make the taxonomic system reflect evolution. When it comes to naming, however, this principle is not always compatible with the traditional rank-based nomenclature. In the latter, only taxa associated with a rank can be named, yet there are not enough ranks to name a long series of nested clades. For these and other reasons, phylogenetic nomenclature has been developed; it is still controversial.

A clade is by definition monophyletic, meaning that it contains one ancestor (which can be an organism, a population, or a species) and all its descendants. The ancestor can be known or unknown; any and all members of a clade can be extant or extinct.

The science that tries to reconstruct phylogenetic trees and thus discover clades is called phylogenetics or cladistics, the latter term coined by Ernst Mayr (1965), derived from "clade". The results of phylogenetic/cladistic analyses are tree-shaped diagrams called "cladograms"; they, and all their branches, are phylogenetic hypotheses.

Three methods of defining clades are featured in phylogenetic nomenclature: node-, stem-, and apomorphy-based (see here for detailed definitions).

The relationship between clades can be described in several ways:


"Clade" is the title of a novel by James Bradley, who chose it both because of its biological meaning and also because of the larger implications of the word.

An episode of "Elementary" was titled "Dead Clade Walking" and dealt with a case involving a rare fossil.



</doc>
<doc id="6684" url="https://en.wikipedia.org/wiki?curid=6684" title="Communications in Afghanistan">
Communications in Afghanistan

Communications in Afghanistan is under the control of the Ministry of Communications and Information Technology (MCIT). It has rapidly expanded after the Karzai administration took over in late 2001, and has embarked on wireless companies, internet, radio stations and television channels.

The Afghan government signed a $64.5 billion agreement in 2006 with China's ZTE on the establishment of a countrywide optical fiber cable network. The project began to improve telephone, internet, television and radio broadcast services throughout Afghanistan. About 90% of the country's population had access to communication services in 2014.

Afghanistan uses its own space satellite called Afghansat 1. There are about 18 million mobile phone users in the country. Telecom companies include Afghan Telecom, Afghan Wireless, Etisalat, MTN, Roshan, Salaam and a few others. Over 50% of the population have access to the internet.

There are about 32 million GSM mobile phone subscribers in Afghanistan as of 2016, with over 114,192 fixed-telephone-lines and over 264,000 CDMA subscribers. Mobile communications have improved because of the introduction of wireless carriers into this developing country. The first was Afghan Wireless, which is US based that was founded by Ehsan Bayat. The second was Roshan, which began providing services to all major cities within Afghanistan. There are also a number of VSAT stations in major cities such as Kabul, Kandahar, Herat, Mazari Sharif, and Jalalabad, providing international and domestic voice/data connectivity. The international calling code for Afghanistan is +93. The following is a partial list of mobile phone companies in the country:


All the companies providing communication services are obligated to deliver 2.5% of their income to the communication development fund annually. According to the Ministry of Communication and Information Technology there are 4760 active towers throughout the country which covers 85% of the population. The Ministry of Communication and Information Technology plans to expand its services in remote parts of the country where the remaining 15% of the population will be covered with the installation of 700 new towers.

Phone calls in Afghanistan have been monitored by the National Security Agency according to WikiLeaks.

MTN 21 According to a three-year duopoly agreement between the MCIT and mobile operators AWCC and Roshan, no mobile operator could enter the Afghan telecom market until July 2006. The third GSM license was awarded to Areeba in September 2005 for a period of 15 years, and a total license fee of $40.1 million. Areeba was a subsidiary of the Lebanon-based firm Investcom in consortium with Alokozai-FZE. After commencing services in July 2006, Areeba had an estimated subscribership of 200,000 by the end of that year. Areeba was later acquired by the South African-based Mobile Telephone Network (MTN) in mid-2007 as part of a $5.53 billion global merger between the two companies. MTN-Afghanistan is a subsidiary of the South African-based MTN Group, a multinational telecommunications company operating across the Middle East and Africa. MTN is the majority (90%) shareholder, while International Finance Corporation (IFC) at 9% is also a debt and equity shareholder of MTN-Afghanistan. MTN operates at 900-1800 MHz GSM band, and as of 2012 has 4.5 million subscribers and service coverage in most major cities, 464 districts, and all 34 provincial capitals. With over $400 million in total investment, MTN offers mobile voice, SMS, MMS, SRS, GPRS, fax, voicemail and PCO services through prepaid, postpaid and corporate tariffs.

MTN has interconnection agreements with all national telecom operators and provides international voice and SMS roaming in 121 countries and across 227 operators through prepaid and postpaid roaming tariffs. MTN also has a national ISP license which the company received in November 2008. MTN was the first company to introduce the popular per-second billing system in the country (also known as "pay as you talk") allowing its subscribers to transparently track their talk-time and receive billing summaries via SMS. The scheme was so popular that other GSM companies quickly adopted this method.

Afghanistan was given legal control of the ".af" domain in 2003, and the Afghanistan Network Information Center (AFGNIC) was established to administer domain names. As of 2016, there are at least 55 internet service providers (ISPs) in the country. Internet in Afghanistan is also at the peak with over 5 million users as of 2016.

According to the Ministry of Communications, the following are some of the different ISPs operating in Afghanistan:

There are over 106 television operators in Afghanistan and 320 television transmitters, many of which are based Kabul, while others are broadcast from other provinces. Selected foreign channels are also shown to the public in Afghanistan, but with the use of the internet, over 3,500 international TV channels may be accessed in Afghanistan.

There are an estimated 150 FM radio operators throughout the country. Broadcasts are in Dari, Pashto, English, Uzbeki and a number of other languages.

Radio listeners are generally decreasing and are being slowly outnumbered by television. Of Afghanistan's 6 main cities, Kandahar and Khost have the maximum number of radio listeners. Kabul and Jalalabad have moderate number of listeners. However, Mazar-e-Sharif and especially Herat have very few radio listeners.

In 1870, a central post office was established at Bala Hissar in Kabul and a post office in the capital of each province. The service was slowly being expanded over the years as more postal offices were established in each large city by 1918. Afghanistan became a member of the Universal Postal Union in 1928, and the postal administration elevated to the Ministry of Communication in 1934. Civil war caused a disruption in issuing official stamps during the 1980s-90s war but in 1999 postal service was operating again. Postal services to/from Kabul worked remarkably well all throughout the war years. Postal services to/from Herat resumed in 1997. The Afghan government has reported to the UPU several times about illegal stamps being issued and sold in 2003 and 2007.

Afghanistan Post has been reorganizing the postal service in 2000s with assistance from Pakistan Post. The Afghanistan Postal commission was formed to prepare a written policy for the development of the postal sector, which will form the basis of a new postal services law governing licensing of postal services providers. The project was expected to finish by 2008.

In January 2014 the Afghan Ministry of Communications and Information Technology signed an agreement with Eutelsat for the use of satellite resources to enhance deployment of Afghanistan's national broadcasting and telecommunications infrastructure as well as its international connectivity. Afghansat 1 was officially launched in May 2014, with expected service for at least seven years in Afghanistan. The Afghan government plans to launch Afghansat 2 after the lease of Afghansat 1 ends.



</doc>
<doc id="6689" url="https://en.wikipedia.org/wiki?curid=6689" title="Christian of Oliva">
Christian of Oliva

Christian of Oliva (), also Christian of Prussia () (died 4 December(?) 1245) was the first missionary bishop of Prussia. 
Christian was born about 1180 in the Duchy of Pomerania, possibly in the area of Chociwel (according to Johannes Voigt). Probably as a juvenile he joined the Cistercian Order at newly established Kołbacz ("Kolbatz") Abbey and in 1209 entered Oliwa Abbey near Gdańsk, founded in 1178 by the Samboride dukes of Pomerelia. At this time the Piast duke Konrad I of Masovia with the consent of Pope Innocent III had started the first of several unsuccessful Prussian Crusades into the adjacent Chełmno Land and Christian acted as a missionary among the Prussians east of the Vistula River.
In 1209, Christian was commissioned by the Pope to be responsible for the Prussian missions between the Vistula and Neman Rivers and in 1212 he was appointed bishop. In 1215 he went to Rome in order to report to the Curia on the condition and prospects of his mission, and was consecrated first "Bishop of Prussia" at the Fourth Council of the Lateran. His seat as a bishop remained at Oliwa Abbey on the western side of the Vistula, whereas the pagan Prussian (later East Prussian) territory was on the eastern side of it.

The attempts by Konrad of Masovia to subdue the Prussian lands had picked long-term and intense border quarrels, whereby the Polish lands of Masovia, Cuyavia and even Greater Poland became subject to continuous Prussian raids. Bishop Christian asked the new Pope Honorius III for the consent to start another Crusade, however a first campaign in 1217 proved a failure and even the joint efforts by Duke Konrad with the Polish High Duke Leszek I the White and Duke Henry I the Bearded of Silesia in 1222/23 only led to the reconquest of Chełmno Land but did not stop the Prussian invasions. At least Christian was able to establish the Diocese of Chełmno east of the Vistula, adopting the episcopal rights from the Masovian Bishop of Płock, confirmed by both Duke Konrad and the Pope.

Duke Konrad of Masovia still was not capable to end the Prussian attacks on his territory and in 1224 began to conduct negotiations with the Teutonic Knights under Grand Master Hermann von Salza in order to strengthen his forces. As von Salza initially hesitated to offer his services, Christian created the military Order of Dobrzyń ("Fratres Milites Christi") in 1228, however to little avail.

Meanwhile, von Salza had to abandon his hope to establish an Order's State in the Burzenland region of Transylvania, which had led to an éclat with King Andrew II of Hungary. He obtained a charter by Emperor Frederick II issued in the 1226 Golden Bull of Rimini, whereby Chełmno Land would be the unshared possession of the Teutonic Knights, which was confirmed by Duke Konrad of Masovia in the 1230 Treaty of Kruszwica. Christian ceded his possessions to the new State of the Teutonic Order and in turn was appointed Bishop of Chełmno the next year.

Bishop Christian continued his mission in Sambia ("Samland"), where from 1233 to 1239 he was held captive by pagan Prussians, and freed in trade for five other hostages who then in turn were released for a ransom of 800 Marks, granted to him by Pope Gregory IX. He had to deal with the constant cut-back of his autonomy by the Knights and asked the Roman Curia for mediation. In 1243, the Papal legate William of Modena divided the Prussian lands of the Order's State into four dioceses, whereby the bishops retained the secular rule over about on third of the diocesan territory:
all suffragan dioceses under the Archbishopric of Riga. Christian was supposed to choose one of them, but did not agree to the division. He possibly retired to the Cistercians Abbey in Sulejów, where he died before the conflict was solved.



</doc>
<doc id="6690" url="https://en.wikipedia.org/wiki?curid=6690" title="Coca-Cola">
Coca-Cola

Coca-Cola, or Coke, is a carbonated soft drink manufactured by The Coca-Cola Company. Originally intended as a patent medicine, it was invented in the late 19th century by John Stith Pemberton and was bought out by businessman Asa Griggs Candler, whose marketing tactics led Coca-Cola to its dominance of the world soft-drink market throughout the 20th century. The drink's name refers to two of its original ingredients: coca leaves, and kola nuts (a source of caffeine). The current formula of Coca-Cola remains a trade secret, although a variety of reported recipes and experimental recreations have been published.

The Coca-Cola Company produces concentrate, which is then sold to licensed Coca-Cola bottlers throughout the world. The bottlers, who hold exclusive territory contracts with the company, produce the finished product in cans and bottles from the concentrate, in combination with filtered water and sweeteners. A typical can contains of sugar (usually in the form of high fructose corn syrup). The bottlers then sell, distribute, and merchandise Coca-Cola to retail stores, restaurants, and vending machines throughout the world. The Coca-Cola Company also sells concentrate for soda fountains of major restaurants and foodservice distributors.

The Coca-Cola Company has on occasion introduced other cola drinks under the Coke name. The most common of these is Diet Coke, along with others including Caffeine-Free Coca-Cola, Diet Coke Caffeine-Free, Coca-Cola Zero Sugar, Coca-Cola Cherry, Coca-Cola Vanilla, and special versions with lemon, lime, and coffee. Based on Interbrand's "best global brand" study of 2015, Coca-Cola was the world's third most valuable brand, after Apple and Google. In 2013, Coke products were sold in over 200 countries worldwide, with consumers drinking more than 1.8 billion company beverage servings each day. Coca-Cola ranked No. 87 in the 2018 Fortune 500 list of the largest United States corporations by total revenue.

Confederate Colonel John Pemberton, who was wounded in the American Civil War and became addicted to morphine, began a quest to find a substitute for the problematic drug. In 1885 at Pemberton's Eagle Drug and Chemical House, a drugstore in Columbus, Georgia, he registered Pemberton's French Wine Coca nerve tonic. Pemberton's tonic may have been inspired by the formidable success of Vin Mariani, a French-Corsican coca wine, but his recipe additionally included the African kola nut, the beverage's source of caffeine.

It is also worth noting that a Spanish drink called "Kola Coca" was presented at a contest in Philadelphia in 1885, a year before the official birth of Coca-Cola. The rights for this Spanish drink were bought by Coca-Cola in 1953.

In 1886, when Atlanta and Fulton County passed prohibition legislation, Pemberton responded by developing Coca-Cola, a nonalcoholic version of Pemberton's French Wine Coca. The first sales were at Jacob's Pharmacy in Atlanta, Georgia, on May 8, 1886, where it initially sold for five cents a glass. Drugstore soda fountains were popular in the United States at the time due to the belief that carbonated water was good for the health, and Pemberton's new drink was marketed and sold as a patent medicine, Pemberton claiming it a cure for many diseases, including morphine addiction, indigestion, nerve disorders, headaches, and impotence. Pemberton ran the first advertisement for the beverage on May 29 of the same year in the "Atlanta Journal".

By 1888, three versions of Coca-Cola – sold by three separate businesses – were on the market. A co-partnership had been formed on January 14, 1888, between Pemberton and four Atlanta businessmen: J.C. Mayfield, A.O. Murphey, C.O. Mullahy, and E.H. Bloodworth. Not codified by any signed document, a verbal statement given by Asa Candler years later asserted under testimony that he had acquired a stake in Pemberton's company as early as 1887. John Pemberton declared that the "name" "Coca-Cola" belonged to his son, Charley, but the other two manufacturers could continue to use the "formula".

Charley Pemberton's record of control over the "Coca-Cola" name was the underlying factor that allowed for him to participate as a major shareholder in the March 1888 Coca-Cola Company incorporation filing made in his father's place. Charley's exclusive control over the "Coca-Cola" name became a continual thorn in Asa Candler's side.
Candler's oldest son, Charles Howard Candler, authored a book in 1950 published by Emory University. In this definitive biography about his father, Candler specifically states: "..., on April 14, 1888, the young druggist Asa Griggs Candler purchased a one-third interest in the formula of an almost completely unknown proprietary elixir known as Coca-Cola."
The deal was actually between John Pemberton's son Charley and Walker, Candler & Co. – with John Pemberton acting as cosigner for his son. For $50 down and $500 in 30 days, Walker, Candler & Co. obtained all of the one-third interest in the Coca-Cola Company that Charley held, all while Charley still held on to the name. After the April 14 deal, on April 17, 1888, one-half of the Walker/Dozier interest shares were acquired by Candler for an additional $750.

In 1892, Candler set out to incorporate a second company; "The Coca-Cola Company" (the current corporation). When Candler had the earliest records of the "Coca-Cola Company" destroyed in 1910, the action was claimed to have been made during a move to new corporation offices around this time.

After Candler had gained a better foothold on Coca-Cola in April 1888, he nevertheless was forced to sell the beverage he produced with the recipe he had under the names "Yum Yum" and "Koke". This was while Charley Pemberton was selling the elixir, although a cruder mixture, under the name "Coca-Cola", all with his father's blessing. After both names failed to catch on for Candler, by the middle of 1888, the Atlanta pharmacist was quite anxious to establish a firmer legal claim to Coca-Cola, and hoped he could force his two competitors, Walker and Dozier, completely out of the business, as well.

John Pemberton died suddenly on August 16, 1888; Asa Candler then decided to move swiftly forward to attain full control of the entire Coca-Cola operation.

Charley Pemberton, an alcoholic and opium addict, was the one person who unnerved Asa Candler more than anyone else. Candler is said to have quickly maneuvered to purchase the exclusive rights to the name "Coca-Cola" from Pemberton's son Charley immediately after he learned of Dr. Pemberton's death. One of several stories states that Candler approached Charley's mother at John Pemberton's funeral and offered her $300 in cash for the title to the name. Charley Pemberton was found on June 23, 1894, unconscious, with a stick of opium by his side. Ten days later, Charley died at Atlanta's Grady Hospital at the age of 40.

In Charles Howard Candler's 1950 book about his father, he stated: "On August 30th [1888], he [Asa Candler] became sole proprietor of Coca-Cola, a fact which was stated on letterheads, invoice blanks and advertising copy."

With this action on August 30, 1888, Candler's sole control became technically all true. Candler had negotiated with Margaret Dozier and her brother Woolfolk Walker a full payment amounting to $1,000, which all agreed Candler could pay off with a series of notes over a specified time span. By May 1, 1889, Candler was now claiming full ownership of the Coca-Cola beverage, with a total investment outlay by Candler for the drink enterprise over the years amounting to $2,300.

In 1914, Margaret Dozier, as co-owner of the original Coca-Cola Company in 1888, came forward to claim that her signature on the 1888 Coca-Cola Company bill of sale had been forged. Subsequent analysis of other similar transfer documents had also indicated John Pemberton's signature had most likely been forged as well, which some accounts claim was precipitated by his son Charley.

On September 12, 1919, Coca-Cola Co. was purchased by a group of investors for $25 million and reincorporated. The company publicly offered 500,000 shares of the company for $40 a share.

In 1986, The Coca-Cola Company merged with two of their bottling operators (owned by JTL Corporation and BCI Holding Corporation) to form Coca-Cola Enterprises Inc. (CCE).

In December 1991, Coca-Cola Enterprises merged with the Johnston Coca-Cola Bottling Group, Inc.

The first bottling of Coca-Cola occurred in Vicksburg, Mississippi, at the Biedenharn Candy Company on March 12, 1894. The proprietor of the bottling works was Joseph A. Biedenharn. The original bottles were Hutchinson bottles, very different from the much later hobble-skirt design of 1915 now so familiar.

It was then a few years later that two entrepreneurs from Chattanooga, Tennessee, namely Benjamin F. Thomas and Joseph B. Whitehead, proposed the idea of bottling and were so persuasive that Candler signed a contract giving them control of the procedure for only one dollar. Candler never collected his dollar, but in 1899, Chattanooga became the site of the first Coca-Cola bottling company. Candler remained very content just selling his company's syrup. The loosely termed contract proved to be problematic for The Coca-Cola Company for decades to come. Legal matters were not helped by the decision of the bottlers to subcontract to other companies, effectively becoming parent bottlers. This contract specified that bottles would be sold at 5¢ each and had no fixed duration, leading to the fixed price of Coca-Cola from 1886 to 1959.

The first outdoor wall advertisement that promoted the Coca-Cola drink was painted in 1894 in Cartersville, Georgia. Cola syrup was sold as an over-the-counter dietary supplement for upset stomach. By the time of its 50th anniversary, the soft drink had reached the status of a national icon in the USA. In 1935, it was certified kosher by Atlanta Rabbi Tobias Geffen, after the company made minor changes in the sourcing of some ingredients.

The longest running commercial Coca-Cola soda fountain anywhere was Atlanta's Fleeman's Pharmacy, which first opened its doors in 1914. Jack Fleeman took over the pharmacy from his father and ran it until 1995; closing it after 81 years. On July 12, 1944, the one-billionth gallon of Coca-Cola syrup was manufactured by The Coca-Cola Company. Cans of Coke first appeared in 1955.

On April 23, 1985, Coca-Cola, amid much publicity, attempted to change the formula of the drink with "New Coke". Follow-up taste tests revealed most consumers preferred the taste of New Coke to both Coke and Pepsi but Coca-Cola management was unprepared for the public's nostalgia for the old drink, leading to a backlash. The company gave in to protests and returned to the old formula under the name Coca-Cola Classic, on July 10, 1985.

On July 5, 2005, it was revealed that Coca-Cola would resume operations in Iraq for the first time since the Arab League boycotted the company in 1968.

In April 2007, in Canada, the name "Coca-Cola Classic" was changed back to "Coca-Cola". The word "Classic" was removed because "New Coke" was no longer in production, eliminating the need to differentiate between the two. The formula remained unchanged. In January 2009, Coca-Cola stopped printing the word "Classic" on the labels of bottles sold in parts of the southeastern United States. The change is part of a larger strategy to rejuvenate the product's image. The word "Classic" was removed from all Coca-Cola products by 2011.

In November 2009, due to a dispute over wholesale prices of Coca-Cola products, Costco stopped restocking its shelves with Coke and Diet Coke for two months; a separate pouring rights deal in 2013 saw Coke products removed from Costco food courts in favor of Pepsi. Some Costco locations (such as the ones in Tucson, Arizona) additionally sell imported Coca-Cola from Mexico with cane sugar instead of corn syrup from separate distributors. Coca-Cola introduced the 7.5-ounce mini-can in 2009, and on September 22, 2011, the company announced price reductions, asking retailers to sell eight-packs for $2.99. That same day, Coca-Cola announced the 12.5-ounce bottle, to sell for 89 cents. A 16-ounce bottle has sold well at 99 cents since being re-introduced, but the price was going up to $1.19.

In 2012, Coca-Cola resumed business in Myanmar after 60 years of absence due to U.S.-imposed investment sanctions against the country. Coca-Cola's bottling plant will be located in Yangon and is part of the company's five-year plan and $200 million investment in Myanmar. Coca-Cola with its partners is to invest US$5 billion in its operations in India by 2020. In 2013, it was announced that Coca-Cola Life would be introduced in Argentina and other parts of the world that would contain stevia and sugar. However, the drink was discontinued in Britain on June 2017.

A typical can of Coca-Cola (12 fl ounces/355 ml) contains 38 grams of sugar (usually in the form of HFCS), 50 mg of sodium, 0 grams fat, 0 grams potassium, and 140 calories.
On May 5, 2014, Coca-Cola said it is working to remove a controversial ingredient, brominated vegetable oil, from all of its drinks.

The exact formula of Coca-Cola's natural flavorings (but not its other ingredients, which are listed on the side of the bottle or can) is a trade secret. The original copy of the formula was held in SunTrust Bank's main vault in Atlanta for 86 years. Its predecessor, the Trust Company, was the underwriter for the Coca-Cola Company's initial public offering in 1919. On December 8, 2011, the original secret formula was moved from the vault at SunTrust Banks to a new vault containing the formula which will be on display for visitors to its World of Coca-Cola museum in downtown Atlanta.

According to Snopes, a popular myth states that only two executives have access to the formula, with each executive having only half the formula. However, several sources state that while Coca-Cola does have a rule restricting access to only two executives, each knows the entire formula and others, in addition to the prescribed duo, have known the formulation process.

On February 11, 2011, Ira Glass said on his PRI radio show, "This American Life", that "TAL" staffers had found a recipe in "Everett Beal's Recipe Book", reproduced in the February 28, 1979, issue of "The Atlanta Journal-Constitution", that they believed was either Pemberton's original formula for Coca-Cola, or a version that he made either before or after the product hit the market in 1886. The formula basically matched the one found in Pemberton's diary. Coca-Cola archivist Phil Mooney acknowledged that the recipe "could ... be a precursor" to the formula used in the original 1886 product, but emphasized that Pemberton's original formula is not the same as the one used in the current product.

When launched, Coca-Cola's two key ingredients were cocaine and caffeine. The cocaine was derived from the coca leaf and the caffeine from kola nut (also spelled "cola nut" at the time), leading to the name Coca-Cola.

Although Coca-Cola denies usage of cocaine, Pemberton called for five ounces of coca leaf per gallon of syrup (approximately 37 g/L), a significant dose; in 1891, Candler claimed his formula (altered extensively from Pemberton's original) contained only a tenth of this amount. Coca-Cola once contained an estimated nine milligrams of cocaine per glass. (For comparison, a typical dose or "line" of cocaine is 50–75 mg.) In 1903, it was removed.

After 1904, instead of using fresh leaves, Coca-Cola started using "spent" leaves – the leftovers of the cocaine-extraction process with trace levels of cocaine. Since then, Coca-Cola uses a cocaine-free coca leaf extract prepared at a Stepan Company plant in Maywood, New Jersey.

In the United States, the Stepan Company is the only manufacturing plant authorized by the Federal Government to import and process the coca plant, which it obtains mainly from Peru and, to a lesser extent, Bolivia. Besides producing the coca flavoring agent for Coca-Cola, the Stepan Company extracts cocaine from the coca leaves, which it sells to Mallinckrodt, a St. Louis, Missouri, pharmaceutical manufacturer that is the only company in the United States licensed to purify cocaine for medicinal use.

Long after the syrup had ceased to contain any significant amount of cocaine, in the southeastern U.S., "dope" remained a common colloquialism for Coca-Cola, and "dope-wagons" were trucks that transported it. The traditional shape of the bottle resembles the seed-pod of the coca bush, memorializing the cocaine recipe.

Kola nuts act as a flavoring and the source of caffeine in Coca-Cola. In Britain, for example, the ingredient label states "Flavourings (Including Caffeine)." Kola nuts contain about 2.0 to 3.5% caffeine, are of bitter flavor, and are commonly used in cola soft drinks. In 1911, the U.S. government initiated "United States v. Forty Barrels and Twenty Kegs of Coca-Cola", hoping to force Coca-Cola to remove caffeine from its formula. The case was decided in favor of Coca-Cola. Subsequently, in 1912, the U.S. Pure Food and Drug Act was amended, adding caffeine to the list of "habit-forming" and "deleterious" substances which must be listed on a product's label.

Coca-Cola contains 34 mg of caffeine per 12 fluid ounces (9.8 mg per 100 ml).

The actual production and distribution of Coca-Cola follows a franchising model. The Coca-Cola Company only produces a syrup concentrate, which it sells to bottlers throughout the world, who hold Coca-Cola franchises for one or more geographical areas. The bottlers produce the final drink by mixing the syrup with filtered water and sweeteners, putting the mixture into cans and bottles, and carbonating it, which the bottlers then sell and distribute to retail stores, vending machines, restaurants, and food service distributors.

The Coca-Cola Company owns minority shares in some of its largest franchises, such as Coca-Cola Enterprises, Coca-Cola Amatil, Coca-Cola Hellenic Bottling Company, and Coca-Cola FEMSA, but fully independent bottlers produce almost half of the volume sold in the world.
Independent bottlers are allowed to sweeten the drink according to local tastes.

The bottling plant in Skopje, Macedonia, received the 2009 award for "Best Bottling Company".

Since it announced its intention to begin distribution in Myanmar in June 2012, Coca-Cola has been officially available in every country in the world except Cuba and North Korea. However, it is reported to be available in both countries as a grey import.

Coca-Cola has been a point of legal discussion in the Middle East. In the early 20th century, a fatwa was created in Egypt to discuss the question of "whether Muslims were permitted to drink Coca-Cola and Pepsi cola." The fatwa states: "According to the Muslim Hanefite, Shafi'ite, etc., the rule in Islamic law of forbidding or allowing foods and beverages is based on the presumption that such things are permitted unless it can be shown that they are forbidden on the basis of the Qur'an." The Muslim jurists stated that, unless the Qu'ran specifically prohibits the consumption of a particular product, it is permissible to consume. Another clause was discussed, whereby the same rules apply if a person is unaware of the condition or ingredients of the item in question.

This is a list of variants of Coca-Cola introduced around the world. In addition to the caffeine-free version of the original, additional fruit flavors have been included over the years. Not included here are versions of Diet Coke and Coca-Cola Zero Sugar; variant versions of those no-calorie colas can be found at their respective articles.


The Coca-Cola logo was created by John Pemberton's bookkeeper, Frank Mason Robinson, in 1885. Robinson came up with the name and chose the logo's distinctive cursive script. The writing style used, known as Spencerian script, was developed in the mid-19th century and was the dominant form of formal handwriting in the United States during that period.

Robinson also played a significant role in early Coca-Cola advertising. His promotional suggestions to Pemberton included giving away thousands of free drink coupons and plastering the city of Atlanta with publicity banners and streetcar signs.

The Coca-Cola bottle, called the "contour bottle" within the company, was created by bottle designer Earl R. Dean and Coca-Cola's general counsel, Harold Hirsch. In 1915, The Coca-Cola Company was represented by their general counsel to launch a competition among its bottle suppliers as well as any competition entrants to create a new bottle for their beverage that would distinguish it from other beverage bottles, "a bottle which a person could recognize even if they felt it in the dark, and so shaped that, even if broken, a person could tell at a glance what it was."

Chapman J. Root, president of the Root Glass Company of Terre Haute, Indiana, turned the project over to members of his supervisory staff, including company auditor T. Clyde Edwards, plant superintendent Alexander Samuelsson, and Earl R. Dean, bottle designer and supervisor of the bottle molding room. Root and his subordinates decided to base the bottle's design on one of the soda's two ingredients, the coca leaf or the kola nut, but were unaware of what either ingredient looked like. Dean and Edwards went to the Emeline Fairbanks Memorial Library and were unable to find any information about coca or kola. Instead, Dean was inspired by a picture of the gourd-shaped cocoa pod in the Encyclopædia Britannica. Dean made a rough sketch of the pod and returned to the plant to show Root. He explained to Root how he could transform the shape of the pod into a bottle. Root gave Dean his approval.

Faced with the upcoming scheduled maintenance of the mold-making machinery, over the next 24 hours Dean sketched out a concept drawing which was approved by Root the next morning. Dean then proceeded to create a bottle mold and produced a small number of bottles before the glass-molding machinery was turned off.

Chapman Root approved the prototype bottle and a design patent was issued on the bottle in November 1915. The prototype never made it to production since its middle diameter was larger than its base, making it unstable on conveyor belts. Dean resolved this issue by decreasing the bottle's middle diameter. During the 1916 bottler's convention, Dean's contour bottle was chosen over other entries and was on the market the same year. By 1920, the contour bottle became the standard for The Coca-Cola Company. A revised version was also patented in 1923. Because the Patent Office releases the "Patent Gazette" on Tuesday, the bottle was patented on December 25, 1923, and was nicknamed the "Christmas bottle." Today, the contour Coca-Cola bottle is one of the most recognized packages on the planet..."even in the dark!".

As a reward for his efforts, Dean was offered a choice between a $500 bonus or a lifetime job at the Root Glass Company. He chose the lifetime job and kept it until the Owens-Illinois Glass Company bought out the Root Glass Company in the mid-1930s. Dean went on to work in other Midwestern glass factories.

One alternative depiction has Raymond Loewy as the inventor of the unique design, but, while Loewy did serve as a designer of Coke cans and bottles in later years, he was in the French Army the year the bottle was invented and did not emigrate to the United States until 1919. Others have attributed inspiration for the design not to the cocoa pod, but to a Victorian hooped dress.

In 1944, Associate Justice Roger J. Traynor of the Supreme Court of California took advantage of a case involving a waitress injured by an exploding Coca-Cola bottle to articulate the doctrine of strict liability for defective products. Traynor's concurring opinion in "Escola v. Coca-Cola Bottling Co." is widely recognized as a landmark case in U.S. law today.

Karl Lagerfeld is the latest designer to have created a collection of aluminum bottles for Coca-Cola. Lagerfeld is not the first fashion designer to create a special version of the famous Coca-Cola Contour bottle. A number of other limited edition bottles by fashion designers for Coca-Cola Light soda have been created in the last few years.

In 2009, in Italy, Coca-Cola Light had a Tribute to Fashion to celebrate 100 years of the recognizable contour bottle. Well known Italian designers Alberta Ferretti, Blumarine, Etro, Fendi, Marni, Missoni, Moschino, and Versace each designed limited edition bottles.

Pepsi, the flagship product of PepsiCo, The Coca-Cola Company's main rival in the soft drink industry, is usually second to Coke in sales, and outsells Coca-Cola in some markets. RC Cola, now owned by the Dr Pepper Snapple Group, the third largest soft drink manufacturer, is also widely available.

Around the world, many local brands compete with Coke. In South and Central America Kola Real, known as Big Cola in Mexico, is a growing competitor to Coca-Cola. On the French island of Corsica, Corsica Cola, made by brewers of the local Pietra beer, is a growing competitor to Coca-Cola. In the French region of Brittany, Breizh Cola is available. In Peru, Inca Kola outsells Coca-Cola, which led The Coca-Cola Company to purchase the brand in 1999. In Sweden, Julmust outsells Coca-Cola during the Christmas season. In Scotland, the locally produced Irn-Bru was more popular than Coca-Cola until 2005, when Coca-Cola and Diet Coke began to outpace its sales. In the former East Germany, Vita Cola, invented during Communist rule, is gaining popularity.

In India, Coca-Cola ranked third behind the leader, Pepsi-Cola, and local drink Thums Up. The Coca-Cola Company purchased Thums Up in 1993. , Coca-Cola held a 60.9% market-share in India. Tropicola, a domestic drink, is served in Cuba instead of Coca-Cola, due to a United States embargo. French brand Mecca Cola and British brand Qibla Cola are competitors to Coca-Cola in the Middle East.

In Turkey, Cola Turka, in Iran and the Middle East, Zamzam Cola and Parsi Cola, in some parts of China, China Cola, in Slovenia, Cockta, and the inexpensive Mercator Cola, sold only in the country's biggest supermarket chain, Mercator, are some of the brand's competitors. Classiko Cola, made by Tiko Group, the largest manufacturing company in Madagascar, is a competitor to Coca-Cola in many regions.

Coca-Cola's advertising has significantly affected American culture, and it is frequently credited with inventing the modern image of Santa Claus as an old man in a red-and-white suit. Although the company did start using the red-and-white Santa image in the 1930s, with its winter advertising campaigns illustrated by Haddon Sundblom, the motif was already common. Coca-Cola was not even the first soft drink company to use the modern image of Santa Claus in its advertising: White Rock Beverages used Santa in advertisements for its ginger ale in 1923, after first using him to sell mineral water in 1915. Before Santa Claus, Coca-Cola relied on images of smartly dressed young women to sell its beverages. Coca-Cola's first such advertisement appeared in 1895, featuring the young Bostonian actress Hilda Clark as its spokeswoman.

1941 saw the first use of the nickname "Coke" as an official trademark for the product, with a series of advertisements informing consumers that "Coke means Coca-Cola". In 1971, a song from a Coca-Cola commercial called "I'd Like to Teach the World to Sing", produced by Billy Davis, became a hit single.

Coke's advertising is pervasive, as one of Woodruff's stated goals was to ensure that everyone on Earth drank Coca-Cola as their preferred beverage. This is especially true in southern areas of the United States, such as Atlanta, where Coke was born.

Some Coca-Cola television commercials between 1960 through 1986 were written and produced by former Atlanta radio veteran Don Naylor (WGST 1936–1950, WAGA 1951–1959) during his career as a producer for the McCann Erickson advertising agency. Many of these early television commercials for Coca-Cola featured movie stars, sports heroes, and popular singers.

During the 1980s, Pepsi-Cola ran a series of television advertisements showing people participating in taste tests demonstrating that, according to the commercials, "fifty percent of the participants who said they preferred Coke "actually" chose the Pepsi." Statisticians pointed out the problematic nature of a 50/50 result: most likely, the taste tests showed that in blind tests, most people cannot tell the difference between Pepsi and Coke. Coca-Cola ran ads to combat Pepsi's ads in an incident sometimes referred to as the "cola wars"; one of Coke's ads compared the so-called Pepsi challenge to two chimpanzees deciding which tennis ball was furrier. Thereafter, Coca-Cola regained its leadership in the market.

Selena was a spokesperson for Coca-Cola from 1989 until the time of her death. She filmed three commercials for the company. During 1994, to commemorate her five years with the company, Coca-Cola issued special Selena coke bottles.

The Coca-Cola Company purchased Columbia Pictures in 1982, and began inserting Coke-product images into many of its films. After a few early successes during Coca-Cola's ownership, Columbia began to under-perform, and the studio was sold to Sony in 1989.

Coca-Cola has gone through a number of different advertising slogans in its long history, including "The pause that refreshes", "I'd like to buy the world a Coke", and "Coke is it".

In 2006, Coca-Cola introduced My Coke Rewards, a customer loyalty campaign where consumers earn points by entering codes from specially marked packages of Coca-Cola products into a website. These points can be redeemed for various prizes or sweepstakes entries.

In Australia in 2011, Coca-Cola began the "share a Coke" campaign, where the Coca-Cola logo was replaced on the bottles and replaced with first names. Coca-Cola used the 150 most popular names in Australia to print on the bottles. The campaign was paired with a website page, Facebook page, and an online "share a virtual Coke". The same campaign was introduced to Coca-Cola, Diet Coke & Coke Zero bottles and cans in the UK in 2013.

Coca-Cola has also advertised its product to be consumed as a breakfast beverage, instead of coffee or tea for the morning caffeine.

From 1886 to 1959, the price of Coca-Cola was fixed at five cents, in part due to an advertising campaign.

Throughout the years, Coca-Cola has released limited time collector bottles for Christmas.

The "Holidays are coming!" advertisement features a train of red delivery trucks, emblazoned with the Coca-Cola name and decorated with Christmas lights, driving through a snowy landscape and causing everything that they pass to light up and people to watch as they pass through.

The advertisement fell into disuse in 2001, as the Coca-Cola company restructured its advertising campaigns so that advertising around the world was produced locally in each country, rather than centrally in the company's headquarters in Atlanta, Georgia. In 2007, the company brought back the campaign after, according to the company, many consumers telephoned its information center saying that they considered it to mark the beginning of Christmas. The advertisement was created by U.S. advertising agency Doner, and has been part of the company's global advertising campaign for many years.

Keith Law, a producer and writer of commercials for Belfast CityBeat, was not convinced by Coca-Cola's reintroduction of the advertisement in 2007, saying that "I don't think there's anything Christmassy about HGVs and the commercial is too generic."

In 2001, singer Melanie Thornton recorded the campaign's advertising jingle as a single, "Wonderful Dream (Holidays are Coming)", which entered the pop-music charts in Germany at no. 9. In 2005, Coca-Cola expanded the advertising campaign to radio, employing several variations of the jingle.

In 2011, Coca-Cola launched a campaign for the Indian holiday Diwali. The campaign included commercials, a song, and an integration with Shah Rukh Khan's film "Ra.One".

Coca-Cola was the first commercial sponsor of the Olympic games, at the 1928 games in Amsterdam, and has been an Olympics sponsor ever since. This corporate sponsorship included the 1996 Summer Olympics hosted in Atlanta, which allowed Coca-Cola to spotlight its hometown. Most recently, Coca-Cola has released localized commercials for the 2010 Winter Olympics in Vancouver; one Canadian commercial referred to Canada's hockey heritage and was modified after Canada won the gold medal game on February 28, 2010 by changing the ending line of the commercial to say "Now they know whose game they're playing".

Since 1978, Coca-Cola has sponsored the FIFA World Cup, and other competitions organized by FIFA. One FIFA tournament trophy, the FIFA World Youth Championship from Tunisia in 1977 to Malaysia in 1997, was called "FIFA — Coca-Cola Cup". In addition, Coca-Cola sponsors the annual Coca-Cola 600 and Coke Zero 400 for the NASCAR Sprint Cup Series at Charlotte Motor Speedway in Concord, North Carolina and Daytona International Speedway in Daytona, Florida.

Coca-Cola has a long history of sports marketing relationships, which over the years have included Major League Baseball, the National Football League, the National Basketball Association, and the National Hockey League, as well as with many teams within those leagues. Coca-Cola has had a longtime relationship with the NFL's Pittsburgh Steelers, due in part to the now-famous 1979 television commercial featuring "Mean Joe" Greene, leading to the two opening the Coca-Cola Great Hall at Heinz Field in 2001 and a more recent Coca-Cola Zero commercial featuring Troy Polamalu.

Coca-Cola is the official soft drink of many collegiate football teams throughout the nation, partly due to Coca-Cola providing those schools with upgraded athletic facilities in exchange for Coca-Cola's sponsorship. This is especially prevalent at the high school level, which is more dependent on such contracts due to tighter budgets.

Coca-Cola was one of the official sponsors of the 1996 Cricket World Cup held on the Indian subcontinent. Coca-Cola is also one of the associate sponsors of Delhi Daredevils in the Indian Premier League.

In England, Coca-Cola was the main sponsor of The Football League between 2004 and 2010, a name given to the three professional divisions below the Premier League in soccer (football). In 2005, Coca-Cola launched a competition for the 72 clubs of The Football League — it was called "Win a Player". This allowed fans to place one vote per day for their favorite club, with one entry being chosen at random earning £250,000 for the club; this was repeated in 2006. The "Win A Player" competition was very controversial, as at the end of the 2 competitions, Leeds United A.F.C. had the most votes by more than double, yet they did not win any money to spend on a new player for the club. In 2007, the competition changed to "Buy a Player". This competition allowed fans to buy a bottle of Coca-Cola or Coca-Cola Zero and submit the code on the wrapper on the Coca-Cola website. This code could then earn anything from 50p to £100,000 for a club of their choice. This competition was favored over the old "Win a Player" competition, as it allowed all clubs to win some money. Between 1992 and 1998, Coca-Cola was the title sponsor of the Football League Cup (Coca-Cola Cup), the secondary cup tournament of England.

Between 1994 and 1997, Coca-Cola was also the title sponsor of the Scottish League Cup, renaming it the Coca-Cola Cup like its English counterpart. From 1998 to 2001, the company were the title sponsor of the Irish League Cup in Northern Ireland, where it was named the Coca-Cola League Cup.

Coca-Cola is the presenting sponsor of the Tour Championship, the final event of the PGA Tour held each year at East Lake Golf Club in Atlanta, GA.

Introduced March 1, 2010, in Canada, to celebrate the 2010 Winter Olympics, Coca-Cola sold gold colored cans in packs of 12 each, in select stores.

Coca-Cola has been prominently featured in many films and television programs. It was a major plot element in films such as One, Two, Three, The Coca-Cola Kid, and The Gods Must Be Crazy, among many others. In music, in the Beatles' song, "Come Together", the lyrics say, "He shoot Coca-Cola", he say... The Beach Boys also referenced Coca-Cola in their 1964 song "All Summer Long" (i.e. Member when you spilled Coke all over your blouse?)

The best selling artist of all time Elvis Presley, promoted Coca-Cola during his last tour of 1977. The Coca-Cola Company used Elvis' image to promote the product. For example, the company used a song performed by Presley, A Little Less Conversation, in a Japanese Coca-Cola commercial.

Other artists that promoted Coca-Cola include David Bowie, George Michael, Elton John, and Whitney Houston, who appeared in the Diet Coke commercial, among many others.

Not all musical references to Coca-Cola went well. A line in "Lola" by the Kinks was originally recorded as "You drink champagne and it tastes just like Coca-Cola." When the British Broadcasting Corporation refused to play the song because of the commercial reference, lead singer Ray Davies re-recorded the lyric as "it tastes just like cherry cola" to get airplay for the song.

Political cartoonist Michel Kichka satirized a famous Coca-Cola billboard in his 1982 poster "And I Love New York." On the billboard, the Coca-Cola wave is accompanied by the words "Enjoy Coke." In Kichka's poster, the lettering and script above the Coca-Cola wave instead read "Enjoy Cocaine."

Coca-Cola is sometimes used for the treatment of gastric phytobezoars. In about 50% of cases studied, Coca-Cola alone was found to be effective in gastric phytobezoar dissolution. Unfortunately, this treatment can result in the potential of developing small bowel obstruction in a minority of cases, necessitating surgical intervention.

Criticism of Coca-Cola has arisen from various groups around the world, concerning a variety of issues, including health effects, environmental issues, and business practices. The drink's coca flavoring, and the nickname "Coke", remain a common theme of criticism due to the relationship with the illegal drug cocaine. In 1911, the US government seized 40 barrels and 20 kegs of Coca-Cola syrup in Chattanooga, Tennessee, alleging the caffeine in its drink was "injurious to health", leading to amended food safety legislation.

The Coca-Cola Company, its subsidiaries and products have been subject to sustained criticism by consumer groups, environmentalists, and watchdogs, particularly since the early 2000s.

Coca-Cola Classic is rich in sugar (or sweetners in some countries) especially sucrose, which causes dental caries when consumed regularly. Besides this, the high caloric value of the sugars themselves can contribute to obesity. Both are major health issues in the developed world.

Coca-Cola produces over 3 million tonnes of plastic packaging every year.

In July 2001, the Coca-Cola company was sued over its alleged use of political far-right wing death squads (the United Self-Defense Forces of Colombia) to kidnap, torture, and kill Colombian bottler workers that were linked with trade union activity. Coca-Cola was sued in a US federal court in Miami by the Colombian food and drink union Sinaltrainal. The suit alleged that Coca-Cola was indirectly responsible for having "contracted with or otherwise directed paramilitary security forces that utilized extreme violence and murdered, tortured, unlawfully detained or otherwise silenced trade union leaders". This sparked campaigns to boycott Coca-Cola in the UK, US, Germany, Italy, and Australia. Javier Correa, the president of Sinaltrainal, said the campaign aimed to put pressure on Coca-Cola "to mitigate the pain and suffering" that union members had suffered.

Speaking from the Coca-Cola company's headquarters in Atlanta, company spokesperson Rafael Fernandez Quiros said "Coca-Cola denies any connection to any human-rights violation of this type" and added "We do not own or operate the plants".

A documentary on the controversy, titled "The Coca-Cola Case", was released in 2010.

Coca-Cola has a high degree of identification with the United States, being considered by some an "American Brand" or as an item representing America. During World War II, this gave rise to brief production of the White Coke as a neutral brand. The drink is also often a metonym for the Coca-Cola Company.

Coca-Cola was introduced to China in 1927, and was very popular until 1949. After the Chinese Civil War ended in 1949, the beverage was no longer imported into China, as it was perceived to be a symbol of decadent Western culture and the capitalist lifestyle. Importation and sales of the beverage resumed in 1979, after diplomatic relations between the United States and China were restored.

There are some consumer boycotts of Coca-Cola in Arab countries due to Coke's early investment in Israel during the Arab League boycott of Israel (its competitor Pepsi stayed out of Israel). Mecca Cola and Pepsi have been successful alternatives in the Middle East.

A Coca-Cola fountain dispenser (officially a Fluids Generic Bioprocessing Apparatus or FGBA) was developed for use on the Space Shuttle as a test bed to determine if carbonated beverages can be produced from separately stored carbon dioxide, water, and flavored syrups and determine if the resulting fluids can be made available for consumption without bubble nucleation and resulting foam formation. FGBA-1 flew on STS-63 in 1995 and dispensed pre-mixed beverages, followed by FGBA-2 on STS-77 the next year. The latter mixed CO₂, water, and syrup to make beverages. It supplied 1.65 liters each of Coca-Cola and Diet Coke.






</doc>
<doc id="6693" url="https://en.wikipedia.org/wiki?curid=6693" title="Cofinality">
Cofinality

In mathematics, especially in order theory, the cofinality cf("A") of a partially ordered set "A" is the least of the cardinalities of the cofinal subsets of "A".

This definition of cofinality relies on the axiom of choice, as it uses the fact that every non-empty set of cardinal numbers has a least member. The cofinality of a partially ordered set "A" can alternatively be defined as the least ordinal "x" such that there is a function from "x" to "A" with cofinal image. This second definition makes sense without the axiom of choice. If the axiom of choice is assumed, as will be the case in the rest of this article, then the two definitions are equivalent.

Cofinality can be similarly defined for a directed set and is used to generalize the notion of a subsequence in a net.


If "A" admits a totally ordered cofinal subset, then we can find a subset "B" which is well-ordered and cofinal in "A". Any subset of "B" is also well-ordered. If two cofinal subsets of "B" have minimal cardinality (i.e. their cardinality is the cofinality of "B"), then they are order isomorphic to each other.

The cofinality of an ordinal α is the smallest ordinal δ which is the order type of a cofinal subset of α. The cofinality of a set of ordinals or any other well-ordered set is the cofinality of the order type of that set.

Thus for a limit ordinal α, there exists a δ-indexed strictly increasing sequence with limit α. For example, the cofinality of ω² is ω, because the sequence ω·"m" (where "m" ranges over the natural numbers) tends to ω²; but, more generally, any countable limit ordinal has cofinality ω. An uncountable limit ordinal may have either cofinality ω as does ω or an uncountable cofinality.

The cofinality of 0 is 0. The cofinality of any successor ordinal is 1. The cofinality of any nonzero limit ordinal is an infinite regular cardinal.

A regular ordinal is an ordinal which is equal to its cofinality. A singular ordinal is any ordinal which is not regular.

Every regular ordinal is the initial ordinal of a cardinal. Any limit of regular ordinals is a limit of initial ordinals and thus is also initial but need not be regular. Assuming the axiom of choice, formula_1 is regular for each α. In this case, the ordinals 0, 1, formula_2, formula_3, and formula_4 are regular, whereas 2, 3, formula_5, and ω are initial ordinals which are not regular.

The cofinality of any ordinal "α" is a regular ordinal, i.e. the cofinality of the cofinality of "α" is the same as the cofinality of "α". So the cofinality operation is idempotent.

If κ is an infinite cardinal number, then cf(κ) is the least cardinal such that there is an unbounded function from cf(κ) to κ; cf(κ) is also the cardinality of the smallest set of strictly smaller cardinals whose sum is κ; more precisely

That the set above is nonempty comes from the fact that

i.e. the disjoint union of κ singleton sets. This implies immediately that cf(κ) ≤ κ.
The cofinality of any totally ordered set is regular, so one has cf(κ) = cf(cf(κ)).

Using König's theorem, one can prove κ < κ and κ < cf(2) for any infinite cardinal κ.

The last inequality implies that the cofinality of the cardinality of the continuum must be uncountable. On the other hand,

the ordinal number ω being the first infinite ordinal, so that the cofinality of formula_9 is card(ω) = formula_10. (In particular, formula_9 is singular.) Therefore,

Generalizing this argument, one can prove that for a limit ordinal δ

On the other hand, if the axiom of choice holds, then for a successor or zero ordinal δ




</doc>
<doc id="6695" url="https://en.wikipedia.org/wiki?curid=6695" title="Citadel">
Citadel

A citadel is the core fortified area of a town or city. It may be a castle, fortress, or fortified center. The term is a diminutive of "city" and thus means "little city", so called because it is a smaller part of the city of which it is the defensive core. Ancient Sparta had a citadel as did many other Greek cities and towns.

In a fortification with bastions, the citadel is the strongest part of the system, sometimes well inside the outer walls and bastions, but often forming part of the outer wall for the sake of economy. It is positioned to be the last line of defense, should the enemy breach the other components of the fortification system. A citadel is also a term of the third part of a medieval castle, with higher walls than the rest. It was to be the last line of defense before the keep.

Some of the oldest known structures which have served as citadels were built by the Indus Valley Civilisation, where the citadel represented a centralised authority. The main citadel in Indus Valley was almost 12 meters tall. The purpose of these structures, however, remains debated. Though the structures found in the ruins of Mohenjo-daro were walled, it is far from clear that these structures were defensive against enemy attacks. Rather, they may have been built to divert flood waters.

Several settlements in Anatolia, including the Assyrian city of Kaneš in modern-day Kültepe, featured citadels. Kaneš' citadel contained the city's palace, temples, and official buildings. The citadel of the Greek city of Mycenae was built atop a highly-defensible rectangular hill and was later surrounded by walls in order to increase its defensive capabilities.

In Ancient Greece, the Acropolis (literally: "high city"), placed on a commanding eminence, was important in the life of the people, serving as a refuge and stronghold in peril and containing military and food supplies, the shrine of the god and a royal palace. The most well-known is the Acropolis of Athens, but nearly every Greek city-state had one – the Acrocorinth famed as a particularly strong fortress. In a much later period, when Greece was ruled by the Latin Empire, the same strong points were used by the new feudal rulers for much the same purpose.

In the first millennium BCE, the Castro culture emerged in Northernwestern Portugal and Spain in the region extending from the Douro river up to the Minho, but soon expanding north along the coast, and east following the river valleys. It was an autochthonous evolution of Atlantic Bronze Age communities. In 2008, the origins of the Celts were attributed to this period by John T. Koch and supported by Barry Cunliffe. The Ave River Valley in Portugal was the core region of this culture, with a large number of small settlements (the "castros"), but also settlements known as citadels or oppida by the Roman conquerors. These had several rings of walls and the Roman conquest of the citadels of Abobriga, Lambriaca and Cinania around 138 B.C. was possible only by prolonged siege. Ruins of notable citadels still exist, and are known by archaeologists as Citânia de Briteiros, Citânia de Sanfins, Cividade de Terroso and Cividade de Bagunte.

Rebels who took power in the city but with the citadel still held by the former rulers could by no means regard their tenure of power as secure. One such incident played an important part in the history of the Maccabean Revolt against the Seleucid Empire. The Hellenistic garrison of Jerusalem and local supporters of the Seleucids held out for many years in the Acra citadel, making Maccabean rule in the rest of Jerusalem precarious. When finally gaining possession of the place, the Maccabeans pointedly destroyed and razed the Acra, though they constructed another citadel for their own use in a different part of Jerusalem.

At various periods, and particularly during the Middle Ages and the Renaissance, the citadel – having its own fortifications, independent of the city walls – was the last defence of a besieged army, often held after the town had been conquered. Locals and defending armies have often held out citadels long after the city had fallen. For example, in the 1543 Siege of Nice the Ottoman forces led by Barbarossa conquered and pillaged the town and took many captives – but the citadel held out.

In the Philippines The Ivatan people of the northern islands of Batanes often built fortifications to protect themselves during times of war. They built their so-called "idjangs" on hills and elevated areas.These fortifications were likened to European castles because of their purpose. Usually, the only entrance to the castles would be via a rope ladder that would only be lowered for the villagers and could be kept away when invaders arrived.

In time of war the citadel in many cases afforded retreat to the people living in the areas around the town. However, Citadels were often used also to protect a garrison or political power from the inhabitants of the town where it was located, being designed to ensure loyalty from the town that they defended.

For example, during the Dutch Wars of 1664-67, King Charles II of England constructed a Royal Citadel at Plymouth, an important channel port which needed to be defended from a possible naval attack. However, due to Plymouth's support for the Parliamentarians in the then-recent English Civil War, the Plymouth Citadel was so designed that its guns could fire on the town as well as on the sea approaches.

Barcelona had a great citadel built in 1714 to intimidate the Catalans against repeating their mid-17th- and early-18th-century rebellions against the Spanish central government. In the 19th century, when the political climate had liberalized enough to permit it, the people of Barcelona had the citadel torn down, and replaced it with the city's main central park, the Parc de la Ciutadella. A similar example is the Citadella in Budapest, Hungary.

The attack on the Bastille in the French Revolution – though afterwards remembered mainly for the release of the handful of prisoners incarcerated there – was to considerable degree motivated by the structure's being a Royal citadel in the midst of revolutionary Paris.

Similarly, after Garibaldi's overthrow of Bourbon rule in Palermo, during the 1860 Unification of Italy, Palermo's Castellamare Citadel – symbol of the hated and oppressive former rule – was ceremoniously demolished.

Following Belgium declaring independence in 1830, a Dutch garrison under General David Hendrik Chassé held out in Antwerp Citadel between 1830 and 1832, while the city had already become part of the independent Belgium.

The Siege of the Alcázar in the Spanish Civil War, in which the Nationalists held out against a much larger Republican force for two months until relieved, shows that in some cases a citadel can be effective even in modern warfare; a similar case is the Battle of Huế during the Vietnam war, where a North Vietnamese Army division held the citadel of Huế for 26 days against roughly their own numbers of much better-equipped US and South Vietnamese troops.

The Citadelle of Québec (construction started 1673, completed 1820) still survives as the largest citadel still in official military operation in North America. It is home to the Royal 22nd Regiment of the Canadian Army; and forms part of the Ramparts of Quebec City dating back to 1620s. 

Since the mid 20th century, citadels commonly enclose military command and control centres, rather than cities or strategic points of defense on the boundaries of a country. These modern citadels are built to protect the command center from heavy attacks, such as aerial or nuclear bombardment. The military citadels under London in the UK, including the massive underground complex Pindar beneath the Ministry of Defence, are examples, as is the Cheyenne Mountain nuclear bunker in the US.

On armored warships, the heavily armored section of the ship that protects the ammunition and machinery spaces is called the armored citadel.

A modern naval interpretation refers to the heaviest protected part of the hull as "The Vitals", and the citadel refers to the semi armoured freeboard above the vitals. Generally Anglo-American and German language follow this while Russian sources/language refer to "The Vitals" as "zitadel". Likewise Russian literature often refers to 'the turret' of a tank as 'the tower'.

The safe room on a ship is also called a citadel.



</doc>
<doc id="6696" url="https://en.wikipedia.org/wiki?curid=6696" title="Chain mail">
Chain mail

Mail or maille (also chain mail(le) or chainmail(le)) is a type of armour consisting of small metal rings linked together in a pattern to form a mesh. A coat of this armour is often referred to as a hauberk, and sometimes a byrnie.

The earliest examples of surviving mail were found in the Carpathian Basin at a burial in Horný Jatov, Slovakia dated at 3rd century BC, and in a chieftain's burial located in Ciumești, Romania. Its invention is commonly credited to the Celts, but there are examples of Etruscan pattern mail dating from at least the 4th century BCE. Mail may have been inspired by the much earlier scale armour. Mail spread to North Africa, West Africa, the Middle East, Central Asia, India, Tibet, South East Asia, and Japan.

Herodotus wrote that the ancient Persians wore scale armour, but mail is also distinctly mentioned in the Avesta, the ancient holy scripture of the Persian religion of Zoroastrianism that was founded by the prophet Zoroaster in the 5th century BC.

Mail continues to be used in the 21st century as a component of stab-resistant body armour, cut-resistant gloves for butchers and woodworkers, shark-resistant wetsuits for defense against shark bites, and a number of other applications.

The origins of the word "mail" are not fully known. One theory is that it originally derives from the Latin word "macula", meaning "spot" or "opacity" (as in macula of retina). Another theory relates the word to the old French "maillier", meaning "to hammer" (related to the modern English word "malleable"). In modern French, "maille" refers to a loop or stitch. The Arabic words "burnus", , a burnoose; a hooded cloak, also a chasuble (worn by Coptic priests) and "barnaza", , to bronze, suggest an Arabic influence for the Carolingian armour known as "byrnie" (see below).

The first attestations of the word "mail" are in Old French and Anglo-Norman: "maille", "maile", or "male" or other variants, which became "mailye", "maille", "maile", "male", or "meile" in Middle English.
The modern usage of terms for mail armour is highly contested in popular and, to a lesser degree, academic culture. Medieval sources referred to armour of this type simply as "mail"; however, "chain-mail" has become a commonly used, if incorrect, neologism first attested in Sir Walter Scott's 1822 novel "The Fortunes of Nigel". Since then the word "mail" has been commonly, if incorrectly, applied to other types of armour, such as in "plate-mail" (first attested in 1835). The more correct term is "plate armour".

Civilizations that used mail invented specific terms for each garment made from it. The standard terms for European mail armour derive from French: leggings are called chausses, a hood is a mail coif, and mittens, mitons. A mail collar hanging from a helmet is a camail or aventail. A shirt made from mail is a hauberk if knee-length and a haubergeon if mid-thigh length. A layer (or layers) of mail sandwiched between layers of fabric is called a jazerant.

A waist-length coat in medieval Europe was called a byrnie, although the exact construction of a byrnie is unclear, including whether it was constructed of mail or other armour types. Noting that the byrnie was the "most highly valued piece of armour" to the Carolingian soldier, Bennet, Bradbury, DeVries, Dickie, and Jestice indicate that:

There is some dispute among historians as to what exactly constituted the Carolingian byrnie. Relying... only on artistic and some literary sources because of the lack of archaeological examples, some believe that it was a heavy leather jacket with metal scales sewn onto it. It was also quite long, reaching below the hips and covering most of the arms. Other historians claim instead that the Carolingian byrnie was nothing more than a coat of mail, but longer and perhaps heavier than traditional early medieval mail. Without more certain evidence, this dispute will continue.

The use of mail as battlefield armour was common during the Iron Age and the Middle Ages, becoming less common over the course of the 16th and 17th centuries when plate armor and more advanced firearms were developed. It is believed that the Roman Republic first came into contact with mail fighting the Gauls in Cisalpine Gaul, now Northern Italy, but a different pattern of mail was already in use among the Etruscans. The Roman army adopted the technology for their troops in the form of the lorica hamata which was used as a primary form of armour through the Imperial period.
After the fall of the Western Empire, much of the infrastructure needed to create plate armour diminished. Eventually the word "mail" came to be synonymous with armour. It was typically an extremely prized commodity, as it was expensive and time-consuming to produce and could mean the difference between life and death in a battle. Mail from dead combatants was frequently looted and was used by the new owner or sold for a lucrative price. As time went on and infrastructure improved, it came to be used by more soldiers. Eventually with the rise of the lanced cavalry charge, impact warfare, and high-powered crossbows, mail came to be used as a secondary armour to plate for the mounted nobility.

By the 14th century, plate armour was commonly used to supplement mail. Eventually mail was supplanted by plate for the most part, as it provided greater protection against windlass crossbows, bludgeoning weapons, and lance charges. However, mail was still widely used by many soldiers as well as brigandines and padded jacks. These three types of armour made up the bulk of the equipment used by soldiers, with mail being the most expensive. It was sometimes more expensive than plate armour. Mail typically persisted longer in less technologically advanced areas such as Eastern Europe but was in use everywhere into the 16th century.

During the late 19th and early 20th century, mail was used as a material for bulletproof vests, most notably by the Wilkinson Sword Company. Results were unsatisfactory; Wilkinson mail worn by the Khedive of Egypt's regiment of "Iron Men" was manufactured from split rings which proved to be too brittle, and the rings would fragment when struck by bullets and aggravate the injury. The riveted mail armour worn by the opposing Sudanese Madhists did not have the same problem but also proved to be relatively useless against the firearms of British forces at the battle of Omdurman. During World War I, Wilkinson Sword transitioned from mail to a lamellar design which was the precursor to the flak jacket.

Also during World War I, a mail fringe, designed by Captain Cruise of the British Infantry, was added to helmets to protect the face. This proved unpopular with soldiers, in spite of being proven to defend against a three-ounce (100 g) shrapnel round fired at a distance of . A protective face mask or splatter mask had a mail veil and was used by early tank crews as a measure against flying steel fragments (spalling) inside the vehicle.

Mail armour was introduced to the Middle East and Asia through the Romans and was adopted by the Sassanid Persians starting in the 3rd century AD, where it was supplemental to the scale and lamellar armour already used. Mail was commonly also used as horse armour for cataphracts and heavy cavalry as well as armour for the soldiers themselves. Asian mail could be just as heavy as the European variety and sometimes had prayer symbols stamped on the rings as a sign of their craftsmanship as well as for divine protection. Indeed, mail armour is mentioned in the Quran as being a gift revealed by Allah to David:

21:80 It was We Who taught him the making of coats of mail for your benefit, to guard you from each other's violence: will ye then be grateful? (Yusuf Ali's translation)

From the Middle East, mail was quickly adopted in Central Asia by the Sogdians and by India in the South. Mail armour was introduced by the Turks in late 12th century and commonly used by Turk and the Mughal armies where it eventually became the armour of choice in India. Indian mail was constructed with alternating rows of solid links and round riveted links and it was often integrated with plate protection (mail and plate armour). Mail and plate armour was commonly used in India until the Battle of Plassey and the subsequent British conquest of the sub-continent.

The Ottoman Empire used mail armour as well as mail and plate armour, and it was used in their armies until the 18th century by heavy cavalry and elite units such as the Janissaries. They spread its use into North Africa where it was adopted by Mamluk Egyptians and the Sudanese who produced it until the early 20th century. Ottoman mail was constructed with alternating rows of solid links and round riveted links. The Persians used mail armour as well as mail and plate armour. Persian mail and Ottoman mail were often quite similar in appearance.

Mail was introduced to China when its allies in Central Asia paid tribute to the Tang Emperor in 718 by giving him a coat of "link armour" assumed to be mail. China first encountered the armour in 384 when its allies in the nation of Kuchi arrived wearing "armour similar to chains". Once in China, mail was imported but was not produced widely. Due to its flexibility, comfort, and rarity, it was typically the armour of high-ranking guards and those who could afford the exotic import (to show off their social status) rather than the armour of the rank and file, who used more common brigandine, scale, and lamellar types. However, it was one of the only military products that China imported from foreigners. Mail spread to Korea slightly later where it was imported as the armour of imperial guards and generals.

In Japan mail is called "" which means chain. When the word "kusari" is used in conjunction with an armoured item it usually means that mail makes up the majority of the armour composition. An example of this would be "kusari gusoku" which means chain armour. "Kusari" "", "", "", "", "", shoulder, "", and other armoured clothing were produced, even "" socks.

"" was used in samurai armour at least from the time of the Mongol invasion (1270s) but particularly from the Nambokucho period (1336–1392). The Japanese used many different weave methods including a square 4-in-1 pattern ("so gusari"), a hexagonal 6-in-1 pattern ("hana gusari") and a European 4-in-1 ("nanban gusari"). The rings of Japanese mail were much smaller than their European counterparts; they would be used in patches to link together plates and to drape over vulnerable areas such as the armpits.

Riveted kusari was known and used in Japan. On page 58 of the book "Japanese Arms & Armor: Introduction" by H. Russell Robinson, there is a picture of Japanese riveted kusari, and
this quote from the translated reference of 1800 book, "The Manufacture of Armour and Helmets in Sixteenth-Century Japan", shows that the Japanese not only knew of and used riveted kusari but that they manufactured it as well.

... karakuri-namban (riveted namban), with stout links each closed by a rivet. Its invention is credited to Fukushima Dembei Kunitaka, pupil, of Hojo Awa no Kami Ujifusa, but it is also said to be derived directly from foreign models. It is heavy because the links are tinned (biakuro-nagashi) and these are also sharp-edged because they are punched out of iron plate

Butted and or split (twisted) links made up the majority of "kusari" links used by the Japanese. Links were either "butted" together meaning that the ends touched each other and were not riveted, or the "kusari" was constructed with links where the wire was turned or twisted two or more times; these split links are similar to the modern split ring commonly used on keychains. The rings were lacquered black to prevent rusting, and were always stitched onto a backing of cloth or leather. The kusari was sometimes concealed entirely between layers of cloth.

Kusari gusoku or chain armour was commonly used during the Edo period 1603 to 1868 as a stand-alone defense. According to George Cameron Stone

Entire suits of mail "kusari gusoku" were worn on occasions, sometimes under the ordinary clothing

Ian Bottomley in his book "Arms and Armor of the Samurai: The History of Weaponry in Ancient Japan" shows a picture of a kusari armour and mentions "" (chain jackets) with detachable arms being worn by samurai police officials during the Edo period. The end of the samurai era in the 1860s, along with the 1876 ban on wearing swords in public, marked the end of any practical use for mail and other armour in Japan. Japan turned to a conscription army and uniforms replaced armour.

Mail armour provided an effective defense against slashing blows by edged weapons and some forms of penetration by many thrusting and piercing weapons; in fact, a study conducted at the Royal Armouries at Leeds concluded that "it is almost impossible to penetrate using any conventional medieval weapon". Generally speaking, mail's resistance to weapons is determined by four factors: linkage type (riveted, butted, or welded), material used (iron versus bronze or steel), weave density (a tighter weave needs a thinner weapon to surpass), and ring thickness (generally ranging from 18 to 14 gauge (1.02–1.63 mm diameter) wire in most examples). Mail, if a warrior could afford it, provided a significant advantage when combined with competent fighting techniques.

When the mail was not riveted, a thrust from most sharp weapons could penetrate it. However, when mail was riveted, only a strong well-placed thrust from certain spears, or thin or dedicated mail-piercing swords like the estoc could penetrate, and a pollaxe or halberd blow could break through the armour. Strong projectile weapons such as stronger self bows, recurve bows, and crossbows could also penetrate riveted mail. Some evidence indicates that during armoured combat, the intention was to actually get around the armour rather than through it—according to a study of skeletons found in Visby, Sweden, a majority of the skeletons showed wounds on less well protected legs. Although mail was a formidable protection, due to longswords getting more tapered as time progressed, mail worn under plate armor (and stand-alone mail as well) could be penetrated by the conventional weaponry of another knight.

The flexibility of mail meant that a blow would often injure the wearer, potentially causing serious bruising or fractures, and it was a poor defence against head trauma. Mail-clad warriors typically wore separate rigid helms over their mail coifs for head protection. Likewise, blunt weapons such as maces and warhammers could harm the wearer by their impact without penetrating the armour; usually a soft armour, such as gambeson, was worn under the hauberk. Medieval surgeons were very well capable of setting and caring for bone fractures resulting from blunt weapons. With the poor understanding of hygiene, however, cuts that could get infected were much more of a problem. Thus mail armour proved to be sufficient protection in most situations.

Several patterns of linking the rings together have been known since ancient times, with the most common being the 4-to-1 pattern (where each ring is linked with four others). In Europe, the 4-to-1 pattern was completely dominant. Mail was also common in East Asia, primarily Japan, with several more patterns being utilised and an entire nomenclature developing around them.

Historically, in Europe, from the pre-Roman period on, the rings composing a piece of mail would be riveted closed to reduce the chance of the rings splitting open when subjected to a thrusting attack or a hit by an arrow.

Up until the 14th century European mail was made of alternating rows of round riveted rings and solid rings. Sometime during the 14th century European mail makers started to transition from round rivets to wedge shaped rivets but continued using alternating rows of solid rings. Eventually European mail makers stopped using solid rings and almost all European mail was made from wedge riveted rings only with no solid rings. Both were commonly made of wrought iron, but some later pieces were made of heat-treated steel. Wire for the riveted rings was formed by either of two methods. One was to hammer out wrought iron into plates and cut or slit the plates. These thin pieces were then pulled through a draw plate repeatedly until the desired diameter was achieved. Waterwheel powered drawing mills are pictured in several period manuscripts. Another method was to simply forge down an iron billet into a rod and then proceed to draw it out into wire. The solid links would have been made by punching from a sheet. Guild marks were often stamped on the rings to show their origin and craftsmanship. Forge welding was also used to create solid links, but there are few possible examples known; the only well documented example from Europe is that of the camail (mail neck-defence) of the 7th century Coppergate helmet. Outside of Europe this practice was more common such as "theta" links from India. Very few examples of historic butted mail have been found and it is generally accepted that butted mail was never in wide use historically except in Japan where mail ("kusari") was commonly made from "butted" links. Butted link mail was also used by the Moros of the Philippines in their mail and plate armors.

Mail is used as protective clothing for butchers against meat-packing equipment. Workers may wear up to of mail under their white coats. Butchers also commonly wear a single mail glove to protect themselves from self-inflicted injury while cutting meat, as do many oyster shuckers.

Woodcarvers sometimes use similar mail gloves to protect their hands from cuts and punctures.

Scuba divers use mail to protect them from sharkbite, as do animal control officers for protection against the animals they handle. Shark expert and underwater filmmaker Valerie Taylor was among the first to develop and test shark suits in 1979 while diving with sharks.

Mail is widely used in industrial settings as shrapnel guards and splash guards in metal working operations.

Electrical applications for mail include RF leakage testing and being worn as a faraday cage suit by tesla coil enthusiasts and high voltage electrical workers.

Conventional textile-based ballistic vests are designed to stop soft-nosed bullets but offer little defense from knife attacks. Knife-resistant armour are designed to defend against knife attacks; some of these use layers of metal plates, mail and metallic wires.

Many historical reenactment groups, especially those whose focus is Antiquity or the Middle Ages, commonly use mail both as practical armour and for costuming. Mail is especially popular amongst those groups which use steel weapons. A modern hauberk made from 1.5 mm diameter wire with 10 mm inner diameter rings weighs roughly and contains 15,000–45,000 rings.

One of the drawbacks of mail is the uneven weight distribution; the stress falls mainly on shoulders. Weight can be better distributed by wearing a belt over the mail, which provides another point of support.

Mail worn today for re-enactment and recreational use can be made in a variety of styles and materials. Most recreational mail today is made of butted links which are galvanized or stainless steel. This is historically inaccurate but is much less expensive to procure and especially to maintain than historically accurate reproductions. Mail can also be made of titanium, aluminium, bronze, or copper. Riveted mail offers significantly better protection ability as well as historical accuracy than mail constructed with butted links. Riveted mail can be more labour-intensive and expensive to manufacture. Japanese mail ("kusari") is one of the few historically correct examples of mail being constructed with such "butted links".

Mail remained in use as a decorative and possibly high-status symbol with military overtones long after its practical usefulness had passed. It was frequently used for the epaulettes of military uniforms. It is still used in this form by the British Territorial Army.

Mail has applications in sculpture and jewellery, especially when made out of precious metals or colourful anodized metals. Mail artwork includes headdresses, decorative wall hangings, ornaments, chess sets, macramé, and jewelry. For these non-traditional applications, hundreds of patterns (commonly referred to as "weaves") have been invented.

Large-linked mail is occasionally used as a fetish clothing material, with the large links intended to reveal – in part – the body beneath them.

In some films, knitted string spray-painted with a metallic paint is used instead of actual mail in order to cut down on cost (an example being "Monty Python and the Holy Grail", which was filmed on a very small budget). Films more dedicated to costume accuracy often use ABS plastic rings, for the lower cost and weight. Such ABS mail coats were made for "The Lord of the Rings" film trilogy, in addition to many metal coats. The metal coats are used rarely because of their weight, except in close-up filming where the appearance of ABS rings is distinguishable. A large scale example of the ABS mail used in the "Lord of the Rings" can be seen in the entrance to the Royal Armouries museum in Leeds in the form of a large curtain bearing the logo of the museum. It was acquired from the makers of the film's armour, Weta Workshop, when the museum hosted an exhibition of WETA armour from their films. For the film "Mad Max Beyond Thunderdome", Tina Turner is said to have worn actual mail and she complained how heavy this was. Game of Thrones makes use of mail, notably during the "Red Wedding" scene.


Typically worn under mail armour:

Can be worn over mail armour:

Others:



</doc>
<doc id="6697" url="https://en.wikipedia.org/wiki?curid=6697" title="Cerberus">
Cerberus

In Greek mythology, Cerberus (; "Kerberos" ), often called the "hound of Hades", is a multi-headed dog that guards the gates of the Underworld to prevent the dead from leaving. Cerberus was the offspring of the monsters Echidna and Typhon, and is usually described as having three heads, a serpent for a tail, and snakes protruding from multiple parts of his body. Cerberus is primarily known for his capture by Heracles, one of Heracles' twelve labours.

Descriptions of Cerberus vary, including the number of his heads. Cerberus was usually three-headed, though not always. Cerberus had several multi-headed relatives. His father was the multi snake-headed Typhon, and Cerberus was the brother of three other multi-headed monsters, the multi-snake-headed Lernaean Hydra; Orthrus, the two-headed dog who guarded the Cattle of Geryon; and the Chimera, who had three heads, that of a lion, a goat, and a snake. And, like these close relatives, Cerberus was, with only the rare iconographic exception, multi-headed.

In the earliest description of Cerberus, Hesiod's "Theogony" (c. 8th – 7th century BC), Cerberus has fifty heads, while Pindar (c. 522 – c. 443 BC) gave him one hundred heads. However, later writers almost universally give Cerberus three heads. An exception is the Latin poet Horace's Cerberus which has a single dog head, and one hundred snake heads. Perhaps trying to reconcile these competing traditions, Apollodorus's Cerberus has three dog heads and the heads of "all sorts of snakes" along his back, while the Byzantine poet John Tzetzes (who probably based his account on Apollodorus) gives Cerberus fifty heads, three of which were dog heads, the rest being the "heads of other beasts of all sorts".

In art Cerberus is most commonly depicted with two dog heads (visible), never more than three, but occasionally with only one. On one of the two earliest depictions (c. 590–580 BC), a Corinthian cup from Argos (see below), now lost, Cerberus was shown as a normal single-headed dog. The first appearance of a three-headed Cerberus occurs on a mid-sixth-century BC Laconian cup (see below).

Horace's many snake-headed Cerberus followed a long tradition of Cerberus being part snake. This is perhaps already implied as early as in Hesiod's "Theogony", where Cerberus' mother is the half-snake Echidna, and his father the snake-headed Typhon. In art Cerberus is often shown as being part snake, for example the lost Corinthian cup showed snakes protruding from Cerberus' body, while the mid sixth-century BC Laconian cup gives Cerberus a snake for a tail. In the literary record, the first certain indication of Cerberus' serpentine nature comes from the rationalized account of Hecataeus of Miletus (fl. 500–494 BC), who makes Cerberus a large poisonous snake. Plato refers to Cerberus' composite nature, and Euphorion of Chalcis (3rd century BC) describes Cerberus as having multiple snake tails, and presumably in connection to his serpentine nature, associates Cerberus with the creation of the poisonous aconite plant. Virgil has snakes writhe around Cerberus' neck, Ovid's Cerberus has a venomous mouth, necks "vile with snakes", and "hair inwoven with the threatening snake", while Seneca gives Cerberus a mane consisting of snakes, and a single snake tail.

Cerberus was given various other traits. According to Euripides, Cerberus not only had three heads but three bodies, and according to Virgil he had multiple backs. Cerberus ate raw flesh (according to Hesiod), had eyes which flashed fire (according to Euphorion), a three-tongued mouth (according to Horace), and acute hearing (according to Seneca).

Cerberus' only mythology concerns his capture by Heracles. As early as Homer we learn that Heracles was sent by Eurystheus, the king of Tiryns, to bring back Cerberus from Hades the king of the underworld. According to Apollodorus, this was the twelfth and final labour imposed on Heracles. In a fragment from a lost play "Pirithous", (attributed to either Euripides or Critias) Heracles says that, although Eurystheus commanded him to bring back Cerberus, it was not from any desire to see Cerberus, but only because Eurystheus thought that the task was impossible.

Heracles was aided in his mission by his being an initiate of the Eleusinian Mysteries. Euripides has his initiation being "lucky" for Heracles in capturing Cerberus. And both Diodorus Siculus and Apollodorus say that Heracles was initiated into the Mysteries, in preparation for his descent into the underworld. According to Diodorus, Heracles went to Athens, where Musaeus, the son of Orpheus, was in charge of the initiation rites, while according to Apollodorus, he went to Eumolpus at Eleusis.

Heracles also had the help of Hermes, the usual guide of the underworld, as well as Athena. In the "Odyssey", Homer has Hermes and Athena as his guides. And Hermes and Athena are often shown with Heracles on vase paintings depicting Cerberus' capture. By most accounts, Heracles made his descent into the underworld through an entrance at Tainaron, the most famous of the various Greek entrances to the underworld. The place is first mentioned in connection with the Cerberus story in the rationalized account of Hecataeus of Miletus (fl. 500–494 BC), and Euripides, Seneca, and Apolodorus, all have Heracles descend into the underworld there. However Xenophon reports that Heracles was said to have descended at the Acherusian Chersonese near Heraclea Pontica, on the Black Sea, a place more usually associated with Heracles' exit from the underworld (see below). Heraclea, founded c. 560 BC, perhaps took its name from the association of its site with Heracles' Cerberian exploit.

While in the underworld, Heracles met the heroes Theseus and Pirithous, where the two companions were being held prisoner by Hades for attempting to carry off Hades' wife Persephone. Along with bringing back Cerberus, Heracles also managed (usually) to rescue Theseus, and in some versions Pirithous as well. According to Apollodorus, Heracles found Theseus and Pirithous near the gates of Hades, bound to the "Chair of Forgetfulness, to which they grew and were held fast by coils of serpents", and when they saw Heracles, "they stretched out their hands as if they should be raised from the dead by his might", and Heracles was able to free Theseus, but when he tried to raise up Pirithous, "the earth quaked and he let go."

The earliest evidence for the involvement of Theseus and Pirithous in the Cerberus story, is found on a shield-band relief (c. 560 BC) from Olympia, where Theseus and Pirithous (named) are seated together on a chair, arms held out in supplication, while Heracles approaches, about to draw his sword. The earliest literary mention of the rescue occurs in Euripides, where Heracles saves Theseus (with no mention of Pirithous). In the lost play "Pirithous", both heroes are rescued, while in the rationalized account of Philochorus, Heracles was able to rescue Theseus, but not Pirithous. In one place Diodorus says Heracles brought back both Theseus and Pirithous, by the favor of Persephone, while in another he says that Pirithous remained in Hades, or according to "some writers of myth" that neither Theseus, nor Pirithous returned. Both are rescued in Hyginus.

There are various versions of how Heracles accomplished Cerberus' capture. According to Apollodorus, Heracles asked Hades for Cerberus, and Hades told Heracles he would allow him to take Cerberus only if he "mastered him without the use of the weapons which he carried", and so, using his lion-skin as a shield, Heracles squeezed Cerberus around the head until he submitted.

In some early sources Cerberus' capture seems to involve Heracles fighting Hades. Homer ("Iliad" 5.395–397) has Hades injured by an arrow shot by Heracles. A scholium to the "Iliad" passage, explains that Hades had commanded that Heracles "master Cerberus without shield or Iron". Heracles did this, by (as in Apollodorus) using his lion-skin instead of his shield, and making stone points for his arrows, but when Hades still opposed him, Heracles shot Hades in anger. Consistent with the no iron requirement, on an early-sixth-century BC lost Corinthian cup, Heracles is shown attacking Hades with a stone, while the iconographic tradition, from c. 560 BC, often shows Heracles using his wooden club against Cerberus.

Euripides, has Amphitryon ask Heracles: "Did you conquer him in fight, or receive him from the goddess [i.e. Persephone]? To which, Heracles answers: "In fight", and the "Pirithous" fragment says that Heracles "overcame the beast by force". However, according to Diodorus, Persephone welcomed Heracles "like a brother" and gave Cerberus "in chains" to Heracles. Aristophanes, has Heracles seize Cerberus in a stranglehold and run off, while Seneca has Heracles again use his lion-skin as shield, and his wooden club, to subdue Cerberus, after which a quailing Hades and Persephone, allow Heracles to lead a chained and submissive Cerberus away. Cerberus is often shown being chained, and Ovid tells that Heracles dragged the three headed Cerberus with chains of adamant.

There were several locations which were said to be the place where Heracles brought up Cerberus from the underworld. The geographer Strabo (63/64 BC – c. AD 24) reports that "according to the myth writers" Cerberus was brought up at Tainaron, the same place where Euripides has Heracles enter the underworld. Seneca has Heracles enter and exit at Tainaron. Apollodorus, although he has Heracles enter at Tainaron, has him exit at Troezen. The geographer Pausanias tells us that there was a temple at Troezen with "altars to the gods said to rule under the earth", where it was said that, in addition to Cerberus being "dragged" up by Heracles, Semele was supposed to have been brought up out of the underworld by Dionysus.

Another tradition had Cerberus brought up at Heraclea Pontica (the same place which Xenophon had earlier associated with Heracles' descent) and the cause of the poisonous plant aconite which grew there in abundance. Herodorus of Heraclea and Euphorion said that when Heracles brought Cerberus up from the underworld at Heraclea, Cerberus "vomited bile" from which the aconite plant grew up. Ovid, also makes Cerberus the cause of the poisonous aconite, saying that on the "shores of Scythia", upon leaving the underworld, as Cerberus was being dragged by Heracles from a cave, dazzled by the unaccustomed daylight, Cerberus spewed out a "poison-foam", which made the aconite plants growing there poisonous. Seneca's Cerberus too, like Ovid's, reacts violently to his first sight of daylight. Enraged, the previously submissive Cerberus struggles furiously, and Heracles and Theseus must together drag Cerberus into the light.

Pausanias reports that according to local legend Cerberus was brought up through a chasm in the earth dedicated to Clymenus (Hades) next to the sanctuary of Chthonia at Hermione, and in Euripides' "Heracles", though Euripides does not say that Cerberus was brought out there, he has Cerberus kept for a while in the "grove of Chthonia" at Hermione. Pausanias also mentions that at Mount Laphystion in Boeotia, that there was a statue of Heracles Charops ("with bright eyes"), where the Boeotians said Heracles brought up Cerberus. Other locations which perhaps were also associated with Cerberus being brought out of the underworld include, Hierapolis, Thesprotia, and Emeia near Mycenae.

In some accounts, after bringing Cerberus up from the underworld, Heracles paraded the captured Cerberus through Greece. Euphorion has Heracles lead Cerberus through Midea in Argolis, as women and children watch in fear, and Diodorus Siculus says of Cerberus, that Heracles "carried him away to the amazement of all and exhibited him to men." Seneca has Juno complain of Heracles "highhandedly parading the black hound through Argive cities" and Heracles greeted by laurel-wreathed crowds, "singing" his praises.

Then, according to Apollodorus, Heracles showed Cerberus to Eurystheus, as commanded, after which he returned Cerberus to the underworld. However, according to Hesychius of Alexandria, Cerberus escaped, presumably returning to the underworld on his own.

The earliest mentions of Cerberus (c. 8th – 7th century BC) occur in Homer's "Iliad" and "Odyssey", and Hesiod's "Theogony". Homer does not name or describe Cerberus, but simply refers to Heracles being sent by Eurystheus to fetch the "hound of Hades", with Hermes and Athena as his guides, and, in a possible reference to Cerberus' capture, that Heracles shot Hades with an arrow. According to Hesiod, Cerberus was the offspring of the monsters Echidna and Typhon, was fifty-headed, ate raw flesh, and was the "brazen-voiced hound of Hades", who fawns on those that enter the house of Hades, but eats those who try to leave.

Stesichorus (c. 630 – 555 BC) apparently wrote a poem called "Cerberus", of which virtually nothing remains. However the early-sixth-century BC-lost Corinthian cup from Argos, which showed a single head, and snakes growing out from many places on his body, was possibly influenced by Stesichorus' poem. The mid-sixth-century BC cup from Laconia gives Cerberus three heads and a snake tail, which eventually becomes the standard representation.

Pindar (c. 522 – c. 443 BC) apparently gave Cerberus one hundred heads. Bacchylides (5th century BC) also mentions Heracles bringing Cerberus up from the underworld, with no further details. Sophocles (c. 495 – c. 405 BC), in his "Women of Trachis", makes Cerberus three-headed, and in his "Oedipus at Colonus", the Chorus asks that Oedipus be allowed to pass the gates of the underworld undisturbed by Cerberus, called here the "untamable Watcher of Hades". Euripides (c. 480 – 406 BC) describes Cerberus as three-headed, and three-bodied, says that Heracles entered the underworld at Tainaron, has Heracles say that Cerberus was not given to him by Persephone, but rather he fought and conquered Cerberus, "for I had been lucky enough to witness the rites of the initiated", an apparent reference to his initiation into the Eleusinian Mysteries, and says that the capture of Cerberus was the last of Heracles' labors. The lost play "Pirthous" (attributed to either Euripides or his late contemporary Critias) has Heracles say that he came to the underworld at the command of Eurystheus, who had ordered him to bring back Cerberus alive, not because he wanted to see Cerberus, but only because Eurystheus thought Heracles would not be able to accomplish the task, and that Heracles "overcame the beast" and "received favour from the gods".
Plato (c. 425 – 348 BC) refers to Cerberus' composite nature, citing Cerberus, along with Scylla and the Chimera, as an example from "ancient fables" of a creature composed of many animal forms "grown together in one". Euphorion of Chalcis (3rd century BC) describes Cerberus as having multiple snake tails, and eyes that flashed, like sparks from a blacksmith's forge, or the volcaninc Mount Etna. From Euphorion, also comes the first mention of a story which told that at Heraclea Pontica, where Cerberus was brought out of the underworld, by Heracles, Cerberus "vomited bile" from which the poisonous aconite plant grew up.

According to Diodorus Siculus (1st century BC), the capture of Cerberus was the eleventh of Heracles' labors, the twelfth and last being stealing the Apples of the Hesperides. Diodorus says that Heracles thought it best to first go to Athens to take part in the Eleusinian Mysteries, "Musaeus, the son of Orpheus, being at that time in charge of the initiatory rites", after which, he entered into the underworld "welcomed like a brother by Persephone", and "receiving the dog Cerberus in chains he carried him away to the amazement of all and exhibited him to men."

In Virgil's "Aeneid" (1st century BC), Aeneas and the Sibyl encounter Cerberus in a cave, where he "lay at vast length", filling the cave "from end to end", blocking the entrance to the underworld. Cerberus is described as "triple-throated", with "three fierce mouths", multiple "large backs", and serpents writhing around his neck. The Sybyl throws Cerberus a loaf laced with honey and herbs to induce sleep, enabling Aeneas to enter the underworld, and so apparently for Virgil—contradicting Hesiod—Cerberus guarded the underworld against entrance. Later Virgil describes Cerberus, in his bloody cave, crouching over half-gnawed bones. In his "Georgics", Virgil refers to Cerberus, his "triple jaws agape" being tamed by Orpheus' playing his lyre.

Horace (65 – 8 BC) also refers to Cerberus yielding to Orphesus' lyre, here Cerberus has a single dog head, which "like a Fury's is fortified by a hundred snakes", with a "triple-tongued mouth" oozing "fetid breath and gore".

Ovid (43 BC – AD 17/18) has Cerberus' mouth produce venom, and like Euphorion, makes Cerberus the cause of the poisonous plant aconite. According to Ovid, Heracles dragged Cerberus from the underworld, emerging from a cave "where 'tis fabled, the plant grew / on soil infected by Cerberian teeth", and dazzled by the daylight, Cerberus spewed out a "poison-foam", which made the aconite plants growing there poisonous.
Seneca, in his tragedy "Hercules Furens" gives a detailed description of Cerberus and his capture.
Seneca's Cerberus has three heads, a mane of snakes, and a snake tail, with his three heads being covered in gore, and licked by the many snakes which surround them, and with hearing so acute that he can hear "even ghosts". Seneca has Heracles use his lion-skin as shield, and his wooden club, to beat Cerberus into submission, after which Hades and Persephone, quailing on their thrones, let Heracles lead a chained and submissive Cerberus away. But upon leaving the underworld, at his first sight of daylight, a frightened Cerberus struggles furiously, and Heracles, with the help of Theseus (who had been held captive by Hades, but released, at Heracles' request) drag Cerberus into the light. Seneca, like Diodorus, has Heracles parade the captured Cerberus through Greece.

Apollodorus' Cerberus has three dog-heads, a serpent for a tail, and the heads of many snakes on his back. According to Apollodorus, Heracles' twelfth and final labor was to bring back Cerberus from Hades. Heracles first went to Eumolpus to be initiated into the Eleusinian Mysteries. Upon his entering the underworld, all the dead flee Heracles except for Meleager and the Gorgon Medusa. Heracles drew his sword against Medusa, but Hermes told Heracles that the dead are mere "empty phantoms". Heracles asked Hades (here called Pluto) for Cerberus, and Hades said that Heracles could take Cerberus provided he was able to subdue him without using weapons. Heracles found Cerberus at the gates of Acheron, and with his arms around Cerberus, though being bitten by Cerberus' serpent tail, Heracles squeezed until Cerberus submitted. Heracles carried Cerberus away, showed him to Eurystheus, then returned Cerberus to the underworld.

In an apparently unique version of the story, related by the sixth-century AD Pseudo-Nonnus, Heracles descended into Hades to abduct Persephone, and killed Cerberus on his way back up.

The capture of Cerberus was a popular theme in ancient Greek and Roman art. The earliest depictions date from the beginning of the sixth century BC. One of the two earliest depictions, a Corinthian cup (c. 590–580 BC) from Argos (now lost), shows a naked Heracles, with quiver on his back and bow in his right hand, striding left, accompanied by Hermes. Heracles threatens Hades with a stone, who flees left, while a goddess, perhaps Persephone or possibly Athena, standing in front of Hades' throne, prevents the attack. Cerberus, with a single canine head and snakes rising from his head and body, flees right. On the far right a column indicates the entrance to Hades' palace. Many of the elements of this scene— Hermes, Athena, Hades, Persephone, and a column or portico— are common occurrences in later works. The other earliest depiction, a relief "pithos" fragment from Crete (c. 590–570 BC), is thought to show a single lion-headed Cerberus with a snake (open-mouthed) over his back being led to the right.

A mid-sixth-century BC Laconian cup by the Hunt Painter adds several new features to the scene which also become common in later works: three heads, a snake tail, Cerberus' chain and Heracles' club. Here Cerberus has three canine heads, is covered by a shaggy coat of snakes, and has a tail which ends in a snake head. He is being held on a chain leash by Heracles who holds his club raised over head.

In Greek art, the vast majority of depictions of Heracles and Cerberus occur on Attic vases. Although the lost Corinthian cup shows Cerberus with a single dog head, and the relief "pithos" fragment (c. 590–570 BC) apparently shows a single lion-headed Cerberus, in Attic vase painting Cerberus usually has two dog heads. In other art, as in the Laconian cup, Cerberus is usually three-headed. Occasionally in Roman art Cerberus is shown with a large central lion head and two smaller dog heads on either side.

As in the Corinthian and Laconian cups (and possibly the relief "pithos" fragment), Cerberus is often depicted as part snake. In Attic vase painting, Cerberus is usually shown with a snake for a tail or a tail which ends in the head of a snake. Snakes are also often shown rising from various parts of his body including snout, head, neck, back, ankles, and paws.

Two Attic amphoras from Vulci, one (c. 530–515 BC) by the Bucci Painter (Munich 1493), the other (c. 525–510 BC) by the Andokides painter (Louvre F204), in addition to the usual two heads and snake tail, show Cerberus with a mane down his necks and back, another typical Cerberian feature of Attic vase painting. Andokides' amphora also has a small snake curling up from each of Cerberus' two heads.

Besides this lion-like mane and the occasional lion-head mentioned above, Cerberus was sometimes shown with other leonine features. A pitcher (c. 530–500) shows Cerberus with mane and claws, while a first-century BC sardonyx cameo shows Cerberus with leonine body and paws. In addition, a limestone relief fragment from Taranto (c. 320–300 BC) shows Cerberus with three lion-like heads.

During the second quarter of the 5th century BC the capture of Cerberus disappears from Attic vase painting. After the early third century BC, the subject becomes rare everywhere until the Roman period. In Roman art the capture of Cerberus is usually shown together with other labors. Heracles and Cerberus are usually alone, with Heracles leading Cerberus.

The etymology of Cerberus' name is uncertain. Ogden refers to attempts to establish an Indo-European etymology as "not yet successful". It has been claimed to be related to the Sanskrit word सर्वरा "sarvarā", used as an epithet of one of the dogs of Yama, from a Proto-Indo-European word *"k̑érberos", meaning "spotted". Lincoln (1991), among others, critiques this etymology. Lincoln notes a similarity between Cerberus and the Norse mythological dog Garmr, relating both names to a Proto-Indo-European root "*ger-" "to growl" (perhaps with the suffixes "-*m/*b" and "-*r"). However, as Ogden observes, this analysis actually requires "Kerberos" and "Garmr" to be derived from two "different" Indo-European roots (*"ker-" and *"gher-" respectively), and so does not actually establish a relationship between the two names.

Though probably not Greek, Greek etymologies for Cerberus have been offered. An etymology given by Servius (the late-fourth-century commentator on Virgil)—but rejected by Ogden—derives Cerberus from the Greek word "creoboros" meaning "flesh-devouring". Another suggested etymology derives Cerberus from "Ker berethrou", meaning "evil of the pit".

At least as early as the 6th century BC, some ancient writers attempted to explain away various fantastical features of Greek mythology; included in these are various rationalized accounts of the Cerberus story. The earliest such account (late 6th century BC) is that of Hecataeus of Miletus. In his account Cerberus was not a dog at all, but rather simply a large poisonous snake, which lived on Tainaron. The serpent was called the "hound of Hades" only because anyone bitten by it died immediately, and it was this snake that Heracles brought to Eurystheus. The geographer Pausanias (who preserves for us Hecataeus' version of the story) points out that, since Homer does not describe Cerberus, Hecataeus' account does not necessarily conflict with Homer, since Homer's "Hound of Hades" may not in fact refer to an actual dog.

Other rationalized accounts make Cerberus out to be a normal dog. According to Palaephatus (4th century BC) Cerberus was one of the two dogs who guarded the cattle of Geryon, the other being Orthrus. Geryon lived in a city named Tricranium (in Greek "Tricarenia," "Three-Heads"), from which name both Cerberus and Geryon came to be called "three-headed". Heracles killed Orthus, and drove away Geryon's cattle, with Cerberus following along behind. Molossus, a Mycenaen, offered to buy Cerberus from Eurystheus (presumably having received the dog, along with the cattle, from Heracles). But when Eurystheus refused, Molossus stole the dog and penned him up in a cave in Tainaron. Eurystheus commanded Heracles to find Cerberus and bring him back. After searching the entire Peloponnesus, Heracles found where it was said Cerberus was being held, went down into the cave, and brought up Cerberus, after which it was said: "Heracles descended through the cave into Hades and brought up Cerberus."

In the rationalized account of Philochorus, in which Heracles rescues Theseus, Perithous is eaten by Cerberus. In this version of the story, Aidoneus (i.e., "Hades") is the mortal king of the Molossians, with a wife named Persephone, a daughter named Kore (another name for the goddess Persephone) and a large mortal dog named Cerberus, with whom all suiters of his daughter were required to fight. After having stolen Helen, to be Theseus' wife, Theseus and Perithous, attempt to abduct Kore, for Perithous, but Aidoneus catches the two heroes, imprisons Theseus, and feeds Perithous to Cerberus. Later, while a guest of Aidoneus, Heracles asks Aidoneus to release Theseus, as a favor, which Aidoneus grants.

A 2nd-century AD Greek known as Heraclitus the paradoxographer (not to be confused with the 5th-century BC Greek philosopher Heraclitus) – claimed that Cerberus had two pups that were never away from their father, which made Cerberus appear to be three-headed.

Servius, a medieval commentator on Virgil's "Aeneid", derived Cerberus' name from the Greek word "creoboros" meaning "flesh-devouring" (see above), and held that Cerberus symbolized the corpse-consuming earth, with Heracles' triumph over Cerberus representing his victory over earthly desires. Later the mythographer Fulgentius, allegorizes Cerberus' three heads as representing the three origins of human strife: "nature, cause, and accident", and (drawing on the same flesh-devouring etymology as Servius) as symbolizing "the three ages—infancy, youth, old age, at which death enters the world."

The later Vatican Mythographers repeat and expand upon the traditions of Servius and Fulgentius. All three Vatican Mythographers repeat Servius' derivation of Cerberus' name from "creoboros". The Second Vatican Mythographer repeats (nearly word for word) what Fulgentius had to say about Cerberus, while the Third Vatican Mythographer, in another very similar passage to Fugentius', says (more specifically than Fugentius), that for "the philosophers" Cerberus represented hatred, his three heads symbolizing the three kinds of human hatred: natural, causal, and casual (i.e. accidental).

The Second and Third Vatican Mythographers, note that the three brothers Zeus, Poseidon and Hades each have tripartite insignia, associating Hades' three headed Cerberus, with Zeus' three-forked thunderbolt, and Poseidon's three-pronged trident, while the Third Vatican Mythographer adds that "some philosophers think of Cerberus as the tripartite earth: Asia, Africa, and Europe. This earth, swallowing up bodies, sends souls to Tartarus."

Virgil described Cerberus as "ravenous" ("fame rabida"), and a rapacious Cerberus became proverbial. Thus Cerberus came to symbolize avarice, and so, for example, in Dante's "Inferno," Cerberus is placed in the Third Circle of Hell, guarding over the gluttons, where he "rends the spirits, flays and quarters them," and Dante (perhaps echoing Servius' association of Cerbeus with earth) has his guide Virgil take up handfuls of earth and throw them into Cerberus' "rapacious gullets."

In the constellation Cerberus introduced by Johannes Hevelius in 1687, Cerberus is drawn as a three-headed snake, held in Hercules' hand (previously these stars had been depicted as a branch of the tree on which grew the Apples of the Hesperides).

In 1829 French naturalist Georges Cuvier gave the name "Cerberus" to a genus of Asian snakes, which are commonly called "dog-faced water snakes" in English.



</doc>
<doc id="6698" url="https://en.wikipedia.org/wiki?curid=6698" title="Camel case">
Camel case

Camel case (stylized as camelCase; also known as camel caps or more formally as medial capitals) is the practice of writing phrases such that each word or abbreviation in the middle of the phrase begins with a capital letter, with no intervening spaces or punctuation. Common examples include "iPhone" and "eBay". It is also sometimes used in online usernames such as "johnSmith", and to make multi-word domain names more legible, for example in advertisements.

Camel case is often used for variable names in computer programming. Some programming styles prefer camel case with the first letter capitalised, others not. For clarity, this article calls the two alternatives upper camel case (initial uppercase letter, also known as Pascal case) and lower camel case (initial lowercase letter, also known as Dromedary case). Some people and organizations, notably Microsoft, use the term "camel case" only for lower camel case. "Pascal case" means only upper camel case.

Camel case is distinct from Title Case, which capitalises all words but retains the spaces between them, and from Tall Man lettering, which uses capitals to emphasize the differences between similar-looking words such as "predniSONE" and "predniSOLONE". Camel case is also distinct from snake case, which uses underscores interspersed with lowercase letters (sometimes with the first letter capitalized). The combination of "upper camel case" and "snake case" is known as "Darwin case". Darwin case uses underscores between words with initial uppercase letters, as in "Sample_Type".

The original name of the practice, used in media studies, grammars and the "Oxford English Dictionary", was "medial capitals". Other synonyms include:


The earliest known occurrence of the term "InterCaps" on Usenet is in an April 1990 post to the group alt.folklore.computers by Avi Rappoport. The earliest use of the name "CamelCase" occurs in 1995, in a post by Newton Love. Love has since said, "With the advent of programming languages having these sorts of constructs, the humpiness of the style made me call it HumpyCase at first, before I settled on CamelCase. I had been calling it CamelCase for years. ... The citation above was just the first time I had used the name on USENET."

The use of medial capitals as a convention in the regular spelling of everyday texts is rare, but is used in some languages as a solution to particular problems which arise when two words or segments are combined.

In Italian, pronouns can be suffixed to verbs, and because the honorific form of second-person pronouns is capitalized, this can produce a sentence like "non ho trovato il tempo di risponderLe" ("I have not found time to answer you" – where "Le" means "to you").

In German, the medial capital letter I, called "Binnen-I", is sometimes used in a word like "StudentInnen" ("students") to indicate that both "Studenten" ("male students") and "Studentinnen" ("female students") are intended simultaneously. However, mid-word capitalisation does not conform to German orthography; the previous example could be correctly written using parentheses as "Student(inn)en", analogous to "congress(wo)man" in English.

In Irish, camelcase is used when an inflectional prefix is attached to a proper noun, for example ("in Galway"), from ("Galway"); ("the Scottish person"), from ("Scottish person"); and ("to Ireland"), from ("Ireland"). In recent Scots Gaelic orthography, a hyphen has been inserted: .

This convention is also used by several written Bantu languages (e.g., "kiSwahili", "Swahili language"; "isiZulu", "Zulu language") and several indigenous languages of Mexico (e.g. Nahuatl, Totonacan, Mixe–Zoque, and some Oto-Manguean languages).

In Dutch, when capitalizing the digraph "ij", both the letter "I" and the letter "J" are capitalized, for example in the countryname "IJsland".

In English, medial capitals are usually only found in Scottish or Irish "Mac-" or "Mc-" names, where for example "MacDonald, McDonald," and "Macdonald" are common spelling variants of the same name, and in Anglo-Norman "Fitz-" names, where for example both "FitzGerald" and "Fitzgerald" are found.

In their English style guide "The King's English", first published in 1906, H. W. and F. G. Fowler suggested that medial capitals could be used in triple compound words where hyphens would cause ambiguity—the examples they give are "KingMark-like" (as against "King Mark-like") and "Anglo-SouthAmerican" (as against "Anglo-South American"). However, they described the system as "too hopelessly contrary to use at present."

In the scholarly transliteration of languages written in other scripts, medial capitals are used in similar situations. For example, in transliterated Hebrew, "ha'Ivri" means "the Hebrew person" or "the Jew" and "b'Yerushalayim" means "in Jerusalem". In Tibetan proper names like "rLobsang", the "r" stands for a prefix glyph in the original script that functions as tone marker rather than a normal letter. Another example is "tsIurku", a Latin transcription of the Chechen term for the capping stone of the characteristic Medieval defensive towers of Chechenia and Ingushetia; the capital letter "I" here denoting a phoneme distinct from the one transcribed as "i".

Medial capitals are traditionally used in abbreviations to reflect the capitalization that the words would have when written out in full, for example in the academic titles PhD or BSc. In German, the names of statutes are abbreviated using embedded capitals, e.g. StGB (Strafgesetzbuch) for Criminal Code, PatG (Patentgesetz) for Patent Act, BVerfG (Bundesverfassungsgericht) for Federal Constitutional Court, or the very common GmbH (Gesellschaft mit beschränkter Haftung) for Company with Limited Liability. In this context, there can even be three or more "CamelCase" capitals, e.g. in TzBfG for Teilzeit- und Befristungsgesetz (Act on Part-Time and Limited Term Occupations). In French, camel case acronyms such as OuLiPo (1960) were favored for a time as alternatives to initialisms.

Camel case is often used to transliterate initialisms into alphabets where two letters may be required to represent a single character of the original alphabet, e.g., DShK from Cyrillic ДШК. 

The first systematic and widespread use of medial capitals for technical purposes was the notation for chemical formulae invented by the Swedish chemist Jacob Berzelius in 1813. To replace the multitude of naming and symbol conventions used by chemists until that time, he proposed to indicate each chemical element by a symbol of one or two letters, the first one being capitalized. The capitalization allowed formulae like "NaCl" to be written without spaces and still be parsed without ambiguity.

Berzelius' system continues to be used, augmented with three-letter symbols such as "Uue" for unconfirmed or unknown elements and abbreviations for some common substituents (especially in the field of organic chemistry, for instance "Et" for "ethyl-"). This has been further extended to describe the amino acid sequences of proteins and other similar domains.

Since the early 20th century, medial capitals have occasionally been used for corporate names and product trademarks, such as

In the 1970s and 1980s, medial capitals were adopted as a standard or alternative naming convention for multi-word identifiers in several programming languages. The precise origin of the convention in computer programming has not yet been settled. A 1954 conference proceedings occasionally informally referred to IBM's Speedcoding system as "SpeedCo". Christopher Strachey's paper on GPM (1965), shows a program that includes some medial capital identifiers, including "codice_1" and "codice_2".

Multiple-word descriptive identifiers with embedded spaces such as codice_3 or codice_4 cannot be used in most programming languages because the spaces between the words would be parsed as delimiters between tokens. The alternative of running the words together as in codice_5 or codice_6 is difficult to understand and possibly misleading; for example, codice_6 is an English word (able to be charted).

Some early programming languages, notably Lisp (1958) and COBOL (1959), addressed this problem by allowing a hyphen ("-") to be used between words of compound identifiers, as in "END-OF-FILE": Lisp because it worked well with prefix notation (a Lisp parser would not treat a hyphen in the middle of a symbol as a subtraction operator) and COBOL because its operators were individual English words. This convention remains in use in these languages, and is also common in program names entered on a command line, as in Unix.

However, this solution was not adequate for mathematically-oriented languages such as FORTRAN (1955) and ALGOL (1958), which used the hyphen as an infix subtraction operator. FORTRAN ignored blanks altogether, so programmers could use embedded spaces in variable names. However, this feature was not very useful since the early versions of the language restricted identifiers to no more than six characters.

Exacerbating the problem, common punched card character sets of the time were uppercase only and lacked other special characters. It was only in the late 1960s that the widespread adoption of the ASCII character set made both lowercase and the underscore character codice_8 universally available. Some languages, notably C, promptly adopted underscores as word separators, and identifiers such as codice_9 are still prevalent in C programs and libraries (as well as in later languages influenced by C, such as Perl and Python). However, some languages and programmers chose to avoid underscores—among other reasons to prevent confusing them with whitespace—and adopted camel case instead.

Charles Simonyi, who worked at Xerox PARC in the 1970s and later oversaw the creation of Microsoft's Office suite of applications, invented and taught the use of Hungarian Notation, one version of which uses the lowercase letter(s) at the start of a (capitalized) variable name to denote its type. One account claims that the camel case style first became popular at Xerox PARC around 1978, with the Mesa programming language developed for the Xerox Alto computer. This machine lacked an underscore key (whose place was taken by a left arrow "←"), and the hyphen and space characters were not permitted in identifiers, leaving camel case as the only viable scheme for readable multiword names. The PARC Mesa Language Manual (1979) included a coding standard with specific rules for upper and lower camel case that was strictly followed by the Mesa libraries and the Alto operating system. Niklaus Wirth, the inventor of Pascal, came to appreciate camel case during a sabbatical at PARC and used it in Modula, his next programming language.

The Smalltalk language, which was developed originally on the Alto, also uses camel case instead of underscores. This language became quite popular in the early 1980s, and thus may also have been instrumental in spreading the style outside PARC.

Whatever its origins in the computing field, the convention was used in the names of computer companies and their commercial brands, since the late 1970s — a trend that continues to this day:


In the 1980s and 1990s, after the advent of the personal computer exposed hacker culture to the world, camel case then became fashionable for corporate trade names in non-computer fields as well. Mainstream usage was well established by 1990:


During the dot-com bubble of the late 1990s, the lowercase prefixes "e" (for "electronic") and "i" (for "Internet", "information", "intelligent", etc.) became quite common, giving rise to names like Apple's iMac and the eBox software platform.

In 1998, Dave Yost suggested that chemists use medial capitals to aid readability of long chemical names, e.g. write AmidoPhosphoRibosylTransferase instead of amidophosphoribosyltransferase. This usage was not widely adopted.

Camelcase is sometimes used for abbreviated names of certain neighborhoods, e.g. New York City neighborhoods "SoHo" ("So"uth of "Ho"uston Street) and "TriBeCa" ("Tri"angle "Be"low "Ca"nal Street) and San Francisco's "SoMa" ("So"uth of "Ma"rket). Such usages erode quickly, so the neighborhoods are now typically rendered as "Soho", "Tribeca", and "Soma".

Internal capitalization has also been used for other technical codes like HeLa (1983).

The use of medial caps for compound identifiers is recommended by the coding style guidelines of many organizations or software projects. For some languages (such as Mesa, Pascal, Modula, Java and Microsoft's .NET) this practice is recommended by the language developers or by authoritative manuals and has therefore become part of the language's "culture".

Style guidelines often distinguish between upper and lower camel case, typically specifying which variety should be used for specific kinds of entities: variables, record fields, methods, procedures, types, etc. These rules are sometimes supported by static analysis tools that check source code for adherence.

The original Hungarian notation for programming, for example, specifies that a lowercase abbreviation for the "usage type" (not data type) should prefix all variable names, with the remainder of the name in upper camel case; as such it is a form of lower camel case.

Programming identifiers often need to contain acronyms and initialisms that are already in uppercase, such as "old HTML file". By analogy with the title case rules, the natural camel case rendering would have the abbreviation all in uppercase, namely "oldHTMLFile". However, this approach is problematic when two acronyms occur together (e.g., "parse DBM XML" would become "parseDBMXML") or when the standard mandates lower camel case but the name begins with an abbreviation (e.g. "SQL server" would become "sQLServer"). For this reason, some programmers prefer to treat abbreviations as if they were lowercase words and write "oldHtmlFile", "parseDbmXml" or "sqlServer". However, this can make it harder to recognise that a given word is intended as an acronym.

Camel case is used in some wiki markup languages for terms that should be automatically linked to other wiki pages. This convention was originally used in Ward Cunningham's original wiki software, WikiWikiWeb, and can be activated in most other wikis. Some wiki engines such as TiddlyWiki, Trac and PmWiki make use of it in the default settings, but usually also provide a configuration mechanism or plugin to disable it. Wikipedia formerly used camel case linking as well, but switched to explicit link markup using square brackets and many other wiki sites have done the same. Some wikis that do not use camel case linking may still use the camel case as a naming convention, such as AboutUs.

The NIEM registry requires that XML data elements use upper camel case and XML attributes use lower camel case.

Most popular command-line interfaces and scripting languages cannot easily handle file names that contain embedded spaces (usually requiring the name to be put in quotes). Therefore, users of those systems often resort to camel case (or underscores, hyphens and other "safe" characters) for compound file names like MyJobResume.pdf.

Microblogging and social networking sites that limit the number of characters in a message are potential outlets for medial capitals. Using camel case between words reduces the number of spaces, and thus the number of characters, in a given message, allowing more content to fit into the limited space. Hashtags, especially long ones, often use camel case to maintain readability (e.g. #CollegeStudentProblems is easier to read than #collegestudentproblems).

In website URLs, spaces are percent-encoded as "%20", making the address longer and less human readable. By omitting spaces, camel case does not have this problem.

Camel case has been criticised as negatively impacting readability due to the removal of spaces and uppercasing of every word.

A 2009 study comparing snake case to camel case found that camel case identifiers could be recognised with higher accuracy among both programmers and non-programmers, and that programmers already trained in camel case were able to recognise those identifiers faster than underscored snake-case identifiers.

A 2010 follow-up study, under the same conditions but using an improved measurement method with use of eye-tracking equipment, indicates: "While results indicate no difference in accuracy between the two styles, subjects recognize identifiers in the underscore style more quickly."




</doc>
<doc id="6700" url="https://en.wikipedia.org/wiki?curid=6700" title="Cereal">
Cereal

A cereal (or cereal grain) is any grass cultivated for the edible components of its grain (botanically, a type of fruit called a caryopsis), composed of the endosperm, germ, and bran. The term may also refer to the resulting grain itself. Cereal grain crops are grown in greater quantities and provide more food energy worldwide than any other type of crop and are therefore staple crops. Edible grains from other plant families, such as buckwheat (Polygonaceae), quinoa (Amaranthaceae) and chia (Lamiaceae), are referred to as pseudocereals.

In their natural, unprocessed, "whole grain" form, cereals are a rich source of vitamins, minerals, carbohydrates, fats, oils, and protein. When processed by the removal of the bran, and germ, the remaining endosperm is mostly carbohydrate. In some developing countries, grain in the form of rice, wheat, millet, or maize constitutes a majority of daily sustenance. In developed countries, cereal consumption is moderate and varied but still substantial.

The word "cereal" is derived from "Ceres", the Roman goddess of harvest and agriculture.

Agriculture allowed for the support of an increased population, leading to larger societies and eventually the development of cities. It also created the need for greater organization of political power (and the creation of social stratification), as decisions had to be made regarding labor and harvest allocation and access rights to water and land. Agriculture bred immobility, as populations settled down for long periods of time, which led to the accumulation of material goods.

Early Neolithic villages show evidence of the development of processing grain. The Levant is the ancient home of the ancestors of wheat, barley and peas, in which many of these villages were based. There is evidence of the cultivation of figs in the Jordan Valley as long as 11,300 years ago, and cereal (grain) production in Syria approximately 9,000 years ago. During the same period, farmers in China began to farm rice and millet, using man-made floods and fires as part of their cultivation regimen. Fiber crops were domesticated as early as food crops, with China domesticating hemp, cotton being developed independently in Africa and South America, and Western Asia domesticating flax. The use of soil amendments, including manure, fish, compost and ashes, appears to have begun early, and developed independently in several areas of the world, including Mesopotamia, the Nile Valley and Eastern Asia.

The first cereal grains were domesticated by early primitive humans. About 8,000 years ago, they were domesticated by ancient farming communities in the Fertile Crescent region. Emmer wheat, einkorn wheat, and barley were three of the so-called Neolithic founder crops in the development of agriculture. Around the same time, millets and rices were starting to become domesticated in East Asia. Sorghum and millets were also being domesticated in sub-Saharan West Africa.

During the second half of the 20th century there was a significant increase in the production of high-yield cereal crops worldwide, especially wheat and rice, due to an initiative known as the Green Revolution. The strategies developed by the Green Revolution focused on fending off starvation and were very successful in raising overall yields of cereal grains, but did not give sufficient relevance to nutritional quality. These modern high yield-cereal crops have low quality proteins, with essential amino acid deficiencies, are high in carbohydrates, and lack balanced essential fatty acids, vitamins, minerals and other quality factors.

While each individual species has its own peculiarities, the cultivation of all cereal crops is similar. Most are annual plants; consequently one planting yields one harvest. Wheat, rye, triticale, oats, barley, and spelt are the "cool-season" cereals. These are hardy plants that grow well in moderate weather and cease to grow in hot weather (approximately , but this varies by species and variety). The "warm-season" cereals are tender and prefer hot weather. Barley and rye are the hardiest cereals, able to overwinter in the subarctic and Siberia. Many cool-season cereals are grown in the tropics. However, some are only grown in cooler highlands, where it may be possible to grow multiple crops per year.

For the past few decades, there has also been increasing interest in perennial grain plants. This interest developed due to advantages in erosion control, reduced need for fertiliser, and potential lowered costs to the farmer. Though research is still in early stages, The Land Institute in Salina, Kansas has been able to create a few cultivars that produce a fairly good crop yield.

The warm-season cereals are grown in tropical lowlands year-round and in temperate climates during the frost-free season. Rice is commonly grown in flooded fields, though some strains are grown on dry land. Other warm climate cereals, such as sorghum, are adapted to arid conditions.

Cool-season cereals are well-adapted to temperate climates. Most varieties of a particular species are either winter or spring types. Winter varieties are sown in the autumn, germinate and grow vegetatively, then become dormant during winter. They resume growing in the springtime and mature in late spring or early summer. This cultivation system makes optimal use of water and frees the land for another crop early in the growing season.

Winter varieties do not flower until springtime because they require vernalization: exposure to low temperatures for a genetically determined length of time. Where winters are too warm for vernalization or exceed the hardiness of the crop (which varies by species and variety), farmers grow spring varieties. Spring cereals are planted in early springtime and mature later that same summer, without vernalization. Spring cereals typically require more irrigation and yield less than winter cereals.

Once the cereal plants have grown their seeds, they have completed their life cycle. The plants die, become brown, and dry. As soon as the parent plants and their seed kernels are reasonably dry, harvest can begin.

In developed countries, cereal crops are universally machine-harvested, typically using a combine harvester, which cuts, threshes, and winnows the grain during a single pass across the field. In developing countries, a variety of harvesting methods are in use, depending on the cost of labor, from combines to hand tools such as the scythe or grain cradle.

If a crop is harvested during humid weather, the grain may not dry adequately in the field to prevent spoilage during its storage. In this case, the grain is sent to a dehydrating facility, where artificial heat dries it.

In North America, farmers commonly deliver their newly harvested grain to a grain elevator, a large storage facility that consolidates the crops of many farmers. The farmer may sell the grain at the time of delivery or maintain ownership of a share of grain in the pool for later sale. Storage facilities should be protected from small grain pests, rodents and birds.

The following table shows the annual production of cereals in 1961, 2010, 2011, 2012, and 2013 ranked by 2013 production.

Maize, wheat, and rice together accounted for 89% of all cereal production worldwide in 2012, and 43% of all food calories in 2009, while the production of oats and triticale have drastically fallen from their 1960s levels.

Other cereals worthy of notice, but not included in FAO statistics, include:

Several other species of wheat have also been domesticated, some very early in the history of agriculture:

In 2013 global cereal production reached a record 2,521 million tonnes. A slight dip to 2,498 million tonnes was forecasted for 2014 by the FAO in July 2014.

Some grains are deficient in the essential amino acid, lysine. That is why many vegetarian cultures, in order to get a balanced diet, combine their diet of grains with legumes. Many legumes, however, are deficient in the essential amino acid methionine, which grains contain. Thus, a combination of legumes with grains forms a well-balanced diet for vegetarians. Common examples of such combinations are dal (lentils) with rice by South Indians and Bengalis, dal with wheat in Pakistan and North India, beans with corn tortillas, tofu with rice, and peanut butter with wheat bread (as sandwiches) in several other cultures, including the Americas. The amount of crude protein measured in grains is expressed as grain crude protein concentration.

Cereals contain exogenous opioid peptides called exorphins and include opioid food peptides like Gluten exorphin and opioid food peptides. They mimic the actions of endorphines because they bind to the same opioid receptors in the brain.

The ISO has published a series of standards regarding cereal products which are covered by ICS 67.060.


</doc>
<doc id="6704" url="https://en.wikipedia.org/wiki?curid=6704" title="Christendom">
Christendom

Christendom has several meanings. In one contemporary sense, as used in a secular or Protestant context, it may refer to the "Christian world": Christian-majority countries and the countries in which Christianity dominates or prevails, or, in the historic, Catholic sense of the word, the nations in which Catholic Christianity is the established religion, having a Catholic Christian polity.

Since the spread of Christianity from the Levant to Europe and North Africa during the early Roman Empire, Christendom has been divided in the pre-existing Greek East and Latin West. Consequently, different versions of the Christian religion arose with their own beliefs and practices, centred around the cities of Rome (Western Christianity, whose community was called Western or Latin Christendom) and Constantinople (Eastern Christianity, whose community was called Eastern Christendom). From the 11th to 13th centuries, Latin Christendom rose to the central role of the Western world.

In its historical sense, the term usually refers to the Middle Ages and to the Early Modern period during which the Christian world represented a geopolitical power that was juxtaposed with both the pagan and especially the Muslim world. In the traditional Roman Catholic sense of the word, it refers to the sum total of nations in which the Catholic Church is the established religion of the state or to those with ecclesiastical concordats with the Holy See.

The Anglo-Saxon term "cristendom" appears to have been invented in the 9th century by a scribe somewhere in southern England, possibly at the court of king Alfred the Great of Wessex. The scribe was translating Paulus Orosius' book "History Against the Pagans" (c. 416) and in need for a term to express the concept of the universal culture focused on Jesus Christ. It had the sense now taken by "Christianity" (as is still the case with the cognate Dutch "christendom", where it denotes mostly the religion itself, just like the German "Christentum").

The current sense of the word of "lands where Christianity is the dominant religion" emerged in Late Middle English (by c. 1400). This semantic development happened independently in the languages of late medieval Europe, which leads to the confusing semantics of English "Christendom" equalling German "Christenheit", Dutch "christenheid", French "chrétienté" vs. English "Christianity" equalling German "Christentum", Dutch "christendom", French "christianisme". The reason is the increasing fragmentation of Western Christianity at that time both theologically and politically.
"Christendom" as a geopolitical term is thus meaningful in the context of the Middle Ages, and arguably during the European wars of religion and the Ottoman wars in Europe.

Canadian theology professor Douglas John Hall stated (1997) that "Christendom" [...] means literally the dominion or sovereignty of the Christian religion." Thomas John Curry, Roman Catholic auxiliary bishop of Los Angeles, defined (2001) Christendom as "the system dating from the fourth century by which governments upheld and promoted Christianity." Curry states that the end of Christendom came about because modern governments refused to "uphold the teachings, customs, ethos, and practice of Christianity." British church historian Diarmaid MacCulloch described (2010) Christendom as "the union between Christianity and secular power."

The "Christian world" is also collectively known as the Corpus Christianum, translated as "the Christian body", meaning the community of all Christians. The Christian polity, embodying a less secular meaning, can be compatible with the idea of both a religious and a temporal body: "Corpus Christianum". The "Corpus Christianum" can be seen as a Christian equivalent of the Muslim "Ummah".

The word "Christendom" is also used with its other meaning to frame-true Christianity. A more secular meaning can denote the fact that the term "Christendom" refers to Christians as a group, the "political Christian world", as an informal cultural hegemony that Christianity has traditionally enjoyed in the West.
In its most broad term, it refers to the world's Christian-majority countries, which, share little in common aside from the predominance of the faith. Unlike the Muslim world, which has a geo-political and cultural definition that provides a primary identifier for a large swath of the world, Christendom is more complex.
There is a common and nonliteral sense of the word that is much like the terms "Western world", "known world" or "Free World". When Thomas F. Connolly said, "There isn't enough power in all Christendom to make that airplane what we want!", he was simply using a figure of speech, although it is true that during the Cold War, just as the totalitarianism of the Communist Bloc presented a contrast to the liberty of the Free World, the state atheism of the Communist Bloc contrasted with the religious freedom and the powerful religious institutions in North America and Western Europe. The notion of "Europe" and the "Western World" has been intimately connected with the concept of "Christianity and Christendom"; many even attribute Christianity for being the link that created a unified European identity.

In the beginning of Christendom, early Christianity was a religion spread in the Greek/Roman world and beyond as a 1st-century Jewish sect, which historians refer to as Jewish Christianity. It may be divided into two distinct phases: the apostolic period, when the first apostles were alive and organizing the Church, and the post-apostolic period, when an early episcopal structure developed, whereby bishoprics were governed by bishops (overseers).

The post-apostolic period concerns the time roughly after the death of the apostles when bishops emerged as overseers of urban Christian populations. The earliest recorded use of the terms "Christianity" (Greek ) and "catholic" (Greek ), dates to this period, the 2nd century, attributed to Ignatius of Antioch "c." 107. Early Christendom would close at the end of imperial persecution of Christians after the ascension of Constantine the Great and the Edict of Milan in AD 313 and the First Council of Nicaea in 325.

According to Malcolm Muggeridge (1980), Christ founded Christianity, but Constantine founded Christendom. Canadian theology professor Douglas John Hall dates the 'inauguration of Christendom' to the 4th century, with Constantine playing the primary role (so much so that he equates Christendom with "Constantinianism") and Theodosius I (Edict of Thessalonica, 380) and Justinian I secondary roles.

"Christendom" has referred to the medieval and renaissance notion of the "Christian world" as a sociopolitical polity. In essence, the earliest vision of Christendom was a vision of a Christian theocracy, a government founded upon and upholding Christian values, whose institutions are spread through and over with Christian doctrine. In this period, members of the Christian clergy wield political authority. The specific relationship between the political leaders and the clergy varied but, in theory, the national and political divisions were at times subsumed under the leadership of the church as an institution. This model of church-state relations was accepted by various Church leaders and political leaders in European history.

The Church gradually became a defining institution of the Empire. Emperor Constantine issued the Edict of Milan in 313 proclaiming toleration for the Christian religion, and convoked the First Council of Nicaea in 325 whose Nicene Creed included belief in "one holy catholic and apostolic Church". Emperor Theodosius I made Nicene Christianity the state church of the Roman Empire with the Edict of Thessalonica of 380.

As the Western Roman Empire disintegrated into feudal kingdoms and principalities, the concept of Christendom changed as the western church became one of five patriarchates of the Pentarchy and the Christians of the Eastern Roman Empire developed. The Byzantine Empire was the last bastion of Christendom. Christendom would take a turn with the rise of the Franks, a Germanic tribe who converted to the Christian faith and entered into communion with Rome.

On Christmas Day 800 AD, Pope Leo III crowned Charlemagne resulting in the creation of another Christian king beside the Christian emperor in the Byzantine state. The Carolingian Empire created a definition of "Christendom" in juxtaposition with the Byzantine Empire, that of a distributed versus centralized culture respectively.

The classical heritage flourished throughout the Middle Ages in both the Byzantine Greek East and the Latin West. In the Greek philosopher Plato's ideal state there are three major classes, which was representative of the idea of the “tripartite soul”, which is expressive of three functions or capacities of the human soul: “reason”, “the spirited element”, and “appetites” (or “passions”). Will Durant made a convincing case that certain prominent features of Plato's ideal community where discernible in the organization, dogma and effectiveness of "the" Medieval Church in Europe:

... For a thousand years Europe was ruled by an order of guardians considerably like that which was visioned by our philosopher. During the Middle Ages it was customary to classify the population of Christendom into "laboratores" (workers), "bellatores" (soldiers), and "oratores" (clergy). The last group, though small in number, monopolized the instruments and opportunities of culture, and ruled with almost unlimited sway half of the most powerful continent on the globe. The clergy, like Plato's guardians, were placed in authority... by their talent as shown in ecclesiastical studies and administration, by their disposition to a life of meditation and simplicity, and ... by the influence of their relatives with the powers of state and church. In the latter half of the period in which they ruled [800 AD onwards], the clergy were as free from family cares as even Plato could desire [for such guardians]... [Clerical] Celibacy was part of the psychological structure of the power of the clergy; for on the one hand they were unimpeded by the narrowing egoism of the family, and on the other their apparent superiority to the call of the flesh added to the awe in which lay sinners held them..." "In the latter half of the period in which they ruled, the clergy were as free from family cares as even Plato could desire.

After the collapse of Charlemagne's empire, the southern remnants of the Holy Roman Empire became a collection of states loosely connected to the Holy See of Rome. Tensions between Pope Innocent III and secular rulers ran high, as the pontiff exerted control over their temporal counterparts in the west and vice versa. The pontificate of Innocent III is considered the height of temporal power of the papacy. The "Corpus Christianum" described the then-current notion of the community of all Christians united under the Roman Catholic Church. The community was to be guided by Christian values in its politics, economics and social life. Its legal basis was the "corpus iuris canonica" (body of canon law).

In the East, Christendom became more defined as the Byzantine Empire's gradual loss of territory to an expanding Islam and the muslim conquest of Persia. This caused Christianity to become important to the Byzantine identity. Before the East–West Schism which divided the Church religiously, there had been the notion of a "universal Christendom" that included the East and the West. After the East–West Schism, hopes of regaining religious unity with the West were ended by the Fourth Crusade, when Crusaders conquered the Byzantine capital of Constantinople and hastened the decline of the Byzantine Empire on the path to its destruction. With the breakup of the Byzantine Empire into individual nations with nationalist Orthodox Churches, the term Christendom described Western Europe, Catholicism, Orthodox Byzantines, and other Eastern rites of the Church.

The Catholic Church's peak of authority over all European Christians and their common endeavours of the Christian community — for example, the Crusades, the fight against the Moors in the Iberian Peninsula and against the Ottomans in the Balkans — helped to develop a sense of communal identity against the obstacle of Europe's deep political divisions. The popes, formally just the bishops of Rome, claimed to be the focus of all Christendom, which was largely recognised in Western Christendom from the 11th century until the Reformation, but not in Eastern Christendom. Moreover, this authority was also sometimes abused, and fostered the Inquisition and anti-Jewish pogroms, to root out divergent elements and create a religiously uniform community. Ultimately, the Inquisition was done away with by order of Pope Innocent III.

Christendom ultimately was led into specific crisis in the late Middle Ages, when the kings of France managed to establish a French national church during the 14th century and the papacy became ever more aligned with the Holy Roman Empire of the German Nation. Known as the Western Schism, western Christendom was a split between three men, who were driven by politics rather than any real theological disagreement for simultaneously claiming to be the true pope. The Avignon Papacy developed a reputation for corruption that estranged major parts of Western Christendom. The Avignon schism was ended by the Council of Constance.

Before the modern period, Christendom was in a general crisis at the time of the Renaissance Popes because of the moral laxity of these pontiffs and their willingness to seek and rely on temporal power as secular rulers did. Many in the Catholic Church's hierarchy in the Renaissance became increasingly entangled with insatiable greed for material wealth and temporal power, which led to many reform movements, some merely wanting a moral reformation of the Church's clergy, while others repudiated the Church and separated from it in order to form new sects. The Italian Renaissance produced ideas or institutions by which men living in society could be held together in harmony. In the early 16th century, Baldassare Castiglione (The Book of the Courtier) laid out his vision of the ideal gentleman and lady, while Machiavelli cast a jaundiced eye on "la verità effetuale delle cose" — the actual truth of things — in "The Prince", composed, humanist style, chiefly of parallel ancient and modern examples of Virtù. Some Protestant movements grew up along lines of mysticism or renaissance humanism (cf. Erasmus). The Catholic Church fell partly into general neglect under the Renaissance Popes, whose inability to govern the Church by showing personal example of high moral standards set the climate for what would ultimately become the Protestant Reformation. During the Renaissance, the papacy was mainly run by the wealthy families and also had strong secular interests. To safeguard Rome and the connected Papal States the popes became necessarily involved in temporal matters, even leading armies, as the great patron of arts Pope Julius II did. It during these intermediate times popes strove to make Rome the capital of Christendom while projecting it, through art, architecture, and literature, as the center of a Golden Age of unity, order, and peace.

Professor Frederick J. McGinness described Rome as essential in understanding the legacy the Church and its representatives encapsulated best by The Eternal City: No other city in Europe matches Rome in its traditions, history, legacies, and influence in the Western world. Rome in the Renaissance under the papacy not only acted as guardian and transmitter of these elements stemming from the Roman Empire but also assumed the role as artificer and interpreter of its myths and meanings for the peoples of Europe from the Middle Ages to modern times... Under the patronage of the popes, whose wealth and income were exceeded only by their ambitions, the city became a cultural center for master architects, sculptors, musicians, painters, and artisans of every kind...In its myth and message, Rome had become the sacred city of the popes, the prime symbol of a triumphant Catholicism, the center of orthodox Christianity, a new Jerusalem.

It is clearly noticeable that the popes of the Italian Renaissance have been subjected by many writers with an overly harsh tone. Pope Julius II, for example, was not only an effective secular leader in military affairs, a deviously effective politician but foremost one of the greatest patron of the Renaissance period and person who also encouraged open criticism from noted humanists.

The blossoming of renaissance humanism was made very much possible due to the universality of the institutions of Catholic Church and represented by personalities such as Pope Pius II, Nicolaus Copernicus, Leon Battista Alberti, Desiderius Erasmus, sir Thomas More, Bartolomé de Las Casas, Leonardo da Vinci and Teresa of Ávila. George Santayana in his work "The Life of Reason" postulated the tenets of the all encompassing order the Church had brought and as the repository of the legacy of classical antiquity:

The enterprise of individuals or of small aristocratic bodies has meantime sown the world which we call civilised with some seeds and nuclei of order. There are scattered about a variety of churches, industries, academies, and governments. But the universal order once dreamt of and nominally almost established, the empire of universal peace, all-permeating rational art, and philosophical worship, is mentioned no more. An unformulated conception, the prerational ethics of private privilege and national unity, fills the background of men's minds. It represents feudal traditions rather than the tendency really involved in contemporary industry, science, or philanthropy. Those dark ages, from which our political practice is derived, had a political theory which we should do well to study; for their theory about a universal empire and a Catholic church was in turn the echo of a former age of reason, when a few men conscious of ruling the world had for a moment sought to survey it as a whole and to rule it justly.

Developments in western philosophy and European events brought change to the notion of the "Corpus Christianum". The Hundred Years' War accelerated the process of transforming France from a feudal monarchy to a centralized state. The rise of strong, centralized monarchies denoted the European transition from feudalism to capitalism. By the end of the Hundred Years' War, both France and England were able to raise enough money through taxation to create independent standing armies. In the Wars of the Roses, Henry Tudor took the crown of England. His heir, the absolute king Henry VIII establishing the English church.

In modern history, the Reformation and rise of modernity in the early 16th century entailed a change in the "Corpus Christianum". In the Holy Roman Empire, the Peace of Augsburg of 1555 officially ended the idea among secular leaders that all Christians must be united under one church. The principle of "cuius regio, eius religio" ("whose the region is, his religion") established the religious, political and geographic divisions of Christianity, and this was established with the Treaty of Westphalia in 1648, which legally ended the concept of a single Christian hegemony in the territories of the Holy Roman Empire, despite the Catholic Church's doctrine that it alone is the one true Church founded by Christ.
Subsequently, each government determined the religion of their own state. Christians living in states where their denomination was "not" the established one were guaranteed the right to practice their faith in public during allotted hours and in private at their will. At times there were mass expulsions of dissenting faiths as happened with the Salzburg Protestants. Some people passed as adhering to the official church, but instead lived as Nicodemites or crypto-protestants.

The European wars of religion are usually taken to have ended with the Treaty of Westphalia (1648), or arguably, including the Nine Years' War and the War of the Spanish Succession in this period, with the Treaty of Utrecht of 1713. In the 18th century, the focus shifts away from religious conflicts, either between Christian factions or against the external threat of Islamic factions.

The European Miracle, the Age of Enlightenment and the formation of the great colonial empires together with the beginning decline of the Ottoman Empire mark the end of the geopolitical "history of Christendom". Instead, the focus of Western history shifts to the development of the nation-state, accompanied by increasing atheism and secularism, culminating with the French Revolution and the Napoleonic Wars at the turn of the 19th century.

Writing in 1997, Canadian theology professor Douglas John Hall argued that Christendom had either fallen already or was in its death throes; although its end was gradual and not as clear to pin down as its 4th-century establishment, the "transition to the post-Constantinian, or post-Christendom, situation (...) has already been in process for a century or two," beginning with the 18th-century rationalist Enlightenment and the French Revolution (the first attempt to topple the Christian establishment). American Catholic bishop Thomas John Curry stated (2001) that the end of Christendom came about because modern governments refused to "uphold the teachings, customs, ethos, and practice of Christianity." He argued the First Amendment to the United States Constitution (1791) and the Second Vatican Council's Declaration on Religious Freedom (1965) are two of the most important documents setting the stage for its end. According to British historian Diarmaid MacCulloch (2010), Christendom was 'killed' by the First World War (1914–18), which led to the fall of the three main Christian empires (Russian, German and Austrian) of Europe, as well as the Ottoman Empire, rupturing the Eastern Christian communities that had existed on its territory. The Christian empires were replaced by secular, even anti-clerical republics seeking to definitively keep the churches out of politics. The only surviving monarchy with an established church, Britain, was severely damaged by the war, lost most of Ireland due to Catholic–Protestant infighting, and was starting to lose grip on its colonies.

Western culture, throughout most of its history, has been nearly equivalent to Christian culture, and many of the population of the Western hemisphere could broadly be described as cultural Christians. The notion of "Europe" and the "Western World" has been intimately connected with the concept of "Christianity and Christendom"; many even attribute Christianity for being the link that created a unified European identity. Historian Paul Legutko of Stanford University said the Catholic Church is "at the center of the development of the values, ideas, science, laws, and institutions which constitute what we call Western civilization."

Though Western culture contained several polytheistic religions during its early years under the Greek and Roman Empires, as the centralized Roman power waned, the dominance of the Catholic Church was the only consistent force in Western Europe. Until the Age of Enlightenment, Christian culture guided the course of philosophy, literature, art, music and science. Christian disciplines of the respective arts have subsequently developed into Christian philosophy, Christian art, Christian music, Christian literature etc. Art and literature, law, education, and politics were preserved in the teachings of the Church, in an environment that, otherwise, would have probably seen their loss. The Church founded many cathedrals, universities, monasteries and seminaries, some of which continue to exist today. Medieval Christianity created the first modern universities. The Catholic Church established a hospital system in Medieval Europe that vastly improved upon the Roman "valetudinaria". These hospitals were established to cater to "particular social groups marginalized by poverty, sickness, and age," according to historian of hospitals, Guenter Risse. Christianity also had a strong impact on all other aspects of life: marriage and family, education, the humanities and sciences, the political and social order, the economy, and the arts.

Christianity had a significant impact on education and science and medicine as the church created the bases of the Western system of education, and was the sponsor of founding universities in the Western world as the university is generally regarded as an institution that has its origin in the Medieval Christian setting. Many clerics throughout history have made significant contributions to science and Jesuits in particular have made numerous significant contributions to the development of science. The cultural influence of Christianity includes social welfare, founding hospitals, economics (as the Protestant work ethic), natural law (which would later influence the creation of international law), politics, architecture, literature, personal hygiene, and family life. Christianity played a role in ending practices common among pagan societies, such as human sacrifice, slavery, infanticide and polygamy.

Christian literature is writing that deals with Christian themes and incorporates the Christian world view. This constitutes a huge body of extremely varied writing. Christian poetry is any poetry that contains Christian teachings, themes, or references. The influence of Christianity on poetry has been great in any area that Christianity has taken hold. Christian poems often directly reference the Bible, while others provide allegory.

Christian art is art produced in an attempt to illustrate, supplement and portray in tangible form the principles of Christianity. Virtually all Christian groupings use or have used art to some extent. The prominence of art and the media, style, and representations change; however, the unifying theme is ultimately the representation of the life and times of Jesus and in some cases the Old Testament. Depictions of saints are also common, especially in Anglicanism, Roman Catholicism, and Eastern Orthodoxy.

An illuminated manuscript is a manuscript in which the text is supplemented by the addition of decoration. The earliest surviving substantive illuminated manuscripts are from the period AD 400 to 600, primarily produced in Ireland, Constantinople and Italy. The majority of surviving manuscripts are from the Middle Ages, although many illuminated manuscripts survive from the 15th century Renaissance, along with a very limited number from Late Antiquity.

Most illuminated manuscripts were created as codices, which had superseded scrolls; some isolated single sheets survive. A very few illuminated manuscript fragments survive on papyrus. Most medieval manuscripts, illuminated or not, were written on parchment (most commonly of calf, sheep, or goat skin), but most manuscripts important enough to illuminate were written on the best quality of parchment, called vellum, traditionally made of unsplit calfskin, though high quality parchment from other skins was also called "parchment".

Christian art began, about two centuries after Christ, by borrowing motifs from Roman Imperial imagery, classical Greek and Roman religion and popular art. Religious images are used to some extent by the Abrahamic Christian faith, and often contain highly complex iconography, which reflects centuries of accumulated tradition. In the Late Antique period iconography began to be standardised, and to relate more closely to Biblical texts, although many gaps in the canonical Gospel narratives were plugged with matter from the apocryphal gospels. Eventually the Church would succeed in weeding most of these out, but some remain, like the ox and ass in the Nativity of Christ.

An icon is a religious work of art, most commonly a painting, from Eastern Christianity. Christianity has used symbolism from its very beginnings. In both East and West, numerous iconic types of Christ, Mary and saints and other subjects were developed; the number of named types of icons of Mary, with or without the infant Christ, was especially large in the East, whereas Christ Pantocrator was much the commonest image of Christ.

Christian symbolism invests objects or actions with an inner meaning expressing Christian ideas. Christianity has borrowed from the common stock of significant symbols known to most periods and to all regions of the world. Religious symbolism is effective when it appeals to both the intellect and the emotions. Especially important depictions of Mary include the Hodegetria and Panagia types. Traditional models evolved for narrative paintings, including large cycles covering the events of the Life of Christ, the Life of the Virgin, parts of the Old Testament, and, increasingly, the lives of popular saints. Especially in the West, a system of attributes developed for identifying individual figures of saints by a standard appearance and symbolic objects held by them; in the East they were more likely to identified by text labels.

Each saint has a story and a reason why he or she led an exemplary life. Symbols have been used to tell these stories throughout the history of the Church. A number of Christian saints are traditionally represented by a symbol or iconic motif associated with their life, termed an attribute or emblem, in order to identify them. The study of these forms part of iconography in Art history. They were particularly

Christian architecture encompasses a wide range of both secular and religious styles from the foundation of Christianity to the present day, influencing the design and construction of buildings and structures in Christian culture.

Buildings were at first adapted from those originally intended for other purposes but, with the rise of distinctively ecclesiastical architecture, church buildings came to influence secular ones which have often imitated religious architecture. In the 20th century, the use of new materials, such as concrete, as well as simpler styles has had its effect upon the design of churches and arguably the flow of influence has been reversed. From the birth of Christianity to the present, the most significant period of transformation for Christian architecture in the west was the Gothic cathedral. In the east, Byzantine architecture was a continuation of Roman architecture.

Christian philosophy is a term to describe the fusion of various fields of philosophy with the theological doctrines of Christianity. Scholasticism, which means "that [which] belongs to the school", and was a method of learning taught by the academics (or "school people") of medieval universities c. 1100–1500. Scholasticism originally started to reconcile the philosophy of the ancient classical philosophers with medieval Christian theology. Scholasticism is not a philosophy or theology in itself but a tool and method for learning which places emphasis on dialectical reasoning.

The Byzantine Empire, which was the most sophisticated culture during antiquity, suffered under Muslim conquests limiting its scientific prowess during the Medieval period. Christian Western Europe had suffered a catastrophic loss of knowledge following the fall of the Western Roman Empire. But thanks to the Church scholars such as Aquinas and Buridan, the West carried on at least the spirit of scientific inquiry which would later lead to Europe's taking the lead in science during the Scientific Revolution using translations of medieval works.

Medieval technology refers to the technology used in medieval Europe under Christian rule. After the Renaissance of the 12th century, medieval Europe saw a radical change in the rate of new inventions, innovations in the ways of managing traditional means of production, and economic growth. The period saw major technological advances, including the adoption of gunpowder and the astrolabe, the invention of spectacles, and greatly improved water mills, building techniques, agriculture in general, clocks, and ships. The latter advances made possible the dawn of the Age of Exploration. The development of water mills was impressive, and extended from agriculture to sawmills both for timber and stone, probably derived from Roman technology. By the time of the Domesday Book, most large villages in Britain had mills. They also were widely used in mining, as described by Georg Agricola in De Re Metallica for raising ore from shafts, crushing ore, and even powering bellows.

Significant in this respect were advances within the fields of navigation. The compass and astrolabe along with advances in shipbuilding, enabled the navigation of the World Oceans and thus domination of the worlds economic trade. Gutenberg’s printing press made possible a dissemination of knowledge to a wider population, that would not only lead to a gradually more egalitarian society, but one more able to dominate other cultures, drawing from a vast reserve of knowledge and experience.

During the Renaissance, great advances occurred in geography, astronomy, chemistry, physics, math, manufacturing, and engineering. The rediscovery of ancient scientific texts was accelerated after the Fall of Constantinople, and the invention of printing which would democratize learning and allow a faster propagation of new ideas. "Renaissance technology" is the set of artifacts and customs, spanning roughly the 14th through the 16th century. The era is marked by such profound technical advancements like the printing press, linear perspectivity, patent law, double shell domes or Bastion fortresses. Draw-books of the Renaissance artist-engineers such as Taccola and Leonardo da Vinci give a deep insight into the mechanical technology then known and applied.

Renaissance science spawned the Scientific Revolution; science and technology began a cycle of mutual advancement. The "Scientific Renaissance" was the early phase of the Scientific Revolution. In the two-phase model of early modern science: a "Scientific Renaissance" of the 15th and 16th centuries, focused on the restoration of the natural knowledge of the ancients; and a "Scientific Revolution" of the 17th century, when scientists shifted from recovery to innovation.

In 2009, according to the "Encyclopædia Britannica", Christianity was the majority religion in Europe (including Russia) with 80%, Latin America with 92%, North America with 81%, and Oceania with 79%. There are also large Christian communities in other parts of the world, such as China, India and Central Asia, where Christianity is the second-largest religion after Islam. The United States is home to the world's largest Christian population, followed by Brazil and Mexico.
Many Christians not only live under, but also have an official status in, a state religion of the following nations: Armenia (Armenian Apostolic Church), Costa Rica (Roman Catholic Church), Denmark (Church of Denmark), El Salvador (Roman Catholic Church), England (Church of England), Georgia (Georgian Orthodox church), Greece (Church of Greece), Iceland (Church of Iceland), Liechtenstein (Roman Catholic Church), Malta (Roman Catholic Church), Monaco (Roman Catholic Church), Romania (Romanian Orthodox Church), Norway (Church of Norway), Vatican City (Roman Catholic Church), Switzerland (Roman Catholic Church, Swiss Reformed Church and Christian Catholic Church of Switzerland).

The estimated number of Christians in the world ranges from 2.2 billion to 2.4 billion people. The faith represents approximately one-third of the world's population and is the largest religion in the world, with the three largest groups of Christians being the Catholic Church, Protestantism, and the Eastern Orthodox Church. The largest Christian denomination is the Catholic Church, with an estimated 1.2 billion adherents.

A religious order is a lineage of communities and organizations of people who live in some way set apart from society in accordance with their specific religious devotion, usually characterized by the principles of its founder's religious practice. In contrast, the term Holy Orders is used by many Christian churches to refer to ordination or to a group of individuals who are set apart for a special role or ministry. Historically, the word "order" designated an established civil body or corporation with a hierarchy, and ordinatio meant legal incorporation into an ordo. The word "holy" refers to the Church. In context, therefore, a holy order is set apart for ministry in the Church. Religious orders are composed of initiates (laity) and, in some traditions, ordained clergies.

Various organizations include:

Within the framework of Christianity, there are at least three possible definitions for Church law. One is the Torah/Mosaic Law (from what Christians consider to be the Old Testament) also called Divine Law or Biblical law. Another is the instructions of Jesus of Nazareth in the Gospel (sometimes referred to as the Law of Christ or the New Commandment or the New Covenant). A third is canon law which is the internal ecclesiastical law governing the Roman Catholic Church, the Eastern Orthodox churches, and the Anglican Communion of churches. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was initially a rule adopted by a council (From Greek "kanon" / κανών, Hebrew kaneh / קנה, for rule, standard, or measure); these canons formed the foundation of canon law.

Christian ethics in general has tended to stress the need for grace, mercy, and forgiveness because of human weakness and developed while Early Christians were subjects of the Roman Empire. From the time Nero blamed Christians for setting Rome ablaze (64 AD) until Galarius (311 AD), persecutions against Christians erupted periodically. Consequently, Early Christian ethics included discussions of how believers should relate to Roman authority and to the empire.

Under the Emperor Constantine I (312-337), Christianity became a legal religion. While some scholars debate whether Constantine's conversion to Christianity was authentic or simply matter of political expediency, Constantine's decree made the empire safe for Christian practice and belief. Consequently, issues of Christian doctrine, ethics and church practice were debated openly, see for example the First Council of Nicaea and the First seven Ecumenical Councils. By the time of Theodosius I (379-395), Christianity had become the state religion of the empire. With Christianity in power, ethical concerns broaden and included discussions of the proper role of the state.

Render unto Caesar… is the beginning of a phrase attributed to Jesus in the synoptic gospels which reads in full, ""Render unto Caesar the things which are Caesar’s, and unto God the things that are God’s"". This phrase has become a widely quoted summary of the relationship between Christianity and secular authority. The gospels say that when Jesus gave his response, his interrogators "marvelled, and left him, and went their way." Time has not resolved an ambiguity in this phrase, and people continue to interpret this passage to support various positions that are poles apart. The traditional division, carefully determined, in Christian thought is the state and church have separate spheres of influence.

Thomas Aquinas thoroughly discussed that "human law" is positive law which means that it is natural law applied by governments to societies. All human laws were to be judged by their conformity to the natural law. An unjust law was in a sense no law at all. At this point, the natural law was not only used to pass judgment on the moral worth of various laws, but also to determine what the law said in the first place. This could result in some tension. Late ecclesiastical writers followed in his footsteps.

Christian democracy is a political ideology that seeks to apply Christian principles to public policy. It emerged in 19th-century Europe, largely under the influence of Catholic social teaching. In a number of countries, the democracy's Christian ethos has been diluted by secularisation. In practice, Christian democracy is often considered conservative on cultural, social and moral issues and progressive on fiscal and economic issues. In places, where their opponents have traditionally been secularist socialists and social democrats, Christian democratic parties are moderately conservative, whereas in other cultural and political environments they can lean to the left.

Attitudes and beliefs about the roles and responsibilities of women in Christianity vary considerably today as they have throughout the last two millennia — evolving along with or counter to the societies in which Christians have lived. The Bible and Christianity historically have been interpreted as excluding women from church leadership and placing them in submissive roles in marriage. Male leadership has been assumed in the church and within marriage, society and government.

Some contemporary writers describe the role of women in the life of the church as having been downplayed, overlooked, or denied throughout much of Christian history. Paradigm shifts in gender roles in society and also many churches has inspired reevaluation by many Christians of some long-held attitudes to the contrary. Christian egalitarians have increasingly argued for equal roles for men and women in marriage, as well as for the ordination of women to the clergy. Contemporary conservatives meanwhile have reasserted what has been termed a "complementarian" position, promoting the traditional belief that the Bible ordains different roles and responsibilities for women and men in the Church and family.

A Christian denomination is a distinct religious body within Christianity, identified by traits such as a name, organisation, leadership and doctrine. Worldwide, Christians are divided, often along ethnic and linguistic lines, into separate churches and traditions. Technically, divisions between one group and another are defined by church doctrine and church authority. Centering on language of "professed Christianity" and "true Christianity", issues that separate one group of followers of Jesus from another include:

Christianity is composed of, but not limited to, five major branches of Churches: Catholicism, Eastern Orthodoxy, Oriental Orthodoxy, Anglicanism, and Protestantism. Some listings include Anglicans among Protestants while others list the Eastern Orthodox and Oriental Orthodox together as one group, thus the number of distinct major branches can vary between three and five depending on the listing. The Assyrian Church of the East (Nestorians) and the Old Catholic churches are also distinct Christian bodies of historic importance, but much smaller in adherents and geographic scope. Each of the branches has important subdivisions. Because the Protestant subdivisions do not maintain a common theology or earthly leadership, they are far more distinct than the subdivisions of the other four groupings. "Denomination" typically refers to one of the many Christian groupings including each of the multitude of Protestant subdivisions.

In Christendom, the largest denominations are:








</doc>
<doc id="6710" url="https://en.wikipedia.org/wiki?curid=6710" title="Coyote">
Coyote

The coyote (from Nahuatl "coyōtl" ), prairie wolf or brush wolf, "Canis latrans", is a canine native to North America. It is smaller than its close relative, the gray wolf, and slightly smaller than the closely related eastern wolf and red wolf. It fills much of the same ecological niche as the golden jackal does in Eurasia, though it is larger and more predatory, and is sometimes called the American jackal by zoologists.

The coyote is listed as least concern by the International Union for Conservation of Nature due to its wide distribution and abundance throughout North America, southwards through Mexico, and into Central America. The species is versatile, able to adapt to and expand into environments modified by humans. It is enlarging its range, with coyotes moving into urban areas in the Eastern U.S., and was sighted in eastern Panama (across the Panama Canal from their home range) for the first time in 2013.

, 19 coyote subspecies are recognized. The average male weighs and the average female . Their fur color is predominantly light gray and red or fulvous interspersed with black and white, though it varies somewhat with geography. It is highly flexible in social organization, living either in a family unit or in loosely knit packs of unrelated individuals. It has a varied diet consisting primarily of animal meat, including deer, rabbits, hares, rodents, birds, reptiles, amphibians, fish, and invertebrates, though it may also eat fruits and vegetables on occasion. Its characteristic vocalization is a howl made by solitary individuals. Humans are the coyote's greatest threat, followed by cougars and gray wolves. In spite of this, coyotes sometimes mate with gray, eastern, or red wolves, producing "coywolf" hybrids. In the northeastern United States and eastern Canada, the eastern coyote (a larger subspecies, though still smaller than wolves) is the result of various historical and recent matings with various types of wolves. Genetic studies show that most North American wolves contain some level of coyote DNA.

The coyote is a prominent character in Native American folklore, mainly in the Southwestern United States and Mexico, usually depicted as a trickster that alternately assumes the form of an actual coyote or a man. As with other trickster figures, the coyote uses deception and humor to rebel against social conventions. The animal was especially respected in Mesoamerican cosmology as a symbol of military might. After the European colonization of the Americas, it was reviled in Anglo-American culture as a cowardly and untrustworthy animal. Unlike wolves (gray, eastern, or red), which have undergone an improvement of their public image, attitudes towards the coyote remain largely negative.

Coyote males average in weight, while females average , though size varies geographically. Northern subspecies, which average , tend to grow larger than the southern subspecies of Mexico, which average . Body length ranges on average from , and tail length , with females being shorter in both body length and height. The largest coyote on record was a male killed near Afton, Wyoming, on November19, 1937, which measured from nose to tail, and weighed . Scent glands are located at the upper side of the base of the tail and are a bluish-black color.

The color and texture of the coyote's fur varies somewhat geographically. The hair's predominant color is light gray and red or fulvous, interspersed around the body with black and white. Coyotes living at high elevations tend to have more black and gray shades than their desert-dwelling counterparts, which are more fulvous or whitish-gray. The coyote's fur consists of short, soft underfur and long, coarse guard hairs. The fur of northern subspecies is longer and denser than in southern forms, with the fur of some Mexican and Central American forms being almost hispid (bristly). Generally, adult coyotes (including coywolf hybrids) have a sable coat color, dark neonatal coat color, bushy tail with an active supracaudal gland, and a white facial mask. Albinism is extremely rare in coyotes; out of a total of 750,000 coyotes killed by federal and cooperative hunters between March22, 1938, and June30, 1945, only two were albinos.

The coyote is typically smaller than the gray wolf, but has longer ears and a relatively larger braincase, as well as a thinner frame, face, and muzzle. The scent glands are smaller than the gray wolf's, but are the same color. Its fur color variation is much less varied than that of a wolf. The coyote also carries its tail downwards when running or walking, rather than horizontally as the wolf does.

Coyote tracks can be distinguished from those of dogs by their more elongated, less rounded shape. Unlike dogs, the upper canines of coyotes extend past the mental foramina.

At the time of the European colonization of the Americas, coyotes were largely confined to open plains and arid regions of the western half of the continent. In early post-Columbian historical records, distinguishing between coyotes and wolves is often difficult. One record from 1750 in Kaskaskia, Illinois, written by a local priest, noted that the "wolves" encountered there were smaller and less daring than European wolves. Another account from the early 1800s in Edwards County mentioned wolves howling at night, though these were likely coyotes. This species was encountered several times during the Lewis and Clark Expedition (1804–1806), though it was already well known to European traders on the upper Missouri. Lewis, writing on 5 May 1805, in northeastern Montana, described the coyote in these terms:
The coyote was first scientifically described by naturalist Thomas Say in September 1819, on the site of Lewis and Clark's Council Bluffs, fifteen miles up the Missouri River from the mouth of the Platte during a government-sponsored expedition with Major Stephen Long. He had the first edition of the Lewis and Clark journals in hand, which contained Biddle's edited version of Lewis's observations dated 5 May 1805. His account was published in 1823. Say was the first person to document the difference between a ""prairie wolf"" (coyote) and on the next page of his journal a wolf which he named "Canis nubilus" (Great Plains wolf). Say described the coyote as:
The earliest written reference to the species comes from the naturalist Francisco Hernández's "Plantas y Animales de la Nueva España" (1651), where it is described as a "Spanish fox" or "jackal". The first published usage of the word "coyote" (which is a Spanish borrowing of its Nahuatl name "coyōtl") comes from the historian Francisco Javier Clavijero's "Historia de México" in 1780. The first time it was used in English occurred in William Bullock's "Six months' residence and travels in Mexico" (1824), where it is variously transcribed as "cayjotte" and "cocyotie". The word's spelling was standardized as "coyote" by the 1880s. Alternative English names for the coyote include "prairie wolf", "brush wolf", "cased wolf", "little wolf" and "American jackal". Its binomial name "Canis latrans" translates to "barking dog", a reference to the many vocalizations they produce.

Xiaoming Wang and Richard H. Tedford, one of the foremost authorities on carnivore evolution, proposed that the genus "Canis" was the descendant of the coyote-like "Eucyon davisi" and its remains first appeared in the Miocene 6million years ago (Mya) in the southwestern US and Mexico. By the Pliocene (5Mya), the larger "Canis lepophagus" appeared in the same region and by the early Pleistocene (1Mya) "C.latrans" (the coyote) was in existence. They proposed that the progression from "Eucyon davisi" to "Clepophagus" to the coyote was linear evolution. Additionally, "C.latrans" and "C. aureus" are closely related to "C.edwardii", a species that appeared earliest spanning the mid-Blancan (late Pliocene) to the close of the Irvingtonian (late Pleistocene), and coyote remains indistinguishable from "C.latrans" were contemporaneous with "C.edwardii" in North America. Johnston describes "C.lepophagus" as having a more slender skull and skeleton than the modern coyote. Ronald Nowak found that the early populations had small, delicate, narrowly proportioned skulls that resemble small coyotes and appear to be ancestral to "C.latrans".

"C. lepophagus" was similar in weight to modern coyotes, but had shorter limb bones that indicates a less cursorial lifestyle. The coyote represents a more primitive form of "Canis" than the gray wolf, as shown by its relatively small size and its comparatively narrow skull and jaws, which lack the grasping power necessary to hold the large prey in which wolves specialize. This is further corroborated by the coyote's sagittal crest, which is low or totally flattened, thus indicating a weaker bite than the wolf's. The coyote is not a specialized carnivore as the wolf is, as shown by the larger chewing surfaces on the molars, reflecting the species' relative dependence on vegetable matter. In these respects, the coyote resembles the fox-like progenitors of the genus more so than the wolf.

The oldest fossils that fall within the range of the modern coyote date to 0.74–0.85 Ma (million years) in Hamilton Cave, West Virginia; 0.73 Ma in Irvington, California; 0.35–0.48 Ma in Porcupine Cave, Colorado and in Cumberland Cave, Pennsylvania. Modern coyotes arose 1,000 years after the Quaternary extinction event. Compared to their modern Holocene counterparts, Pleistocene coyotes ("C.l. orcutti") were larger and more robust, likely in response to larger competitors and prey. Pleistocene coyotes were likely more specialized carnivores than their descendants, as their teeth were more adapted to shearing meat, showing fewer grinding surfaces suited for processing vegetation. Their reduction in size occurred within 1000 years of the Quaternary extinction event, when their large prey died out. Furthermore, Pleistocene coyotes were unable to exploit the big-game hunting niche left vacant after the extinction of the dire wolf ("C.dirus"), as it was rapidly filled by gray wolves, which likely actively killed off the large coyotes, with natural selection favoring the modern gracile morph.

In 1993, a study proposed that the wolves of North America display skull traits more similar to the coyote than wolves from Eurasia. In 2010, a study found that the coyote was a basal member of the clade that included the Tibetan wolf, the domestic dog, the Mongolian wolf and the Eurasian wolf, with the Tibetan wolf diverging early from wolves and domestic dogs. In 2016, a whole-genome DNA study proposed, based on the assumptions made, that all of the North American wolves and coyotes diverged from a common ancestor less than 6,000–117,000 years ago. The study also indicated that all North American wolves have a significant amount of coyote ancestry and all coyotes some degree of wolf ancestry, and that the red wolf and eastern wolf are highly admixed with different proportions of gray wolf and coyote ancestry. The proposed timing of the wolf/coyote divergence conflicts with the finding of a coyote-like specimen in strata dated to 1 Mya.

Genetic studies relating to wolves or dogs have inferred phylogenetic relationships based on the only reference genome available, that of the Boxer dog. In 2017, the first reference genome of the wolf "Canis lupus lupus" was mapped to aid future research. In 2018, a study looked at the genomic structure and admixture of North American wolves, wolf-like canids, and coyotes using specimens from across their entire range that mapped the largest dataset of nuclear genome sequences against the wolf reference genome. The study supports the findings of previous studies that North American gray wolves and wolf-like canids were the result of complex gray wolf and coyote mixing. A polar wolf from Greenland and a coyote from Mexico represented the purest specimens. The coyotes from
Alaska, California, Alabama, and Quebec show almost no wolf ancestry. Coyotes from Missouri, Illinois, and Florida exhibit 5–10% wolf ancestry. There was 40%:60% wolf to coyote ancestry in red wolves, 60%:40% in Eastern timber wolves, and 75%:25% in the Great Lakes wolves. There was 10% coyote ancestry in Mexican wolves and the Atlantic Coast wolves, 5% in Pacific Coast and Yellowstone wolves, and less than 3% in Canadian archipelago wolves. If a third canid had been involved in the admixture of the North American wolf-like canids then its genetic signature would have been found in coyotes and wolves, which it has not.

In 2018, whole genome sequencing was used to compare members of genus "Canis". The study indicates that the common ancestor of the coyote and gray wolf has genetically admixed with a ghost population of an extinct unidentified canid. The canid was genetically close to the dhole and had evolved after the divergence of the African wild dog from the other canid species. The basal position of the coyote compared to the wolf is proposed to be due to the coyote retaining more of the mitochondrial genome of this unknown canid.

, 19 subspecies are recognized. Geographic variation in coyotes is not great, though taken as a whole, the eastern subspecies ("C. l. thamnos" and "C. l. frustor") are large, dark-colored animals, with a gradual paling in color and reduction in size westward and northward ("C. l. texensis", "C. l. latrans", "C. l. lestes", and "C. l. incolatus"), a brightening of ochraceous tones–deep orange or brown–towards the Pacific coast ("C. l. ochropus", "C. l. umpquensis"), a reduction in size in the Southwestern United States ("C. l. microdon", "C. l. mearnsi") and a general trend towards dark reddish colors and short muzzles in Mexican and Central American populations.

Coyotes have occasionally mated with domestic dogs, sometimes producing crosses colloquially known as "coydogs". Such matings are rare in the wild, as the mating cycles of dogs and coyotes do not coincide, and coyotes are usually antagonistic towards dogs. Hybridization usually only occurs when coyotes are expanding into areas where conspecifics are few, and dogs are the only alternatives. Even then, pup survival rates are lower than normal, as dogs do not form pair bonds with coyotes, thus making the rearing of pups more difficult. In captivity, F hybrids (first generation) tend to be more mischievous and less manageable as pups than dogs, and are less trustworthy on maturity than wolf-dog hybrids. Hybrids vary in appearance, but generally retain the coyote's usual characteristics. F hybrids tend to be intermediate in form between dogs and coyotes, while F hybrids (second generation) are more varied. Both F and F hybrids resemble their coyote parents in terms of shyness and intrasexual aggression. Hybrids are fertile and can be successfully bred through four generations. Melanistic coyotes owe their black pelts to a mutation that first arose in domestic dogs. A population of nonalbino white coyotes in Newfoundland owe their coloration to a melanocortin 1 receptor mutation inherited from Golden Retrievers.
Coyotes have hybridized with wolves to varying degrees, particularly in the Eastern United States and Canada. The so-called "eastern coyote" of northeastern North America probably originated in the aftermath of the extermination of gray and eastern wolves in the northeast, thus allowing coyotes to colonize former wolf ranges and mix with remnant wolf populations. This hybrid is smaller than either the gray or eastern wolf, and holds smaller territories, but is in turn larger and holds more extensive home ranges than the typical western coyote. , the eastern coyote's genetic makeup is fairly uniform, with minimal influence from eastern wolves or western coyotes. Adult eastern coyotes are larger than western coyotes, with female eastern coyotes weighing 21% more than male western coyotes. Physical differences become more apparent by the age of 35 days, with eastern coyote pups having longer legs than their western counterparts. Differences in dental development also occurs, with tooth eruption being later, and in a different order in the eastern coyote. Aside from its size, the eastern coyote is physically similar to the western coyote. The four color phases range from dark brown to blond or reddish blond, though the most common phase is gray-brown, with reddish legs, ears, and flanks. No significant differences exist between eastern and western coyotes in aggression and fighting, though eastern coyotes tend to fight less, and are more playful. Unlike western coyote pups, in which fighting precedes play behavior, fighting among eastern coyote pups occurs after the onset of play. Eastern coyotes tend to reach sexual maturity at two years of age, much later than in western coyotes.

Eastern and red wolves are also products of varying degrees of wolf-coyote hybridization. The eastern wolf probably was a result of a wolf-coyote admixture, combined with extensive backcrossing with parent gray wolf populations. The red wolf may have originated during a time of declining wolf populations in the southeastern United States, forcing a wolf-coyote hybridization, as well as backcrossing with local parent coyote populations to the extent that about 75–80% of the modern red wolf's genome is of coyote derivation.

Like the Eurasian golden jackal, the coyote is gregarious, but not as dependent on conspecifics as more social canid species like wolves are. This is likely because the coyote is not a specialized hunter of large prey as the latter species is. The basic social unit of a coyote pack is a family containing a reproductive female. However, unrelated coyotes may join forces for companionship, or to bring down prey too large to attack singly. Such "nonfamily" packs are only temporary, and may consist of bachelor males, nonreproductive females and subadult young. Families are formed in midwinter, when females enter estrus. Pair bonding can occur 2–3 months before actual copulation takes place. The copulatory tie can last 5–45 minutes. A female entering estrus attracts males by scent marking and howling with increasing frequency. A single female in heat can attract up to seven reproductive males, which can follow her for as long as a month. Although some squabbling may occur among the males, once the female has selected a mate and copulates, the rejected males do not intervene, and move on once they detect other estrous females. Unlike the wolf, which has been known to practice both monogamous and bigamous matings, the coyote is strictly monogamous, even in areas with high coyote densities and abundant food. Females that fail to mate sometimes assist their sisters or mothers in raising their pups, or join their siblings until the next time they can mate. The newly mated pair then establishes a territory and either constructs their own den or cleans out abandoned badger, marmot, or skunk earths. During the pregnancy, the male frequently hunts alone and brings back food for the female. The female may line the den with dried grass or with fur pulled from her belly. The gestation period is 63 days, with an average litter size of six, though the number fluctuates depending on coyote population density and the abundance of food.

Coyote pups are born in dens, hollow trees, or under ledges, and weigh at birth. They are altricial, and are completely dependent on milk for their first 10 days. The incisors erupt at about 12 days, the canines at 16, and the second premolars at 21. Their eyes open after 10 days, by which point the pups become increasingly more mobile, walking by 20 days, and running at the age of six weeks. The parents begin supplementing the pup's diet with regurgitated solid food after 12–15 days. By the age of four to six weeks, when their milk teeth are fully functional, the pups are given small food items such as mice, rabbits, or pieces of ungulate carcasses, with lactation steadily decreasing after two months. Unlike wolf pups, coyote pups begin seriously fighting (as opposed to play fighting) prior to engaging in play behavior. A common play behavior includes the coyote "hip-slam". By three weeks of age, coyote pups bite each other with less inhibition than wolf pups. By the age of four to five weeks, pups have established dominance hierarchies, and are by then more likely to play rather than fight. The male plays an active role in feeding, grooming, and guarding the pups, but abandons them if the female goes missing before the pups are completely weaned. The den is abandoned by June to July, and the pups follow their parents in patrolling their territory and hunting. Pups may leave their families in August, though can remain for much longer. The pups attain adult dimensions at eight months, and gain adult weight a month later.

Individual feeding territories vary in size from , with the general concentration of coyotes in a given area depending on food abundance, adequate denning sites, and competition with conspecifics and other predators. The coyote generally does not defend its territory outside of the denning season, and is much less aggressive towards intruders than the wolf is, typically chasing and sparring with them, but rarely killing them. Conflicts between coyotes can arise during times of food shortage. Coyotes mark their territories by raised-leg urination and ground-scratching.

Like wolves, coyotes use a den (usually the deserted holes of other species) when gestating and rearing young, though they may occasionally give birth under sagebrushes in the open. Coyote dens can be located in canyons, washouts, coulees, banks, rock bluffs, or level ground. Some dens have been found under abandoned homestead shacks, grain bins, drainage pipes, railroad tracks, hollow logs, thickets, and thistles. The den is continuously dug and cleaned out by the female until the pups are born. Should the den be disturbed or infested with fleas, the pups are moved into another den. A coyote den can have several entrances and passages branching out from the main chamber. A single den can be used year after year.

While the popular consensus is that olfaction is very important for hunting, two studies that experimentally investigated the role of olfactory, auditory, and visual cues found that visual cues are the most important ones for hunting in red foxes and coyotes.

When hunting large prey, the coyote often works in pairs or small groups. Success in killing large ungulates depends on factors such as snow depth and crust density. Younger animals usually avoid participating in such hunts, with the breeding pair typically doing most of the work. Unlike the wolf, which attacks large prey from the rear, the coyote approaches from the front, lacerating its prey's head and throat. Like other canids, the coyote caches excess food. Coyotes catch mouse-sized rodents by pouncing, whereas ground squirrels are chased. Although coyotes can live in large groups, small prey is typically caught singly. Coyotes have been observed to kill porcupines in pairs, using their paws to flip the rodents on their backs, then attacking the soft underbelly. Only old and experienced coyotes can successfully prey on porcupines, with many predation attempts by young coyotes resulting in them being injured by their prey's quills. Coyotes sometimes urinate on their food, possibly to claim ownership over it. Recent evidence demonstrates that at least some coyotes have become more nocturnal in hunting, presumably to avoid humans.

Coyotes may occasionally form mutualistic hunting relationships with American badgers, assisting each other in digging up rodent prey. The relationship between the two species may occasionally border on apparent "friendship", as some coyotes have been observed laying their heads on their badger companions or licking their faces without protest. The amicable interactions between coyotes and badgers were known to pre-Columbian civilizations, as shown on a Mexican jar dated to 1250–1300 CE depicting the relationship between the two.

Being both a gregarious and solitary animal, the variability of the coyote's visual and vocal repertoire is intermediate between that of the solitary foxes and the highly social wolf. The aggressive behavior of the coyote bears more similarities to that of foxes than it does that of wolves and dogs. An aggressive coyote arches its back and lowers its tail. Unlike dogs, which solicit playful behavior by performing a "play-bow" followed by a "play-leap", play in coyotes consists of a bow, followed by side-to-side head flexions and a series of "spins" and "dives". Although coyotes will sometimes bite their playmates' scruff as dogs do, they typically approach low, and make upward-directed bites. Pups fight each other regardless of sex, while among adults, aggression is typically reserved for members of the same sex. Combatants approach each other waving their tails and snarling with their jaws open, though fights are typically silent. Males tend to fight in a vertical stance, while females fight on all four paws. Fights among females tend to be more serious than ones among males, as females seize their opponents' forelegs, throat, and shoulders.

The coyote has been described as "the most vocal of all [wild] North American mammals". Its loudness and range of vocalizations was the cause for its binomial name "Canis latrans", meaning "barking dog". At least 11 different vocalizations are known in adult coyotes. These sounds are divided into three categories: agonistic and alarm, greeting, and contact. Vocalizations of the first category include woofs, growls, huffs, barks, bark howls, yelps, and high-frequency whines. Woofs are used as low-intensity threats or alarms, and are usually heard near den sites, prompting the pups to immediately retreat into their burrows. Growls are used as threats at short distances, but have also been heard among pups playing and copulating males. Huffs are high-intensity threat vocalizations produced by rapid expiration of air. Barks can be classed as both long-distance threat vocalizations and as alarm calls. Bark howls may serve similar functions. Yelps are emitted as a sign of submission, while high-frequency whines are produced by dominant animals acknowledging the submission of subordinates. Greeting vocalizations include low-frequency whines, 'wow-oo-wows', and group yip howls. Low-frequency whines are emitted by submissive animals, and are usually accompanied by tail wagging and muzzle nibbling. The sound known as 'wow-oo-wow' has been described as a "greeting song". The group yip howl is emitted when two or more pack members reunite, and may be the final act of a complex greeting ceremony. Contact calls include lone howls and group howls, as well as the previously mentioned group yip howls. The lone howl is the most iconic sound of the coyote, and may serve the purpose of announcing the presence of a lone individual separated from its pack. Group howls are used as both substitute group yip howls and as responses to either lone howls, group howls, or group yip howls.

Prior to the near extermination of wolves and cougars, the coyote was most numerous in grasslands inhabited by bison, pronghorn, elk, and other deer, doing particularly well in short-grass areas with prairie dogs, though it was just as much at home in semiarid areas with sagebrush and jackrabbits or in deserts inhabited by cactus, kangaroo rats, and rattlesnakes. As long as it was not in direct competition with the wolf, the coyote ranged from the Sonoran Desert to the alpine regions of adjoining mountains or the plains and mountainous areas of Alberta. With the extermination of the wolf, the coyote's range expanded to encompass broken forests from the tropics of Guatemala and the northern slope of Alaska.

Coyotes walk around per day, often along trails such as logging roads and paths; they may use iced-over rivers as travel routes in winter. They are often crepuscular, being more active around evening and the beginning of the night than during the day. Like many canids, coyotes are competent swimmers, reported to be able to travel at least across water.

The coyote is roughly the North American equivalent to the Eurasian golden jackal. Likewise, the coyote is highly versatile in its choice of food, but is primarily carnivorous, with 90% of its diet consisting of meat. Prey species include bison, deer, sheep, rabbits, rodents, birds, amphibians (except toads), lizards, snakes, fish, crustaceans, and insects. Coyotes may be picky over the prey they target, as animals such as shrews, moles, and brown rats do not occur in their diet in proportion to their numbers. More unusual prey include fishers, young black bear cubs, harp seals and rattlesnakes. Coyotes kill rattlesnakes mostly for food (but also to protect their pups at their dens) by teasing the snakes until they stretch out and then biting their heads and snapping and shaking the snakes. Birds taken by coyotes may range in size from thrashers, larks and sparrows to adult wild turkeys and, possibly, brooding adult swans and pelicans. If working in packs or pairs, coyotes may have access to larger prey than lone individuals normally take, such as various prey weighing more than . In some cases, packs of coyotes have dispatched much larger prey such as adult "Odocoileus" deer, cow elk, pronghorns and wild sheep, although the young fawn, calves and lambs of these animals are considerably more often taken even by packs, as well as domestic sheep and domestic cattle. In some cases, coyotes can bring down prey weighing up to or more. When it comes to adult ungulates such as wild deer, they often exploit them when vulnerable such as those that are infirm, stuck in snow or ice, otherwise winter-weakened or heavily pregnant, whereas less wary domestic ungulates may be more easily exploited.

Although coyotes prefer fresh meat, they will scavenge when the opportunity presents itself. Excluding the insects, fruit, and grass eaten, the coyote requires an estimated of food daily, or annually. The coyote readily cannibalizes the carcasses of conspecifics, with coyote fat having been successfully used by coyote hunters as a lure or poisoned bait. The coyote's winter diet consists mainly of large ungulate carcasses, with very little plant matter. Rodent prey increases in importance during the spring, summer, and fall.

The coyote feeds on a variety of different produce, including blackberries, blueberries, peaches, pears, apples, prickly pears, chapotes, persimmons, peanuts, watermelons, cantaloupes, and carrots. During the winter and early spring, the coyote eats large quantities of grass, such as green wheat blades. It sometimes eats unusual items such as cotton cake, soybean meal, domestic animal droppings, beans, and cultivated grain such as maize, wheat, and sorghum.

In coastal California, coyotes now consume a higher percentage of marine-based food than their ancestors, which is thought to be due to the extirpation of the grizzly bear from this region. In Death Valley, coyotes may consume great quantities of hawkmoth caterpillars or beetles in the spring flowering months.

In areas where the ranges of coyotes and gray wolves overlap, interference competition and predation by wolves has been hypothesized to limit local coyote densities. Coyote ranges expanded during the 19th and 20th centuries following the extirpation of wolves, while coyotes were driven to extinction on Isle Royale after wolves colonized the island in the 1940s. One study conducted in Yellowstone National Park, where both species coexist, concluded that the coyote population in the Lamar River Valley declined by 39% following the reintroduction of wolves in the 1990s, while coyote populations in wolf inhabited areas of the Grand Teton National Park are 33% lower than in areas where they are absent. Wolves have been observed to not tolerate coyotes in their vicinity, though coyotes have been known to trail wolves to feed on their kills.

Coyotes may compete with cougars in some areas. In the eastern Sierra Nevadas, coyotes compete with cougars over mule deer. Cougars normally outcompete and dominate coyotes, and may kill them occasionally, thus reducing coyote predation pressure on smaller carnivores such as foxes and bobcats. Coyotes that are killed are sometimes not eaten, perhaps indicating that these compromise competitive interspecies interactions, however there are multiple confirmed cases of cougars also eating coyotes. In northeastern Mexico, cougar predation on coyotes continues apace but coyotes were absent from the prey spectrum of sympatric jaguars, apparently due to differing habitat usages.

Other than by gray wolves and cougars, predation on adult coyotes is relatively rare but multiple other predators can be occasional threats. In some cases, adult coyotes have been preyed upon by both American black and grizzly bears, American alligators, large Canada lynx and golden eagles. At kill sites and carrion, coyotes, especially if working alone, tend to be dominated by wolves, cougars, bears, wolverines and, usually but not always, eagles (i.e., bald and golden). When such larger, more powerful and/or more aggressive predators such as these come to a shared feeding site, a coyote may either try to fight, wait until the other predator is done or occasionally share a kill, but if a major danger such as wolves or an adult cougar is present, the coyote will tend to flee.

Coyotes rarely kill healthy adult red foxes, and have been observed to feed or den alongside them, though they often kill foxes caught in traps. Coyotes may kill fox kits, but this is not a major source of mortality. In southern California, coyotes frequently kill gray foxes, and these smaller canids tend to avoid areas with high coyote densities.

In some areas, coyotes share their ranges with bobcats. These two similarly-sized species rarely physically confront one another, though bobcat populations tend to diminish in areas with high coyote densities. However, several studies have demonstrated interference competition between coyotes and bobcats, and in all cases coyotes dominated the interaction. Multiple researchers reported instances of coyotes killing bobcats, whereas bobcats killing coyotes is more rare. Coyotes attack bobcats using a bite-and-shake method similar to what is used on medium-sized prey. Coyotes (both single individuals and groups) have been known to occasionally kill bobcats – in most cases, the bobcats were relatively small specimens, such as adult females and juveniles. However, coyote attacks (by an unknown number of coyotes) on adult male bobcats have occurred. In California, coyote and bobcat populations are not negatively correlated across different habitat types, but predation by coyotes is an important source of mortality in bobcats. Biologist Stanley Paul Young noted that in his entire trapping career, he had never successfully saved a captured bobcat from being killed by coyotes, and wrote of two incidents wherein coyotes chased bobcats up trees. Coyotes have been documented to directly kill Canada lynx on occasion, and compete with them for prey, especially snowshoe hares. In some areas, including central Alberta, lynx are more abundant where coyotes are few, thus interactions with coyotes appears to influence lynx populations more than the availability of snowshoe hares.

Due to the coyote's wide range and abundance throughout North America, it is listed as Least Concern by the International Union for Conservation of Nature (IUCN). The coyote's pre-Columbian range was limited to the Southwest and Plains regions of the United States and Canada, and northern and central Mexico. By the 19th century, the species expanded north and east, expanding further after 1900, coinciding with land conversion and the extirpation of wolves. By this time, its range encompassed all of the United States and Mexico, southward into Central America, and northward into most of Canada and Alaska. This expansion is ongoing, and the species now occupies the majority of areas between 8°N (Panama) and 70°N (northern Alaska).

Although it was once widely believed that coyotes are recent immigrants to southern Mexico and Central America, aided in their expansion by deforestation, Pleistocene and Early Holocene records, as well as records from the pre-Columbian period and early European colonization show that the animal was present in the area long before modern times. Nevertheless, range expansion did occur south of Costa Rica during the late 1970s and northern Panama in the early 1980s, following the expansion of cattle-grazing lands into tropical rain forests. The coyote is predicted to appear in northern Belize in the near future, as the habitat there is favorable to the species. Concerns have been raised of a possible expansion into South America through the Panamanian Isthmus, should the Darién Gap ever be closed by the Pan-American Highway. This fear was partially confirmed in January 2013, when the species was recorded in eastern Panama's Chepo District, beyond the Panama Canal.

A recent genetic study proposes that coyotes were originally not found in the eastern United States. From the 1890s, dense forests were transformed into agricultural land and wolf control implemented on a large scale, leaving a niche for coyotes to disperse into. There were two major dispersals from two populations of genetically distinct coyotes. The first major dispersal to the northeast came in the early 20th century from those coyotes living in the northern Great Plains. These came to New England via the northern Great Lakes region and southern Canada, and to Pennsylvania via the southern Great Lakes region, meeting together in the 1940s in New York and Pennsylvania. These coyotes have hybridized with the remnant gray wolf and eastern wolf populations, which has added to coyote genetic diversity and may have assisted adaptation to the new niche. The second major dispersal to the southeast came in the mid-20th century from Texas and reached the Carolinas in the 1980s. These coyotes have hybridized with the remnant red wolf populations before the 1970s when the red wolf was extirpated in the wild, which has also added to coyote genetic diversity and may have assisted adaptation to this new niche as well. Both of these two major coyote dispersals have experienced rapid population growth and are forecast to meet along the mid-Atlantic coast. The study concludes that for coyotes the long range dispersal, gene flow from local populations, and rapid population growth may be inter-related.

Among large North American carnivores, the coyote probably carries the largest number of diseases and parasites, likely due to its wide range and varied diet. Viral diseases known to infect coyotes include rabies, canine distemper, infectious canine hepatitis, four strains of equine encephalitis, and oral papillomatosis. By the late 1970s, serious rabies outbreaks in coyotes had ceased to be a problem for over 60 years, though sporadic cases every 1–5 years did occur. Distemper causes the deaths of many pups in the wild, though some specimens can survive infection. "Tularemia", a bacterial disease, infects coyotes from tick bites and through their rodent and lagomorph prey, and can be deadly for pups.

Coyotes can be infected by both demodectic and sarcoptic mange, the latter being the most common. Mite infestations are rare and incidental in coyotes, while tick infestations are more common, with seasonal peaks depending on locality (May–August in the Northwest, March–November in Arkansas). Coyotes are only rarely infested with lice, while fleas infest coyotes from puphood, though they may be more a source of irritation than serious illness. "Pulex simulans" is the most common species to infest coyotes, while "Ctenocephalides canis" tends to occur only in places where coyotes and dogs (its primary host) inhabit the same area. Although coyotes are rarely host to flukes, they can nevertheless have serious effects on coyotes, particularly "Nanophyetus salmincola", which can infect them with salmon poisoning disease, a disease with a 90% mortality rate. Trematode "Metorchis conjunctus" can also infect coyotes. Tapeworms have been recorded to infest 60–95% of all coyotes examined. The most common species to infest coyotes are "Taenia pisiformis" and "Taenia crassiceps", which uses cottontail rabbits as intermediate hosts. The largest species known in coyotes is "T. hydatigena", which enters coyotes through infected ungulates, and can grow to lengths of . Although once largely limited to wolves, "Echinococcus granulosus" has expanded to coyotes since the latter began colonizing former wolf ranges. The most frequent ascaroid roundworm in coyotes is "Toxascaris leonina", which dwells in the coyote's small intestine and has no ill effects, except for causing the host to eat more frequently. Hookworms of the genus "Ancylostoma" infest coyotes throughout their range, being particularly prevalent in humid areas. In areas of high moisture, such as coastal Texas, coyotes can carry up to 250 hookworms each. The blood-drinking "A. caninum" is particularly dangerous, as it damages the coyote through blood loss and lung congestion. A 10-day-old pup can die from being host to as few as 25 "A. caninum" worms.

Coyote features as a trickster figure and skin-walker in the folktales of some Native Americans in the United States, notably several nations in the Southwestern and Plains regions, where he alternately assumes the form of an actual coyote or that of a man. As with other trickster figures, Coyote acts as a picaresque hero who rebels against social convention through deception and humor. Folklorists such as Harris believe coyotes came to be seen as tricksters due to the animal's intelligence and adaptability. After the European colonization of the Americas, Anglo-American depictions of Coyote are of a cowardly and untrustworthy animal. Unlike the gray wolf, which has undergone a radical improvement of its public image, Anglo-American cultural attitudes towards the coyote remain largely negative.

In the Maidu creation story, Coyote introduces work, suffering, and death to the world. Zuni lore has Coyote bringing winter into the world by stealing light from the kachinas. The Chinook, Maidu, Pawnee, Tohono O'odham, and Ute portray the coyote as the companion of The Creator. A Tohono O'odham flood story has Coyote helping Montezuma survive a global deluge that destroys humanity. After The Creator creates humanity, Coyote and Montezuma teach people how to live. The Crow creation story portrays Old Man Coyote as The Creator. In The Dineh creation story, Coyote was present in the First World with First Man and First Woman, though a different version has it being created in the Fourth World. The Navajo Coyote brings death into the world, explaining that without death, too many people would exist, thus no room to plant corn.
Prior to the Spanish conquest of the Aztec Empire, Coyote played a significant role in Mesoamerican cosmology. The coyote symbolized military might in Classic era Teotihuacan, with warriors dressing up in coyote costumes to call upon its predatory power. The species continued to be linked to Central Mexican warrior cults in the centuries leading up to the post-Classic Aztec rule. In Aztec mythology, Huehuecóyotl (meaning "old coyote"), the god of dance, music and carnality, is depicted in several codices as a man with a coyote's head. He is sometimes depicted as a womanizer, responsible for bringing war into the world by seducing Xochiquetzal, the goddess of love. Epigrapher David H. Kelley argued that the god Quetzalcoatl owed its origins to pre-Aztec Uto-Aztecan mythological depictions of the coyote, which is portrayed as mankind's "Elder Brother", a creator, seducer, trickster, and culture hero linked to the morning star.

Coyote attacks on humans are uncommon and rarely cause serious injuries, due to the relatively small size of the coyote, but have been increasingly frequent, especially in California. There have been only two confirmed fatal attacks: one on a three-year-old named Kelly Keen in Glendale, California and another on a nineteen-year-old named Taylor Mitchell in Nova Scotia, Canada. In the 30 years leading up to March 2006, at least 160 attacks occurred in the United States, mostly in the Los Angeles County area. Data from United States Department of Agriculture (USDA) Wildlife Services, the California Department of Fish and Game, and other sources show that while 41 attacks occurred during the period of 1988–1997, 48 attacks were verified from 1998 through 2003. The majority of these incidents occurred in Southern California near the suburban-wildland interface.

In the absence of the harassment of coyotes practiced by rural people, urban coyotes are losing their fear of humans, which is further worsened by people intentionally or unintentionally feeding coyotes. In such situations, some coyotes have begun to act aggressively toward humans, chasing joggers and bicyclists, confronting people walking their dogs, and stalking small children. Non-rabid coyotes in these areas sometimes target small children, mostly under the age of 10, though some adults have been bitten.

Although media reports of such attacks generally identify the animals in question as simply "coyotes", research into the genetics of the eastern coyote indicates those involved in attacks in northeast North America, including Pennsylvania, New York, New England, and eastern Canada, may have actually been coywolves, hybrids of "Canis latrans" and "C. lupus," not fully coyotes.

Coyotes are presently the most abundant livestock predators in western North America, causing the majority of sheep, goat, and cattle losses. For example, according to the National Agricultural Statistics Service, coyotes were responsible for 60.5% of the 224,000 sheep deaths attributed to predation in 2004. The total number of sheep deaths in 2004 comprised 2.22% of the total sheep and lamb population in the United States, which, according to the National Agricultural Statistics Service USDA report, totaled 4.66 million and 7.80 million heads respectively as of July 1, 2005. Because coyote populations are typically many times greater and more widely distributed than those of wolves, coyotes cause more overall predation losses. United States government agents routinely shoot, poison, trap, and kill about 90,000 coyotes each year to protect livestock. An Idaho census taken in 2005 showed that individual coyotes were 5% as likely to attack livestock as individual wolves. In Utah, more than 11,000 coyotes were killed for bounties totaling over $500,000 in the fiscal year ending June 30, 2017.

Livestock guardian dogs are commonly used to aggressively repel predators and have worked well in both fenced pasture and range operations. A 1986 survey of sheep producers in the USA found that 82% reported the use of dogs represented an economic asset.

Re-wilding cattle, which involves increasing the natural protective tendencies of cattle, is a method for controlling coyotes discussed by Temple Grandin of Colorado State University. This method is gaining popularity among producers who allow their herds to calve on the range and whose cattle graze open pastures throughout the year.

Coyotes typically bite the throat just behind the jaw and below the ear when attacking adult sheep or goats, with death commonly resulting from suffocation. Blood loss is usually a secondary cause of death. Calves and heavily fleeced sheep are killed by attacking the flanks or hindquarters, causing shock and blood loss. When attacking smaller prey, such as young lambs, the kill is made by biting the skull and spinal regions, causing massive tissue and bone damage. Small or young prey may be completely carried off, leaving only blood as evidence of a kill. Coyotes usually leave the hide and most of the skeleton of larger animals relatively intact, unless food is scarce, in which case they may leave only the largest bones. Scattered bits of wool, skin, and other parts are characteristic where coyotes feed extensively on larger carcasses.

Tracks are an important factor in distinguishing coyote from dog predation. Coyote tracks tend to be more oval-shaped and compact than those of domestic dogs, and their claw marks are less prominent and the tracks tend to follow a straight line more closely than those of dogs. With the exception of sighthounds, most dogs of similar weight to coyotes have a slightly shorter stride. Coyote kills can be distinguished from wolf kills by less damage to the underlying tissues in the former. Also, coyote scat tends to be smaller than wolf scat.

Coyotes are often attracted to dog food and animals that are small enough to appear as prey. Items such as garbage, pet food, and sometimes feeding stations for birds and squirrels attract coyotes into backyards. About three to five pets attacked by coyotes are brought into the Animal Urgent Care hospital of South Orange County (California) each week, the majority of which are dogs, since cats typically do not survive the attacks. Scat analysis collected near Claremont, California, revealed that coyotes relied heavily on pets as a food source in winter and spring. At one location in Southern California, coyotes began relying on a colony of feral cats as a food source. Over time, the coyotes killed most of the cats, and then continued to eat the cat food placed daily at the colony site by people who were maintaining the cat colony.
Coyotes usually attack smaller-sized dogs, but they have been known to attack even large, powerful breeds such as the Rottweiler in exceptional cases. Dogs larger than coyotes, such as greyhounds, are generally able to drive them off, and have been known to kill coyotes. Smaller breeds are more likely to suffer injury or death.

Prior to the mid-19th century, coyote fur was considered worthless. This changed with the diminution of beavers, and by 1860, the hunting of coyotes for their fur became a great source of income (75 cents to $1.50 per skin) for wolfers in the Great Plains. Coyote pelts were of significant economic importance during the early 1950s, ranging in price from $5 to $25 per pelt, depending on locality. The coyote's fur is not durable enough to make rugs, but can be used for coats and jackets, scarves, or muffs. The majority of pelts are used for making trimmings, such as coat collars and sleeves for women's clothing. Coyote fur is sometimes dyed black as imitation silver fox.

Coyotes were occasionally eaten by trappers and mountain men during the western expansion. Coyotes sometimes featured in the feasts of the Plains Indians, and coyote pups were eaten by the indigenous people of San Gabriel, California. The taste of coyote meat has been likened to that of the wolf, and is more tender than pork when boiled. Coyote fat, when taken in the fall, has been used on occasion to grease leather or eaten as a spread.

Coyotes were probably semidomesticated by various pre-Columbian cultures. Some 19th-century writers wrote of coyotes being kept in native villages in the Great Plains. The coyote is easily tamed as a pup, but can become destructive as an adult. Both full-blooded and hybrid coyotes can be playful and confiding with their owners, but are suspicious and shy of strangers, though coyotes being tractable enough to be used for practical purposes like retrieving and pointing have been recorded. A tame coyote named "Butch", caught in the summer of 1945, had a short-lived career in cinema, appearing in "Smoky" and "Ramrod" before being shot while raiding a henhouse.







</doc>
<doc id="6711" url="https://en.wikipedia.org/wiki?curid=6711" title="Compressor (disambiguation)">
Compressor (disambiguation)

A gas compressor is a device that pressurizes fluids.

Compressor may also refer to:



</doc>
<doc id="6713" url="https://en.wikipedia.org/wiki?curid=6713" title="Conan the Barbarian">
Conan the Barbarian

Conan the Barbarian (also known as Conan the Cimmerian) is a fictional sword and sorcery hero who originated in pulp fiction magazines and has since been adapted to books, comics, several films (including "Conan the Barbarian" and "Conan the Destroyer"), television programs (animated and live-action), video games, role-playing games, and other media. The character was created by writer Robert E. Howard in 1932 in a series of fantasy stories published in "Weird Tales" magazine.

Conan the Barbarian was created by Robert E. Howard in a series of fantasy stories published in "Weird Tales" in 1932. For months, Howard had been in search of a new character to market to the burgeoning pulp outlets of the early 1930s. In October 1931, he submitted the short story "People of the Dark" to Clayton Publications' new magazine, "Strange Tales of Mystery and Terror" (June 1932). "People of the Dark" is a remembrance story of "past lives", and in its first-person narrative the protagonist describes one of his previous incarnations; Conan is a black-haired barbarian hero who swears by a deity called Crom. Some Howard scholars believe this Conan to be a forerunner of the more famous character.

In February 1932, Howard vacationed at a border town on the lower Rio Grande. During this trip, he further conceived the character of Conan and also wrote the poem "Cimmeria", much of which echoes specific passages in Plutarch's "Lives". According to some scholars, Howard's conception of Conan and the Hyborian Age may have originated in Thomas Bulfinch's "The Outline of Mythology" (1913) which inspired Howard to "coalesce into a coherent whole his literary aspirations and the strong physical, autobiographical elements underlying the creation of Conan".

Having digested these prior influences after he returned from his trip, Howard rewrote a rejected story, "By This Axe I Rule!" (May 1929), replacing his existing character Kull of Atlantis with his new hero, and retitling it "The Phoenix on the Sword". Howard also wrote "The Frost-Giant's Daughter", inspired by the Greek myth of Daphne, and submitted both stories to "Weird Tales" magazine. Although "The Frost-Giant's Daughter" was rejected, the magazine accepted "The Phoenix on the Sword" after it received the requested polishing.

"The Phoenix on the Sword" appeared in "Weird Tales" cover-dated December 1932. Editor Farnsworth Wright subsequently prompted Howard to write an 8,000-word essay for personal use detailing "the Hyborian Age", the fictional setting for Conan. Using this essay as his guideline, Howard began plotting "The Tower of the Elephant", a new Conan story that was the first to truly integrate his new conception of the Hyborian world.

The publication and success of "The Tower of the Elephant" spurred Howard to write many more Conan stories for "Weird Tales". By the time of Howard's suicide in 1936, he had written 21 complete stories, 17 of which had been published, as well as a number of unfinished fragments.

Following Howard's death, the copyright of the Conan stories passed through several hands. Eventually, under the guidance of L. Sprague de Camp and Lin Carter, the stories were edited, revised, and sometimes rewritten. For roughly 40 years, the original versions of Howard's Conan stories remained out of print. In 1977, the publisher Berkley Books issued three volumes using the earliest published form of the texts from "Weird Tales", but these failed to displace the edited versions. In the 1980s and 1990s, the copyright holders of the Conan franchise permitted Howard's stories to go out of print entirely, while continuing to sell Conan works by other authors.

In 2000, the British publisher Gollancz Science Fiction issued a two-volume, complete edition of Howard's Conan stories as part of its Fantasy Masterworks imprint, which included several stories that had never seen print in their original form. The Gollancz edition mostly used the versions of the stories as published in "Weird Tales".

The two volumes were combined and the stories resorted into chronological order as "The Complete Chronicles of Conan: Centenary Edition" (Gollancz Science Fiction, 2006; edited and with an Afterword by Steve Jones).

In 2003, another British publisher, Wandering Star Books, made an effort both to restore Howard's original manuscripts and to provide a more scholarly and historical view of the Conan stories. It published hardcover editions in England, which were republished in the United States by the Del Rey imprint of Ballantine Books. The first book, "Conan of Cimmeria: Volume One (1932–1933)" (2003; published in the US as "The Coming of Conan the Cimmerian") includes Howard's notes on his fictional setting, as well as letters and poems concerning the genesis of his ideas. This was followed by "Conan of Cimmeria: Volume Two (1934)" (2004; published in the US as "The Bloody Crown of Conan") and "Conan of Cimmeria: Volume Three (1935–1936)" (2005; published in the US as "The Conquering Sword of Conan"). These three volumes combined include all of the original, unedited Conan stories.

The various stories of Conan the Barbarian occur in the pseudo-historical "Hyborian Age", set after the destruction of Atlantis and before the rise of any known ancient civilization. This is a specific epoch in a fictional timeline created by Howard for many of the low fantasy tales of his artificial legendary.

The reasons behind the invention of the Hyborian Age were perhaps commercial: Howard had an intense love for history and historical dramas; however, at the same time, he recognized the difficulties and the time-consuming research work needed in maintaining historical accuracy—and moreover, the poorly-stocked libraries in the rural part of Texas where Howard lived just didn't have the material needed for such historical research. By conceiving a timeless setting—"a "vanished" age"—and by carefully choosing names that resembled human history, Howard shrewdly avoided the problem of historical anachronisms and his need for lengthy exposition.

According to "The Phoenix on the Sword", the adventures of Conan take place "Between the years when the oceans drank Atlantis and the gleaming cities, and the years of the rise of the Sons of Aryas."

Conan is a Cimmerian. From the writings of Robert E. Howard (The Hyborian Age among others) it is known that his Cimmerians are based on the Celts or perhaps the historic Cimmerians, based on the described geography and the existence of said people. He was born on a battlefield and is the son of a village blacksmith. Conan matured quickly as a youth and, by age fifteen, he was already a respected warrior who had participated in the destruction of the Aquilonian fortress of Venarium. After its demise, he was struck by wanderlust and began the adventures chronicled by Howard, encountering skulking monsters, evil wizards, tavern wenches, and beautiful princesses. He roamed throughout the Hyborian Age nations as a thief, outlaw, mercenary, and pirate. As he grew older, he began commanding vast units of warriors and escalating his ambitions. In his forties, he seized the crown from the tyrannical king of Aquilonia, the most powerful kingdom of the Hyborian Age, having strangled the previous ruler on the steps of his own throne. Conan's adventures often result in him performing heroic feats, though his motivation for doing so is largely to protect his own survival or for personal gain.

A conspicuous element of Conan's character is his chivalry. He is extremely reluctant to fight women (even when they fight him) and has a strong tendency to save damsels in distress. In "Jewels of Gwahlur", he has to make a split-second decision whether to save the dancing girl Muriela or the chest of priceless gems which he spent months in search of. So, without hesitation, he rescues Muriela and allows for the treasure to be irrevocably lost. In "The Black Stranger", Conan saves the exile Zingaran Lady Belesa at considerable risk to himself, giving her as a parting gift his fortune in gems big enough to have a comfortable and wealthy life in Zingara, while asking for no sexual favors in return. Reviewer Jennifer Bard also noted that when Conan is in a pirate crew or a robber gang led by another male, his tendency is to subvert and undermine the leader's authority, and eventually supplant (and often, kill) him (e.g. "Pool of the Black One", "A Witch Shall be Born", "Shadows in the Moonlight"). Conversely, in "Queen of the Black Coast", it is noted that Conan "generally agreed to Belit's plan. Hers was the mind that directed their raids, his the arm that carried out her ideas. It was a good life." And at the end of "Red Nails", Conan and Valeria seem to be headed towards a reasonably amicable piratical partnership.

Conan has "sullen", "smoldering", and "volcanic" blue eyes with a black "square-cut mane". Howard once describes him as having a hairy chest and, while comic book interpretations often portray Conan as wearing a loincloth or other minimalist clothing to give him a more barbaric image, Howard describes the character as wearing whatever garb is typical for the kingdom and culture in which Conan finds himself. Howard never gave a strict height or weight for Conan in a story, only describing him in loose terms like "giant" and "massive". In the tales, no human is ever described as being stronger than Conan, although several are mentioned as taller (such as the strangler Baal-pteor) or of larger bulk. In a letter to P. Schuyler Miller and John D. Clark in 1936, only three months before Howard's death, Conan is described as standing 6ft/183cm and weighing when he takes part in an attack on Venarium at only 14 years old, though being far from fully grown. Conan himself says in "Beyond the Black River" that he had "...not yet seen 15 snows".
Although Conan is muscular, Howard frequently compares his agility and way of moving to that of a panther (see, for instance, "Jewels of Gwahlur", "Beyond the Black River", or "Rogues in the House"). His skin is frequently characterized as bronzed from constant exposure to the sun. In his younger years, he is often depicted wearing a light chain shirt and a horned helmet, though appearances vary with different stories.

During his reign as king of Aquilonia, Conan was

Howard imagined the Cimmerians as a pre-Celtic people with mostly black hair and blue or grey eyes. Ethnically, the Cimmerians to which Conan belongs are descendants of the Atlanteans, though they don't remember their ancestry. In his fictional historical essay "The Hyborian Age", Howard describes how the people of Atlantis—the land where his character King Kull originated—had to move east after a great cataclysm changed the face of the world and sank their island, settling where Ireland and Scotland would eventually be located. Thus they are (in Howard's work) the ancestors of the Irish and Scottish (the Celtic Gaels) and not the Picts, the other ancestor of modern Scots who also appear in Howard's work. In the same work, Howard also described how the Cimmerians eventually moved south and east after the age of Conan (presumably in the vicinity of the Black Sea, where the historical Cimmerians dwelt).

Despite his brutish appearance, Conan uses his brains as well as his brawn. The Cimmerian is a highly skilled warrior, possibly without peer with a sword, but his travels have given him vast experience in other trades, especially as a thief. He's also a talented commander, tactician, and strategist, as well as a born leader. In addition, Conan has advanced knowledge of languages and codes and is able to recognize, or even decipher, certain ancient or secret signs and writings. For example, in "Jewels of Gwahlur" Howard states: "In his roaming about the world the giant adventurer had picked up a wide smattering of knowledge, particularly including the speaking and reading of many alien tongues. Many a sheltered scholar would have been astonished at the Cimmerian's linguistic abilities." He also has incredible stamina, enabling him to go without sleep for a few days. In "A Witch Shall be Born", Conan fights armed men until he is overwhelmed, captured, and crucified, before going an entire night and day without water. However, Conan still possesses the strength to pull the nails from his feet, while hoisting himself into a horse's saddle and riding for ten miles.

Another noticeable trait is his sense of humor, largely absent in the comics and movies, but very much a part of Howard's original vision of the character (particularly apparent in "Xuthal of the Dusk", also known as "The Slithering Shadow.") His sense of humor can also be rather grimly ironic, as was demonstrated by how he unleashes his own version of justice on the treacherous—and ill-fated—innkeeper Aram Baksh in "Shadows in Zamboula".

He is a loyal friend to those true to him, with a barbaric code of conduct that often marks him as more honorable than the more sophisticated people he meets in his travels. Indeed, his straightforward nature and barbarism are constants in all the tales.

Conan is a formidable combatant both armed and unarmed. With his back to the wall, Conan is capable of engaging and killing opponents by the score. This is seen in several stories, such as "Queen of the Black Coast", "The Scarlet Citadel", and "A Witch Shall Be Born". Conan is not superhuman, though; he needed the providential help of Zelata's wolf to defeat four Nemedian soldiers in Howard's novel "The Hour of the Dragon". Some of his hardest victories have come from fighting single opponents of inhuman strength: one such as Thak, an ape-like humanoid from "Rogues in the House", or the strangler Baal-Pteor in "Shadows in Zamboula". Conan is far from untouchable and has been captured or defeated several times (on one occasion, knocking himself out drunkenly after running into a wall).

Howard frequently corresponded with H. P. Lovecraft, and the two would sometimes insert references or elements of each other's settings in their works. Later editors reworked many of the original Conan stories by Howard, thus diluting this connection. Nevertheless, many of Howard's unedited Conan stories are arguably part of the Cthulhu Mythos. Additionally, many of the Conan stories by Howard, de Camp, and Carter used geographical place names from Clark Ashton Smith's Hyperborean Cycle.





A number of untitled synopses for Conan stories also exist.


The character of Conan has proven durably popular, resulting in Conan stories by later writers such as Poul Anderson, Leonard Carpenter, Lin Carter, L. Sprague de Camp, Roland J. Green, John C. Hocking, Robert Jordan, Sean A. Moore, Björn Nyberg, Andrew J. Offutt, Steve Perry, John Maddox Roberts, Harry Turtledove, and Karl Edward Wagner. Some of these writers have finished incomplete Conan manuscripts by Howard. Others were created by rewriting Howard stories which originally featured entirely different characters from entirely different milieus. Most, however, are completely original works. In total, more than fifty novels and dozens of short stories featuring the Conan character have been written by authors other than Howard.

The Gnome Press edition (1950–1957) was the first hardcover collection of Howard's Conan stories, including all the original Howard material known to exist at the time, some left unpublished in his lifetime. The later volumes contain some stories rewritten by L. Sprague de Camp (like "The Treasure of Tranicos"), including several non-Conan Howard stories, mostly historical exotica situated in the Levant at the time of the Crusades, which he turned into Conan yarns. The Gnome edition also issued the first Conan story written by an author other than Howard—the final volume published, which is by Björn Nyberg and revised by de Camp.

The Lancer/Ace editions (1966–1977), under the direction of de Camp and Lin Carter, were the first comprehensive paperbacks, compiling the material from the Gnome Press series together in chronological order with all the remaining original Howard material, including that left unpublished in his lifetime and fragments and outlines. These were completed by de Camp and Carter. The series also included Howard stories originally featuring other protagonists that were rewritten by de Camp as Conan stories. New Conan stories written entirely by de Camp and Carter were added as well. Lancer Books went out of business before bringing out the entire series, the publication of which was completed by Ace Books. Eight of the eventual twelve volumes published featured dynamic cover paintings by Frank Frazetta that, for many fans, presented the definitive, iconic impression of Conan and his world. For decades to come, most other portrayals of the Cimmerian and his imitators were heavily influenced by the cover paintings of this series.

Most editions after the Lancer/Ace series have been of either the original Howard stories or Conan material by others, but not both. The exception are the Ace Maroto editions (1978–1981), which include both new material by other authors and older material by Howard, though the latter are some of the non-Conan tales rewritten as Conan stories by de Camp. Notable later editions of the original Howard Conan stories include the Donald M. Grant editions (1974–1989, incomplete); Berkley editions (1977); Gollancz editions (2000–2006), and Wandering Star/Del Rey editions (2003–2005). Later series of new Conan material include the Bantam editions (1978–1982) and Tor editions (1982–2004).

In an attempt to provide a coherent timeline which fit the numerous adventures of Conan penned by Robert E. Howard and later writers, various "Conan chronologies" have been prepared by many people from the 1930s onward. Note that no consistent timeline has yet accommodated every single Conan story. The following are the principal theories that have been advanced over the years.

The very first Conan cinematic project was planned by Edward Summer. Summer envisioned a series of Conan films, much like the James Bond franchise. He outlined six stories for this film series, but none were ever made. An original screenplay by Summer and Roy Thomas was written, but their lore-authentic screen story was never filmed. However, the resulting film, "Conan the Barbarian" (1982), was a combination of director John Milius' ideas and plots from Conan stories (written also by Howard's successors, notably Lin Carter and L. Sprague de Camp). The addition of Nietzschean motto and Conan's life philosophy were crucial for bringing the spirit of Howard's literature to the screen.

The plot of "Conan the Barbarian" (1982) begins with Conan being enslaved by the Vanir raiders of Thulsa Doom, a malevolent warlord who is responsible for the slaying of Conan's parents and the genocide of his people. Later, Thulsa Doom becomes a cult leader of a religion that worships Set, a Snake God. The vengeful Conan, the archer Subotai and the thief Valeria set out on a quest to rescue a princess held captive by Thulsa Doom. The film was directed by John Milius and produced by Dino De Laurentiis. The character of Conan was played by Arnold Schwarzenegger and was his break-through role as an actor.

This film was followed by a less popular sequel, "Conan the Destroyer" in 1984. This sequel was a more typical fantasy-genre film and was even less faithful to Howard's Conan stories, being just a picaresque story of an assorted bunch of adventurers.

The third film in the "Conan" trilogy was planned for 1987 to be titled "Conan the Conqueror". The director was to be either Guy Hamilton or John Guillermin. Since Arnold Schwarzenegger was committed to the film "Predator" and De Laurentiis's contract with the star had expired after his obligation to "Red Sonja" and "Raw Deal", he wasn't keen to negotiate a new one; thus the third Conan film sank into development hell. The script was eventually turned into "Kull the Conqueror".

There were rumors in the late 1990s of another Conan sequel, a story about an older Conan titled "King Conan: Crown of Iron", but Schwarzenegger's election in 2003 as governor of California ended this project. Warner Bros. spent seven years trying to get the project off the ground. However, in June 2007 the rights reverted to Paradox Entertainment, though all drafts made under Warner remained with them. In August 2007, it was announced that Millennium Films had acquired the rights to the project. Production was aimed for a Spring 2006 start, with the intention of having stories more faithful to the Robert E. Howard creation. In June 2009, Millennium hired Marcus Nispel to direct. In January 2010, Jason Momoa was selected for the role of Conan. The film was released in August 2011, and met poor critical reviews and box office results.

In 2012, producers Chris Morgan and Frederick Malmberg announced plans for a sequel to the 1982 "Conan the Barbarian" titled "The Legend of Conan", with Arnold Schwarzenegger reprising his role as Conan. A year later, "Deadline" reported that Andrea Berloff would write the script. Years passed since the initial announcement as Schwarzenegger worked on other films, but as late as 2016, Schwarzenegger affirmed his enthusiasm for making the film, saying, "Interest is high ... but we are not rushing." The script was finished, and Schwarzenegger and Morgan were meeting with possible directors. In April 2017, producer Chris Morgan stated that Universal had dropped the project, although there was a possibility of a TV show. The story of the film was supposed to be set 30 years after the first, with some inspiration from Clint Eastwood's "Unforgiven".

There have been three television series related to Conan:

Conan the Barbarian has appeared in comics nearly non-stop since 1970. The comics are arguably, apart from the books, the vehicle that had the greatest influence on the longevity and popularity of the character. Aside from an earlier and unofficial Conan comic published in Mexico, the two main publishers of Conan comics have been Marvel Comics and Dark Horse Comics. Marvel Comics launched "Conan the Barbarian" (1970–1993) and the classic "Savage Sword of Conan" (1974–1995). Dark Horse launched their "Conan" series in 2003. Dark Horse Comics is currently publishing compilations of the 1970s Marvel Comics series in trade paperback format.

Barack Obama, former President of the United States, is a collector of Conan the Barbarian comic books and a big fan of the character and appeared as a character in a comic book called "Barack the Barbarian" from Devil's Due.

Marvel Comics introduced a relatively lore-faithful version of Conan the Barbarian in 1970 with "Conan the Barbarian", written by Roy Thomas and illustrated by Barry Windsor-Smith. Smith was succeeded by penciller John Buscema, while Thomas continued to write for many years. Later writers included J.M. DeMatteis, Bruce Jones, Michael Fleisher, Doug Moench, Jim Owsley, Alan Zelenetz, Chuck Dixon and Don Kraar. In 1974, "Conan the Barbarian" series spawned the more adult-oriented, black-and-white comics magazine "Savage Sword of Conan", written by Thomas with art mostly by Buscema or Alfredo Alcala. Marvel also published several graphic novels starring the character , and a handbook with detailed information about the Hyborian world.

The Marvel Conan stories were also adapted as a newspaper comic strip which appeared daily and Sunday from 4 September 1978 to 12 April 1981. Originally written by Roy Thomas and illustrated by John Buscema, the strip was continued by several different Marvel artists and writers.

Dark Horse Comics began their comic adaptation of the Conan saga in 2003. Entitled simply "Conan", the series was first written by Kurt Busiek and pencilled by Cary Nord. Tim Truman replaced Busiek when Busiek signed an exclusive contract with DC Comics; however, Busiek issues were sometimes used for filler. This series is an interpretation of the original Conan material by Robert E. Howard with no connection whatsoever to the earlier Marvel comics or any Conan story not written or envisioned by Howard supplemented by wholly original material.

A second series, "Conan the Cimmerian" was released in 2008 by Tim Truman (writer) and Tomás Giorello (artist). The series ran for twenty-six issues, including an introductory "zero" issue.

Dark Horse's third series, "", began in December 2010 by Roy Thomas (writer) and Mike Hawthorne (artist) and ran for twelve issues.

A fourth series, "Conan the Barbarian", began in February 2012 by Brian Wood (writer) and Becky Cloonan (artist). It ran for twenty-five issues, and expanded on Robert E. Howard's "Queen of the Black Coast".

A fifth series, "Conan the Avenger", began in April 2014 by Fred Van Lente (writer) and Brian Ching (artist). It ran for twenty-five issues, and expanded on Robert E. Howard's The Snout in the Dark and A Witch Shall Be Born.

Dark Horse's sixth series, "Conan the Slayer", began in July 2016 by Cullen Bunn (writer) and Sergio Dávila (artist).

In 2018, Marvel reacquired the rights and started new runs of both Conan the Barbarian and Savage Sword of Conan in January/February 2019.

Seven video games have been released based on the Conan mythos.



TSR, Inc. signed a license agreement in 1984 to publish Conan-related gaming material:

In 1988 Steve Jackson Games acquired a Conan license and started publishing Conan solo adventures for its "GURPS" generic system of rules as of 1988 and a "GURPS Conan" core rulebook in 1989:

In 2003 the British company Mongoose Publishing bought a license and acquired in turn the rights to make use of the Conan gaming franchise, publishing a Conan role-playing game from 2004 until 2010. The game ran the OGL System of rules that Mongoose established for its "OGL series" of games:

In 2010 Mongoose Publishing dropped the Conan license. In February 2015, another British company, Modiphius Entertainment, acquired the license, announcing plans to put out a new Conan role-playing game in August of that year. Actually, the core rulebook was not launched (via Kickstarter) until a whole year later, in February 2016, reaching by far all funds needed for publication. Long after the Kickstarter ended the core rulebook was launched in PDF format on January the 31st, 2017. The physical core rulebook finally started distribution in June 2017 :









</doc>
<doc id="6715" url="https://en.wikipedia.org/wiki?curid=6715" title="Chris Marker">
Chris Marker

Chris Marker (; 29 July 1921 – 29 July 2012) was a French writer, photographer, documentary film director, multimedia artist and film essayist. His best known films are "La Jetée" (1962), "Le Joli Mai" (1963), "A Grin Without a Cat" (1977) and "Sans Soleil" (1983). Marker is often associated with the Left Bank Cinema movement that occurred in the late 1950s and included such other filmmakers as Alain Resnais, Agnès Varda, Henri Colpi and Armand Gatti.

His friend and sometime collaborator Alain Resnais called him "the prototype of the twenty-first-century man." Film theorist Roy Armes has said of him: "Marker is unclassifiable because he is unique...The French Cinema has its dramatists and its poets, its technicians, and its autobiographers, but only has one true essayist: Chris Marker."

Marker was born Christian François Bouche-Villeneuve. He was always elusive about his past and known to refuse interviews and not allow photographs to be taken of him; his place of birth is highly disputed. Some sources and Marker himself claim that he was born in Ulaanbaatar, Mongolia. Other sources say he was born in Belleville, Paris, and others, in Neuilly-sur-Seine. The 1949 edition of "Le Cœur Net" specifies his birthday as 22 July. Film critic David Thomson has stated: "Marker told me himself that Mongolia is correct. I have since concluded that Belleville is correct – but that does not spoil the spiritual truth of Ulan Bator." When asked about his secretive nature, Marker has said "My films are enough for them [the audience]."

Marker was a philosophy student in France prior to World War II. During the German occupation of France, he joined the Maquis (FTP), a part of the French Resistance. At some point during the war he left France and joined the United States Air Force as a paratrooper, although some sources claim that this is not true. After the war, he began a career as a journalist, first writing for the journal "Esprit", a neo-Catholic, Marxist magazine where he met fellow journalist André Bazin. At "Esprit", Marker wrote political commentaries, poems, short stories, and (with Bazin) film reviews. He would later become an early contributor to Bazin's "Cahiers du cinéma".

During this time period, Marker began to travel around the world as a journalist and photographer, a vocation he would continue the rest of his life. He was hired by the French publishing company Éditions du Seuil as editor of the series "Petite Planète" ("Small World"). This collection devoted one edition to each country and included information and photographs. In 1949 Marker published his first novel, "Le Coeur net" ("The Forthright Spirit"), which was about aviation. In 1952 Marker published an illustrated essay on French writer Jean Giraudoux, "Giraudoux Par Lui-Même".

During his early journalism career, Marker became increasingly interested in filmmaking and experimented with photography in the early 1950s. Around this time Marker met and befriended many members of what would be called the Left Bank Film Movement, including Alain Resnais, Agnès Varda, Henri Colpi, Armand Gatti and the novelists Marguerite Duras and Jean Cayrol. This group is often associated with the French New Wave directors who came to prominence during the same time period, and indeed both groups were often friends and journalistic co-workers. The term "Left Bank" was first coined by film critic Richard Roud, who has described them as having "fondness for a kind of Bohemian life and an impatience with the conformity of the Right Bank, a high degree of involvement in literature and the plastic arts, and a consequent interest in experimental filmmaking", as well as an identification with the political left. Many of Marker's earliest films were produced by Anatole Dauman.

In 1952 Marker made his first film, "Olympia 52", a 16mm feature documentary about the 1952 Helsinki Olympic Games. In 1953 Marker collaborated with Resnais on the documentary "Statues Also Die". The film examines traditional African art such as sculptures and masks, and its decline with coming of Western colonialism. The film won the 1954 Prix Jean Vigo, but was banned by French censors for its criticism of French colonialism.

After working as assistant director on Resnais's "Night and Fog" in 1955, Marker made "Sunday in Peking", a short documentary "film essay" that would characterize Marker's unique film style for most of his career. The film was shot in two weeks by Marker while he was traveling through China with Armand Gatti in September 1955. In the film, Marker's commentary overlaps scenes from China, such as tombs which, contrary to Westernized understandings of Chinese legends, do not contain the remains of any Ming Dynasty emperors.

After working on the commentary for Resnais' film "Le mystère de l'atelier quinze" in 1957, Marker continued to form his own cinematic style with the feature documentary "Letter from Siberia". An essay film on the narrativization of Siberia, it contains Marker's signature commentary, which takes the form of a letter from the director, in the long tradition of epistolary treatments by French explorers of the "undeveloped" world. "Letter " looks at the modernization of Siberia with its movement into the twentieth century, but with a look back at some of the tribal cultural practices now receding into the past. It combines footage that Marker shot in Siberia with old newsreel footage, cartoon sequences, stills, and even an illustration of Alfred E. Neuman from "Mad Magazine" as well as a fake TV commercial as part of a humorous attack on Western mass culture. In producing a meta-commentary on narrativity and film, Marker uses the same brief filmic sequence three times but with different commentary—the first one praising the Soviet Union, the second denouncing it, and the third taking an apparently neutral or "objective" stance.

In 1959 Marker made the animated film "Les Astronautes" with Walerian Borowczyk. The film was a combination of traditional drawings with still photography. In 1960 Marker made "Description d'un combat", a documentary on the State of Israel which reflects on the country's past and future. The film won the Golden Bear for Best Documentary at the 1961 Berlin Film Festival.

In January 1961, Marker traveled to Cuba and shot the film "¡Cuba Sí!" The film promotes and defends Fidel Castro and includes two interviews with the Comandante. The film ends with an anti-American epilogue in which the United States is embarrassed by the Bay of Pigs Invasion fiasco, and was subsequently banned. The banned essay was included in Marker's first volume of collected film commentaries, "Commentaires I", published in 1961. The following year Marker published "Coréennes", a collection of photographs and essays on the conditions of Korea.

Marker became known internationally for the short film "La Jetée" ("The Pier") in 1962. It tells of a post-nuclear war experiment in time travel by using a series of filmed photographs developed as a photomontage of varying pace, with limited narration and sound effects. In the film, a survivor of a futuristic third World War is obsessed with distant and disconnected memories of a pier at the Orly Airport, the image of a mysterious woman, and a man's death. Scientists experimenting in time travel choose him for their studies, and the man travels back in time to contact the mysterious woman, and discovers that the man's death at the Orly Airport was his own. Except for one shot of the woman mentioned above sleeping and suddenly waking up, the film is composed entirely of photographs by Jean Chiabaud and stars Davos Hanich as the man, Hélène Chatelain as the woman and filmmaker William Klein as a man from the future.

"La Jetée" was the inspiration for Mamoru Oshii's 1987 debut live action feature "The Red Spectacles" (and later for parts of Oshii's 2001 film "Avalon" as well) and also inspired Terry Gilliam's "12 Monkeys" (1995) and Jonás Cuarón's "Year of the Nail" (2007). It also inspired many of director Mira Nair's shots for the 2006 film "The Namesake".

While making "La Jetee", Marker was simultaneously making the 150-minute documentary essay-film "Le joli mai", released in 1963. Beginning in the Spring of 1962, Marker and his camera operator Pierre Lhomme shot 55 hours of footage interviewing random people on the streets of Paris. The questions, asked by the unseen Marker, range from their personal lives, as well as social and political issues of relevance at that time. As he had with montages of landscapes and indigenous art, Marker created a film essay that contrasted and juxtaposeed a variety of lives with his signature commentary (spoken by Marker's friends, singer-actor Yves Montand in the French version and Simone Signoret in the English version). The film has been compared to the "Cinéma vérité" films of Jean Rouch, and criticized by its practitioners at the time. The term "Cinéma vérité" was itself anathema to Marker; he would never use it. It was shown in competition at the 1963 Venice Film Festival, where it won the award for Best First Work. It also won the Golden Dove Award at the Leipzig DOK Festival.

After the documentary "Le Mystère Koumiko" in 1965, Marker made "Si j'avais quatre dromadaires", an essay-film that, like "La jetée", is a photomontage of over 800 photographs that Marker had taken over the past 10 years from 26 countries. The commentary takes on a slightly different dimension from his previous commentaries by using a conversation between a fictitious photographer and two friends, who discuss the photos. The film's title is an allusion to a poem by Guillaume Apollinaire. It was the last film in which Marker included "travel footage" for many years.

In 1967 Marker published his second volume of collected film essays, "Commentaires II". That same year, Marker organized the omnibus film "Loin du Vietnam", a protest against the Vietnam War with segments contributed by Marker, Jean-Luc Godard, Alain Resnais, Agnès Varda, Claude Lelouch, William Klein, Michele Ray and Joris Ivens. The film includes footage of the war, from both sides, as well as anti-war protests in New York and Paris and other anti-war activities.

From this initial collection of filmmakers with left-wing political agendas, Marker created the group S.L.O.N. ("Société pour le lancement des oeuvres nouvelles","Society for launching new works", but also the Russian word for "elephant"). SLON was a film collective whose objectives were to make films and to encourage industrial workers to create film collectives of their own. Its members included Valerie Mayoux, Jean-Claude Lerner, Alain Adair and John Tooker. Marker is usually credited as director or co-director of all of the films made by SLON.

After the events of May 1968, Marker felt a moral obligation to abandon his own personal film career and devote himself to SLON and its activities. SLON's first film was about a strike at a Rhodiacéta factory in France, "À bientôt, j'espère" ("Rhodiacéta") in 1968. Later that year SLON made "La Sixième face du pentagone", about an anti-war protest in Washington, D.C. and was a reaction to what SLON considered to be the unfair and censored reportage of such events on mainstream television. The film was shot by François Reichenbach, who received co-director credit. "La Bataille des dix millions" was made in 1970 with Mayoux as co-director and Santiago Álvarez as cameraman and is about the 1970 sugar crop in Cuba and its disastrous effects on the country. In 1971, SLON made "Le Train en marche", a new prologue to Soviet filmmaker Aleksandr Medvedkin's 1935 film "Schastye", which had recently been re-released in France.

In 1974, SLON became I.S.K.R.A. ("Images, Sons, Kinescope, Réalisations, Audiovisuelles", but also the name of Vladimir Lenin's political newspaper "Iskra," which also is a Russian word for "spark").

In 1974 returned to his personal work and made a film outside of ISKRA. "La Solitude du chanteur de fond" is a one-hour documentary about Marker's friend Yves Montand's benefit concert for Chilean refugees. The concert was Montand's first public performance in four years, and the documentary includes film clips from his long career as a singer and actor.

Marker had been working on a film about Chile with ISKRA since 1973. Marker had collaborated with Belgian sociologist Armand Mattelart and ISKRA members Valérie Mayoux and Jacqueline Meppiel to shoot and collect the visual materials, which Marker then edited together and provided the commentary for. The resulting film was the two and a half-hour documentary "La Spirale", released in 1975. The film chronicles events in Chile, beginning with the 1970 election of socialist President Salvador Allende until his murder and the resulting coup in 1973.

Marker then began work on one of his most ambitious films, "A Grin Without a Cat", released in 1977. The film's title refers to the Cheshire Cat from "Alice in Wonderland". The metaphor compares the promise of the global socialist movement before May 1968 (the grin) with its actual presence in the world after May 1968 (the cat). The film's original French title is "Le fond de l'air est rouge", which means "the air is essentially red", or "revolution is in the air", implying that the socialist movement was everywhere around the world.

The film was intended to be an all-encompassing portrait of political movements since May 1968, a summation of the work which he had taken part in for ten years. The film is divided into two parts: the first half focuses on the hopes and idealism before May 1968, and the second half on the disillusion and disappointments since those events. Marker begins the film with the Odessa Steps sequence from Sergei Eisenstein's film "The Battleship Potemkin", which Marker points out is a fictitious creation of Eisenstein which has still influenced the image of the historical event. Marker used very little commentary in this film, but the film's montage structure and preoccupation with memory make it a Marker film. Upon release, the film was criticized for not addressing many current issues of the New Left such as the woman's movement, sexual liberation and worker self-management. The film was re-released in the US in 2002.

In the late 1970s, Marker traveled extensively throughout the world, including an extended period in Japan. From this inspiration, he first published the photo-essay "Le Dépays" in 1982, and then used the experience for his next film "Sans Soleil", released in 1982.

"Sans Soleil" stretches the limits of what could be called a documentary. It is an essay, a montage, mixing pieces of documentary with fiction and philosophical comments, creating an atmosphere of dream and science fiction. The main themes are Japan, Africa, memory and travel. A sequence in the middle of the film takes place in San Francisco, and heavily references Alfred Hitchcock's "Vertigo". Marker has said that "Vertigo" is the only film "capable of portraying impossible memory, insane memory." The film's commentary are credited to the fictitious cameraman Sandor Krasna, and read in the form of letters by an unnamed woman. Though centered around Japan, the film was also shot in such other countries as Guinea Bissau, Ireland and Iceland. "Sans Soleil" was shown at the 1983 Berlin Film Festival where it won the OCIC Award. It was also awarded the Sutherland Trophy at the 1983 British Film Institute Awards.

In 1984, Marker was invited by producer Serge Silberman to document the making of Akira Kurosawa's film "Ran". From this Marker made "A.K.", released in 1985. The film focuses more on Kurosawa's remote but polite personality than on the making of the film. The film was screened in the Un Certain Regard section at the 1985 Cannes Film Festival, before "Ran" itself had been released.

In 1985, Marker's long-time friend and neighbor Simone Signoret died of cancer. Marker then made the one-hour TV documentary "Mémoires pour Simone" as a tribute to her in 1986.

Beginning with "Sans Soleil", he developed a deep interest in digital technology. From 1985 to 1988, he worked on a conversational program (a prototypical chatbot) called "Dialector," which he wrote in Applesoft BASIC on an Apple II. He incorporated audiovisual elements in addition to the snippets of dialogue and poetry that "Computer" exchanged with the user. Version 6 of this program was revived from a floppy disk (with Marker's help and permission) and emulated online in 2015.

His interests in digital technology also led to his film "Level Five" (1996) and "Immemory" (1998, 2008), an interactive multimedia CD-ROM, produced for the Centre Pompidou (French language version) and from Exact Change (English version). Marker created a 19-minute multimedia piece in 2005 for the Museum of Modern Art in New York City titled "Owls at Noon Prelude: The Hollow Men" which was influenced by T. S. Eliot's poem.

Marker lived in Paris, and very rarely granted interviews. One exception was a lengthy interview with "Libération" in 2003 in which he explained his approach to filmmaking. When asked for a picture of himself, he usually offered a photograph of a cat instead. (Marker was represented in Agnes Varda's 2008 documentary "The Beaches of Agnes" by a cartoon drawing of a cat, speaking in a technologically altered voice.) Marker's own cat was named "Guillaume-en-égypte". In 2009, Marker commissioned an to represent him in machinima works. The avatar was created by Exosius Woolley and first appeared in the short film / machinima, "Ouvroir the Movie by Chris Marker".

In the 2007 Criterion Collection release of "La Jetée" and "Sans Soleil", Marker included a short essay, "Working on a Shoestring Budget". He confessed to shooting all of "Sans Soleil" with a silent film camera, and recording all the audio on a primitive audio cassette recorder. Marker also reminds the reader that only one short scene in "La Jetée" is of a moving image, as Marker could only borrow a movie camera for one afternoon while working on the film.

From 2007 through 2011 Chris Marker collaborated with the art dealer and publisher Peter Blum on a variety of projects which were exhibited at the Peter Blum galleries in the SoHo and Chelsea neighborhoods. Marker's works have also been exhibited at the current Peter Blum Gallery location in the Midtown neighborhood on 57th Street in 2014. These projects include several series of printed photographs titled "PASSENGERS", "Koreans", "Crush Art", "Quelle heure est-elle?", and "Staring Back"; a set of photogravures titled "After Dürer"; a book titled "PASSENGERS"; and digital prints of movie posters, whose titles were often appropriated, titled "Breathless", "Hiroshima Mon Amour", "Owl People", and "Rin Tin Tin". The video installations titled "Silent Movie" and "Owls at Noon Prelude: The Hollow Men" were exhibited through Peter Blum in 2009. These mentioned works have also been exhibited at the 2014 & 2015 Venice Biennale, Whitechapel Gallery in London, the MIT List Visual Arts Center in Cambridge, Massachusetts, the Carpenter Center for the Visual Arts at Harvard University in Cambridge, Massachusetts, the Moscow Photobiennale, Les Recontres d'Arles de la Photographie in Arles, France, the Centre de la Photographie in Geneva, Switzerland, the Walker Art Center in Minneapolis, Minnesota, the Wexner Center for the Arts in Columbus, Ohio, The Museum of Modern Art in New York, and the Pacific Film Archive in Berkeley, California. Since 2014 the artworks of the Estate of Chris Marker are represented by Peter Blum Gallery, New York.

Marker died on 29 July 2012, 91 years to the day after his birth.










</doc>
<doc id="6716" url="https://en.wikipedia.org/wiki?curid=6716" title="Cardinal vowels">
Cardinal vowels

Cardinal vowels are a set of reference vowels used by phoneticians in describing the sounds of languages. For instance, the vowel of the English word "feet" can be described with reference to cardinal vowel 1, , which is the cardinal vowel closest to it. It is often stated that to be able to use the cardinal vowel system effectively one must undergo training with an expert phonetician, working both on the recognition and the production of the vowels. Daniel Jones wrote "The values of cardinal vowels cannot be learnt from written descriptions; they should be learnt by oral instruction from a teacher who knows them".

A cardinal vowel is a vowel sound produced when the tongue is in an extreme position, either front or back, high or low. The current system was systematised by Daniel Jones in the early 20th century, though the idea goes back to earlier phoneticians, notably Ellis and Bell.

Cardinal vowels are not vowels of any particular language, but a measuring system. However, some languages contain vowel or vowels that are close to the cardinal vowel(s). An example of such language is Ngwe, which is spoken in Cameroon. It has been cited as a language with a vowel system that has 8 vowels which are rather similar to the 8 primary cardinal vowels (Ladefoged 1971:67).

Three of the cardinal vowels—, and —have articulatory definitions. The vowel is produced with the tongue as far forward and as high in the mouth as is possible (without producing friction), with spread lips. The vowel is produced with the tongue as far back and as high in the mouth as is possible, with protruded lips. This sound can be approximated by adopting the posture to whistle a very low note, or to blow out a candle. And is produced with the tongue as low and as far back in the mouth as possible.

The other vowels are 'auditorily equidistant' between these three 'corner vowels', at four degrees of aperture or 'height': close (high tongue position), close-mid, open-mid, and open (low tongue position).

These degrees of aperture plus the front-back distinction define 8 reference points on a mixture of articulatory and auditory criteria. These eight vowels are known as the eight 'primary cardinal vowels', and vowels like these are common in the world's languages.

The lip positions can be reversed with the lip position for the corresponding vowel on the opposite side of the front-back dimension, so that e.g. Cardinal 1 can be produced with rounding somewhat similar to that of Cardinal 8 (though normally compressed rather than protruded); these are known as 'secondary cardinal vowels'. Sounds such as these are claimed to be less common in the world's languages. Other vowel sounds are also recognised on the vowel chart of the International Phonetic Alphabet.

In the , the cardinal vowels have the same numbers used above, but added to 300.

The usual explanation of the cardinal vowel system implies that the competent user can reliably distinguish between sixteen Primary and Secondary vowels plus a small number of central vowels. The provision of diacritics by the International Phonetic Association further implies that intermediate values may also be reliably recognized, so that a phonetician might be able to produce and recognize not only a close-mid front unrounded vowel and an open-mid front unrounded vowel but also a mid front unrounded vowel , a centralized mid front unrounded vowel , and so on. This suggests a range of vowels nearer to forty or fifty than to twenty in number. Empirical evidence for this ability in trained phoneticians is hard to come by.

Ladefoged, in a series of pioneering experiments published in the 1950s and 60s, studied how trained phoneticians coped with the vowels of a dialect of Scottish Gaelic. He asked eighteen phoneticians to listen to a recording of ten words spoken by a native speaker of Gaelic and to place the vowels on a cardinal vowel quadrilateral. He then studied the degree of agreement or disagreement among the phoneticians. Ladefoged himself drew attention to the fact that the phoneticians who were trained in the British tradition established by Daniel Jones were closer to each other in their judgments than those who had not had this training. However, the most striking result is the great divergence of judgments among "all" the listeners regarding vowels that were distant from Cardinal values.





</doc>
<doc id="6719" url="https://en.wikipedia.org/wiki?curid=6719" title="Columbia, Missouri">
Columbia, Missouri

Columbia is a city in the U.S. state of Missouri. It is the county seat of Boone County and home to the University of Missouri. Founded in 1821, it is the principal city of the five-county Columbia metropolitan area. It is Missouri's fourth most-populous and fastest growing city, with an estimated 123,180 residents in 2018.

As a Midwestern college town, Columbia has a reputation for progressive politics, persuasive journalism, and public art. The tripartite establishment of Stephens College (1833), the University of Missouri (1839), and Columbia College (1851), which surround the city's Downtown to the east, south, and north, has made the city a center of learning. At its center is 8th Street, also known as the Avenue of the Columns, which connects Francis Quadrangle and Jesse Hall to the Boone County Courthouse and the City Hall. Originally an agricultural town, the cultivation of the mind is Columbia's chief economic concern today. Never a major center of manufacturing, the city also depends on healthcare, insurance, and technology businesses. Companies such as Shelter Insurance, Carfax, and Slackers CDs and Games, were founded in the city. Cultural institutions include the State Historical Society of Missouri, the Museum of Art and Archaeology, and the annual True/False Film Festival. The Missouri Tigers, the state's only major college athletic program, play football at Faurot Field and basketball at Mizzou Arena as members of the rigorous Southeastern Conference.

The city rests upon the forested hills and rolling prairies of Mid-Missouri, near the Missouri River valley, where the Ozark Mountains begin to transform into plains and savanna. Limestone forms bluffs and glades while rain dissolves the bedrock, creating caves and springs which water the Hinkson, Roche Perche, and Bonne Femme creeks. Surrounding the city, Rock Bridge Memorial State Park, Mark Twain National Forest, and Big Muddy National Fish and Wildlife Refuge form a greenbelt preserving sensitive and rare environments.

The first humans who entered the area at least twelve thousand years ago were nomadic hunters. Later, woodland tribes lived in villages along waterways and built mounds in high places. The Osage and Missouria nations were expelled by the exploration of French traders and the rapid settlement of American pioneers. The latter arrived by the Boone's Lick Road and hailed from the culture of the Upland South, especially Virginia, Kentucky, and Tennessee. From 1812 on the Boonslick area would play a pivotal role in Missouri's early history and the nation's westward expansion. German, Irish, and other European immigrants soon joined. The modern populace is unusually diverse, over eight percent foreign-born. While White and Black remain the largest ethnicities, people of Asian descent are now the third-largest group. The city has been called the "Athens of Missouri" for its classic beauty and educational emphasis, but is more commonly called "CoMo".

Columbia's origins begin with the settlement of American pioneers from Kentucky and Virginia in an early 1800s region known as the Boonslick. Before 1815 settlement in the region was confined to small log forts because of the threat of Native American attack during the War of 1812. When the war ended settlers came on foot, horseback, and wagon, often moving entire households along the Boone's Lick Road and sometimes bringing enslaved African Americans. By 1818 it was clear that the increased population would necessitate a new county be created from territorial Howard County. The Moniteau Creek on the west and Cedar Creek on the east were obvious natural boundaries.

Believing it was only a matter of time before a county seat was chosen, the Smithton Land Company was formed to purchase over to established the village of Smithton near the present-day intersection of Walnut and Garth. In 1819 Smithton was a small cluster of log cabins in an ancient forest of oak and hickory; chief among them was the cabin of Richard Gentry, a trustee of the Smithton Company who would become first mayor of Columbia. In 1820 Boone County was formed and named after the recently deceased explorer Daniel Boone. The Missouri Legislature appointed John Gray, Jefferson Fulcher, Absalom Hicks, Lawrence Bass, and David Jackson as commissioners to select and establish a permanent county seat. Smithton never had more than twenty people, and it was quickly realized that well digging was difficult because of the bedrock.

Springs were discovered across the Flat Branch Creek, so in the Spring of 1821 Columbia was laid off and the inhabitants of Smithton moved their cabins to the new town. The first house in Columbia was built by Thomas Duly in 1820 at what would become Fifth and Broadway. Columbia's permanence was ensured when it was chosen as county seat in 1821 and the Boone's Lick Road was rerouted down Broadway.

The roots of Columbia's three economic foundations—education, medicine, and insurance— can be traced to the city's incorporation in 1821. Original plans for the town set aside land for a state university. In 1833, Columbia Baptist Female College opened, which later became Stephens College. Columbia College, distinct from today's and later to become the University of Missouri, was founded in 1839. When the state legislature decided to establish a state university, Columbia raised three times as much money as any competing city, and James S. Rollins donated the land that is today the Francis Quadrangle. Soon other educational institutions were founded in Columbia, such as Christian Female College, the first college for women west of the Mississippi, which later became Columbia College.

The city benefited from being a stagecoach stop of the Santa Fe and Oregon trails, and later from the Missouri–Kansas–Texas Railroad. In 1822, William Jewell set up the first hospital. In 1830, the first newspaper began; in 1832, the first theater in the state was opened; and in 1835, the state's first agricultural fair was held. By 1839, the population of 13,000 and wealth of Boone County was exceeded in Missouri only by that of St. Louis County, which, at that time, included the City of St. Louis.

Columbia's infrastructure was relatively untouched by the Civil War. As a slave state, Missouri had many residents with Southern sympathies, but it stayed in the Union. The majority of the city was pro-Union; however, the surrounding agricultural areas of Boone County and the rest of central Missouri were decidedly pro-Confederate. Because of this, the University of Missouri became a base from which Union troops operated. No battles were fought within the city because the presence of Union troops dissuaded Confederate guerrillas from attacking, though several major battles occurred at nearby Boonville and Centralia.

After Reconstruction, race relations in Columbia followed the Southern pattern of increasing violence of whites against blacks in efforts to suppress voting and free movement: George Burke, a black man who worked at the university, was lynched in 1889. In the spring of 1923, James T. Scott, an African-American janitor at the University of Missouri, was arrested on allegations of raping a university professor's daughter. He was taken from the county jail and lynched on April 29 before a white mob of several hundred, hanged from the Old Stewart Road Bridge.
In the 21st century, a number of efforts have been undertaken to recognize Scott's death. In 2010 his death certificate was changed to reflect that he was never tried or convicted of charges, and that he had been lynched. In 2011 a headstone was put at his grave at Columbia Cemetery; it includes his wife's and parents' names and dates, to provide a fuller account of his life. In 2016, a marker was erected at the lynching site to memorialize Scott.

In 1963, University of Missouri System and the Columbia College system established their headquarters in Columbia. The insurance industry also became important to the local economy as several companies established headquarters in Columbia, including Shelter Insurance, Missouri Employers Mutual, and Columbia Insurance Group. State Farm Insurance has a regional office in Columbia. In addition, the now-defunct Silvey Insurance was a large local employer.

Columbia became a transportation crossroads when U.S. Route 63 and U.S. Route 40 (which was improved as present-day Interstate 70) were routed through the city. Soon after, the city opened the Columbia Regional Airport. By 2000, the city's population was nearly 85,000.
In 2017, Columbia was in the path of totality for the Solar eclipse of August 21, 2017. The city was expecting upwards of 400,000 tourists coming to view the eclipse.

Columbia, in northern mid-Missouri, is away from both St. Louis and Kansas City, and north of the state capital of Jefferson City. The city is near the Missouri River, between the Ozark Plateau and the Northern Plains.

According to the United States Census Bureau, the city has a total area of of which is land and is water.

The city generally slopes from the highest point in the Northeast to the lowest point in the Southwest towards the Missouri River. Prominent tributaries of the river are Perche Creek, Hinkson Creek, and Flat Branch Creek. Along these and other creeks in the area can be found large valleys, cliffs, and cave systems such as that in Rock Bridge State Park just south of the city. These creeks are largely responsible for numerous stream valleys giving Columbia hilly terrain similar to the Ozarks while also having prairie flatland typical of northern Missouri. Columbia also operates several greenbelts with trails and parks throughout town.

Large mammals found in the city include urbanized coyotes, red foxes, and numerous whitetail deer. Eastern gray squirrel, and other rodents are abundant, as well as cottontail rabbits and the nocturnal opossum and raccoon. Large bird species are abundant in parks and include the Canada goose, mallard duck, as well as shorebirds, including the great egret and great blue heron. Turkeys are also common in wooded areas and can occasionally be seen on the MKT recreation trail. Populations of bald eagles are found by the Missouri River. The city is on the Mississippi Flyway, used by migrating birds, and has a large variety of small bird species, common to the eastern U.S. The Eurasian tree sparrow, an introduced species, is limited in North America to the counties surrounding St. Louis. Columbia has large areas of forested and open land and many of these areas are home to wildlife.

Columbia has a humid continental climate (Köppen "Dfa)" marked by sharp seasonal contrasts in temperature, and is in USDA Plant Hardiness Zone 6a. The monthly daily average temperature ranges from in January to in July, while the high reaches or exceeds on an average of 32 days per year, on two days, while four nights of sub- lows can be expected. Precipitation tends to be greatest and most frequent in the latter half of spring, when severe weather is also most common. Snow averages per season, mostly from December to March, with occasional November accumulation and falls in April being rarer; historically seasonal snow accumulation has ranged from in 2005–06 to in 1977–78. Extreme temperatures have ranged from on February 12, 1899 to on July 12 and 14, 1954. Readings of or are uncommon, the last occurrences being January 7, 2014 and July 31, 2012.

Columbia's most significant and well-known architecture is in buildings downtown and within the university campuses. The University of Missouri's Jesse Hall and the neo-gothic Memorial Union have become icons of the city. The David R. Francis Quadrangle is an example of Thomas Jefferson's academic village concept.

Four historic districts within the city are listed on the National Register of Historic Places: Downtown Columbia, the East Campus Neighborhood, Francis Quadrangle, and the North Ninth Street Historic District. The downtown skyline is relatively low and is dominated by the 10-story Tiger Hotel, built in 1928, and the 15-story Paquin Tower.

Downtown Columbia is an area of approximately one square mile surrounded by the University of Missouri on the south, Stephens College to the east, and Columbia College on the north. The area serves as Columbia's financial and business district.
Since the early 21st century, a large number of high-rise apartment complexes have been built in downtown Columbia. Many of these buildings also offer mixed-use business and retail space on the lower levels. These developments have not been without criticism, with some expressing concern the buildings hurt the historic feel of the area, or that the city does not yet have the infrastructure to support them.

The city's historic residential core lies in a ring around downtown, extending especially to the west along Broadway, and south into the East Campus Neighborhood. The city government recognizes 63 neighborhood associations. The city's most dense commercial areas are primarily along Interstate 70, U.S. Route 63, Stadium Boulevard, Grindstone Parkway, and Downtown.

As of the census of 2010, 108,500 people, 43,065 households, and 21,418 families resided in the city. The population density was . There were 46,758 housing units at an average density of . The racial makeup of the city was 79.0% White, 11.3% African American, 0.3% Native American, 5.2% Asian, 0.1% Pacific Islander, 1.1% from other races, and 3.1% from two or more races. Hispanic or Latino of any race were 3.4% of the population.

There were 43,065 households of which 26.1% had children under the age of 18 living with them, 35.6% were married couples living together, 10.6% had a female householder with no husband present, 3.5% had a male householder with no wife present, and 50.3% were non-families. 32.0% of all households were made up of individuals and 6.6% had someone living alone who was 65 years of age or older. The average household size was 2.32 and the average family size was 2.94.

In the city the population was spread out with 18.8% of residents under the age of 18; 27.3% between the ages of 18 and 24; 26.7% from 25 to 44; 18.6% from 45 to 64; and 8.5% who were 65 years of age or older. The median age in the city was 26.8 years. The gender makeup of the city was 48.3% male and 51.7% female.

As of the census of 2000, there were 84,531 people, 33,689 households, and 17,282 families residing in the city. The population density was 1,592.8 people per square mile (615.0/km²). There were 35,916 housing units at an average density of 676.8 per square mile (261.3/km²). The racial makeup of the city was 81.54% White, 10.85% Black or African American, 0.39% Native American, 4.30% Asian, 0.04% Pacific Islander, 0.81% from other races, and 2.07% from two or more races. Hispanic or Latino of any race were 2.05% of the population.

There were 33,689 households out of which 26.1% had children under the age of 18 living with them, 38.2% were married couples living together, 10.3% had a female householder with no husband present, and 48.7% were non-families. 33.1% of all households were made up of individuals and 6.5% had someone living alone who was 65 years of age or older. The average household size was 2.26 and the average family size was 2.92.

In the city, the population was spread out with 19.7% under the age of 18, 26.7% from 18 to 24, 28.7% from 25 to 44, 16.2% from 45 to 64, and 8.6% who were 65 years of age or older. The median age was 27 years. For every 100 females, there were 91.8 males. For every 100 females age 18 and over, there were 89.1 males.

The median income for a household in the city was $33,729, and the median income for a family was $52,288. Males had a median income of $34,710 versus $26,694 for females. The per capita income for the city was $19,507. About 9.4% of families and 19.2% of the population were below the poverty line, including 14.8% of those under age 18 and 5.2% of those age 65 or over. However, traditional statistics of income and poverty can be misleading when applied to cities with high student populations, such as Columbia.

Columbia's economy is historically dominated by education, healthcare, and the insurance industry. Jobs in government are also common, either in Columbia or a half-hour south in Jefferson City. Commutes into the city are also common and, in 2000, the city had a day time population of 106,487—a 26% increase over the census population of the same year. The Columbia Regional Airport and the Missouri River Port of Rocheport connect the region with trade and transportation.

The economy of Columbia's metro area is slightly larger than the Bahamas. With a Gross Metropolitan Product of $5.84 billion in 2004, Columbia's economy makes up 2.9% of the Gross State Product of Missouri. Insurance corporations headquartered in Columbia include Shelter Insurance, and the Columbia Insurance Group. Other organizations include Veterans United Home Loans, MFA Incorporated, the Missouri State High School Activities Association, and MFA Oil. Companies such as Socket, Datastorm Technologies, Inc. (no longer existent), Slackers CDs and Games, Carfax, and MBS Textbook Exchange were founded in Columbia.

According to Columbia's 2017 Comprehensive Annual Financial Report, the top employers in the city are:

The Missouri Theatre Center for the Arts and Jesse Auditorium are Columbia's largest fine arts venues. Ragtag Cinema annually hosts the True/False Film Festival. In 2008, filmmaker Todd Sklar completed "Box Elder", which was filmed entirely in and around Columbia and the University of Missouri.

The University of Missouri's Museum of Art and Archaeology displays 14,000 works of art and archaeological objects in five galleries for no charge to the public. Libraries include the Columbia Public Library, the University of Missouri Libraries, with over three million volumes in Ellis Library, and the State Historical Society of Missouri.

The "We Always Swing" Jazz Series and the Roots N Blues N BBQ Festival is held in Columbia. "9th Street Summerfest" (Now hosted in Rose Park at Rose Music Hall) closes part of that street several nights each summer to hold outdoor performances and has featured Willie Nelson (2009), Snoop Dogg (2010), The Flaming Lips (2010), Weird Al Yankovic (2013), and others. The "University Concert Series" regularly includes musicians and dancers from various genres, typically in Jesse Hall. Other musical venues in town include the Missouri Theatre, the University's multipurpose Hearnes Center, The Blue Note, and Rose Music Hall. Shelter Gardens, a park on the campus of Shelter Insurance headquarters, also hosts outdoor performances during the summer.

The University of Missouri School of Music attracts hundreds of musicians to Columbia, student performances are held in Whitmore Recital Hall. Among many non-profit organizations for classical music are included the "Odyssey Chamber Music Series", "Missouri Symphony Society" and "Columbia Civic Orchestra". Founded in 2006, the "Plowman Chamber Music Competition" is a biennial competition held in March/April of odd-numbered years, considered to be one of the finest, top five chamber music competitions in the nation.

The University of Missouri's sports teams, the Missouri Tigers, play a significant role in the city's sports culture. Faurot Field at Memorial Stadium, which has a capacity of 71,168, hosts home football games. The Hearnes Center and Mizzou Arena are two other large sport and event venues, the latter being the home arena for Mizzou's basketball team. Taylor Stadium is host to their baseball team and was the regional host for the 2007 NCAA Baseball Championship. Columbia College has several men and women collegiate sports teams as well. In 2007, Columbia hosted the National Association of Intercollegiate Athletics Volleyball National Championship, which the Lady Cougars participated in.

Columbia also hosts the Show-Me State Games, a non-profit program of the Missouri Governor's Council on Physical Fitness and Health. They are the largest state games in the United States.

Situated midway between St. Louis and Kansas City, Columbians will often have allegiances to the professional sports teams housed there, such as the St. Louis Cardinals, the Kansas City Royals, the Kansas City Chiefs, the St. Louis Blues and Sporting Kansas City.

The city has two daily newspapers: the "Columbia Missourian" and the "Columbia Daily Tribune", both morning deliveries. The "Missourian" is directed by professional editors and staffed by Missouri School of Journalism students who do reporting, design, copy editing, information graphics, photography and multimedia. The "Missourian" publishes the weekly city magazine, "Vox". With a daily circulation of nearly 20,000, the "Daily Tribune" is the most widely read newspaper in central Missouri. The University of Missouri has the independent but official student newspaper called "The Maneater", which is printed bi-weekly. The now-defunct "Prysms Weekly" was also published in Columbia. In Fall 2009, KCOU News launched full operations out of KCOU 88.1 FM on the MU Campus. The entirely student-run news organization airs a daily newscast, "The Pulse", weekdays from 4:30 to 5:30 p.m.

The city has 14 radio stations and 4 television channels. Columbia's Public Access channel, Columbia Access Television, is commonly referred to by its acronym, CAT, or CAT-TV. The Education Access channel in Columbia, CPSTV, is managed by Columbia Public Schools as a function of the Columbia Public Schools Community Relations Department. The city's Government Access channel broadcast City Council, Planning and Zoning Commission and Board of Adjustment meetings

Columbia has many bars and restaurants that provide diverse styles of cuisine, which is in part due to it being a college town with three colleges. One such establishment is the historic Booches bar, restaurant and pool hall, which was established in 1884 and is frequented by college students, as well as others from all walks of life. Shakespeare's Pizza is known across the nation for its college town pizza.

The City of Columbia's current government was established by a home rule charter adopted by voters on November 11, 1974, which established a Council-manager government that invested power in the City Council. The City Council is made up of seven members – six elected by each of Columbia's six single-member districts or wards, plus an at-large council member, the Mayor, who is elected by all city voters. The Mayor receives a $9,000 annual stipend and the six remaining council members receive a $6,000 annual stipend. They are elected to staggered three-year terms. As well as serving as a voting member of the City Council, the mayor is also recognized as the head of city government for ceremonial purposes. Chief executive authority is invested in a hired city manager, who oversees the day-to-day operations of government.

Columbia is the county seat of Boone County, and houses the county court and government center. The city lies within Missouri's 4th congressional district. The 19th Missouri State Senate district covers all of Boone County. There are five Missouri House of Representatives districts (9, 21, 23, 24, and 25) in the city. The principal law enforcement agency is the Columbia Police Department, while the Columbia Fire Department provides fire protection. The University of Missouri Police Department patrols areas on and around the MU campus and has jurisdiction throughout the city and Boone County. The Public Service Joint Communications Center coordinates efforts between the two organizations as well as the Boone County Fire Protection District, which operates Urban Search and Rescue Missouri Task Force 1.
The population generally supports progressive causes such as the extensive city recycling programs and the decriminalization of cannabis both for medical and recreational use at the municipal level (though the scope of the latter of the two cannabis ordinances has since been restricted). The city is one of only four in the state to offer medical benefits to same-sex partners of city employees. The new health plan extends health benefits to unmarried heterosexual domestic partners of city employees.

On October 10, 2006, the City Council approved an ordinance to prohibit smoking in public places, including restaurants and bars. The ordinance was passed over protest, and several amendments to the ordinance reflect this. Over half of residents possess at least a bachelor's degree, while over a quarter hold a graduate degree. Columbia is the thirteenth most-highly educated municipality in the United States.

Columbia and much of the surrounding area lies within the Columbia Public School District. The district enrolls over 17,000 students and had a revenue of nearly $200 million for the 2007–2008 school year. It is above the state average in both attendance percentage and graduation rate. The city operates four public high schools which cover grades 9–12: David H. Hickman High School, Rock Bridge High School, Muriel Battle High School, and Frederick Douglass High School. Rock Bridge is one of two Missouri high schools to receive a silver medal by U.S. News & World Report, putting it in the top 3% of all high schools in the nation. Hickman has been on Newsweek magazine's list of top 1,300 schools in the country for the past three years and has more named presidential scholars than any other public high school in the United States. There are also several private high schools including Christian Fellowship School, Columbia Independent School, Heritage Academy, Christian Chapel Academy, and Father Augustine Tolton Regional Catholic High School.

CPS also manages six middle schools: West, Jefferson, Lange, Oakland, Smithton, and Gentry. There is also a seventh school that will open in the 2019-2020 school year.

The city has three institutions of higher education: the University of Missouri, Stephens College, and Columbia College, all of which surround Downtown Columbia. The city is the headquarters of the University of Missouri System, which also operates campuses in St. Louis, Kansas City, and Rolla. Moberly Area Community College, Central Methodist University, and William Woods University also operate satellite campuses in Columbia.

The Columbia Transit provides public bus and para-transit service, and is owned and operated by the city. In 2008, 1,414,400 passengers boarded along the system's six fixed routes and nine University of Missouri shuttle routes, and 27,000 boarded the Para-transit service. The system is constantly experiencing growth in service and technology. A $3.5 million project to renovate and expand the Wabash Station, a rail depot built in 1910 and converted into the city's transit center in the mid-1980s, was completed in summer of 2007. In 2007, a Transit Master Plan was created to address the future transit needs of the city and county with a comprehensive plan to add infrastructure in three key phases. The five to 15-year plan intends to add service along the southwest, southeast and northeast sections of Columbia and develop alternative transportation models for Boone County.

Columbia is also known for its MKT Trail, a spur of the Katy Trail State Park, which allows foot and bike traffic across the city, and, conceivably, the state. It consists of a soft gravel surface for running and biking. Columbia also is preparing to embark on construction of several new bike paths and street bike lanes thanks to a $25 million grant from the federal government. The city is also served by American Airlines and United Airlines at the Columbia Regional Airport, the only commercial airport in mid-Missouri.

I-70 (concurrent with US 40) and US 63 are the two main freeways used for travel to and from Columbia. Within the city, there are also three state highways: Routes 763 (Rangeline Street & College Avenue), 163 (Providence Road), and 740 (Stadium Boulevard).

Rail service is provided by the city-owned Columbia Terminal Railroad (COLT), which runs from the north side of Columbia to Centralia and a connection to the Norfolk Southern Railway. Columbia would be at the center of the proposed Missouri Hyperloop, reducing travel times to Kansas City and St. Louis to around 15 minuets.

Health care is a big part of Columbia's economy, with nearly one in six people working in a health-care related profession and a physician density that is about three times the United States average. Columbia's hospitals and supporting facilities are a large referral center for the state, and medical related trips to the city are common. There are three hospital systems within the city and five hospitals with a total of 1,105 beds.

The University of Missouri Health Care operates three hospitals in Columbia: the University of Missouri Hospital, the University of Missouri Women's and Children's Hospital (formerly Columbia Regional Hospital), and the Ellis Fischel Cancer Center. Boone Hospital Center is administered by BJC Healthcare and operates several clinics as well as outpatient locations. The Harry S. Truman Memorial Veterans' Hospital, next to University Hospital, is administered by the United States Department of Veterans Affairs.

There are also a large number of medical-related industries in Columbia. The University of Missouri School of Medicine uses university-owned facilities as teaching hospitals. The University of Missouri Research Reactor Center is the largest research reactor in the United States and produces radioisotopes used in nuclear medicine. The center serves as the sole supplier of the active ingredients in two U.S. Food and Drug Administration-approved radiopharmaceuticals and produces Fluorine-18 used in PET imaging with its cyclotron.

In accordance with the Columbia Sister Cities Program, which operates in conjunction with Sister Cities International, Columbia has been paired with five international sister cities in an attempt to foster cross-cultural understanding:







</doc>
<doc id="6720" url="https://en.wikipedia.org/wiki?curid=6720" title="Charlton Athletic F.C.">
Charlton Athletic F.C.

Charlton Athletic Football Club is an English professional association football club based in Charlton, south-east London. They currently compete in the EFL Championship, the second tier of English football. The club was founded on 9 June 1905 when a number of youth clubs in south-east London, including East Street Mission and Blundell Mission, combined to form Charlton Athletic. Their home ground is the Valley, where the club have played since 1919, apart from one year in Catford, during 1923–24, and seven years at Crystal Palace and West Ham United between 1985 and 1992, due to the Valley being redeveloped. 

Charlton turned professional in 1920 and first entered the Football League in 1921. Since then the club has had four separate periods in the top flight of English football: 1936–1957, 1986–1990, 1998–1999, and 2000–2007. Historically, Charlton's most successful period was the 1930s, when the club's highest league finishes were recorded, including runners-up of the First Division in 1937. After World War II, Charlton reached two consecutive FA Cup finals, losing in 1946, and winning in 1947. 

The club's traditional kit consists of red shirts, white shorts and red socks, and their most commonly used nickname is "The Addicks". Charlton share local rivalries with fellow South East London clubs Crystal Palace and Millwall.

Charlton Athletic F.C. were formed on 9 June 1905 by a group of 15- to 17-year-olds in East Street, Charlton, which is now known as Eastmoor Street and no longer residential. Charlton spent most of the years before the First World War playing in youth leagues. They became a senior side in 1913 the same year that nearby Woolwich Arsenal relocated to North London. After the war, they joined the Kent League for one season (1919–20) before becoming professional, appointing Walter Rayner as the first full-time manager. They were accepted by the Southern League and played just a single season (1920–21) before being voted into the Football League. Charlton's first Football League match was against Exeter City in August 1921, which they won 1–0. In 1923, Charlton became "giant killers" in the FA Cup beating top flight sides Manchester City, West Bromwich Albion, and Preston North End before losing to eventual winners Bolton Wanderers in the Quarter-Finals. Later that year, it was proposed that Charlton merge with Catford Southend to create a larger team with bigger support. In the 1923–24 season Charlton played in Catford at The Mount stadium and wore the colours of "The Enders", light and dark blue vertical stripes. However, the move fell through and the Addicks returned to the Charlton area in 1924, returning to the traditional red and white colours in the process. 

Charlton finished second bottom in the Football League in 1926 and were forced to apply for re-election which was successful. Three years later the Addicks won the Division Three championship in 1929<ref name="England 1928/29"></ref> and they remained at the Division Two level for four years. After relegation into the Third Division south at the end of the 1932–33 season the club appointed Jimmy Seed as manager and he oversaw the most successful period in Charlton's history either side of the Second World War. Seed, an ex-miner who had made a career as a footballer despite suffering the effects of poison gas in the First World War, remains the most successful manager in Charlton's history. He is commemorated in the name of a stand at the Valley. Seed was an innovative thinker about the game at a time when tactical formations were still relatively unsophisticated. He later recalled "a simple scheme that enabled us to pull several matches out of the fire" during the 1934–35 season: when the team was in trouble "the centre-half was to forsake his defensive role and go up into the attack to add weight to the five forwards." The organisation Seed brought to the team proved effective and the Addicks gained successive promotions from the Third Division to the First Division between 1934 and 1936, becoming the first club to ever do so. Charlton finally secured promotion to the First Division by beating local rivals West Ham United at the Boleyn Ground, with their centre-half John Oakes playing on despite concussion and a broken nose.

In 1937, Charlton finished runners up in the First Division,<ref name="1936/1937 English Division 1 (old) Table"></ref> in 1938 finished fourth<ref name="1937/1938 English Division 1 (old) Table"></ref> and 1939 finished third.<ref name="1938/1939 English Division 1 (old) Table"></ref> They were the most consistent team in the top flight of English football over the three seasons immediately before the Second World War. This continued during the war years and they won the Football League War Cup and appeared in finals.

Charlton reached the 1946 FA Cup Final, but lost 4–1 to Derby County at Wembley. Charlton's Bert Turner scored an own goal in the eightieth minute before equalising for the Addicks a minute later to take them into extra time, but they conceded three further goals in the extra period. When the full league programme resumed in 1946–47 Charlton could finish only 19th in the First Division, just above the relegation spots, but they made amends with their performance in the FA Cup, reaching the 1947 FA Cup Final. This time they were successful, beating Burnley 1–0, with Chris Duffy scoring the only goal of the day. In this period of renewed football attendances, Charlton became one of only thirteen English football teams to average over 40,000 as their attendance during a full season. The Valley was the largest football ground in the League, drawing crowds in excess of 70,000. However, in the 1950s little investment was made either for players or to The Valley, hampering the club's growth. In 1956, the then board undermined Jimmy Seed and asked for his resignation; Charlton were relegated the following year.

From the late 1950s until the early 1970s, Charlton remained a mainstay of the Second Division before relegation to the Third Division in 1972<ref name="England 1971/72"></ref> caused the team's support to drop, and even a promotion in 1975 back to the second division<ref name="England 1974/75"></ref> did little to re-invigorate the team's support and finances. In 1979–80 Charlton were relegated again to the Third Division,<ref name="England 1979/80"></ref> but won immediate promotion back to the Second Division in 1980–81.<ref name="England 1980/81"></ref> Even though it did not feel like it, this was a turning point in the club's history leading to a period of turbulence and change including further promotion and exile. A change in management and shortly after a change in club ownership led to severe problems, such as the reckless signing of former European Footballer of the Year Allan Simonsen, and the club looked like it would go out of business.

In 1984 financial matters came to a head and the club went into administration, to be reformed as Charlton Athletic. (1984) Ltd. although the club's finances were still far from secure. They were forced to leave the Valley just after the start of the 1985–86 season, after its safety was criticised by Football League officials in the wake of the Bradford City stadium fire.
The club began to groundshare with Crystal Palace at Selhurst Park and this arrangement looked to be for the long-term, as Charlton did not have enough funds to revamp the Valley to meet safety requirements.
Despite the move away from the Valley, Charlton were promoted to the First Division as Second Division runners-up at the end of 1985–86,<ref name="England 1985/86"></ref> and remained at this level for four years (achieving a highest league finish of 14th) often with late escapes, most notably against Leeds in 1987, where the Addicks triumphed in extra-time of the play-off final replay to secure their top flight place. In 1987 Charlton also returned to Wembley for the first time since the 1947 FA Cup final for the Full Members Cup final against Blackburn.
Eventually, Charlton were relegated in 1990 along with Sheffield Wednesday and bottom club Millwall. Manager Lennie Lawrence remained in charge for one more season before he accepted an offer to take charge of Middlesbrough. He was replaced by joint player-managers Alan Curbishley and Steve Gritt. The pair had unexpected success in their first season finishing just outside the play-offs, and 1992–93 began promisingly and Charlton looked good bets for promotion in the new Division One (the new name of the old Second Division following the formation of the Premier League). However, the club was forced to sell players such as Rob Lee to help pay for a return to The Valley, which eventually happened in December 1992.

There was a tragedy at the club late in the 1992–93 season. Defender Tommy Caton, who had been out of action due to injury since January 1991, announced his retirement from playing on medical advice in March 1993 having failed to recover full fitness, and he died suddenly at the end of the following month at the age of 30.

In 1995, new chairman Richard Murray appointed Alan Curbishley as sole manager of Charlton. Under his sole leadership Charlton made an appearance in the playoffs in 1996 but were eliminated by Crystal Palace in the semi-finals and the following season brought a disappointing 15th-place finish. 1997–98 was Charlton's best season for years. They reached the Division One playoff final and battled against Sunderland in a thrilling game which ended with a 4–4 draw after extra time. Charlton won 7–6 on penalties, with the match described as "arguably the most dramatic game of football in Wembley's history", and were promoted to the Premier League.

Charlton's first Premier League campaign began promisingly (they went top after two games) but they were unable to keep up their good form and were soon battling relegation. The battle was lost on the final day of the season but the club's board kept faith in Curbishley, confident that they could bounce back. Curbishley rewarded the chairman's loyalty with the Division One title in 2000 which signalled a return to the Premier League.

After the club's return, Curbishley proved an astute spender and by 2003 he had succeeded in establishing Charlton in the top flight. Charlton spent much of the 2003–04 Premier League season challenging for a Champions League place, but a late-season slump in form and the sale of star player Scott Parker to Chelsea, left Charlton in seventh place, which was still the club's highest finish since the 1950s. Charlton were unable to build on this level of achievement and Curbishley departed in 2006, with the club still established as a solid mid-table side.

In May 2006, Iain Dowie was named as Curbishley's successor, but was sacked after 12 league matches in November 2006, with only two wins. Les Reed replaced Dowie as manager, however he too failed to improve Charlton's position in the league table and on Christmas Eve 2006, Reed was replaced by former player Alan Pardew. Although results did improve, Pardew was unable to keep Charlton up and relegation was confirmed in the penultimate match of the season.

Charlton's return to the second tier of English football was a disappointment, with their promotion campaign tailing off to an 11th-place finish. Early in the following season the Addicks were linked with a foreign takeover, but this was swiftly denied by the club. On 10 October 2008, Charlton received an indicative offer for the club from a Dubai-based diversified investment company. However, the deal later fell through. The full significance of this soon became apparent as the club recorded net losses of over £13 million for that financial year. Pardew left on 22 November after a 2–5 home loss to Sheffield United that saw the team fall into the relegation places. Matters did not improve under caretaker manager Phil Parkinson, and the team went a club record 18 games without a win, a new club record, before finally achieving a 1–0 away victory over Norwich City in an FA Cup Third Round replay; Parkinson was hired on a permanent basis. The team were relegated to League One after a 2–2 draw against Blackpool on 18 April 2009.

After spending almost the entire 2009–10 season in the top six of League One, Charlton were defeated in the Football League One play-offs semi-final second leg on penalties against Swindon Town.
After a change in ownership, Parkinson and Charlton legend Mark Kinsella left after a poor run of results. Another Charlton legend, Chris Powell, was appointed manager of the club in January 2011, winning his first game in charge 2–0 over Plymouth at the Valley. This was Charlton's first league win since November. Powell's bright start continued with a further three victories, before running into a downturn which saw the club go 11 games in succession without a win. Yet the fans' respect for Powell saw him come under remarkably little criticism. The club's fortunes picked up towards the end of the season, but leaving them far short of the playoffs. In a busy summer, Powell brought in 19 new players and after a successful season, on 14 April 2012, Charlton Athletic won promotion back to the Championship with a 1–0 away win at Carlisle United. A week later, on 21 April 2012, they were confirmed as champions after a 2–1 home win over Wycombe Wanderers. Charlton then lifted the League One trophy on 5 May 2012, having been in the top position since 15 September 2011, and after recording a 3–2 victory over Hartlepool United, recorded their highest ever league points score of 101, the highest in any professional European league that year.

In the first season back in the Championship, the 2012–13 season saw Charlton finish ninth place with 65 points, just three points short of the play-off places to the Premier League.

In early January 2014 during the 2013–14 season, Belgian businessman Roland Duchâtelet took over Charlton as owner in a deal worth £14million. This made Charlton a part of a network of football clubs owned by Duchâtelet. On 11 March 2014, two days after an FA Cup quarter-final loss to Sheffield United, and with Charlton sitting bottom of the table, Powell was sacked and leaked private emails suggested that this was due to a rift with the owner.

New manager Jose Riga, despite having to join Charlton long after the transfer window had closed, was able to improve Charlton's form and eventually guide them to 18th place, successfully avoiding relegation. After Riga's departure to manage Blackpool, former Millwall player Bob Peeters was appointed as manager in May 2014 on a 12-month contract. Charlton started strong, but a long run of draws meant that after only 25 games in charge Peeters was dismissed with the team in 14th place. His replacement, Guy Luzon, was able to ensure there was no danger of a relegation battle by winning the majority of the remaining matches and finishing in 12th place.

The 2015–16 season began promisingly but results under Luzon deteriorated and on 24 October 2015 after a 3–0 defeat at home to Brentford he was sacked. Luzon said in an interview with the News Shopper that he "was not the one who chose how to do the recruitment" as the reason why he was failed as manager.

Karel Fraeye was appointed "interim head coach", but was sacked after 14 games, only two of which were won, with the club now second from bottom in the Championship. On 14 January 2016, Jose Riga was appointed head coach for a second spell, but could not prevent Charlton from being relegated to League One for the 2016–17 season. Riga resigned at the end of the season. To many fans, the managerial changes and subsequent relegation to League One were symptomatic of the mismanagement of the club under Duchâtelet's ownership and a number of protests began and still continue to this day.

After a slow start to the new season, with the club in 15th place of League One, the club announced that it had "parted company" with Russell Slade in November 2016. Karl Robinson was appointed on a permanent basis soon after. He led the Addicks to a uneventful 13th place finish. The following season Robinson had the team challenging for the playoffs, but a drop in form in March led him to resign by mutual consent. He was replaced by former player Lee Bowyer as caretaker manager and guided them to a 6th place finish, but lost in the playoffs semi-final.

Bowyer was appointed permanently in September on a one year contract and after finishing third in the regular 2018-19 EFL League One season, Charlton beat Sunderland 2-1 in the League One play-off final to earn promotion back to the EFL Championship after a three season absence.

The club's first ground was Siemens Meadow (1905–1907), a patch of rough ground by the River Thames. This was over-shadowed by the Siemens Brothers Telegraph Works. Then followed Woolwich Common (1907–1908), Pound Park (1908–1913), and Angerstein Lane (1913–1915). After the end of the First World War, a chalk quarry known as the Swamps was identified as Charlton's new ground, and in the summer of 1919 work began to create the level playing area and remove debris from the site. The first match at this site, now known as the club's current ground The Valley, was in September 1919. Charlton stayed at The Valley until 1923, when the club moved to The Mount stadium in Catford as part of a proposed merger with Catford Southend Football Club. However, after this move collapsed in 1924 Charlton returned to The Valley.

During the 1930s and 1940s, significant improvements were made to the ground, making it one of the largest in the country at that time. In 1938 the highest attendance to date at the ground was recorded at over 75,000 for a FA Cup match against Aston Villa. During the 1940s and 1950s the attendance was often above 40,000, and Charlton had one of the largest support bases in the country. However, after the club's relegation little investment was made in The Valley as it fell into decline.

In the 1980s matters came to a head as the ownership of the club and The Valley was divided. The large East Terrace had been closed down by the authorities after the Bradford City stadium fire and the ground's owner wanted to use part of the site for housing. In September 1985, Charlton made the controversial move to ground-share with South London neighbours Crystal Palace at Selhurst Park. This move was unpopular with supporters and in the late 1980s significant steps were taken to bring about the club's return to The Valley.

A single issue political party, the Valley Party, contested the 1990 local Greenwich Borough Council elections on a ticket of reopening the stadium, capturing 11% of the vote, aiding the club's return. The Valley Gold investment scheme was created to help supporters fund the return to The Valley, and several players were also sold to raise funds. For the 1991–92 season and part of the 1992–93 season, the Addicks played at West Ham's Upton Park as Wimbledon had moved into Selhurst Park alongside Crystal Palace. Charlton finally returned to The Valley in December 1992, celebrating with a 1–0 victory against Portsmouth.

Since the return to The Valley, three sides of the ground have been completely redeveloped turning The Valley into a modern, all-seater stadium with a 27,111 capacity. There are plans in place to increase the ground's capacity to approximately 31,000 and even around 40,000 in the future.

The bulk of the club's support base comes from South East London and Kent, particularly the London boroughs of Greenwich, Bexley and Bromley. Supporters played a key role in the return of the club to The Valley in 1992 and were rewarded by being granted a voice on the board in the form of an elected supporter director. Any season ticket holder could put themselves forward for election, with a certain number of nominations, and votes were cast by all season ticket holders over the age of 18. The last such director, Ben Hayes, was elected in 2006 to serve until 2008, when the role was discontinued as a result of legal issues. Its functions were replaced by a fans forum which met for the first time in December 2008 and is still active to this day.

Charlton's most common nickname is The Addicks. The most likely origin of this name is from a local fishmonger, Arthur "Ikey" Bryan, who rewarded the team with meals of haddock and chips.

The progression of the nickname can be seen in the book "The Addicks Cartoons: An Affectionate Look into the Early History of Charlton Athletic", which covers the pre-First World War history of Charlton through a narrative based on 56 cartoons which appeared in the now defunct Kentish Independent. The very first cartoon, from 31 October 1908, calls the team the Haddocks. By 1910, the name had changed to Addicks although it also appeared as Haddick. The club has had two other nicknames, The Red Robins, adopted in 1931, and The Valiants, chosen in a fan competition in the 1960s which also led to the adoption of the sword badge which is still in use. The Addicks nickname never went away and was revived by fans after the club lost its Valley home in 1985 and went into exile at Crystal Palace. It is now once again the official nickname of the club.

Charlton fans' chants have included "Valley, Floyd Road", a song noting the stadium's address to the tune of "Mull of Kintyre", and "The Red, Red Robin".

Charlton Athletic featured in the ITV one-off drama "Albert's Memorial", shown on 12 September 2010 and starring David Jason and David Warner.

In the long-running BBC sitcom "Only Fools and Horses", Rodney Charlton Trotter is named after the club.

Charlton's ground and the then manager, Alan Curbishley, made appearances in the Sky One TV series, "Dream Team".

Charlton Athletic has also featured in a number of book publications, in both the realm of fiction and factual/sports writing. These include works by Charlie Connelly and Paul Breen's work of popular fiction which is entitled "The Charlton Men". The book is set against Charlton's successful 2011–12 season when they won the League One title and promotion back to the Championship in concurrence with the 2011 London riots.

Charlton have used a number of crests and badges during their history, although the current design has not been changed since 1968. The first known badge, from the 1930s, consisted of the letters CAF in the shape of a club from a pack of cards. In the 1940s, Charlton used a design featuring a robin sitting in a football within a shield, sometimes with the letters CAFC in the four-quarters of the shield, which was worn for the 1946 FA Cup Final. In the late 1940s and early 1950s, the crest of the former metropolitan borough of Greenwich was used as a symbol for the club but this was not used on the team's shirts.

In 1963, a competition was held to find a new badge for the club, and the winning entry was a hand holding a sword, which complied with Charlton's nickname of the time, the Valiants. Over the next five years modifications were made to this design, such as the addition of a circle surrounding the hand and sword and including the club's name in the badge. By 1968, the design had reached the one known today, and has been used continuously from this year, apart from a period in the 1970s when just the letters CAFC appeared on the team's shirts.

With the exception of one season, Charlton have always played in red and white. The colours had been chosen by the group of boys who had founded Charlton Athletic in 1905 after having to play their first matches in the borrowed kits of their local rivals Woolwich Arsenal, who also played in red and white. The exception came during the 1923–24 season when Charlton wore the colours of Catford Southend as part of the proposed move to Catford, which were light and dark blue stripes. However, after the move fell through, Charlton returned to wearing red and white as their home colours.

Charlton's main rivals are Millwall and Crystal Palace.

The rivalry with Crystal Palace grew substantially in the mid-1980s, when the Addicks left their traditional home at The Valley because of safety concerns and played their home fixtures at The Eagles' Selhurst Park stadium. The ground-sharing arrangement – although seen by Crystal Palace chairman Ron Noades as essential for the future of football – was unpopular with both sets of fans. Indeed, the Charlton fans campaigned for a return to The Valley throughout the club's time at Selhurst Park.

Charlton left Selhurst Park in 1991, and the rivalry between the teams once again returned to a nominal level until two incidents 14 years later:

In 2005, having already lost 1–0 to Charlton at Selhurst Park earlier in the season, Palace were relegated at The Valley after a 2–2 draw. After the match there was a well publicised altercation between the two chairmen Richard Murray and Simon Jordan, which only served to renew old hostilities between the fans.

The rivalry began when Millwall moved south of the river in 1910 to The Den in New Cross, South East London situated fewer than four miles from The Valley. Matches between the two sides are always fiercely contested.

<ref name="2018/19 squad numbers revealed"></ref>



</doc>
<doc id="6721" url="https://en.wikipedia.org/wiki?curid=6721" title="Cross-country skiing">
Cross-country skiing

Cross-country skiing is a form of skiing where skiers rely on their own locomotion to move across snow-covered terrain, rather than using ski lifts or other forms of assistance. Cross-country skiing is widely practiced as a sport and recreational activity; however, some still use it as a means of transportation. Variants of cross-country skiing are adapted to a range of terrain which spans unimproved, sometimes mountainous terrain to groomed courses that are specifically designed for the sport.

Modern cross-country skiing is similar to the original form of skiing, from which all skiing disciplines evolved, including alpine skiing, ski jumping and Telemark skiing. Skiers propel themselves either by striding forward (classic style) or side-to-side in a skating motion (skate skiing), aided by arms pushing on ski poles against the snow. It is practised in regions with snow-covered landscapes, including Northern Europe, Canada, Russia, the United States, Australia and New Zealand.
Competitive cross-country skiing is one of the Nordic skiing sports. Cross-country skiing and rifle marksmanship are the two components of biathlon, ski-orienteering is a form of cross-country skiing, which includes map navigation along snow trails and tracks.

The word ski comes from the Old Norse word which means stick of wood. Skiing started as a technique for traveling cross-country over snow on skis, starting almost five millennia ago with beginnings in Scandinavia. It may have been practised as early as 600 BCE in Daxing'anling, in what is now China. Early historical evidence includes Procopius's (around CE 550) description of Sami people as "skrithiphinoi" translated as "ski running samis". Birkely argues that the Sami people have practiced skiing for more than 6000 years, evidenced by the very old Sami word "čuoigat" for skiing. Egil Skallagrimsson's 950 CE saga describes King Haakon the Good's practice of sending his tax collectors out on skis. The Gulating law (1274) stated that "No moose shall be disturbed by skiers on private land." Cross-country skiing evolved from a utilitarian means of transportation to being a worldwide recreational activity and sport, which branched out into other forms of skiing starting in the mid-1800s.

Early skiers used one long pole or spear in addition to the skis. The first depiction of a skier with two ski poles dates to 1741. Traditional skis, used for snow travel in Norway and elsewhere into the 1800s, often comprised one short ski with a natural fur traction surface, the "andor", and one long for gliding, the "langski"—one being up to longer than the other—allowing skiers to propel themselves with a scooter motion. This combination has a long history among the Sami people. Skis up to 280 cm have been produced in Finland, and the longest recorded ski in Norway is 373 cm.

Ski warfare, the use of ski-equipped troops in war, is first recorded by the Danish historian Saxo Grammaticus in the 13th century. These troops were reportedly able to cover distances comparable to that of light cavalry. The garrison in Trondheim used skis at least from 1675, and the Danish-Norwegian army included specialized skiing battalions from 1747—details of military ski exercises from 1767 are on record. Skis were used in military exercises in 1747. In 1799 French traveller Jacques de la Tocnaye recorded his visit to Norway in his travel diary: Norwegian immigrants used skis ("Norwegian snowshoes") in the US midwest from around 1836. Norwegian immigrant "Snowshoe Thompson" transported mail by skiing across the Sierra Nevada between California and Nevada from 1856. In 1888 Norwegian explorer Fridtjof Nansen and his team crossed the Greenland icecap on skis. Norwegian workers on the Buenos Aires - Valparaiso railway line introduced skiing in South America around 1890. In 1910 Roald Amundsen used skis on his South Pole Expedition. In 1902 the Norwegian consul in Kobe imported ski equipment and introduced skiing to the Japanese, motivated by the death of Japanese soldiers during a snow storm.

Norwegian skiing regiments organized military skiing contests in the 18th century, divided in four classes: shooting at a target while skiing at "top speed", downhill racing among trees, downhill racing on large slopes without falling, and "long racing" on "flat ground". An early record of a public ski competition occurred in Tromsø, 1843. In Norwegian, refers to "competitive skiing where the goal is to complete a specific distance in groomed tracks in the shortest possible time". In Norway, "ski touring competitions" () are long-distance cross-country competitions open to the public, competition is usually within age intervals.

A new technique, skate skiing, was experimented with early in the 20th Century, but was not widely adopted until the 1980s. Johan Grøttumsbråten used the skating technique at the 1931 World Championship in Oberhof, one of the earliest recorded use of skating in competitive cross-country skiing. This technique was later used in ski orienteering in the 1960s on roads and other firm surfaces. It became widespread during the 1980s after the success of Bill Koch (United States) in 1982 Cross-country Skiing Championships drew more attention to the skating style. Norwegian skier Ove Aunli started using the technique in 1984, when he found it to be much faster than classic style. Finnish skier, Pauli Siitonen, developed a one-sided variant of the style in the 1970s, leaving one ski in the track while skating to the side with the other one during endurance events; this became known as the "marathon skate".

While the noun "ski" originates from the Norwegian language, unlike the English "skiing" there is no corresponding verb in Norwegian. Fridtjov Nansen, for instance, describes the crossing of Greenland as , literally "On skis across Greenland", while the English edition of the report was titled, "The first crossing of Greenland". Nansen referred to the activity of traversing snow on skis as (he used the term also in the English translation), which may be translated as "ski running". Nansen used , regarding all forms of skiing, but noted that ski jumping is purely a competitive sport and not for amateurs. He further noted that in some competitions the skier "is also required to show his skill in turning his ski to one side or the other within given marks" at full speed on a steep hill. Nansen regarded these forms (i.e., jumping and slalom) as "special arts", and believed that the most important branch of skiing was travel "in an ordinary way across the country". In Germany, Nansen's Greenland report was published as (literally "On snowshoes through Greenland"). The German term, , was supplanted by the borrowed Norwegian word, , in the late 19th century. The Norwegian encyclopedia of sports also uses the term, , (literally "ski running") for all forms of skiing. Around 1900 the word was used in German in the same sense as . In modern Norwegian, a variety of terms refer to cross-country skiing, including:


In contrast, alpine skiing is referred to as (literally "stand on skis").

Recreational cross-country skiing includes ski touring and groomed-trail skiing, typically at resorts or in parklands. It is an accessible form of recreation for persons with vision and mobility impairments. A related form of recreation is dog skijoring—a winter sport where a cross-country skier is assisted by one or more dogs.

Ski touring takes place off-piste and outside of ski resorts. Tours may extend over multiple days. Typically, skis, bindings, and boots allow for free movement of the heel to enable a walking pace, as with Nordic disciplines and unlike Alpine skiing. Ski touring's subgenre ski mountaineering involves independently navigating and route finding through potential avalanche terrain and often requires familiarity with meteorology along with skiing skills. Ski touring can be faster and easier than summer hiking in some terrain, allowing for traverses and ascents that would be harder in the summer. Skis can also be used to access backcountry alpine climbing routes when snow is off the technical route, but still covers the hiking trail. In some countries, organizations maintain a network of huts for use by cross-country skiers in wintertime. For example, the Norwegian Trekking Association maintains over 400 huts stretching across thousands of kilometres of trails which hikers can use in the summer and skiers in the winter.

Groomed trail skiing occurs at facilities such as Nordmarka (Oslo), Royal Gorge Cross Country Ski Resort and Gatineau Park in Quebec, where trails are laid out and groomed for both classic and skate-skiing. Such grooming and track setting (for classic technique) requires specialized equipment and techniques that adapt to the condition of the snow. Trail preparation employs snow machines which tow snow-compaction, texturing and track-setting devices. Groomers must adapt such equipment to the condition of the snow—crystal structure, temperature, degree of compaction, moisture content, etc. Depending on the initial condition of the snow, grooming may achieve an increase in density for new-fallen snow or a decrease in density for icy or compacted snow. Cross-country ski facilities may incorporate a course design that meets homologation standards for such organizations as the International Olympic Committee, the International Ski Federation, or national standards. Standards address course distances, degree of difficulty with maximums in elevation difference and steepness—both up and downhill, plus other factors.
Some facilities have night-time lighting on select trails—called "lysløype" (light trails) in Norwegian and "elljusspår" (electric-light trails) in Swedish. The first "lysløype" opened in 1946 in Nordmarka and at Byåsen (Trondheim).

Cross-country ski competition encompasses a variety of formats for races over courses of varying lengths according to rules sanctioned by the International Ski Federation (FIS) and by national organizations, such as the U.S. Ski and Snowboard Association and Cross Country Ski Canada. It also encompasses cross-country ski marathon events, sanctioned by the Worldloppet Ski Federation, cross-country ski orienteering events, sanctioned by the International Orienteering Federation, and Paralympic cross-country skiing, sanctioned by the International Paralympic Committee.

The FIS Nordic World Ski Championships have been held in various numbers and types of events since 1925 for men and since 1954 for women. From 1924 to 1939, the World Championships were held every year, including the Winter Olympic Games. After World War II, the World Championships were held every four years from 1950 to 1982. Since 1985, the World Championships have been held in odd-numbered years. Notable cross-country ski competitions include the Winter Olympics, the FIS Nordic World Ski Championships, and the FIS World Cup events (including the Holmenkollen).

Cross-country ski marathons—races with distances greater than 40 kilometers—have two cup series, the Ski Classics, which started in 2011, and the Worldloppet. Skiers race in classic or free-style (skating) events, depending on the rules of the race. Notable ski marathons, include the "Vasaloppet" in Sweden, "Birkebeineren" in Norway, the Engadin Skimarathon in Switzerland, the American Birkebeiner, the Tour of Anchorage in Anchorage, Alaska, and the Boreal Loppet, held in Forestville, Quebec, Canada.

Biathlon combines cross-country skiing and rifle shooting. Depending on the shooting performance, extra distance or time is added to the contestant's total running distance/time. For each shooting round, the biathlete must hit five targets; the skier receives a penalty for each missed target, which varies according to the competition rules.

Ski orienteering is a form of cross-country skiing competition that requires navigation in a landscape, making optimal route choices at racing speeds. Standard orienteering maps are used, but with special green overprinting of trails and tracks to indicate their navigability in snow; other symbols indicate whether any roads are snow-covered or clear. Standard skate-skiing equipment is used, along with a map holder attached to the chest. It is one of the four orienteering disciplines recognized by the International Orienteering Federation. Upper body strength is especially important because of frequent double poling along narrow snow trails.

Paralympic cross-country ski competition is an adaptation of cross-country skiing for athletes with disabilities. Paralympic cross-country skiing includes standing events, sitting events (for wheelchair users), and events for visually impaired athletes under the rules of the International Paralympic Committee. These are divided into several categories for people who are missing limbs, have amputations, are blind, or have any other physical disability, to continue their sport.

Cross-country skiing has two basic propulsion techniques, which apply to different surfaces: classic (undisturbed snow and tracked snow) and skate skiing (firm, smooth snow surfaces). The classic technique relies on a wax or texture on the ski bottom under the foot for traction on the snow to allow the skier to slide the other ski forward in virgin or tracked snow. With the skate skiing technique a skier slides on alternating skis on a firm snow surface at an angle from each other in a manner similar to ice skating. Both techniques employ poles with baskets that allow the arms to participate in the propulsion. Specialized equipment is adapted to each technique and each type of terrain. A variety of turns are used, when descending.

Both poles can be used simultaneously ("double-poling"), or alternating, in classic the alternating technique is most common (the "diagonal stride") while in the skating technique double poles are more common.

The classic style is often used on prepared trails (pistes) that have pairs of parallel grooves (tracks) cut into the snow. It is also the most usual technique where no tracks have been prepared. With this technique, each ski is pushed forward from the other stationary ski in a striding and gliding motion, alternating foot to foot. With the "diagonal stride" variant the poles are planted alternately on the opposite side of the forward-striding foot; with the "kick-double-pole" variant the poles are planted simultaneously with every other stride. At times, especially with gentle descents, double poling is the sole means of propulsion. On uphill terrain, techniques include the "side step" for steep slopes, moving the skis perpendicular to the fall line, the "herringbone" for moderate slopes, where the skier takes alternating steps with the skis splayed outwards, and, for gentle slopes, the skier uses the diagonal technique with shorter strides and greater arm force on the poles.

With skate skiing, the skier provides propulsion on a smooth, firm snow surface by pushing alternating skis away from one another at an angle, in a manner similar to ice skating. Skate-skiing usually involves a coordinated use of poles and the upper body to add impetus, sometimes with a double pole plant each time the ski is extended on a temporarily "dominant" side ("V1") or with a double pole plant each time the ski is extended on either side ("V2"). Skiers climb hills with these techniques by widening the angle of the "V" and by making more frequent, shorter strides and more forceful use of poles. A variant of the technique is the "marathon skate" or "Siitoten step", where the skier leaves one ski in the track while skating outwards to the side with the other ski.

Turns, used while descending or for braking, include the snowplough (or "wedge turn"), the stem christie (or "wedge christie"), parallel turn, and the Telemark turn. The step turn is used for maintaining speed during descents or out of track on flats.

Equipment comprises skis, poles, boots and bindings; these vary according to:

Skis used in cross-country are lighter and narrower than those used in alpine skiing. Ski bottoms are designed to provide a gliding surface and, for classic skis, a traction zone under foot. The base of the gliding surface is a plastic material that is designed both to minimize friction and, in many cases, to accept waxes. Glide wax may be used on the tails and tips of classic skis and across the length of skate skis.

Each type of ski is sized and designed differently. Length affects maneuverability; camber affects pressure on the snow beneath the feet of the skier; side-cut affects the ease of turning; width affects forward friction; overall area on the snow affects bearing capacity; and tip geometry affects the ability to penetrate new snow or to stay in a track. Each of the following ski types has a different combination of these attributes:


Glide waxes enhance the speed of the gliding surface, and are applied by ironing them onto the ski and then polishing the ski bottom. Three classes of glide wax are available, depending on the level of desired performance with higher performance coming at higher cost. Hydrocarbon glide waxes, based on paraffin are common for recreational use. Race waxes comprise a combination of fluorinated hydrocarbon waxes and fluorocarbon overlays. Fluorocarbons decrease surface tension and surface area of the water between the ski and the snow, increasing speed and glide of the ski under specific conditions. Either combined with the wax or applied after in a spray, powder, or block form, fluorocarbons significantly improve the glide of the ski and are widely used in cross-country ski races.

Skis designed for classic technique, both in track and in virgin snow, rely on a traction zone, called the "grip zone" or "kick zone", underfoot. This comes either from a) "texture", designed to slide forward but not backwards, that is built into the grip zone of waxless skis, or from applied devices, e.g. climbing skins, or b) from "grip waxes". Grip waxes are classified according to their hardness: harder waxes are for colder and newer snow. An incorrect choice of grip wax for the snow conditions encountered may cause ski slippage (wax too hard for the conditions) or snow sticking to the grip zone (wax too soft for the conditions). Grip waxes generate grip by interacting with snow crystals, which vary with temperature, age and compaction. Hard grip waxes don't work well for snow which has metamorphosed to having coarse grains, whether icy or wet. In these conditions, skiers opt for a stickier substance, called "klister".

Ski boots are attached to the ski only at the toe, leaving the heel free. Depending on application, boots may be lightweight (performance skiing) or heavier and more supportive (back-country skiing).

Bindings connect the boot to the ski. There are three primary groups of binding systems used in cross-country skiing (in descending order of importance):

Ski poles are used for balance and propulsion. Modern cross-country ski poles are made from aluminium, fibreglass-reinforced plastic, or carbon fibre, depending on weight, cost and performance parameters. Formerly they were made of wood or bamboo. They feature a foot (called a basket) near the end of the shaft that provides a pushing platform, as it makes contact with the snow. Baskets vary in size, according to the expected softness/firmness of the snow. Racing poles feature smaller, lighter baskets than recreational poles. Poles designed for skating are longer than those designed for classic skiing. Traditional skiing in the 1800s used a single pole for both cross-country and downhill. The single pole was longer and stronger than the poles that are used in pairs. In competitive cross-country poles in pairs were introduced around 1900.



</doc>
<doc id="6724" url="https://en.wikipedia.org/wiki?curid=6724" title="Copacabana, Rio de Janeiro">
Copacabana, Rio de Janeiro

Copacabana () is a "bairro" (neighbourhood) located in the South Zone of the city of Rio de Janeiro, Brazil. It is known for its 4 km (2.5 miles) balneario beach, which is one of the most famous in the world.

The district was originally called "Sacopenapã" (translated from the Tupi language, it means "the way of the socós", the socós being a kind of bird) until the mid-18th century. It was renamed after the construction of a chapel holding a replica of the Virgen de Copacabana, the patron saint of Bolivia. The name may be derived from the Aymara "kota kahuana", meaning "view of the lake." The social scientist Mario Montaño Aragón found in the "archives of Indias" in Sevilla, Spain, a different history: "Kotakawana" is the god of fertility in ancient Andean mythology, the equivalent to the classical Greek goddess Aphrodite or the Roman Venus. This god is androgynous and lives in the Titicaca, and his court consists of creatures (male and female) that are represented in colonial sculptures and in Catholic churches. They were called "Umantuus", known as mermaids in Western culture.

Copacabana begins at Princesa Isabel Avenue and ends at Posto Seis (lifeguard watchtower Six). Beyond Copacabana, there are two small beaches: one, inside Fort Copacabana and the other, right after it: Diabo ("Devil") Beach. Arpoador beach, where surfers go after its perfect waves, comes next, followed by the famous borough of Ipanema. The area was one of the four "Olympic Zones" during the 2016 Summer Olympics. According to Riotur, the Tourism Secretariat of Rio de Janeiro, there are 63 hotels and 10 hostels in Copacabana.

Copacabana beach, located at the Atlantic shore, stretches from Posto Dois (lifeguard watchtower Two) to Posto Seis (lifeguard watchtower Six). Leme is at Posto Um (lifeguard watchtower One). There are historic forts at both ends of Copacabana beach; Fort Copacabana, built in 1914, is at the south end by Posto Seis and Fort Duque de Caxias, built in 1779, at the north end. One curiosity is that the lifeguard watchtower of Posto Seis never existed. Hotels, restaurants, bars, nightclubs and residential buildings dot the promenade facing Avenida Atlântica. On Sundays and holidays, one side of Avenida Atlântica is closed to cars, giving residents and tourists more space for activities along the beach. 

Copacabana Beach plays host to millions of revellers during the annual New Year's Eve celebrations, and in most years, has been the official venue of the FIFA Beach Soccer World Cup. 
The Copacabana promenade is a pavement landscape in large scale (4 kilometres long). It was rebuilt in 1970 and has used a black and white Portuguese pavement design since its origin in the 1930s: a geometric wave. The Copacabana promenade was designed by Roberto Burle Marx.

Copacabana has the 11th highest Human Development Index in Rio; the 2000 census put the HDI of Copacabana at 0.902.

According to the IBGE, 160,000 people live in Copacabana and 44,000 or 27.5% of them are 60 years old or older. Copacabana covers an area of 7.84 km² which gives the borough a population density of 20,400 people per km². Residential buildings eleven to thirteen stories high built next to each other dominate the borough. Houses and two-story buildings are rare.

When Rio was the capital of Brazil, Copacabana was considered one of the best neighborhoods in the country.

More than 40 different bus routes serve Copacabana, as do three subway Metro stations: Cantagalo, Siqueira Campos and Cardeal Arcoverde.

Three major arteries parallel to each other cut across the entire borough: Avenida Atlântica (Atlantic Avenue), which is a 6-lane, 4 km avenue by the beachside, Nossa Senhora de Copacabana Avenue and Barata Ribeiro/Raul Pompéia Street both of which are 4 lanes and 3.5 km in length. Barata Ribeiro Street changes its name to Raul Pompéia Street after the Sá Freire Alvim Tunnel. Twenty-four streets intersect all three major arteries, and seven other streets intersect some of the three.


The fireworks display in Rio de Janeiro to celebrate New Year's Eve is one of the largest in the world, lasting 15 to 20 minutes. It is estimated that 2 million people go to Copacabana Beach to see the spectacle. The festival also includes a concert that extends throughout the night. The celebration has become one of the biggest tourist attractions of Rio de Janeiro, attracting visitors from all over Brazil as well as from different parts of the world, and the city hotels generally stay fully booked.

New Year's Eve has been celebrated on Copacabana beach since the 1950s when cults of African origin such as Candomblé and Umbanda gathered in small groups dressed in white for ritual celebrations. The first fireworks display occurred in 1976, sponsored by a hotel on the waterfront and this has been repeated ever since. In the 1990s the city saw it as a great opportunity to promote the city and organized and expanded the event.

An assessment made during the New Year's Eve 1992 highlighted the risks associated with increasing crowd numbers on Copacabana beach after the fireworks display. Since the 1993-94 event concerts have been held on the beach to retain the public. The result was a success with egress spaced out over a period of 2 hours without the previous turmoil, although critics claimed that it denied the spirit of the New Year's tradition of a religious festival with fireworks by the sea. The following year Rod Stewart beat attendance records. Finally, the Tribute to Tom Jobim - with Gal Costa, Gilberto Gil, Caetano Veloso, Chico Buarque, and Paulinho da Viola - consolidated the shows at the Copacabana Réveillon.

There was a need to transform the fireworks display in a show of the same quality. The fireworks display was created by entrepreneurs Ricardo Amaral and Marius. From the previous 8–10 minutes the time was extended to 20 minutes and the quality and diversity of the fireworks was improved. A technical problem in fireworks 2000 required the use of ferries from New Year's Eve 2001-02. New Year's Eve has begun to compete with the Carnival, and since 1992 it has been a tourist attraction in its own right.



</doc>
<doc id="6725" url="https://en.wikipedia.org/wiki?curid=6725" title="Cy Young Award">
Cy Young Award

The Cy Young Award is given annually to the best pitchers in Major League Baseball (MLB), one each for the American League (AL) and National League (NL). The award was first introduced in 1956 by Baseball Commissioner Ford Frick in honor of Hall of Fame pitcher Cy Young, who died in 1955. The award was originally given to the single best pitcher in the major leagues, but in 1967, after the retirement of Frick, the award was given to one pitcher in each league.

Each league's award is voted on by members of the Baseball Writers' Association of America, with one representative from each team. As of the 2010 season, each voter places a vote for first, second, third, fourth and fifth place among the pitchers of each league. The formula used to calculate the final scores is a weighted sum of the votes. The pitcher with the highest score in each league wins the award. If two pitchers receive the same number of votes, the award is shared. The current formula started in the 2010 season. Before that, dating back to 1970, writers voted for three pitchers, with the formula of 5 points for a first place vote, 3 for a second place vote and 1 for a third place vote. Prior to 1970, writers only voted for the best pitcher and used a formula of one point per vote.

The Cy Young Award was first introduced in 1956 by Commissioner of Baseball Ford C. Frick in honor of Hall of Fame pitcher Cy Young, who died in 1955. The award would be given to pitchers only. Originally given to the single best pitcher in the major leagues, the award changed its format over time. From 1956 to 1966, the award was given to one pitcher in Major League Baseball. After Frick retired in 1967, William Eckert became the new Commissioner of Baseball. Due to fan requests, Eckert announced that the Cy Young Award would be given out both in the American League and the National League. From 1956 to 1958, a pitcher was not allowed to win the award on more than one occasion; this rule was eliminated in 1959. After a tie in the 1969 voting for the Cy Young Award, the process was changed, in which each writer was to vote for three different pitchers: the first-place vote received five points, the second-place vote received three points, and the third-place vote received one point.

The first recipient of the Cy Young Award was Don Newcombe of the Dodgers. In 1957, Warren Spahn became the first left-handed pitcher to win the award. In 1963, Sandy Koufax became the first pitcher to win the award in a unanimous vote; two years later he became the first multiple winner. In 1978, Gaylord Perry (age 40) became the oldest pitcher to receive the award, a record that stood until broken in 2004 by Roger Clemens (age 42). The youngest recipient was Dwight Gooden (age 20 in 1985). In 2012, R.A. Dickey became the first knuckleball pitcher to win the award.

In 1974, Mike Marshall won the award, becoming the first relief pitcher to win the award. In 1992, Dennis Eckersley was the first modern closer (first player to be used almost exclusively in ninth-inning situations) to win the award, and since then only one other relief pitcher has won the award, Éric Gagné in 2003 (also a closer). A total of nine relief pitchers have won the Cy Young Award across both leagues.

Steve Carlton in 1982 became the first pitcher to win more than three Cy Young Awards, while Greg Maddux in 1994 became the first to win at least three in a row (and received a fourth straight the following year), a feat later repeated by Randy Johnson.

Nineteen pitchers have won the award multiple times. Roger Clemens currently holds the record for the most awards won, with seven - his first and last wins separated by eighteen years. Greg Maddux (1992–1995) and Randy Johnson (1999–2002) share the record for the most consecutive awards won. Clemens, Johnson, Pedro Martínez, Gaylord Perry, Roy Halladay and Max Scherzer are the only pitchers to have won the award in both the American League and National League; Sandy Koufax is the only pitcher who won multiple awards during the period when only one award was presented for all of Major League Baseball. Roger Clemens was the youngest pitcher to win a second Cy Young Award, while Tim Lincecum is the youngest pitcher to do so in the National League and Clayton Kershaw is the youngest left-hander to do so. Clayton Kershaw is the youngest pitcher to win a third Cy Young Award.

Only four teams have never had a pitcher win the Cy Young Award. The Brooklyn/Los Angeles Dodgers have won more than any other team with 12.
There have been 17 players who unanimously won the Cy Young Award, for a total of 23 wins.

Five of these unanimous wins were accompanied with a win of the Most Valuable Player award (marked with * below; ** denotes that the player's unanimous win was accompanied with a unanimous win of the MVP).

In the National League, 11 players have unanimously won the Cy Young Award, for a total of 14 wins.

In the American League, 6 players have unanimously won the Cy Young Award, for a total of 9 wins.





</doc>
<doc id="6728" url="https://en.wikipedia.org/wiki?curid=6728" title="Antisemitism in Christianity">
Antisemitism in Christianity

Antisemitism in Christianity is the hostility of Christian Churches, Christian groups, and by Christians in general to Judaism and the Jewish people.

Christian rhetoric and antipathy towards Jews developed in the early years of Christianity and was reinforced by the belief that Jews had killed Christ and ever increasing anti-Jewish measures over the ensuing centuries. The action taken by Christians against Jews included acts of ostracism, humiliation, violence, and murder, culminating in the Holocaust.

Christian antisemitism has been attributed to numerous factors including theological differences, competition between Church and Synagogue, the Christian drive for converts, decreed by the Great Commission, misunderstanding of Jewish beliefs and practices, and a perceived Jewish hostility toward Christians. These attitudes were reinforced in Christian preaching, art and popular teaching for two millennia, containing contempt for Jews, as well as statutes which were designed to humiliate and stigmatise Jews.

Modern antisemitism has been described as primarily hatred against Jews as a race with its modern expression rooted in 18th-century racial theories, while anti-Judaism is described as hostility to Jewish religion, but in Western Christianity it effectively merged into antisemitism during the 12th century. Scholars have debated how Christian antisemitism played a role in the Nazi Third Reich, World War II and the Holocaust. The Holocaust has driven many within Christianity to reflect on the relationship between Christian theology, practices, and that genocide.

Christianity and Judaism differed in their legal status in the Roman Empire: Judaism, restricted to the Jewish people and Jewish proselytes, was generally exempt from obligation to the Roman imperial cult and since the reign of Julius Caesar enjoyed the status of a "licit religion", though there were also occasional persecutions, for example in 19 Tiberius expelled the Jews from Rome, as Claudius did again in 49. Christianity however was not restricted to one people, and as Jewish Christians were excluded from the synagogue (see Council of Jamnia), they also lost the protection of the status of Judaism, though said "protection" did have its limits (see for example Titus Flavius Clemens (consul), Rabbi Akiva, and Ten Martyrs).

From the reign of Nero onwards, who is said by Tacitus to have blamed the Great Fire of Rome on Christians, Christianity was considered to be illegal and Christians were frequently subjected to persecution, differing regionally. Comparably, Judaism suffered the setbacks of the Jewish-Roman wars, remembered in the legacy of the Ten Martyrs. Robin Lane Fox traces the origin of much later hostility to the period of persecution, where the commonest test by the authorities of a suspected Christian was to require homage to be paid to the deified emperor. Jews were exempt from this requirement as long as they paid the Fiscus Judaicus, and Christians (many or mostly of Jewish origins) would say they were Jewish but refused to pay the tax. This had to be confirmed by the local Jewish authorities, who were likely to refuse to accept the Christians as Jewish, often leading to their execution. The Birkat haMinim was often brought forward as support for this charge that the Jews were responsible for the Persecution of Christians in the Roman Empire. In the 3rd century systematic persecution of Christians began and lasted until Constantine's conversion to Christianity. In 390 Theodosius I made Christianity the state church of the Roman Empire. While pagan cults and Manichaeism were suppressed, Judaism retained its legal status as a licit religion, though anti-Jewish violence still occurred. In the 5th century, some legal measures worsened the status of the Jews in the Roman Empire (now more properly called the Byzantine Empire since relocating to Constantinople).

Another point of contention for Christians concerning Judaism, which according to the modern KJV of the Protestant Bible, is attributed more to a religious bias, rather than an issue of race or being a "Semite". Paul (a Benjamite Hebrew) clarifies this point in the letter to the Galatians where he makes plain his declaration ″28There is neither Jew nor Greek, there is neither bond nor free, there is neither male nor female: for ye are all one in Christ Jesus. 29And if ye be Christ's, then are ye Abraham's seed, and heirs according to the promise.″ Further Paul states: ″15Brethren, I speak after the manner of men; Though it be but a man's covenant, yet if it be confirmed, no man disannulleth, or addeth thereto. 16Now to Abraham and his seed were the promises made. He saith not, And to seeds, as of many; but as of one, And to thy seed, which is Christ.″ Many misled Christians read Matthew 23, John 8:44, Revelations 2:9, 3:9, and wrongly believe the term "Jew" is meaning a Hebrew or Semite...it does not, but rather to the religious belief of Judaism. A perfect real life example of this clarification is "Brother Nathanael" a Christian Hebrew. 

Jesus is rejected in Judaism as a failed Jewish Messiah claimant and a false prophet. Belief in the divinity of any human being is incompatible with Judaism: However, since the traditional Jewish belief is that the messiah has not yet come and the Messianic Age is not yet present, the total rejection of Jesus as either messiah or deity has never been a central issue for Judaism.

If Christianity had come from Judaism and Jesus was a Jew, how and why did the Jews reject such core elements of Christianity? The most widely held explanation, believed by most Christians for early history, was that Jews had been the chosen people but broke the covenant with God, and were condemned to spend the rest of eternity suffering on earth, a symbol of degradation and sub-humanity. According to Jews, however, it is because Jesus failed to meet various messianic prophesies. It was because of this belief that many Medieval European rulers protected the Jews, but considered them property of the king.

Many New Testament passages criticise the Pharisees and it has been argued that these passages have shaped the way that Christians viewed Jews. Like most Bible passages, however, they can be and have been interpreted in a variety of ways.

Mainstream Talmudic Rabbinical Judaism today directly descends from the Pharisees whom Jesus often criticized. During Jesus' life and at the time of his execution, the Pharisees were only one of several Jewish groups such as the Sadducees, Zealots, and Essenes who mostly died out not long after the period; indeed, Jewish scholars such as Harvey Falk and Hyam Maccoby have suggested that Jesus was himself a Pharisee. Arguments by Jesus and his disciples against the Pharisees and what he saw as their hypocrisy were most likely examples of disputes among Jews and internal to Judaism that were common at the time, see for example Hillel and Shammai.

Professor Lillian C. Freudmann, author of "Antisemitism in the New Testament" (University Press of America, 1994) has published a detailed study of the description of Jews in the New Testament, and the historical effects that such passages have had in the Christian community throughout history. Similar studies of such verses have been made by both Christian and Jewish scholars, including, Professors Clark Williamsom (Christian Theological Seminary), Hyam Maccoby (The Leo Baeck Institute), Norman A. Beck (Texas Lutheran College), and Michael Berenbaum (Georgetown University). Most rabbis feel that these verses are antisemitic, and many Christian scholars, in America and Europe, have reached the same conclusion. Another example is John Dominic Crossan's 1995 "book titled Who Killed Jesus? Exposing the Roots of Anti-Semitism in the Gospel Story of the Death of Jesus".

Some biblical scholars have also been accused of holding antisemitic beliefs. Bruce J. Malina, a founding member of The Context Group, has come under criticism for going as far as to deny the Semitic ancestry of modern Israelis. He then ties this back to his work on first century cultural anthropology.

After Paul's death, Christianity emerged as a separate religion, and Pauline Christianity emerged as the dominant form of Christianity, especially after Paul, James and the other apostles agreed on a compromise set of requirements. Some Christians continued to adhere to aspects of Jewish law, but they were few in number and often considered heretics by the Church. One example is the Ebionites, who seem to have denied the virgin birth of Jesus, the physical Resurrection of Jesus, and most of the books that were later canonized as the New Testament. For example, the Ethiopian Orthodox still continue Old Testament practices such as the Sabbath. As late as the 4th century Church Father John Chrysostom complained that some Christians were still attending Jewish synagogues.

The Church Fathers identified Jews and Judaism with heresy and declared the people of Israel to be "extra Deum" (lat. "outside of God"). Saint Peter of Antioch referred to Christians that refused to worship religious images as having "Jewish minds". In the early second century AD, the heretic Marcion of Sinope ( 85 – 160 AD) declared that the Jewish God was a different God, inferior to the Christian one, and rejected the Jewish scriptures as the product of a lesser deity. Marcion's teachings, which were extremely popular, rejected Judaism not only as an incomplete revelation, but as a false one as well, but, at the same time, allowed less blame to be placed on the Jews personally for having not recognized Jesus, since, in Marcion's worldview, Jesus was not sent by the lesser Jewish God, but by the supreme Christian God, whom the Jews had no reason to recognize.

In combating Marcion, orthodox apologists conceded that Judaism was an incomplete and inferior religion to Christianity, while also defending the Jewish scriptures as canonical. The Church Father Tertullian ( 155 – 240 AD) had a particularly intense personal dislike towards the Jews and argued that the Gentiles had been chosen by God to replace the Jews, because they were worthier and more honorable. Origen of Alexandria ( 184 – 253) was more knowledgeable about Judaism than any of the other Church Fathers, having studied Hebrew, met Rabbi Hillel the Younger, consulted and debated with Jewish scholars, and been influenced by the allegorical interpretations of Philo of Alexandria. Origen defended the canonicity of the Old Testament and defended Jews of the past as having been chosen by God for their merits. Nonetheless, he condemned contemporary Jews for not understanding their own Law, insisted that Christians were the "true Israel", and blamed the Jews for the death of Christ. He did, however, maintain that Jews would eventually attain salvation in the final "apocatastasis".

Patristic bishops of the patristic era such as Augustine argued that the Jews should be left alive and suffering as a perpetual reminder of their murder of Christ. Like his anti-Jewish teacher, Ambrose of Milan, he defined Jews as a special subset of those damned to hell. As "Witness People", he sanctified collective punishment for the Jewish deicide and enslavement of Jews to Catholics: "Not by bodily death, shall the ungodly race of carnal Jews perish ... 'Scatter them abroad, take away their strength. And bring them down O Lord. Augustine claimed to "love" the Jews but as a means to convert them to Christianity. Sometimes he identified all Jews with the evil Judas and developed the doctrine (together with St. Cyprian) that there was "no salvation outside the Church".

Other Church Fathers, such as John Chrysostom, went further in their condemnation. The Catholic editor Paul Harkins wrote that St. John Chrysostom's anti-Jewish theology "is no longer tenable (..) For these objectively unchristian acts he cannot be excused, even if he is the product of his times." John Chrysostom held, as most Church Fathers did, that the sins of all Jews were communal and endless, to him his Jewish neighbours were the collective representation of all alleged crimes of all preexisting Jews. All Church Fathers applied the passages of the New Testament concerning the alleged advocation of the crucifixion of Christ to all Jews of his day, the Jews were the ultimate evil. However, John Chrysostom went so far to say that because Jews rejected the Christian God in human flesh, Christ, they therefore deserved to be killed: "grew fit for slaughter." In citing the New Testament, he claimed that Jesus was speaking about Jews when he said, "as for these enemies of mine who did not want me to reign over them, bring them here and "slay them" before me."

St. Jerome identified Jews with Judas Iscariot and the immoral use of money ("Judas is cursed, that in Judas the Jews may be accursed... their prayers turn into sins"). Jerome's homiletical assaults, that may have served as the basis for the anti-Jewish Good Friday liturgy, contrasts Jews with the evil, and that "the ceremonies of the Jews are harmful and deadly to Christians", whoever keeps them was doomed to the devil: "My enemies are the Jews; they have conspired in hatred against Me, crucified Me, heaped evils of all kinds upon Me, blasphemed Me."

Ephraim the Syrian wrote polemics against Jews in the 4th century, including the repeated accusation that Satan dwells among them as a partner. The writings were directed at Christians who were being proselytized by Jews. Ephraim feared that they were slipping back into Judaism; thus, he portrayed the Jews as enemies of Christianity, like Satan, to emphasize the contrast between the two religions, namely, that Christianity was Godly and true and Judaism was Satanic and false. Like John Chrysostom, his objective was to dissuade Christians from reverting to Judaism by emphasizing what he saw as the wickedness of the Jews and their religion.

However, there are also positive remarks from the Church Fathers on the issue, such as Eusebius of Caesarea (circa. 263-340 A.D) in his Ecclesiastical History, who said, "The race of the Hebrews is not new, but is honoured among all men for its antiquity and is itself well known to all."

Bernard of Clairvaux (1090-1153), a Doctor of the Catholic Church, said "For us the Jews are Scripture's living words, because they remind us of what Our Lord suffered. They are not to be persecuted, killed, or even put to flight."

Jews were subject to a wide range of legal disabilities and restrictions in Medieval Europe. Jews were excluded from many trades, the occupations varying with place and time, and determined by the influence of various non-Jewish competing interests. Often Jews were barred from all occupations but money-lending and peddling, with even these at times forbidden. Jews' association to money lending would carry on throughout history in the stereotype of Jews being greedy and perpetuating capitalism.

In the later medieval period, the number of Jews permitted to reside in certain places was limited; they were concentrated in ghettos, and were not allowed to own land; they were subject to discriminatory taxes on entering cities or districts other than their own,
The Oath More Judaico, the form of oath required from Jewish witnesses, in some places developed bizarre or humiliating forms, e.g. in Swabian law of the 13th century, the Jew would be required to stand on the hide of a sow or a bloody lamb.

The Fourth Lateran Council in 1215 was the first to proclaim the requirement for Jews to wear something that distinguished them as Jews (and Muslims the same).
On many occasions, Jews were accused of a blood libel, the supposed drinking of blood of Christian children in mockery of the Christian Eucharist.

"Sicut Judaeis" (the "Constitution for the Jews") was the official position of the papacy regarding Jews throughout the Middle Ages and later. The first bull was issued in about 1120 by Calixtus II, intended to protect Jews who suffered during the First Crusade, and was reaffirmed by many popes, even until the 15th century although they were not always strictly upheld.

The bull forbade, besides other things, Christians from coercing Jews to convert, or to harm them, or to take their property, or to disturb the celebration of their festivals, or to interfere with their cemeteries, on pain of excommunication.

Antisemitism in popular European Christian culture escalated beginning in the 13th century. Blood libels and host desecration drew popular attention and led to many cases of persecution against Jews. Many believed Jews poisoned wells to cause plagues. In the case of blood libel it was widely believed that the Jews would kill a child before Easter and needed Christian blood to bake matzo.Throughout history if a Christian child was murdered accusations of blood libel would arise no matter how small the Jewish population. The Church often added to the fire by portraying the dead child as a martyr who had been tortured and child had powers like Jesus was believed to. Sometimes the children were even made into Saints. Antisemitic imagery such as Judensau and Ecclesia et Synagoga recurred in Christian art and architecture.

In Iceland, one of the hymns repeated in the days leading up to Easter includes the lines,

During the Middle Ages in Europe persecutions and formal expulsions of Jews were liable to occur at intervals, although it should be said that this was also the case for other minority communities, regardless of whether they were religious or ethnic. There were particular outbursts of riotous persecution during the Rhineland massacres of 1096 in Germany accompanying the lead-up to the First Crusade, many involving the crusaders as they travelled to the East. There were many local expulsions from cities by local rulers and city councils. In Germany the Holy Roman Emperor generally tried to restrain persecution, if only for economic reasons, but he was often unable to exert much influence. In the Edict of Expulsion, King Edward I expelled all the Jews from England in 1290 (only after ransoming some 3,000 among the most wealthy of them), on the accusation of usury and undermining loyalty to the dynasty. In 1306 there was a wave of persecution in France, and there were widespread Black Death Jewish persecutions as the Jews were blamed by many Christians for the plague, or spreading it. As late as 1519, the Imperial city of Regensburg took advantage of the recent death of Emperor Maximilian I to expel its 500 Jews.

The largest expulsion of Jews followed the Reconquista or the reunification of Spain, and it preceded the expulsion of the Muslims who would not convert, whose religious rights were protected by the Treaty of Granada (1491). On 31 March 1492 Ferdinand II of Aragon and Isabella I of Castile, the rulers of Spain who financed Christopher Columbus' voyage to the New World just a few months later in 1492, declared that all Jews in their territories should either convert to Christianity or leave the country. While some converted, many others left for Portugal, France, Italy (including the Papal States), Netherlands, Poland, the Ottoman Empire, and North Africa. Many of those who had fled to Portugal were later expelled by King Manuel in 1497 or left to avoid forced conversion and persecution.

On 14 July 1555, Pope Paul IV issued papal bull Cum nimis absurdum which revoked all the rights of the Jewish community and placed religious and economic restrictions on Jews in the Papal States, renewed anti-Jewish legislation and subjected Jews to various degradations and restrictions on their personal freedom.

The bull established the Roman Ghetto and required Jews of Rome, which had existed as a community since before Christian times and which numbered about 2,000 at the time, to live in it. The Ghetto was a walled quarter with three gates that were locked at night. Jews were also restricted to one synagogue per city.

Paul IV's successor, Pope Pius IV, enforced the creation of other ghettos in most Italian towns, and his successor, Pope Pius V, recommended them to other bordering states.

Martin Luther at first made overtures towards the Jews, believing that the "evils" of Catholicism had prevented their conversion to Christianity. When his call to convert to his version of Christianity was unsuccessful, he became hostile to them.

In his book "On the Jews and Their Lies", Luther excoriates them as "venomous beasts, vipers, disgusting scum, canders, devils incarnate." He provided detailed recommendations for a pogrom against them, calling for their permanent oppression and expulsion, writing "Their private houses must be destroyed and devastated, they could be lodged in stables. Let the magistrates burn their synagogues and let whatever escapes be covered with sand and mud. Let them be forced to work, and if this avails nothing, we will be compelled to expel them like dogs in order not to expose ourselves to incurring divine wrath and eternal damnation from the Jews and their lies." At one point he wrote: "...we are at fault in not slaying them..." a passage that "may be termed the first work of modern antisemitism, and a giant step forward on the road to the Holocaust."

Luther's harsh comments about the Jews are seen by many as a continuation of medieval Christian antisemitism. In his final sermon shortly before his death, however, Luther preached: "We want to treat them with Christian love and to pray for them, so that they might become converted and would receive the Lord."

In accordance with the anti-Jewish precepts of the Russian Orthodox Church, Russia's discriminatory policies towards Jews intensified when the partition of Poland in the 18th century resulted, for the first time in Russian history, in the possession of land with a large Jewish population. This land was designated as the Pale of Settlement from which Jews were forbidden to migrate into the interior of Russia. In 1772 Catherine II, the empress of Russia, forced the Jews living in the Pale of Settlement to stay in their "shtetls" and forbade them from returning to the towns that they occupied before the partition of Poland.

Throughout the 19th century and into the 20th, the Roman Catholic Church still incorporated strong antisemitic elements, despite increasing attempts to separate anti-Judaism (opposition to the Jewish religion on religious grounds) and racial antisemitism. Brown University historian David Kertzer, working from the Vatican archive, has argued in his book "The Popes Against the Jews" that in the 19th and early 20th centuries the Roman Catholic Church adhered to a distinction between "good antisemitism" and "bad antisemitism". The "bad" kind promoted hatred of Jews because of their descent. This was considered un-Christian because the Christian message was intended for all of humanity regardless of ethnicity; anyone could become a Christian. The "good" kind criticized alleged Jewish conspiracies to control newspapers, banks, and other institutions, to care only about accumulation of wealth, etc. Many Catholic bishops wrote articles criticizing Jews on such grounds, and, when they were accused of promoting hatred of Jews, they would remind people that they condemned the "bad" kind of antisemitism. Kertzer's work is not without critics. Scholar of Jewish-Christian relations Rabbi David G. Dalin, for example, criticized Kertzer in the "Weekly Standard" for using evidence selectively.

The counter-revolutionary Catholic royalist Louis de Bonald stands out among the earliest figures to explicitly call for the reversal of Jewish emancipation in the wake of the French Revolution. Bonald's attacks on the Jews are likely to have influenced Napoleon's decision to limit the civil rights of Alsatian Jews. Bonald's article "Sur les juifs" (1806) was one of the most venomous screeds of its era and furnished a paradigm which combined anti-liberalism, a defense of a rural society, traditional Christian antisemitism, and the identification of Jews with bankers and finance capital, which would in turn influence many subsequent right-wing reactionaries such as Roger Gougenot des Mousseaux, Charles Maurras, and Édouard Drumont, nationalists such as Maurice Barrès and Paolo Orano, and antisemitic socialists such as Alphonse Toussenel. Bonald furthermore declared that the Jews were an "alien" people, a "state within a state", and should be forced to wear a distinctive mark to more easily identify and discriminate against them.

In the 1840s, the popular counter-revolutionary Catholic journalist Louis Veuillot propagated Bonald's arguments against the Jewish "financial aristocracy" along with vicious attacks against the Talmud and the Jews as a "deicidal people" driven by hatred to "enslave" Christians. Gougenot des Mousseaux's "Le Juif, le judaïsme et la judaïsation des peuples chrétiens" (1869) has been called a "Bible of modern antisemitism" and was translated into German by Nazi ideologue Alfred Rosenberg. Between 1882 and 1886 alone, French priests published twenty antisemitic books blaming France's ills on the Jews and urging the government to consign them back to the ghettos, expel them, or hang them from the gallows. 

In Italy the Jesuit priest Antonio Bresciani's highly popular novel 1850 novel "L'Ebreo di Verona" ("The Jew of Verona") shaped religious anti-Semitism for decades, as did his work for "La Civiltà Cattolica", which he helped launch.

Pope Pius VII (1800–1823) had the walls of the Jewish ghetto in Rome rebuilt after the Jews were emancipated by Napoleon, and Jews were restricted to the ghetto through the end of the Papal States in 1870. Official Catholic organizations, such as the Jesuits, banned candidates "who are descended from the Jewish race unless it is clear that their father, grandfather, and great-grandfather have belonged to the Catholic Church" until 1946.

In Russia, under the Tsarist regime, antisemitism intensified in the early years of the 20th century and was given official favour when the secret police forged the notorious "Protocols of the Elders of Zion", a document purported to be a transcription of a plan by Jewish elders to achieve global domination. Violence against the Jews in the Kishinev pogrom in 1903 was continued after the 1905 revolution by the activities of the Black Hundreds. The Beilis Trial of 1913 showed that it was possible to revive the blood libel accusation in Russia.

Catholic writers such as Ernest Jouin, who published the "Protocols" in French, seamlessly blended racial and religious anti-Semitism, as in his statement that "from the triple viewpoint of race, of nationality, and of religion, the Jew has become the enemy of humanity." Pope Pius XI praised Jouin for "combating our mortal [Jewish] enemy" and appointed him to high papal office as a protonotary apostolic.

In 1916, in the midst of the First World War, American Jews petitioned Pope Benedict XV on behalf of the Polish Jews.

On April 26, 1933 Hitler declared during a meeting with Roman Catholic Bishop of Osnabrück:

“I have been attacked because of my handling of the Jewish question. The Catholic Church considered the Jews pestilent for fifteen hundred years, put them in ghettos, etc., because it recognized the Jews for what they were. In the epoch of liberalism the danger was no longer recognized. I am moving back toward the time in which a fifteen-hundred-year-long tradition was implemented. I do not set race over religion, but I recognize the representatives of this race as pestilent for the state and for the Church, and perhaps I am thereby doing Christianity a great service by pushing them out of schools and public functions.”
The transcript of this discussion contains no response by Bishop Berning. Martin Rhonheimer does not consider this unusual since, in his opinion, for a Catholic Bishop in 1933 there was nothing particularly objectionable "in this historically correct reminder".

The Nazis used Martin Luther's book, "On the Jews and Their Lies" (1543), to claim a moral righteousness for their ideology. Luther even went so far as to advocate the murder of those Jews who refused to convert to Christianity, writing that "we are at fault in not slaying them".

Archbishop Robert Runcie has asserted that: "Without centuries of Christian antisemitism, Hitler's passionate hatred would never have been so fervently echoed...because for centuries Christians have held Jews collectively responsible for the death of Jesus. On Good Friday Jews, have in times past, cowered behind locked doors with fear of a Christian mob seeking 'revenge' for deicide. Without the poisoning of Christian minds through the centuries, the Holocaust is unthinkable." The dissident Catholic priest Hans Küng has written that "Nazi anti-Judaism was the work of godless, anti-Christian criminals. But it would not have been possible without the almost two thousand years' pre-history of 'Christian' anti-Judaism..."

The document Dabru Emet was issued by over 220 rabbis and intellectuals from all branches of Judaism in 2000 as a statement about Jewish-Christian relations. This document states,
"Nazism was not a Christian phenomenon. Without the long history of Christian anti-Judaism and Christian violence against Jews, Nazi ideology could not have taken hold nor could it have been carried out. Too many Christians participated in, or were sympathetic to, Nazi atrocities against Jews. Other Christians did not protest sufficiently against these atrocities. But Nazism itself was not an inevitable outcome of Christianity."
According to American historian Lucy Dawidowicz, antisemitism has a long history within Christianity. The line of "antisemitic descent" from Luther, the author of "On the Jews and Their Lies", to Hitler is "easy to draw." In her "The War Against the Jews, 1933-1945", she contends that Luther and Hitler were obsessed by the "demonologized universe" inhabited by Jews. Dawidowicz writes that the similarities between Luther's anti-Jewish writings and modern antisemitism are no coincidence, because they derived from a common history of "Judenhass", which can be traced to Haman's advice to Ahasuerus. Although modern German antisemitism also has its roots in German nationalism and the liberal revolution of 1848, Christian antisemitism she writes is a foundation that was laid by the Roman Catholic Church and "upon which Luther built."


The Confessing Church was, in 1934, the first Christian opposition group. The Catholic Church officially condemned the Nazi theory of racism in Germany in 1937 with the encyclical ""Mit brennender Sorge"", signed by Pope Pius XI, and Cardinal Michael von Faulhaber led the Catholic opposition, preaching against racism.

Many individual Christian clergy and laypeople of all denominations had to pay for their opposition with their life, including:

By the 1940s fewer Christians were willing to oppose Nazi policy publicly, but many secretly helped save the lives of Jews. There are many sections of Israel's Holocaust Remembrance Museum, Yad Vashem, dedicated to honoring these "Righteous Among the Nations".

Before becoming Pope, Cardinal Pacelli addressed the International Eucharistic Congress in Budapest on 25–30 May 1938 during which he made reference to the Jews "whose lips curse [Christ] and whose hearts reject him even today"; at this time antisemitic laws were in the process of being formulated in Hungary.

The 1937 encyclical "Mit brennender Sorge" was issued by Pope Pius XI, but drafted by the future Pope Pius XII and read from the pulpits of all German Catholic churches, it condemned Nazi ideology and has been characterized by scholars as the "first great official public document to dare to confront and criticize Nazism" and "one of the greatest such condemnations ever issued by the Vatican."

In the summer of 1942, Pius explained to his college of Cardinals the reasons for the great gulf that existed between Jews and Christians at the theological level: ""Jerusalem has responded to His call and to His grace with the same rigid blindness and stubborn ingratitude that has led it along the path of guilt to the murder of God."" Historian Guido Knopp describes these comments of Pius as being ""incomprehensible"" at a time when ""Jerusalem was being murdered by the million"". This traditional adversarial relationship with Judaism would be reversed in "Nostra aetate" issued during the Second Vatican Council.

Prominent members of the Jewish community have contradicted the criticisms of Pius and spoke highly of his efforts to protect Jews. The Israeli historian Pinchas Lapide interviewed war survivors and concluded that Pius XII "was instrumental in saving at least 700,000, but probably as many as 860,000 Jews from certain death at Nazi hands". Some historians dispute this estimate.

The Christian Identity movement, the Ku Klux Klan and other White supremacist groups have expressed antisemitic views. They claim that their antisemitism is based on purported Jewish control of the media, international banks, radical left-wing politics, and the Jews' promotion of multiculturalism, anti-Christian groups, liberalism and perverse organizations. They rebuke charges of racism by claiming that Jews who share their ideology maintain membership in their organizations. A racial belief which is common among these groups, but not universal among them, is an alternative history doctrine, which is sometimes called British Israelism. In some of its forms, this doctrine absolutely denies the view that modern Jews have any racial connection to the Israel of the Bible. Instead, according to extreme forms of this doctrine, the true racial Israelites and the true humans are the members of the Adamic (white) race. These groups are often rejected and not even considered Christian groups by mainstream Christian denominations and the vast majority of Christians around the world.

Antisemitism in Europe remains a substantial problem. Antisemitism exists to a lesser or greater degree in many other nations as well, including Eastern Europe, the former Soviet Union, and the increasingly frequent tensions between some Muslim immigrants and Jews across Europe. The US State Department reports that antisemitism has increased dramatically in Europe and Eurasia since 2000.

While in a decline since the 1940s, there is still a measurable amount of antisemitism in the United States of America as well, although acts of violence are rare. For example, the influential evangelical preacher Billy Graham and then-president Richard Nixon were captured on tape in the early 1970s discussing matters like how to address the Jews' control of the American media. This belief in Jewish conspiracies and domination of the media was similar to those of Graham's former mentors: William Bell Riley chose Graham to succeed him as the second president of Northwestern Bible and Missionary Training School and evangelist Mordecai Ham led the meetings where Graham first believed in Christ. Both held strongly antisemitic views. The 2001 survey by the Anti-Defamation League reported 1432 acts of antisemitism in the United States that year. The figure included 877 acts of harassment, including verbal intimidation, threats and physical assaults. A minority of American churches engage in anti-Israel activism, including support for the controversial BDS (Boycott, Divestment and Sanctions) movement. While not directly indicative of anti-semitism, this activism often conflates the Israeli government's treatment of Palestinians with that of Jesus, thereby promoting the anti-semitic doctrine of Jewish guilt. Many Christian Zionists are also accused of anti-semitism, such as John Hagee, who argued that the Jews brought the Holocaust upon themselves by angering God.

Many Christians do not consider anti-Judaism to be antisemitism. They regard anti-Judaism as a disagreement of religiously sincere people with the tenets of Judaism, while regarding antisemitism as an emotional bias or hatred not specifically targeting the religion of Judaism. Under this approach, anti-Judaism is not regarded as antisemitism as it only rejects the religious ideas of Judaism and does not involve actual hostility to the Jewish people.

Others see anti-Judaism as the rejection of or opposition to beliefs and practices "essentially because" of their source in Judaism or because a belief or practice is associated with the Jewish people. (But see supersessionism)

The position that "Christian theological anti-Judaism is a phenomenon distinct from modern antisemitism, which is rooted in economic and racial thought, so that Christian teachings should not be held responsible for antisemitism" has been articulated, among other places, by Pope John Paul II in 'We Remember: A Reflection on the Shoah,' and the Jewish declaration on Christianity, Dabru Emet. Several scholars, including Susannah Heschel, Gavin I Langmuir and Uriel Tal have challenged this position, arguing that anti-Judaism led directly to modern antisemitism.

Although some Christians in the past did consider anti-Judaism to be contrary to Christian teaching, this view was not widely expressed by leaders and lay people. In many cases, the practical tolerance towards the Jewish religion and Jews prevailed. Some Christian groups, particularly in early years, condemned verbal anti-Judaism.

The Southern Baptist Convention (SBC), the largest Protestant Christian denomination in the U.S., has explicitly rejected suggestions that it should back away from seeking to convert Jews, a position which critics have called antisemitic, but a position which Baptists believe is consistent with their view that salvation is found solely through faith in Christ. In 1996 the SBC approved a resolution calling for efforts to seek the conversion of Jews "as well as for the salvation of 'every kindred and tongue and people and nation.'"

Most Evangelicals agree with the SBC's position, and some support efforts which specifically seek the Jews' conversion. At the same time these Evangelical groups are among the most pro-Israel groups. ("For more information, see Christian Zionism".) Among the controversial groups that have found support from some Evangelical churches is Jews for Jesus, which claims that Jews can "complete" their Jewish faith by accepting Jesus as the Messiah.

The Presbyterian Church (USA), the United Methodist Church, and the United Church of Canada have ended their efforts to convert Jews. While Anglicans do not, as a rule, seek converts from other Christian denominations, the General Synod has affirmed that "the good news of salvation in Jesus Christ is for all and must be shared with all including people from other faiths or of no faith and that to do anything else would be to institutionalize discrimination".

The Roman Catholic Church formerly had religious congregations specifically aimed at the conversion of Jews. Some of these were founded by Jewish converts themselves, like the Congregation of Our Lady of Sion, which was composed of nuns and ordained priests. Many Catholic saints were noted specifically because of their missionary zeal in converting Jews, such as Vincent Ferrer. After the Second Vatican Council many missionary orders aimed at converting Jews to Christianity no longer actively sought to missionize (or proselytize) among Jews. Traditionalist Roman Catholic groups, congregations and clergymen, however, continue to support missionizing Jews according to traditional patterns, sometimes with success ("e.g.", the Society of St. Pius X which has notable Jewish converts among its faithful, many of whom have become traditionalist priests).

Some Jewish organizations have described evangelism and missionary activity directed specifically at Jews as antisemitic.

In recent years there has been much to note in the way of reconciliation between some Christian groups and the Jews.



</doc>
<doc id="6731" url="https://en.wikipedia.org/wiki?curid=6731" title="Boeing C-17 Globemaster III">
Boeing C-17 Globemaster III

The Boeing C-17 Globemaster III is a large military transport aircraft. It was developed for the United States Air Force (USAF) from the 1980s to the early 1990s by McDonnell Douglas. The C-17 carries forward the name of two previous piston-engined military cargo aircraft, the Douglas C-74 Globemaster and the Douglas C-124 Globemaster II. The C-17 commonly performs tactical and strategic airlift missions, transporting troops and cargo throughout the world; additional roles include medical evacuation and airdrop duties. It was designed to replace the Lockheed C-141 Starlifter, and also fulfill some of the duties of the Lockheed C-5 Galaxy, freeing the C-5 fleet for outsize cargo.

Boeing, which merged with McDonnell Douglas in 1997, continued to manufacture C-17s for export customers following the end of deliveries to the U.S. Air Force. Aside from the United States, the C-17 is in service with the United Kingdom, Australia, Canada, Qatar, United Arab Emirates, NATO Heavy Airlift Wing, India, and Kuwait. The final C-17 was completed at the Long Beach, California plant and flown on 29 November 2015.

In the 1970s, the U.S. Air Force began looking for a replacement for its Lockheed C-130 Hercules tactical cargo aircraft. The Advanced Medium STOL Transport (AMST) competition was held, with Boeing proposing the YC-14, and McDonnell Douglas proposing the YC-15. Though both entrants exceeded specified requirements, the AMST competition was canceled before a winner was selected. The Air Force started the C-X program in November 1979 to develop a larger AMST with longer range to augment its strategic airlift.
By 1980, the USAF found itself with a large fleet of aging C-141 Starlifter cargo aircraft. Compounding matters, USAF needed increased strategic airlift capabilities to fulfill its rapid-deployment airlift requirements. The USAF set mission requirements and released a request for proposals (RFP) for C-X in October 1980. McDonnell Douglas elected to develop a new aircraft based on the YC-15. Boeing bid an enlarged three-engine version of its AMST YC-14. Lockheed submitted two designs, a C-5-based design and an enlarged C-141 design. On 28 August 1981, McDonnell Douglas was chosen to build its proposed aircraft, then designated "C-17". Compared to the YC-15, the new aircraft differed in having swept wings, increased size, and more powerful engines. This would allow it to perform the work done by the C-141, and to fulfill some of the duties of the Lockheed C-5 Galaxy, freeing the C-5 fleet for outsize cargo.

Alternative proposals were pursued to fill airlift needs after the C-X contest. These were lengthening of C-141As into C-141Bs, ordering more C-5s, continued purchases of KC-10s, and expansion of the Civil Reserve Air Fleet. Limited budgets reduced program funding, requiring a delay of four years. During this time contracts were awarded for preliminary design work and for the completion of engine certification. In December 1985, a full-scale development contract was awarded, under Program Manager Bob Clepper. At this time, first flight was planned for 1990. The Air Force had formed a requirement for 210 aircraft.

Development problems and limited funding caused delays in the late 1980s. Criticisms were made of the developing aircraft and questions were raised about more cost-effective alternatives during this time. In April 1990, Secretary of Defense Dick Cheney reduced the order from 210 to 120 aircraft. The maiden flight of the C-17 took place on 15 September 1991 from the McDonnell Douglas's plant in Long Beach, California, about a year behind schedule. The first aircraft (T-1) and five more production models (P1-P5) participated in extensive flight testing and evaluation at Edwards Air Force Base. Two complete airframes were built for static and repeated load testing.

A static test of the C-17 wing in October 1992 resulted in the wing failing at 128% of design limit load, which was below the 150% requirement. Both wings buckled rear to the front and failures occurred in stringers, spars and ribs. Some $100 million were spent to redesign the wing structure; the wing failed at 145% during a second test in September 1993. A careful review of the test data, however, showed that the wing was not loaded correctly and did indeed meet the requirement. The C-17 received the "Globemaster III" name in early 1993. In late 1993, the Department of Defense gave the contractor two years to solve production and cost overrun problems or face termination of the contract after the delivery of the 40th aircraft. By accepting the 1993 terms, McDonnell Douglas incurred a loss of nearly US$1.5 billion on the development phase of the program.

In April 1994, the C-17 program remained over budget, and did not meet weight, fuel burn, payload and range specifications. It failed several key criteria during airworthiness evaluation tests. Technical problems were found with the mission software, landing gear, and other areas. In May 1994, it was proposed to cut production to as few as 32 aircraft; these cuts were later rescinded. A July 1994 GAO report revealed that Air Force and DoD studies from 1986 and 1991 stated the C-17 could use 6,400 more runways outside the U.S. than the C-5; it was discovered that these studies only considered runway dimensions, but not runway strength or Load Classification Numbers (LCN). The C-5 has a lower LCN, but the USAF classifies both in the same broad Load Classification Group. When considering runway dimensions and load ratings, the C-17's worldwide runway advantage over the C-5 shrank from 6,400 to 911 airfields. The report also stated that "current military doctrine that does not reflect the use of small, austere airfields". So the C-17's short field capability was not yet considered.

A January 1995 GAO report stated that the USAF originally planned to order 210 C-17s at a cost of $41.8 billion, and that the 120 aircraft on order were to cost $39.5 billion based on a 1992 estimate. In March 1994, the U.S. Army decided it did not need the low-altitude parachute extraction system (LAPES) delivery with the C-17 and that the C-130's capability was sufficient. C-17 testing was limited to this lower weight. Airflow issues prevented the C-17 from meeting airdrop requirements. A February 1997 GAO report revealed that a C-17 with a full payload could not land on wet runways; simulations suggested a distance of was required. The YC-15 was transferred to AMARC to be made flightworthy again for further flight tests for the C-17 program in March 1997. In 1995, most of the problems had been reportedly resolved. The first C-17 squadron was declared operational by the USAF in January 1995.

In 1996, DoD ordered another 80 aircraft for a total of 120. In 1997 McDonnell Douglas merged with its former competitor, Boeing. In April 1999, Boeing proposed cutting the price of the C-17 if the Air Force bought 60 more, and in August 2002, the order was increased to 180 aircraft. In 2007, 190 C-17s were on order for the USAF. On 6 February 2009, Boeing was awarded a $2.95 billion contract for 15 additional aircraft, increasing the total USAF C-17 fleet to 205 and extending production from August 2009 to August 2010. On 6 April 2009, U.S. Secretary of Defense Robert Gates stated that there would be no more C-17s ordered beyond the 205 planned. However, on 12 June 2009, the House Armed Services Air and Land Forces Subcommittee added a further 17 C-17s.

In 2010, Boeing reduced the C-17 production rate to 10 aircraft per year from a high of 16 per year. This was due to dwindling orders and to extend the life of the production line while additional international orders were sought. The workforce was reduced by approximately 1,100 through 2012, and a second shift at the Long Beach assembly plant was also eliminated. By April 2011, 230 production C-17s had been delivered, including 210 to the USAF. The C-17 prototype "T-1" was retired in 2012 after being used by the USAF for testing and development. In January 2010, the USAF announced the end of Boeing's performance-based logistics contracts to maintain the aircraft. On 19 June 2012, the USAF ordered its 224th and final C-17, as a replacement for an aircraft that crashed in Alaska in July 2010.

In September 2013, Boeing announced that C-17 production was starting to close down. In October 2014, the main wing spar of the 279th and last aircraft was completed; this C-17 was delivered in 2015, after which Boeing will close the Long Beach plant. Production of spare components will continue until at least 2017. The C-17 is projected to be in service for several decades. In February 2014, Boeing was engaged in sales talks with "five or six" countries for the remaining 15 C-17s, "two to four" of which are not current operators, and Boeing decided to build 10 aircraft without confirmed buyers in anticipation of future purchases. , five aircraft found buyers, including two for the Middle East, two for Australia and one for Canada.

In May 2015, the "Wall Street Journal" reported that Boeing expected to book a charge of under $100 million and cut 3,000 positions associated with the C-17 program, and it also suggested that Airbus' lower cost A400M Atlas has taken international sales away from the C-17.
Sources: C-17 Globemaster III Pocket Guide, Boeing IDS Major Deliveries

The C-17 is long and has a wingspan of . It can airlift cargo close to a battle area. The size and weight of U.S. mechanized firepower and equipment have grown in recent decades from increased air mobility requirements, particularly for large or heavy non-palletized outsize cargo.

The C-17 is powered by four Pratt & Whitney F117-PW-100 turbofan engines, which are based on the commercial Pratt and Whitney PW2040 used on the Boeing 757. Each engine is rated at of thrust. The engine's thrust reversers direct engine exhaust air upwards and forward, reducing the chances of foreign object damage by ingestion of runway debris, and providing enough reverse thrust to back the aircraft up on the ground while taxiing. The thrust reversers can also be used in flight at idle-reverse for added drag in maximum-rate descents. In vortex surfing tests performed by C-17s, up to 10% fuel savings were reported.
For cargo operations the C-17 requires a crew of three: pilot, copilot, and loadmaster. The cargo compartment is long by wide by high. The cargo floor has rollers for palletized cargo but it can be flipped to provide a flat floor suitable for vehicles and other rolling stock. Cargo is loaded through a large aft ramp that accommodates rolling stock, such as a 69-ton (63-metric ton) M1 Abrams main battle tank, other armored vehicles, trucks, and trailers, along with palletized cargo.
Maximum payload of the C-17 is , and its maximum takeoff weight is . With a payload of and an initial cruise altitude of , the C-17 has an unrefueled range of about on the first 71 aircraft, and on all subsequent extended-range models that include a sealed center wing bay as a fuel tank. Boeing informally calls these aircraft the "C-17 ER". The C-17's cruise speed is about (Mach 0.74). It is designed to airdrop 102 paratroopers and their equipment. The U.S. Army's canceled Ground Combat Vehicle was to be transported by the C-17.

The C-17 is designed to operate from runways as short as and as narrow as . In addition, the C-17 can operate from unpaved, unimproved runways (although with greater chance of damage to the aircraft). The thrust reversers can be used to move the aircraft backwards and reverse direction on narrow taxiways using a three- (or more) point turn. The plane is designed for 20 man-hours of maintenance per flight hour, and a 74% mission availability rate.

The first production C-17 was delivered to Charleston Air Force Base, South Carolina, on 14 July 1993. The first C-17 squadron, the 17th Airlift Squadron, became operationally ready on 17 January 1995. The C-17 has broken 22 records for oversized payloads. The C-17 was awarded U.S. aviation's most prestigious award, the Collier Trophy, in 1994. A Congressional report on operations in Kosovo and Operation Allied Force noted "One of the great success stories...was the performance of the Air Force's C-17A" The C-17 flew half of the strategic airlift missions in the operation, the type could use small airfields, easing operations; rapid turnaround times also led to efficient utilization.

On 26 March 2003, nearly 1,000 U.S. service members were parachuted into the Kurdish-controlled area of northern Iraq during Operation Northern Delay in support of Operation Iraqi Freedom. This was the first combat insertion of paratroopers using the C-17.

In 2006, eight C-17s were delivered to March Joint Air Reserve Base, California; controlled by the Air Force Reserve Command (AFRC), assigned to the 452d Air Mobility Wing; and subsequently assigned to AMC's 436th Airlift Wing and its AFRC "associate" unit, the 512th Airlift Wing, at Dover Air Force Base, Delaware, supplementing the Lockheed C-5 Galaxy. The Mississippi Air National Guard, 172 Airlift Group received their first C-17 in 2006. The only Guard Unit to receive sequential serial number aircraft. The Mississippi Air Guard currently operates 8 C-17 aircraft. In 2011, the New York Air National Guard's 105th Airlift Wing at Stewart Air National Guard Base, New York, transitioned from the C-5 to the C-17.
C-17s delivered military goods during Operation Enduring Freedom in Afghanistan and Operation Iraqi Freedom in Iraq as well as humanitarian aid missions in the immediate aftermath of the 2010 Haiti earthquake, the 2011 Sindh floods delivering thousands of food rations, tons of medical and emergency supplies. On 26 March 2003, 15 USAF C-17s participated in the biggest combat airdrop since the United States invasion of Panama in December 1989: the night-time airdrop of 1,000 paratroopers from the 173rd Airborne Brigade occurred over Bashur, Iraq. The airdrop of paratroopers were followed by C-17s ferrying M1 Abrams, M2 Bradleys, M113s and artillery. USAF C-17s have also been used to assist allies in their airlift requirements, including Canadian vehicles to Afghanistan in 2003 and Australian forces during the Australian-led military deployment to East Timor in 2006. In 2006, USAF C-17s flew 15 Canadian Leopard C2 tanks from Kyrgyzstan into Kandahar in support of NATO's Afghanistan mission. In 2013, five USAF C-17s supported French operations in Mali, operating with other nations' C-17s (RAF, NATO and RCAF deployed a single C-17 each).

A C-17 accompanies the President of the United States on his visits to both domestic and foreign arrangements, consultations, and meetings. The C-17 is used to transport the Presidential Limousine, Marine One, and security detachments. There have been several occasions when a C-17 has been used to transport the President himself, temporarily gaining the Air Force One call sign while doing so.

There was debate over follow-on C-17 orders, Air Force having requested line shutdown while Congress attempted to reinstate production. In FY2007, the Air Force requested $1.6 billion in response to "excessive combat use" on the C-17 fleet. In 2008, USAF General Arthur Lichte, Commander of Air Mobility Command, indicated before a House of Representatives subcommittee on air and land forces a need to extend production to another 15 aircraft to increase the total to 205. Pending the delivery of the results of two studies in 2009, Lichte observed that the production line may remain open for further C-17s to satisfy airlift requirements. The USAF eventually decided to cap its C-17 fleet at 223 aircraft; its final delivery was on 12 September 2013.

Boeing has marketed the C-17 to many European nations including Belgium, Germany, France, Italy, Spain and the United Kingdom. The Royal Air Force (RAF) has established an aim of having interoperability and some weapons and capabilities commonality with the USAF. The 1998 Strategic Defence Review identified a requirement for a strategic airlifter. The Short-Term Strategic Airlift (STSA) competition commenced in September of that year, however tendering was canceled in August 1999 with some bids identified by ministers as too expensive, including the Boeing/BAe C-17 bid, and others unsuitable. The project continued, with the C-17 seen as the favorite. In the light of Airbus A400M delays, the UK Secretary of State for Defence, Geoff Hoon, announced in May 2000 that the RAF would lease four C-17s at an annual cost of £100 million from Boeing for an initial seven years with an optional two-year extension. The RAF had the option to buy or return the aircraft to Boeing. The UK committed to upgrading its C-17s in line with the USAF so that if they were returned, the USAF could adopt them. The lease agreement restricted the operational use of the C-17s, meaning that the RAF could not use them for para-drop, airdrop, rough field, low-level operations and air to air refuelling.
The first C-17 was delivered to the RAF at Boeing's Long Beach facility on 17 May 2001 and flown to RAF Brize Norton by a crew from No. 99 Squadron. The RAF's fourth C-17 was delivered on 24 August 2001. The RAF aircraft were some of the first to take advantage of the new center wing fuel tank found in Block 13 aircraft. In RAF service, the C-17 has not been given an official service name and designation (for example, C-130J referred to as Hercules C4 or C5), but is referred to simply as the C-17 or "C-17A Globemaster".

The RAF declared itself delighted with the C-17. Although the Globemaster fleet was to be a fallback for the A400M, the Ministry of Defence (MoD) announced on 21 July 2004 that they had elected to buy their four C-17s at the end of the lease, even though the A400M appeared to be closer to production. The C-17 gives the RAF strategic capabilities that it would not wish to lose, for example a maximum payload of compared to the A400M's . The C-17's capabilities allow the RAF to use it as an airborne hospital for medical evacuation missions.

Another C-17 was ordered in August 2006, and delivered on 22 February 2008. The four leased C-17s were to be purchased later in 2008. Because of fears that the A400M may suffer further delays, the MoD announced in 2006 that it planned to acquire three more C-17s, for a total of eight, with delivery in 2009–2010. On 26 July 2007, Defence Secretary Des Browne announced that the MoD intended to order a sixth C-17 to boost operations in Iraq and Afghanistan. On 3 December 2007, the MoD announced a contract for a sixth C-17, which was received on 11 June 2008.

On 18 December 2009, Boeing confirmed that the RAF had ordered a seventh C-17, which was delivered on 16 November 2010. The UK announced the purchase of its eighth C-17 in February 2012. The RAF showed interest in buying a ninth C-17 in November 2013.

On 13 January 2013, the RAF deployed two C-17s of No. 99 Squadron from RAF Brize Norton to the French Évreux Air Base. The aircraft transported French armored vehicles to the Malian capital of Bamako during the French Intervention in Mali. In June 2015, an RAF C-17 was used to medically evacuate four victims of the 2015 Sousse attacks from Tunisia.

The Royal Australian Air Force (RAAF) began investigating an acquisition of heavy lift aircraft for strategic transport in 2005. In late 2005 the then Minister for Defence Robert Hill stated that such aircraft were being considered due to the limited availability of strategic airlift aircraft from partner nations and air freight companies. The C-17 was considered to be favored over the A400M as it was a "proven aircraft" and in production. One major RAAF requirement was the ability to airlift the Army's M1 Abrams tanks; another requirement was immediate delivery. Though unstated, commonality with the USAF and the United Kingdom's RAF was also considered advantageous. RAAF aircraft were ordered directly from the USAF production run and are identical to American C-17 even in paint scheme, the only difference being the national markings. This allowed delivery to commence within nine months of commitment to the program.

On 2 March 2006, the Australian government announced the purchase of three aircraft and one option with an entry into service date of 2006. In July 2006 a fixed price contract was awarded to Boeing to deliver four C-17s for (). Australia also signed a US$80.7M contract to join the global 'virtual fleet' C-17 sustainment program and the RAAF's C-17s will receive the same upgrades as the USAF's fleet.

The Royal Australian Air Force took delivery of its first C-17 in a ceremony at Boeing's plant at Long Beach, California on 28 November 2006. Several days later the aircraft flew from Hickam Air Force Base, Hawaii to Defence Establishment Fairbairn, Canberra, arriving on 4 December 2006. The aircraft was formally accepted in a ceremony at Fairbairn shortly after arrival. The second aircraft was delivered to the RAAF on 11 May 2007 and the third was delivered on 18 December 2007. The fourth Australian C-17 was delivered on 19 January 2008. All the Australian C-17s are operated by No. 36 Squadron and are based at RAAF Base Amberley in Queensland.
On 18 April 2011, Boeing announced that Australia had signed an agreement with the U.S. government to acquire a fifth C-17 due to an increased demand for humanitarian and disaster relief missions. The aircraft was delivered to the RAAF on 14 September 2011. On 23 September 2011, Australian Minister for Defence Materiel Jason Clare announced that the government was seeking information from the U.S. about the price and delivery schedule for a sixth Globemaster. In November 2011, Australia requested a sixth C-17 through the U.S. Foreign Military Sales program; it was ordered in June 2012, and was delivered on 1 November 2012.

Australia's C-17s have supported ADF operations around the world, including supporting Air Combat Group training deployments to the U.S., transporting Royal Australian Navy Sea Hawk helicopters and making fortnightly supply missions to Australian forces in Iraq and Afghanistan. The C-17s have also carried humanitarian supplies to Papua New Guinea during Operation Papua New Guinea Assist in 2007, supplies and South African Puma helicopters to Burma in 2008 following Cyclone Nargis, relief supplies to Samoa following the 2009 earthquake, aid packages around Queensland following the 2010–2011 floods and Cyclone Yasi, and rescue teams and equipment to New Zealand following the February 2011 Christchurch earthquake, and equipment after the 2011 Tōhoku earthquake and tsunami from Western Australia to Japan. In July 2014, an Australian C-17 transported several bodies of victims of Malaysia Airlines Flight 17 from Ukraine to the Netherlands.

In August 2014, Defence Minister David Johnston announced the intention to purchase one or two additional C-17s. On 3 October 2014, Johnston announced the government's approval to buy two C-17s at a total cost of (). The United States Congress approved the sale under the Foreign Military Sales program. Prime Minister Tony Abbott confirmed in April 2015 that two additional aircraft are to be ordered, with both delivered by 4 November 2015; these are to add to the six C-17s it has .

The Canadian Forces has had a long-standing need for strategic airlift for military and humanitarian operations around the world. It had followed a pattern similar to the German Air Force in leasing Antonovs and Ilyushins for many of its needs, including deploying the Disaster Assistance Response Team (DART) to tsunami-stricken Sri Lanka in 2005. The Canadian Forces was forced to rely entirely on leased An-124 "Ruslan" for a Canadian Army deployment to Haiti in 2003. A combination of leased "Ruslans", Ilyushins and USAF C-17s was also used to move heavy equipment into Afghanistan. In 2002, the Canadian Forces Future Strategic Airlifter Project began to study alternatives, including long-term leasing arrangements.
On 5 July 2006, the Canadian government issued a notice that it intended to negotiate directly with Boeing to procure four airlifters for the Canadian Forces Air Command (Royal Canadian Air Force after August 2011). On 1 February 2007, Canada awarded a contract for four C-17s with delivery beginning in August 2007. Like Australia, Canada was granted airframes originally slated for the U.S. Air Force, to accelerate delivery.

On 23 July 2007, the first Canadian C-17 made its initial flight. It was turned over to Canada on 8 August, and participated at the Abbotsford International Airshow on 11 August prior to arriving at its new home base at 8 Wing, CFB Trenton, Ontario on 12 August. Its first operational mission was delivery of disaster relief to Jamaica following Hurricane Dean. The second C-17 arrived at 8 Wing, CFB Trenton on 18 October 2007. The last of the initial four aircraft was delivered in April 2008. The official Canadian designation is "CC-177 Globemaster III". The aircraft are assigned to 429 Transport Squadron based at CFB Trenton.

On 14 April 2010, a Canadian C-17 landed for the first time at CFS Alert, the world's most northerly airport. Canadian Globemasters have been deployed in support of numerous missions worldwide, including Operation Hestia after the earthquake in Haiti, providing airlift as part of Operation Mobile and support to the Canadian mission in Afghanistan. After Typhoon Haiyan hit the Philippines in 2013, Canadian C-17s established an air bridge between the two nations, deploying Canada's DART Team and delivering humanitarian supplies and equipment. In 2014, they supported Operation Reassurance and Operation Impact.

On 19 December 2014, it was reported that Canada's Defence Department intended to purchase one more C-17. On 30 March 2015, Canada's fifth C-17 arrived at CFB Trenton.

At the 2006 Farnborough Airshow, a number of NATO member nations signed a letter of intent to jointly purchase and operate several C-17s within the Strategic Airlift Capability (SAC). SAC members are Bulgaria, Estonia, Hungary, Lithuania, the Netherlands, Norway, Poland, Romania, Slovenia, the U.S., along with two Partnership for Peace countries Finland and Sweden as of 2010. The purchase was for two C-17s, and a third was contributed by the U.S. On 14 July 2009, Boeing delivered the first C-17 under the SAC program. The second and third C-17s were delivered in September and October 2009.

The SAC C-17s are based at Pápa Air Base, Hungary. The Heavy Airlift Wing is hosted by Hungary, which acts as the flag nation. The aircraft are manned in similar fashion as the NATO E-3 AWACS aircraft. The C-17 flight crew are multi-national, but each mission is assigned to an individual member nation based on the SAC's annual flight hour share agreement. The NATO Airlift Management Programme Office (NAMPO) provides management and support for the Heavy Airlift Wing. NAMPO is a part of the NATO Support Agency (NSPA). In September 2014, Boeing stated that the three C-17s supporting SAC missions had achieved a readiness rate of nearly 94 percent over the last five years and supported over 1,000 missions.

In June 2009, the Indian Air Force (IAF) selected the C-17 for its "Very Heavy Lift Transport Aircraft" requirement to replace several types of transport aircraft. In January 2010, India requested 10 C-17s through the U.S.'s Foreign Military Sales program, the sale was approved by Congress in June 2010. On 23 June 2010, the Indian Air Force successfully test-landed a USAF C-17 at the Gaggal Airport, India to complete the IAF's C-17 trials. In February 2011, the IAF and Boeing agreed terms for the order of 10 C-17s with an option for six more; the US$4.1 billion order was approved by the Indian Cabinet Committee on Security on 6 June 2011. Deliveries began in June 2013 and were to continue to 2014. In 2012, the IAF reportedly finalized plans to buy six more C-17s in its five-year plan for 2017–2022. However, this option is no longer available since C-17 production ended in 2015.
The aircraft provides strategic airlift and the ability to deploy special forces, such as during national emergencies. They are operated in diverse terrain – from Himalayan air bases in North India at to Indian Ocean bases in South India. The C-17s are based at Hindon Air Force Station and are operated by No. 81 Squadron IAF "Skylords". The first C-17 was delivered in January 2013 for testing and training; it was officially accepted on 11 June 2013. The second C-17 was delivered on 23 July 2013 and put into service immediately. IAF Chief of Air Staff Norman AK Browne called the Globemaster III "a major component in the IAF's modernization drive" while taking delivery of the aircraft at Boeing's Long Beach factory. On 2 September 2013, the "Skylords" squadron with three C-17s officially entered IAF service.

The "Skylords" regularly fly missions within India, such as to high-altitude bases at Leh and Thoise. The IAF first used the C-17 to transport an infantry battalion's equipment to Port Blair on Andaman Islands on 1 July 2013. Foreign deployments to date include Tajikistan in August 2013, and Rwanda to support Indian peacekeepers. One C-17 was used for transporting relief materials during Cyclone Phailin. The fifth aircraft was received in November 2013. The sixth aircraft was received in July 2014.

In June 2017, the U.S. Department of State approved the potential sale of one C-17 to India under a proposed $366 million U.S. Foreign Military Sale that includes spare parts and support. This aircraft was the last C-17 produced. The sale, if finalized, would increase the Indian Air Force's fleet to 11 C-17s. The contract was awarded in March 2018 for completion by 22 August 2019 

Boeing delivered Qatar's first C-17 on 11 August 2009 and the second on 10 September 2009 for the Qatar Emiri Air Force. Qatar received its third C-17 in 2012, and fourth C-17 was received on 10 December 2012. In June 2013, the "New York Times" reported that Qatar was allegedly using its C-17s to ship weapons from Libya to the Syrian opposition during the civil war via Turkey. On 15 June 2015, it was announced at the Paris Airshow that Qatar agreed to order four additional C-17s from the five remaining "white tail" C-17s to double Qatar's C-17 fleet.

In February 2009, the United Arab Emirates Air Force agreed to purchase four C-17s. In January 2010, a contract was signed for six C-17s. In May 2011, the first C-17 was handed over and the last of the six was received in June 2012.

Kuwait requested the purchase of one C-17 in September 2010 and a second in April 2013 through the U.S.'s Foreign Military Sales (FMS) program. The nation ordered two C-17s; the first was delivered on 13 February 2014.

The United States Antarctic Program uses C-17s through the United States Air Force for transportation from Christchurch to McMurdo Station, generally shuttling scientific equipment, food and water, and scientific personnel. The New York Air National Guard also operates Lockheed HC-130 aircraft for further transportation.

In 2015 New Zealand Defence Force was considering the purchase of two C-17s for the Royal New Zealand Air Force at an estimated cost of $600 million to replace its aging C-130s. However, the New Zealand Government eventually decided not to acquire any Globemasters.







</doc>
<doc id="6734" url="https://en.wikipedia.org/wiki?curid=6734" title="Garbage collection (computer science)">
Garbage collection (computer science)

In computer science, garbage collection (GC) is a form of automatic memory management. The "garbage collector", or just "collector", attempts to reclaim "garbage", or memory occupied by objects that are no longer in use by the program. Garbage collection was invented by John McCarthy around 1959 to simplify manual memory management in Lisp.

Garbage collection is essentially the opposite of manual memory management, which requires the programmer to specify which objects to deallocate and return to the memory system. However, many systems use a combination of approaches, including other techniques such as stack allocation and region inference. Like other memory management techniques, garbage collection may take a significant proportion of total processing time in a program and, as a result, can have significant influence on performance. With good implementations and with enough memory, depending on application, garbage collection can be faster than manual memory management, while the opposite can also be true and has often been the case in the past with sub-optimal GC algorithms.

Resources other than memory, such as network sockets, database handles, user interaction windows, file and device descriptors, are not typically handled by garbage collection. Methods used to manage such resources, particularly destructors, may suffice to manage memory as well, leaving no need for GC. Some GC systems allow such other resources to be associated with a region of memory that, when collected, causes the resource to be reclaimed; this is called "finalization". Finalization may introduce complications limiting its usability, such as intolerable latency between disuse and reclaim of especially limited resources, or a lack of control over which thread performs the work of reclaiming.

The basic principles of garbage collection are to find data objects in a program that cannot be accessed in the future, and to reclaim the resources used by those objects.

Many programming languages require garbage collection, either as part of the language specification (for example, Java, C#, D, Go and most scripting languages) or effectively for practical implementation (for example, formal languages like lambda calculus); these are said to be "garbage collected languages". Other languages were designed for use with manual memory management, but have garbage-collected implementations available (for example, C and C++). Some languages, like Ada, Modula-3, and C++/CLI, allow both garbage collection and manual memory management to co-exist in the same application by using separate heaps for collected and manually managed objects; others, like D, are garbage-collected but allow the user to manually delete objects and also entirely disable garbage collection when speed is required.

While integrating garbage collection into the language's compiler and runtime system enables a much wider choice of methods, "post-hoc" GC systems exist, such as Automatic Reference Counting (ARC), including some that do not require recompilation. ("Post-hoc" GC is sometimes distinguished as "litter collection".) The garbage collector will almost always be closely integrated with the memory allocator.

Garbage collection frees the programmer from manually dealing with memory deallocation. As a result, certain categories of bugs are eliminated or substantially reduced:


Some of the bugs addressed by garbage collection have security implications.

Typically, garbage collection has certain disadvantages, including consuming additional resources, performance impacts, possible stalls in program execution, and incompatibility with manual resource management.

Garbage collection consumes computing resources in deciding which memory to free, even though the programmer may have already known this information. The penalty for the convenience of not annotating object lifetime manually in the source code is overhead, which can lead to decreased or uneven performance. A peer-reviewed paper came to the conclusion that GC needs five times the memory to compensate for this overhead and to perform as fast as explicit memory management. Interaction with memory hierarchy effects can make this overhead intolerable in circumstances that are hard to predict or to detect in routine testing. The impact on performance was also given by Apple as a reason for not adopting garbage collection in iOS despite being the most desired feature.

The moment when the garbage is actually collected can be unpredictable, resulting in stalls (pauses to shift/free memory) scattered throughout a session. Unpredictable stalls can be unacceptable in real-time environments, in transaction processing, or in interactive programs. Incremental, concurrent, and real-time garbage collectors address these problems, with varying trade-offs.

The modern GC implementations try to minimize blocking "stop-the-world" stalls by doing as much work as possible on the background (i.e. on a separate thread), for example marking unreachable garbage instances while the application process continues to run. In spite of these advancements, for example in the .NET CLR paradigm it is still very difficult to maintain large heaps (millions of objects) with resident objects that get promoted to older generations without incurring noticeable delays (sometimes a few seconds).

The need for explicit manual resource management (release/close) for non-GCed resources in an object oriented language becomes transitive to composition. That is: in a non-deterministic GC system, if a resource or a resource-like object requires manual resource management (release/close), and this object is used as "part of" another object, then the composed object will also become a resource-like object that itself requires manual resource management (release/close).

Tracing garbage collection is the most common type of garbage collection, so much so that "garbage collection" often refers to tracing garbage collection, rather than other methods such as reference counting. The overall strategy consists of determining which objects should be garbage collected by tracing which objects are "reachable" by a chain of references from certain root objects, and considering the rest as garbage and collecting them. However, there are a large number of algorithms used in implementation, with widely varying complexity and performance characteristics.

Reference counting garbage collection is where each object has a count of the number of references to it. Garbage is identified by having a reference count of zero. An object's reference count is incremented when a reference to it is created, and decremented when a reference is destroyed. When the count reaches zero, the object's memory is reclaimed. 

As with manual memory management, and unlike tracing garbage collection, reference counting guarantees that objects are destroyed as soon as their last reference is destroyed, and usually only accesses memory which is either in CPU caches, in objects to be freed, or directly pointed by those, and thus tends to not have significant negative side effects on CPU cache and virtual memory operation.

There are a number of disadvantages to reference counting; this can generally be solved or mitigated by more sophisticated algorithms:


Escape analysis is a compile-time technique that can convert heap allocations to stack allocations, thereby reducing the amount of garbage collection to be done. This analysis determines whether an object allocated inside a function is accessible outside of it. If a function-local allocation is found to be accessible to another function or thread, the allocation is said to “escape” and cannot be done on the stack. Otherwise, the object may be allocated directly on the stack and released when the function returns, bypassing the heap and associated memory management costs.

Generally speaking, higher-level programming languages are more likely to have garbage collection as a standard feature. In some languages that do not have built in garbage collection, it can be added through a library, as with the Boehm garbage collector for C and C++.

Most functional programming languages, such as ML, Haskell, and APL, have garbage collection built in. Lisp is especially notable as both the first functional programming language and the first language to introduce garbage collection.

Other dynamic languages, such as Ruby and Julia (but not Perl 5 or PHP before version 5.3, which both use reference counting), JavaScript and ECMAScript also tend to use GC. Object-oriented programming languages such as Smalltalk and Java usually provide integrated garbage collection. Notable exceptions are C++ and Delphi which have destructors.

Historically, languages intended for beginners, such as BASIC and Logo, have often used garbage collection for heap-allocated variable-length data types, such as strings and lists, so as not to burden programmers with manual memory management. On early microcomputers, with their limited memory and slow processors, BASIC garbage collection could often cause apparently random, inexplicable pauses in the midst of program operation.

Some BASIC interpreters, such as Applesoft BASIC on the Apple II family, repeatedly scanned the string descriptors for the string having the highest address in order to compact it toward high memory, resulting in O(n) performance, which could introduce minutes-long pauses in the execution of string-intensive programs. A replacement garbage collector for Applesoft BASIC published in Call-A.P.P.L.E. (January 1981, pages 40–45, Randy Wigginton) identified a group of strings in every pass over the heap, which cut collection time dramatically. BASIC.System, released with ProDOS in 1983, provided a windowing garbage collector for BASIC that reduced most collections to a fraction of a second.

While the Objective-C traditionally had no garbage collection, with the release of OS X 10.5 in 2007 Apple introduced garbage collection for Objective-C 2.0, using an in-house developed runtime collector.
However, with the 2012 release of OS X 10.8, garbage collection was deprecated in favor of LLVM's automatic reference counter (ARC) that was introduced with OS X 10.7. Furthermore, since May 2015 Apple even forbids the usage of garbage collection for new OS X applications in the App Store. For iOS, garbage collection has never been introduced due to problems in application responsivity and performance; instead, iOS uses ARC.

Garbage collection is rarely used on embedded or real-time systems because of the perceived need for very tight control over the use of limited resources. However, garbage collectors compatible with such limited environments have been developed. The Microsoft .NET Micro Framework, .NET nanoFramework and Java Platform, Micro Edition are embedded software platforms that, like their larger cousins, include garbage collection.

Among the most popular Garbage Collectors for JDK could be named:






Commonly people compare Garbage Collectors in terms of RAM consumption optimization and receive different results. 

Compile-time garbage collection is a form of static analysis allowing memory to be reused and reclaimed based on invariants known during compilation. 

This form of garbage collection has been studied in the Mercury programming language, and it saw greater usage with the introduction of LLVM's automatic reference counter (ARC) into Apple's ecosystem (iOS and OS X) in 2011.

Incremental, concurrent, and real-time garbage collectors have been developed, such as Baker's algorithm or Lieberman's algorithm.

In Baker's algorithm, the allocation is done in either half of a single region of memory. When it becomes half full, a garbage collection is performed which moves the live objects into the other half and the remaining objects are implicitly deallocated. The running program (the 'mutator') has to check that any object it references is in the correct half, and if not move it across, while a background task is finding all of the objects.

Generational garbage collection schemes are based on the empirical observation that most objects die young. In generational garbage collection two or more allocation regions (generations) are kept, which are kept separate based on object's age. New objects are created in the "young" generation that is regularly collected, and when a generation is full, the objects that are still referenced from older regions are copied into the next oldest generation. Occasionally a full scan is performed.

Some high-level language computer architectures include hardware support for real-time garbage collection.

Most implementations of real-time garbage collectors use tracing. Such real-time garbage collectors meet hard real-time constraints when used with a real-time operating system.





</doc>
<doc id="6736" url="https://en.wikipedia.org/wiki?curid=6736" title="Canidae">
Canidae

The biological family Canidae 

The cat-like feliformia and dog-like caniforms emerged within the Carnivoramorpha 43 million years before present. The caniforms included the fox-like genus "Leptocyon" whose various species existed from 34 million years ago (Mya) before branching 11.9 Mya into Vulpini (foxes) and Canini (canines).

Canids are found on all continents except Antarctica, having arrived independently or accompanied human beings over extended periods of time. Canids vary in size from the 2-m-long (6 ft 7 in) gray wolf to the 24-cm-long (9.4 in) fennec fox. The body forms of canids are similar, typically having long muzzles, upright ears, teeth adapted for cracking bones and slicing flesh, long legs, and bushy tails. They are mostly social animals, living together in family units or small groups and behaving co-operatively. Typically, only the dominant pair in a group breeds, and a litter of young is reared annually in an underground den. Canids communicate by scent signals and vocalizations. One canid, the domestic dog, long ago entered into a partnership with humans and today remains one of the most widely kept domestic animals.

In the history of the carnivores, the family Canidae is represented by the two extinct subfamilies designated as Hesperocyoninae and Borophaginae, and the extant subfamily Caninae. This subfamily includes all living canids and their most recent fossil relatives. All living canids as a group form a dental monophyletic relationship with the extinct borophagines, with both groups having a bicuspid (two points) on the lower carnassial talonid, which gives this tooth an additional ability in mastication. This, together with the development of a distinct entoconid cusp and the broadening of the talonid of the first lower molar, and the corresponding enlargement of the talon of the upper first molar and reduction of its parastyle distinguish these late Cenozoic canids and are the essential differences that identify their clade.

Within the Canidae, the results of allozyme and chromosome analyses have previously suggested several phylogenetic divisions:

DNA analysis shows that the first three form monophyletic clades. The wolf-like canids and the South American canids together form the tribe Canini. Molecular data imply a North American origin of living Canidae some 10 Mya and an African origin of wolf-like canines ("Canis", "Cuon", and "Lycaon"), with the jackals being the most basal of this group. The South American clade is rooted by the maned wolf and bush dog, and the fox-like canids by the fennec fox and Blanford's fox. The gray fox and island fox are basal to the other clades; however, this topological difference is not strongly supported.

The cladogram below is based on the phylogeny of Lindblad-Toh et al. (2005), modified to incorporate recent findings on "Canis", "Vulpes", "Lycalopex", and "Dusicyon" species.

The Canidae today include a diverse group of some 34 species ranging in size from the maned wolf with its long limbs to the short-legged bush dog. Modern canids inhabit forests, tundra, savannahs, and deserts throughout tropical and temperate parts of the world. The evolutionary relationships between the species have been studied in the past using morphological approaches, but more recently, molecular studies have enabled the investigation of phylogenetic relationships. In some species, genetic divergence has been suppressed by the high level of gene flow between different populations and where the species have hybridized, large hybrid zones exist.

Carnivorans evolved from miacoids about 55 Mya during the late Paleocene. Some 5 million years later, the carnivorans split into two main divisions: caniforms (dog-like) and feliforms (cat-like). By 40 Mya, the first member of the dog family proper had arisen. Called "Prohesperocyon wilsoni", its fossilized remains have been found in what is now the southwestern part of Texas. The chief features which identify it as a canid include the loss of the upper third molar (part of a trend toward a more shearing bite), and the structure of the middle ear which has an enlarged bulla (the hollow bony structure protecting the delicate parts of the ear). "Prohesperocyon" probably had slightly longer limbs than its predecessors, and also had parallel and closely touching toes which differ markedly from the splayed arrangements of the digits in bears.

The canid family soon subdivided into three subfamilies, each of which diverged during the Eocene: Hesperocyoninae (about 39.74–15 Mya), Borophaginae (about 34–2 Mya), and Caninae (about 34–0 Mya). The Caninae are the only surviving subfamily and all present-day canids, including wolves, foxes, coyotes, jackals, and domestic dogs, belong to it. Members of each subfamily showed an increase in body mass with time and some exhibited specialized hypercarnivorous diets that made them prone to extinction.

By the Oligocene, all three subfamilies of canids (Hesperocyoninae, Borophaginae, and Caninae) had appeared in the fossil records of North America. The earliest and most primitive branch of the Canidae was the Hesperocyoninae lineage, which included the coyote-sized "Mesocyon" of the Oligocene (38–24 Mya). These early canids probably evolved for the fast pursuit of prey in a grassland habitat; they resembled modern civets in appearance. Hesperocyonines eventually became extinct in the middle Miocene. One of the early members of the Hesperocyonines, the genus "Hesperocyon", gave rise to "Archaeocyon" and "Leptocyon". These branches led to the borophagine and canine radiations.

Around 9–10 Mya during the Late Miocene, the "Canis", "Urocyon", and "Vulpes" genera expanded from southwestern North America, where the canine radiation began. The success of these canines was related to the development of lower carnassials that were capable of both mastication and shearing. Around 8 Mya, the Beringian land bridge allowed members of the genus "Eucyon" a means to enter Asia and they continued on to colonize Europe.

During the Pliocene, around 4–5 Mya, "Canis lepophagus" appeared in North America. This was small and sometimes coyote-like. Others were wolf-like in characteristics. "C. latrans" (the coyote) is theorized to have descended from "C. lepophagus".

The formation of the Isthmus of Panama, about 3 Mya, joined South America to North America, allowing canids to invade South America, where they diversified. However, the most recent common ancestor of the South American canids lived in North America some 4 Mya and more than one incursion across the new land bridge is likely. One of the resulting lineages consisted of the gray fox ("Urocyon cinereoargentus") and the now-extinct dire wolf ("Canis dirus"). The other lineage consisted of the so-called South American endemic species; the maned wolf ("Chrysocyon brachyurus"), the short-eared dog ("Atelocynus microtis"), the bush dog ("Speothos venaticus"), the crab-eating fox ("Cerdocyon thous"), and the South American foxes ("Lycalopex" spp.). The monophyly of this group has been established by molecular means.

During the Pleistocene, the North American wolf line appeared, with "Canis edwardii", clearly identifiable as a wolf, and "Canis rufus" appeared, possibly a direct descendant of "C. edwardii". Around 0.8 Mya, "Canis ambrusteri" emerged in North America. A large wolf, it was found all over North and Central America and was eventually supplanted by its descendant, the dire wolf, which then spread into South America during the late Pleistocene.

By 0.3 Mya, a number of subspecies of the gray wolf ("C. lupus") had developed and had spread throughout Europe and northern Asia. The gray wolf colonized North America during the late Rancholabrean era across the Bering land bridge, with at least three separate invasions, with each one consisting of one or more different Eurasian gray wolf clades. MtDNA studies have shown that there are at least four extant "C. lupus" lineages. The dire wolf shared its habitat with the gray wolf, but became extinct in a large-scale extinction event that occurred around 11,500 years ago. It may have been more of a scavenger than a hunter; its molars appear to be adapted for crushing bones and it may have gone extinct as a result of the extinction of the large herbivorous animals on whose carcasses it relied.

In 2015, a study of mitochondrial genome sequences and whole genome nuclear sequences of African and Eurasian canids indicated that extant wolf-like canids have colonized Africa from Eurasia at least five times throughout the Pliocene and Pleistocene, which is consistent with fossil evidence suggesting that much of African canid fauna diversity resulted from the immigration of Eurasian ancestors, likely coincident with Plio-Pleistocene climatic oscillations between arid and humid conditions. When comparing the African and Eurasian golden jackals, the study concluded that the African specimens represented a distinct monophyletic lineage that should be recognized as a separate species, "Canis anthus" (African golden wolf). According to a phylogeny derived from nuclear sequences, the Eurasian golden jackal ("Canis aureus") diverged from the wolf/coyote lineage 1.9 Mya, but the African golden wolf separated 1.3 Mya. Mitochondrial genome sequences indicated the Ethiopian wolf diverged from the wolf/coyote lineage slightly prior to that.

Wild canids are found on every continent except Antarctica, and inhabit a wide range of different habitats, including deserts, mountains, forests, and grasslands. They vary in size from the fennec fox, which may be as little as in length and weigh , to the gray wolf, which may be up to long, and can weigh up to . Only a few species are arboreal – the gray fox, the closely related island fox and the raccoon dog habitually climb trees.

All canids have a similar basic form, as exemplified by the grey wolf, although the relative length of muzzle, limbs, ears and tail vary considerably between species. With the exceptions of the bush dog, the raccoon dog and some domestic breeds of "Canis lupus", canids have relatively long legs and lithe bodies, adapted for chasing prey. The tails are bushy and the length and quality of the pelage varies with the season. The muzzle portion of the skull is much more elongated than that of the cat family. The zygomatic arches are wide, there is a transverse lambdoidal ridge at the rear of the cranium and in some species, a sagittal crest running from front to back. The bony orbits around the eye never form a complete ring and the auditory bullae are smooth and rounded.

All canids are digitigrade, meaning they walk on their toes. The tip of the nose is always naked, as are the cushioned pads on the soles of the feet. These latter consist of a single pad behind the tip of each toe and a more-or-less three-lobed central pad under the roots of the digits. Hairs grow between the pads and in the Arctic fox, the sole of the foot is densely covered with hair at some times of year. With the exception of the four-toed African wild dog ("Lycaon pictus"), five toes are on the forefeet, but the pollex (thumb) is reduced and does not reach the ground. On the hind feet are four toes, but in some domestic dogs, a fifth vestigial toe, known as a dewclaw, is sometimes present, but has no anatomical connection to the rest of the foot. The slightly curved nails are not retractile and more-or-less blunt.

The penis in male canids is supported by a bone called the baculum. It also contains a structure at the base called the bulbus glandis, which helps to create a copulatory tie during mating, locking the animals together for up to an hour. Young canids are born blind, with their eyes opening a few weeks after birth. All living canids (Caninae) have a ligament analogous to the nuchal ligament of ungulates used to maintain the posture of the head and neck with little active muscle exertion; this ligament allows them to conserve energy while running long distances following scent trails with their nose to the ground. However, based on skeletal details of the neck, at least some of the Borophaginae (such as "Aelurodon") are believed to have lacked this ligament.

Dentition relates to the arrangement of teeth in the mouth, with the dental notation for the upper-jaw teeth using the upper-case letters I to denote incisors, C for canines, P for premolars, and M for molars, and the lower-case letters i, c, p and m to denote the mandible teeth. Teeth are numbered using one side of the mouth and from the front of the mouth to the back. In carnivores, the upper premolar P4 and the lower molar m1 form the carnassials that are used together in a scissor-like action to shear the muscle and tendon of prey.

Canids use their premolars for cutting and crushing except for the upper fourth premolar P4 (the upper carnassial) that is only used for cutting. They use their molars for grinding except for the lower first molar m1 (the lower carnassial) that has evolved for both cutting and grinding depending on the candid's dietary adaptation. On the lower carnassial, the trigonid is used for slicing and the talonid is used for grinding. The ratio between the trigonid and the talonid indicates a carnivore's dietary habits, with a larger trigonid indicating a hypercarnivore and a larger talonid indicating a more omnivorous diet. Because of its low variability, the length of the lower carnassial is used to provide an estimate of a carnivore's body size.

A study of the estimated bite force at the canine teeth of a large sample of living and fossil mammalian predators, when adjusted for their body mass, found that for placental mammals the bite force at the canines was greatest in the extinct dire wolf (163), followed among the modern canids by the four hypercarnivores that often prey on animals larger than themselves: the African wild dog (142), the gray wolf (136), the dhole (112), and the dingo (108). The bite force at the carnassials showed a similar trend to the canines. A predator's largest prey size is strongly influenced by its biomechanical limits.

Most canids have 42 teeth, with a dental formula of: . The bush dog has only one upper molar with two below, the dhole has two above and two below. and the bat-eared fox has three or four upper molars and four lower ones. The molar teeth are strong in most species, allowing the animals to crack open bone to reach the marrow. The deciduous, or baby teeth, formula in canids is , molars being completely absent.

Almost all canids are social animals and live together in groups. In general, they are territorial or have a home range and sleep in the open, using their dens only for breeding and sometimes in bad weather. In most foxes, and in many of the true dogs, a male and female pair work together to hunt and to raise their young. Gray wolves and some of the other larger canids live in larger groups called packs. African wild dogs have packs which may consist of 20 to 40 animals and packs of fewer than about seven individuals may be incapable of successful reproduction. Hunting in packs has the advantage that larger prey items can be tackled. Some species form packs or live in small family groups depending on the circumstances, including the type of available food. In most species, some individuals live on their own. Within a canid pack, there is a system of dominance so that the strongest, most experienced animals lead the pack. In most cases, the dominant male and female are the only pack members to breed.

Canids communicate with each other by scent signals, by visual clues and gestures, and by vocalizations such as growls, barks and howls. In most cases, groups have a home territory from which they drive out other conspecifics. The territory is marked by leaving urine scent marks, which warn trespassing individuals. Social behaviour is also mediated by secretions from glands on the upper surface of the tail near its root and from the anal glands.

Canids as a group exhibit several reproductive traits that are uncommon among mammals as a whole. They are typically monogamous, provide paternal care to their offspring, have reproductive cycles with lengthy proestral and dioestral phases and have a copulatory tie during mating. They also retain adult offspring in the social group, suppressing the ability of these to breed while making use of the alloparental care they can provide to help raise the next generation of offspring.

During the proestral period, increased levels of oestradiol make the female attractive to the male. There is a rise in progesterone during the oestral phase and the female is now receptive. Following this, the level of oestradiol fluctuates and there is a lengthy dioestrous phase during which the female is pregnant. Pseudo-pregnancy frequently occurs in canids that have ovulated but failed to conceive. A period of anoestrus follows pregnancy or pseudo-pregnancy, there being only one oestral period during each breeding season. Small and medium-sized canids mostly have a gestation period of 50 to 60 days, while larger species average 60 to 65 days. The time of year in which the breeding season occurs is related to the length of day, as has been demonstrated in the case of several species that have been translocated across the equator to the other hemisphere and experiences a six-month shift of phase. Domestic dogs and certain small canids in captivity may come into oestrus more frequently, perhaps because the photoperiod stimulus breaks down under conditions of artificial lighting.

The size of a litter varies, with from one to 16 or more pups being born. The young are born small, blind and helpless and require a long period of parental care. They are kept in a den, most often dug into the ground, for warmth and protection. When the young begin eating solid food, both parents, and often other pack members, bring food back for them from the hunt. This is most often vomited up from the adult's stomach. Where such pack involvement in the feeding of the litter occurs, the breeding success rate is higher than is the case where females split from the group and rear their pups in isolation. Young canids may take a year to mature and learn the skills they need to survive. In some species, such as the African wild dog, male offspring usually remain in the natal pack, while females disperse as a group and join another small group of the opposite sex to form a new pack.

One canid, the domestic dog, entered into a partnership with humans a long time ago. The archaeological record shows the first undisputed dog remains buried beside humans 14,700 years ago, with disputed remains occurring 36,000 years ago. These dates imply that the earliest dogs arose in the time of human hunter-gatherers and not agriculturists. The dog was the first domesticated species. The fact that wolves are pack animals with cooperative social structures may have been the reason that the relationship developed. Humans benefited from the canid's loyalty, cooperation, teamwork, alertness and tracking abilities, while the wolf may have benefited from the use of weapons to tackle larger prey and the sharing of food. Humans and dogs may have evolved together.

Among canids, only the gray wolf has widely been known to prey on humans. Nonetheless, at least two records have coyotes killing humans, and two have golden jackals killing children. Human beings have trapped and hunted some canid species for their fur and, especially the gray wolf, the coyote and the red fox, for sport. Canids such as the dhole are now endangered in the wild because of persecution, habitat loss, a depletion of ungulate prey species and transmission of diseases from domestic dogs.

All extant species of family Canidae are in subfamily Caninae.



Except where otherwise stated, the following classification is based on a 1994 paper by Xiaoming Wang, curator of terrestrial mammals at the Natural History Museum of Los Angeles County on the systematics of the subfamily Hesperocyoninae, a 1999 paper by Wang, together with the zoologists Richard H. Tedford and Beryl E. Taylor on the subfamily Borophaginae, and a 2009 paper by Tedford, Wang and Taylor on the North American fossil Caninae.



† (Mya = million years ago)




</doc>
<doc id="6739" url="https://en.wikipedia.org/wiki?curid=6739" title="Subspecies of Canis lupus">
Subspecies of Canis lupus

The historic "Canis lupus" has 38 subspecies listed in the taxonomic authority "Mammal Species of the World" (2005, 3rd edition). These subspecies were named over the past 250 years, and since their naming, a number of them have gone extinct. The nominate subspecies is "Canis lupus lupus".

"Canis lupus" is assessed as least concern by the IUCN, as its relatively widespread range and stable population trend mean that the species, at global level, does not meet, or nearly meet, any of the criteria for the threatened categories. However, some local populations are classified as endangered, and some subspecies are endangered or extinct. Biological taxonomy is not fixed and placement of taxa is reviewed as a result of new research. The current categorization of subspecies of "C. lupus" is shown below. Also included are synonyms, which are now discarded, duplicate or incorrect namings, or in the case of the domestic dog synonyms, old taxa referring to subspecies of the domestic dog, which when the dog was declared a subspecies itself, had nowhere else to go. Common names are given, but may vary, as they have no set meaning.

The species "C. lupus" was first recorded by Carl Linnaeus in his publication "Systema Naturae" in 1758, with the Latin classification translating into the English words "dog wolf".

A subspecies is the taxonomic rank below a species. When geographically separate populations of a species exhibit recognizable phenotypic differences, biologists may identify these as separate subspecies; a subspecies is a recognized local variant of a species. The 38 subspecies of "Canis lupus" are listed in "Mammal Species of the World" (third edition) that was published in 2005, and in the Catalogue of Life. The nominate subspecies is the Eurasian wolf ("Canis lupus lupus"), also known as the common wolf. The subspecies includes the domestic dog, dingo, eastern wolf and red wolf. However, the classification of several as either species or subspecies has recently been challenged.

Living subspecies recognized by "MSW3" and divided into Old World and New World:

For Eurasia, in 1995 mammalogist Robert Nowak recognized five subspecies based on skull morphology, these being: "C. l. lupus", "C. l. albus", "C. l. pallipes", "C. l. cubanensis", and "C. l. communis". In 2003, Nowak also recognized the distinctiveness of "C. l.", "C. l. hattai", and "C. l. hodophilax". In 2005, "MSW3" included "C. l. filchneri". In 2003, two forms were distinguished in southern China and Inner Mongolia as being separate from "C. l. chanco" and "C. l. filchneri" and have yet to be named.

For North America, in 1944 the zoologist Edward Goldman recognized as many as 23 subspecies based on morphology. In 1959, E. Raymond Hall proposed that there had been 24 subspecies of "lupus" in North America. In 1970, L. David Mech proposed that there was "probably far too many subspecific designations...in use" as most did not exhibit enough points of differentiation to be classified as a separate subspecies. The 24 subspecies were accepted by many authorities in 1981 and these were based on morphological or geographical differences, or a unique history. In 1995, the American mammologist Robert M. Nowak analyzed data on the skull morphology of wolf specimens from around the world. For North America, he proposed that there were only five subspecies of gray wolf. These include a large-toothed Arctic wolf named "C. l. arctos", a large wolf from Alaska and western Canada named "C. l. occidentalis", a small wolf from southeastern Canada named "C. l. lycaon", a small wolf from the southwestern U.S. named "C. l. baileyi" and a moderate-sized wolf that was originally found from Texas to Hudson Bay and from Oregon to Newfoundland named "C. l. nubilus". This proposal was not reflected in the taxonomic classification of "Canis lupus" subspecies in "Mammal Species of the World" (third edition, 2005).

Subspecies recognized by "MSW3" which have gone extinct over the past 150 years: 

Subspecies discovered since the publishing of "MSW3" in 2005 which have gone extinct over the past 150 years: 

The Apennine wolf (or Italian wolf) was first recognised as a distinct subspecies "Canis lupus italicus" in 1921 by zoologist Giuseppe Altobello. Altobello's classification was later rejected by several authors, including Reginald Innes Pocock, who synonymised "C. l. italicus" with "C. l. lupus". In 2002, the noted paleontologist R.M. Nowak reaffirmed the morphological distinctiveness of the Italian wolf and recommended the recognition of "Canis lupus italicus". A number of DNA studies have found the Italian wolf to be genetically distinct. In 2004, the genetic distinction of the Italian wolf subspecies was supported by analysis which consistently assigned all the wolf genotypes of a sample in Italy to a single group. This population also showed a unique mitochondrial DNA control-region haplotype, the absence of private alleles and lower heterozygosity at microsatellite loci, as compared to other wolf populations. In 2010, a genetic analysis indicated that a single wolf haplotype (w22) unique to the Apennine Peninsula and one of the two haplotypes (w24, w25), unique to the Iberian Peninsula, belonged to the same haplogroup as the prehistoric wolves of Europe. Another haplotype (w10) was found to be common to the Iberian peninsula and the Balkans. These three populations with geographic isolation exhibited a near lack of gene flow and spatially correspond to three glacial refugia.

The taxonomic reference "Mammal Species of the World" (3rd edition, 2005) does not recognize "Canis lupus italicus"; however, NCBI/Genbank publishes research papers under that name.

The Iberian wolf was first recognised as a distinct subspecies ("Canis lupus signatus") in 1907 by zoologist Ángel Cabrera. The wolves of the Iberian peninsula have morphologically distinct features from other Eurasian wolves and each are considered by their researchers to represent their own subspecies.

The taxonomic reference "Mammal Species of the World" (3rd edition, 2005) does not recognize "Canis lupus signatus"; however, NCBI/Genbank does list it.

The Himalayan wolf is a proposed clade within the Tibetan wolf ("Canis lupus filchneri") that is distinguished by its mitochondrial DNA, which is basal to all other wolves. The taxonomic status of this wolf is disputed, with the species "Canis himalayensis" being proposed based on two limited DNA studies. In 2017, a study of mitochondrial DNA, X-chromosome (maternal lineage) markers and Y-chromosome (male lineage) markers found that the Himalayan wolf was genetically basal to the holarctic grey wolf and has an association with the African golden wolf. 

The taxonomic reference "Mammal Species of the World" (3rd edition, 2005) does not recognize "Canis himalayensis", however NCBI/Genbank lists it as a new subspecies "Canis lupus himalayensis".

The Indian plains wolf is a proposed clade within the Indian wolf ("Canis lupus pallipes") that is distinguished by its mitochondrial DNA, which is basal to all other wolves except for the Himalayan wolf. The taxonomic status of this wolf clade is disputed, with the separate species "Canis indica" being proposed based on two limited DNA studies. The proposal has not been endorsed because they relied on a limited number of museum and zoo samples that may not have been representative of the wild population and a call for further fieldwork has been made.

The taxonomic reference "Mammal Species of the World" (3rd edition, 2005) does not recognize "Canis indica", however NCBI/Genbank lists it as a new subspecies "Canis lupus indica".

A study of the three coastal wolves indicates a close phylogenetic relationship across regions that are geographically and ecologically contiguous, and the study proposed that "Canis lupus ligoni" (Alexander Archipelago wolf), "Canis lupus columbianus" (British Columbian wolf), and "Canis lupus crassodon" (Vancouver Island wolf) should be recognized as a single subspecies of "Canis lupus". They share the same habitat and prey species, and form one study's six identified North American ecotypes - a genetically and ecologically distinct population separated from other populations by their different type of habitat.

The eastern wolf has two proposals over its origin. One is that the eastern wolf is a distinct species ("C. lycaon") that evolved in North America, as opposed to the gray wolf that evolved in the Old World, and is related to the red wolf. The other is that it is derived from admixture between gray wolves which inhabited the Great Lakes area and coyotes, forming a hybrid that was classified as a distinct species by mistake. 

The taxonomic reference "Mammal Species of the World" (3rd edition, 2005) does not recognize "Canis lycaon", however NCBI/Genbank does list it.

The red wolf is an enigmatic taxon, of which there are two proposals over its origin. One is that the red wolf was a distinct species ("C. rufus") that has undergone human-influenced admixture with coyotes. The other is that it was never a distinct species but was derived from admixture between coyotes and gray wolves, due to the gray wolf population being eliminated by humans.

The taxonomic reference "Mammal Species of the World" (3rd edition, 2005) does not recognize "Canis rufus", however NCBI/Genbank does list it.




</doc>
<doc id="6742" url="https://en.wikipedia.org/wiki?curid=6742" title="Central Asia">
Central Asia

Central Asia stretches from the Caspian Sea in the west to China in the east and from Afghanistan in the south to Russia in the north. The region consists of the former Soviet republics of Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan, and Uzbekistan. It is also colloquially referred to as "the stans" as the countries generally considered to be within the region all have names ending with the Persian suffix "-stan", meaning "land of".

Central Asia (2019) has a population of about 72 million, consisting of five republics: Kazakhstan (pop. /1e6 round 0 million), Kyrgyzstan (/1e6 round 0 million), Tajikistan (/1e6 round 0 million), Turkmenistan (/1e6 round 0 million), and Uzbekistan (33 million). Afghanistan (pop. /1e6 round 0 million), which is a part of South Asia, is also sometimes included in Central Asia.

Central Asia has historically been closely tied to its nomadic peoples and the Silk Road. It has acted as a crossroads for the movement of people, goods, and ideas between Europe, Western Asia, South Asia, and East Asia. The Silk Road connected Muslim lands with the people of Europe, India, and China. This crossroads position has intensified the conflict between tribalism and traditionalism and modernization.

In pre-Islamic and early Islamic times, Central Asia was predominantly Iranian, populated by Eastern Iranian-speaking Bactrians, Sogdians, Chorasmians and the semi-nomadic Scythians and Dahae. After expansion by Turkic peoples, Central Asia also became the homeland for the Kazakhs, Uzbeks, Tatars, Turkmen, Kyrgyz, and Uyghurs; Turkic languages largely replaced the Iranian languages spoken in the area.

From the mid-19th century until almost the end of the 20th century, most of Central Asia was part of the Russian Empire and later the Soviet Union, both Slavic-majority countries, and the five former Soviet "-stans" are still home to about 7 million ethnic Russians and 500,000 Ukrainians.

The idea of Central Asia as a distinct region of the world was introduced in 1843 by the geographer Alexander von Humboldt. The borders of Central Asia are subject to multiple definitions. Historically built political geography and geoculture are two significant parameters widely used in the scholarly literature about the definitions of the Central Asia.

The most limited definition was the official one of the Soviet Union, which defined Middle Asia as consisting solely of Uzbekistan, Turkmenistan, Tajikistan, and Kyrgyzstan, hence omitting Kazakhstan. This definition was also often used outside the USSR during this period.

However, the Russian culture has two distinct terms: "Средняя Азия" ("Srednyaya Aziya" or "Middle Asia", the narrower definition, which includes only those traditionally non-Slavic, Central Asian lands that were incorporated within those borders of historical Russia) and "Центральная Азия" ("Tsentralnaya Aziya" or "Central Asia", the wider definition, which includes Central Asian lands that have never been part of historical Russia).

Soon after the dissolution of the Soviet Union in 1991, the leaders of the four former Soviet Central Asian Republics met in Tashkent and declared that the definition of Central Asia should include Kazakhstan as well as the original four included by the Soviets. Since then, this has become the most common definition of Central Asia.

The UNESCO "History of the Civilizations of Central Asia", published in 1992, defines the region as "Afghanistan, northeastern Iran, northern and central Pakistan, northern India, western China, Mongolia and the former Soviet Central Asian republics."

An alternative method is to define the region based on ethnicity, and in particular, areas populated by Eastern Turkic, Eastern Iranian, or Mongolian peoples. These areas include Xinjiang Uyghur Autonomous Region, the Turkic regions of southern Siberia, the five republics, and Afghan Turkestan. Afghanistan as a whole, the northern and western areas of Pakistan and the Kashmir Valley of India may also be included. The Tibetans and Ladakhi are also included. Insofar, most of the mentioned peoples are considered the "indigenous" peoples of the vast region. Central Asia is sometimes referred to as Turkestan.

There are several places that claim to be the geographic center of Asia, for example Kyzyl, the capital of Tuva in the Russian Federation, and a village north of Ürümqi, the capital of the Xinjiang region of China.

Central Asia is an extremely large region of varied geography, including high passes and mountains (Tian Shan), vast deserts (Kyzyl Kum, Taklamakan), and especially treeless, grassy steppes. The vast steppe areas of Central Asia are considered together with the steppes of Eastern Europe as a homogeneous geographical zone known as the Eurasian Steppe.

Much of the land of Central Asia is too dry or too rugged for farming. The Gobi desert extends from the foot of the Pamirs, 77° E, to the Great Khingan (Da Hinggan) Mountains, 116°–118° E.

Central Asia has the following geographic extremes:

A majority of the people earn a living by herding livestock. Industrial activity centers in the region's cities.

Major rivers of the region include the Amu Darya, the Syr Darya, Irtysh, the Hari River and the Murghab River. Major bodies of water include the Aral Sea and Lake Balkhash, both of which are part of the huge west-central Asian endorheic basin that also includes the Caspian Sea.

Both of these bodies of water have shrunk significantly in recent decades due to diversion of water from rivers that feed them for irrigation and industrial purposes. Water is an extremely valuable resource in arid Central Asia and can lead to rather significant international disputes.

Central Asia is bounded on the north by the forests of Siberia. The northern half of Central Asia (Kazakhstan) is the middle part of the Eurasian steppe. Westward the Kazakh steppe merges into the Russian-Ukrainian steppe and eastward into the steppes and deserts of Dzungaria and Mongolia. Southward the land becomes increasingly dry and the nomadic population increasingly thin. The south supports areas of dense population and cities wherever irrigation is possible. The main irrigated areas are along the eastern mountains, along the Oxus and Jaxartes Rivers and along the north flank of the Kopet Dagh near the Persian border. East of the Kopet Dagh is the important oasis of Merv and then a few places in Afghanistan like Herat and Balkh. Two projections of the Tian Shan create three "bays" along the eastern mountains. The largest, in the north, is eastern Kazakhstan, traditionally called Jetysu or Semirechye which contains Lake Balkhash. In the center is the small but densely-populated Ferghana valley. In the south is Bactria, later called Tocharistan, which is bounded on the south by the Hindu Kush mountains of Afghanistan. The Syr Darya (Jaxartes) rises in the Ferghana valley and the Amu Darya (Oxus) rises in Bactria. Both flow northwest into the Aral Sea. Where the Oxus meets the Aral Sea it forms a large delta called Khwarazm and later the Khanate of Khiva. North of the Oxus is the less-famous but equally important Zarafshan River which waters the great trading cities of Bokhara and Samarkand. The other great commercial city was Tashkent northwest of the mouth of the Ferghana valley. The land immediately north of the Oxus was called Transoxiana and also Sogdia, especially when referring to the Sogdian merchants who dominated the silk road trade.

To the east, Dzungaria and the Tarim Basin were united into the Chinese province of Xinjiang about 1759. Caravans from China usually went along the north or south side of the Tarim basin and joined at Kashgar before crossing the mountains northwest to Ferghana or southwest to Bactria. A minor branch of the silk road went north of the Tian Shan through Dzungaria and Zhetysu before turning southwest near Tashkent. Nomadic migrations usually moved from Mongolia through Dzungaria before turning southwest to conquer the settled lands or continuing west toward Europe.

The Kyzyl Kum Desert or semi-desert is between the Oxus and Jaxartes, and the Karakum Desert is between the Oxus and Kopet Dagh in Turkmenistan. Khorasan meant approximately northeast Persia and northern Afghanistan. Margiana was the region around Merv. The Ustyurt Plateau is between the Aral and Caspian Seas.

To the southwest, across the Kopet Dagh, lies Persia. From here Persian and Islamic civilization penetrated Central Asia and dominated its high culture until the Russian conquest. In the southeast is the route to India. In early times Buddhism spread north and throughout much of history warrior kings and tribes would move southeast to establish their rule in northern India. Most nomadic conquerors entered from the northeast. After 1800 western civilization in its Russian and Soviet form penetrated from the northwest.

Because Central Asia is not buffered by a large body of water, temperature fluctuations are severe, outside the sunny, hot summer months. In most areas the climate is dry and continental, with hot summers and cool to cold winters, with occasional snowfall. Outside high-elevation areas, the climate is mostly semi-arid to arid. In lower elevations, summers are hot with blazing sunshine. Winters feature occasional rain and/or snow from low-pressure systems that cross the area from the Mediterranean Sea. Average monthly precipitation is extremely low from July to September, rises in autumn (October and November) and is highest in March or April, followed by swift drying in May and June. Winds can be strong, producing dust storms sometimes, especially toward the end of the dry season in September and October. Specific cities that exemplify Central Asian climate patterns include Tashkent and Samarkand, Uzbekistan, Ashgabat, Turkmenistan, and Dushanbe, Tajikistan, the last of these representing one of the wettest climates in Central Asia, with an average annual precipitation of over 22 inches.

According to the WWF Ecozones system, Central Asia is part of the Palearctic ecozone. The largest biomes in Central Asia are the temperate grasslands, savannas, and shrublands biome. Central Asia also contains the montane grasslands and shrublands, deserts and xeric shrublands as well as temperate coniferous forests biomes.

Although, during the golden age of Orientalism the place of Central Asia in the world history was marginalized, contemporary historiography has rediscovered the "centrality" of the Central Asia. The history of Central Asia is defined by the area's climate and geography. The aridness of the region made agriculture difficult, and its distance from the sea cut it off from much trade. Thus, few major cities developed in the region; instead, the area was for millennia dominated by the nomadic horse peoples of the steppe.

Relations between the steppe nomads and the settled people in and around Central Asia were long marked by conflict. The nomadic lifestyle was well suited to warfare, and the steppe horse riders became some of the most militarily potent people in the world, limited only by their lack of internal unity. Any internal unity that was achieved was most probably due to the influence of the Silk Road, which traveled along Central Asia. Periodically, great leaders or changing conditions would organize several tribes into one force and create an almost unstoppable power. These included the Hun invasion of Europe, the Wu Hu attacks on China and most notably the Mongol conquest of much of Eurasia.

During pre-Islamic and early Islamic times, southern Central Asia was inhabited predominantly by speakers of Iranian languages. Among the ancient sedentary Iranian peoples, the Sogdians and Chorasmians played an important role, while Iranian peoples such as Scythians and the later on Alans lived a nomadic or semi-nomadic lifestyle. The well-preserved Tarim mummies with Caucasoid features have been found in the Tarim Basin.

The main migration of Turkic peoples occurred between the 5th and 10th centuries, when they spread across most of Central Asia. The Tang Chinese were defeated by the Arabs at the battle of Talas in 751, marking the end of the Tang Dynasty's western expansion. The Tibetan Empire would take the chance to rule portion of Central Asia along with South Asia. During the 13th and 14th centuries, the Mongols conquered and ruled the largest contiguous empire in recorded history. Most of Central Asia fell under the control of the Chagatai Khanate. 
The dominance of the nomads ended in the 16th century, as firearms allowed settled peoples to gain control of the region. Russia, China, and other powers expanded into the region and had captured the bulk of Central Asia by the end of the 19th century. After the Russian Revolution, the western Central Asian regions were incorporated into the Soviet Union. The eastern part of Central Asia, known as East Turkistan or Xinjiang, was incorporated into the People's Republic of China. Mongolia remained independent but became a Soviet satellite state. Afghanistan remained relatively independent of major influence by the USSR until the Saur Revolution of 1978.

The Soviet areas of Central Asia saw much industrialization and construction of infrastructure, but also the suppression of local cultures, hundreds of thousands of deaths from failed collectivization programs, and a lasting legacy of ethnic tensions and environmental problems. Soviet authorities deported millions of people, including entire nationalities, from western areas of the USSR to Central Asia and Siberia. According to Touraj Atabaki and Sanjyot Mehendale, "From 1959 to 1970, about two million people from various parts of the Soviet Union migrated to Central Asia, of which about one million moved to Kazakhstan."

With the collapse of the Soviet Union, five countries gained independence. In nearly all the new states, former Communist Party officials retained power as local strongmen. None of the new republics could be considered functional democracies in the early days of independence, although in recent years Kyrgyzstan, Kazakhstan and Mongolia have made further progress towards more open societies, unlike Uzbekistan, Tajikistan, and Turkmenistan, which have maintained many Soviet-style repressive tactics.

At the crossroads of Asia, shamanistic practices live alongside Buddhism. Thus, Yama, Lord of Death, was revered in Tibet as a spiritual guardian and judge. Mongolian Buddhism, in particular, was influenced by Tibetan Buddhism. The Qianlong Emperor of Qing China in the 18th century was Tibetan Buddhist and would sometimes travel from Beijing to other cities for personal religious worship.

Central Asia also has an indigenous form of improvisational oral poetry that is over 1000 years old. It is principally practiced in Kyrgyzstan and Kazakhstan by "akyns", lyrical improvisationists. They engage in lyrical battles, the "aitysh" or the "alym sabak". The tradition arose out of early bardic oral historians. They are usually accompanied by a stringed instrument—in Kyrgyzstan, a three-stringed komuz, and in Kazakhstan, a similar two-stringed instrument, the dombra.

Photography in Central Asia began to develop after 1882, when a Russian Mennonite photographer named Wilhelm Penner moved to the Khanate of Khiva during the Mennonite migration to Central Asia led by Claas Epp, Jr.. Upon his arrival to Khanate of Khiva, Penner shared his photography skills with a local student Khudaybergen Divanov, who later became the founder of Uzbek photography.

Some also learn to sing the "Manas", Kyrgyzstan's epic poem (those who learn the "Manas" exclusively but do not improvise are called "manaschis"). During Soviet rule, "akyn" performance was co-opted by the authorities and subsequently declined in popularity. With the fall of the Soviet Union, it has enjoyed a resurgence, although "akyns" still do use their art to campaign for political candidates. A 2005 "The Washington Post" article proposed a similarity between the improvisational art of "akyns" and modern freestyle rap performed in the West.

As a consequence of Russian colonization, European fine arts – painting, sculpture and graphics – have developed in Central Asia. The first years of the Soviet regime saw the appearance of modernism, which took inspiration from the Russian avant-garde movement. Until the 1980s, Central Asian arts had developed along with general tendencies of Soviet arts. In the 90's, arts of the region underwent some significant changes. Institutionally speaking, some fields of arts were regulated by the birth of the art market, some stayed as representatives of official views, while many were sponsored by international organizations. The years of 1990–2000 were times for the establishment of contemporary arts. In the region, many important international exhibitions are taking place, Central Asian art is represented in European and American museums, and the Central Asian Pavilion at the Venice Biennale has been organized since 2005.

Equestrian sports are traditional in Central Asia, with disciplines like endurance riding, buzkashi, dzhigit and kyz kuu.

Association football is popular across Central Asia. Most countries are members of the Central Asian Football Association, a region of the Asian Football Confederation. However, Kazakhstan is a member of the UEFA.

Wrestling is popular across Central Asia, with Kazakhstan having claimed 14 Olympic medals, Uzbekistan seven, and Kyrgyzstan three. As former Soviet states, Central Asian countries have been successful in gymnastics.

Mixed Martial Arts is one of more common sports in Central Asia, Kyrgyz athlete Valentina Shevchenko holding the UFC Flyweight Champion title.

Cricket is the most popular sport in Afghanistan. The Afghanistan national cricket team, first formed in 2001, has claimed wins over Bangladesh, West Indies and Zimbabwe.

Notable Kazakh competitors include cyclists Alexander Vinokourov and Andrey Kashechkin, boxer Vassiliy Jirov and Gennady Golovkin, runner Olga Shishigina, decathlete Dmitriy Karpov, gymnast Aliya Yussupova, judoka Askhat Zhitkeyev and Maxim Rakov, skier Vladimir Smirnov, weightlifter Ilya Ilyin, and figure skaters Denis Ten and Elizabet Tursynbaeva.

Notable Uzbekistani competitors include cyclist Djamolidine Abdoujaparov, boxer Ruslan Chagaev, canoer Michael Kolganov, gymnast Oksana Chusovitina, tennis player Denis Istomin, chess player Rustam Kasimdzhanov, and figure skater Misha Ge.

Since gaining independence in the early 1990s, the Central Asian republics have gradually been moving from a state-controlled economy to a market economy. The ultimate aim is to emulate the Asian Tigers by becoming the local equivalent, Central Asian snow leopards. However, reform has been deliberately gradual and selective, as governments strive to limit the social cost and ameliorate living standards. All five countries are implementing structural reforms to improve competitiveness. In particular, they have been modernizing the industrial sector and fostering the development of service industries through business-friendly fiscal policies and other measures, to reduce the share of agriculture in GDP. Between 2005 and 2013, the share of agriculture dropped in all but Tajikistan, where it progressed to the detriment of industry. The fastest growth in industry was observed in Turkmenistan, whereas the services sector progressed most in the other four countries.

Public policies pursued by Central Asian governments focus on buffering the political and economic spheres from external shocks. This includes maintaining a trade balance, minimizing public debt and accumulating national reserves. They cannot totally insulate themselves from negative exterior forces, however, such as the persistently weak recovery of global industrial production and international trade since 2008. Notwithstanding this, they have emerged relatively unscathed from the global financial crisis of 2008–2009. Growth faltered only briefly in Kazakhstan, Tajikistan and Turkmenistan and not at all in Uzbekistan, where the economy grew by more than 7% per year on average between 2008 and 2013. Turkmenistan flirted with growth of 15% (14.7%) in 2011. Kyrgyzstan's performance has been more erratic but this phenomenon was visible well before 2008.

The republics which have fared best surfed on the wave of the commodities boom during the first decade of the new century. Kazakhstan and Turkmenistan have abundant oil and natural gas reserves and Uzbekistan's own reserves make it more or less self-sufficient. Kyrgyzstan, Tajikistan and Uzbekistan all have gold reserves and Kazakhstan has the world's largest uranium reserves. Fluctuating global demand for cotton, aluminium and other metals (except gold) in recent years has hit Tajikistan hardest, since aluminium and raw cotton are its chief exports − the Tajik Aluminium Company is the country's primary industrial asset. In January 2014, the Minister of Agriculture announced the government's intention to reduce the acreage of land cultivated by cotton to make way for other crops. Uzbekistan and Turkmenistan are major cotton exporters themselves, ranking fifth and ninth respectively worldwide for volume in 2014.

Although both exports and imports have grown impressively over the past decade, Central Asian republics countries remain vulnerable to economic shocks, owing to their reliance on exports of raw materials, a restricted circle of trading partners and a negligible manufacturing capacity. Kyrgyzstan has the added disadvantage of being considered resource poor, although it does have ample water. Most of its electricity is generated by hydropower.

The Kyrgyz economy was shaken by a series of shocks between 2010 and 2012. In April 2010, President Kurmanbek Bakiyev was deposed by a popular uprising, with former minister of foreign affairs Roza Otunbayeva assuring the interim presidency until the election of Almazbek Atambayev in November 2011. Food prices rose two years in a row and, in 2012, production at the major Kumtor gold mine fell by 60% after the site was perturbed by geological movements. According to the World Bank, 33.7% of the population was living in absolute poverty in 2010 and 36.8% a year later.

Despite high rates of economic growth in recent years, GDP per capita in Central Asia was higher than the average for developing countries only in Kazakhstan in 2013 (PPP$23 206) and Turkmenistan (PPP$14 201). It dropped to PPP$5 167 for Uzbekistan, home to 45% of the region's population, and was even lower for Kyrgyzstan and Tajikistan.

Kazakhstan leads the Central Asian region in terms of foreign direct investments. The Kazakh economy sccounts for more than 70% of all the investment attracted in Central Asia.

In terms of the economic influence of big powers, China is viewed as a one of the key economic players in Central Asia, especially after Beijing launched its grand development strategy known as the Belt and Road Initiative (BRI) in 2013.

Bolstered by strong economic growth in all but Kyrgyzstan, national development strategies are fostering new high-tech industries, pooling resources and orienting the economy towards export markets. Many national research institutions established during the Soviet era have since become obsolete with the development of new technologies and changing national priorities. This has led countries to reduce the number of national research institutions since 2009 by grouping existing institutions to create research hubs. Several of the Turkmen Academy of Science's institutes were merged in 2014: the Institute of Botany was merged with the Institute of Medicinal Plants to become the Institute of Biology and Medicinal Plants; the Sun Institute was merged with the Institute of Physics and Mathematics to become the Institute of Solar Energy; and the Institute of Seismology merged with the State Service for Seismology to become the Institute of Seismology and Atmospheric Physics. In Uzbekistan, more than 10 institutions of the Academy of Sciences have been reorganized, following the issuance of a decree by the Cabinet of Ministers in February 2012. The aim is to orient academic research towards problem-solving and ensure continuity between basic and applied research. For example, the Mathematics and Information Technology Research Institute has been subsumed under the National University of Uzbekistan and the Institute for Comprehensive Research on Regional Problems of Samarkand has been transformed into a problem-solving laboratory on environmental issues within Samarkand State University. Other research institutions have remained attached to the Uzbek Academy of Sciences, such as the Centre of Genomics and Bioinformatics.

Kazakhstan and Turkmenistan are also building technology parks as part of their drive to modernize infrastructure. In 2011, construction began of a technopark in the village of Bikrova near Ashgabat, the Turkmen capital. It will combine research, education, industrial facilities, business incubators and exhibition centres. The technopark will house research on alternative energy sources (sun, wind) and the assimilation of nanotechnologies. Between 2010 and 2012, technological parks were set up in the east, south and north Kazakhstan oblasts (administrative units) and in the capital, Nur-Sultan. A Centre for Metallurgy was also established in the east Kazakhstan oblast, as well as a Centre for Oil and Gas Technologies which will be part of the planned Caspian Energy Hub. In addition, the Centre for Technology Commercialization has been set up in Kazakhstan as part of the Parasat National Scientific and Technological Holding, a joint stock company established in 2008 that is 100% state-owned. The centre supports research projects in technology marketing, intellectual property protection, technology licensing contracts and start-ups. The centre plans to conduct a technology audit in Kazakhstan and to review the legal framework regulating the commercialization of research results and technology.Countries are seeking to augment the efficiency of traditional extractive sectors but also to make greater use of information and communication technologies and other modern technologies, such as solar energy, to develop the business sector, education and research. In March 2013, two research institutes were created by presidential decree to foster the development of alternative energy sources in Uzbekistan, with funding from the Asian Development Bank and other institutions: the SPU Physical−Technical Institute (Physics Sun Institute) and the International Solar Energy Institute. Three universities have been set up since 2011 to foster competence in strategic economic areas: Nazarbayev University in Kazakhstan (first intake in 2011), an international research university, Inha University in Uzbekistan (first intake in 2014), specializing in information and communication technologies, and the International Oil and Gas University in Turkmenistan (founded in 2013). Kazakhstan and Uzbekistan are both generalizing the teaching of foreign languages at school, in order to facilitate international ties. Kazakhstan and Uzbekistan have both adopted the three-tier bachelor's, master's and PhD degree system, in 2007 and 2012 respectively, which is gradually replacing the Soviet system of Candidates and Doctors of Science. In 2010, Kazakhstan became the only Central Asian member of the Bologna Process, which seeks to harmonize higher education systems in order to create a European Higher Education Area.

The Central Asian republics' ambition of developing the business sector, education and research is being hampered by chronic low investment in research and development. Over the decade to 2013, the region's investment in research and development hovered around 0.2–0.3% of GDP. Uzbekistan broke with this trend in 2013 by raising its own research intensity to 0.41% of GDP.

Kazakhstan is the only country where the business enterprise and private non-profit sectors make any significant contribution to research and development – but research intensity overall is low in Kazakhstan: just 0.18% of GDP in 2013. Moreover, few industrial enterprises conduct research in Kazakhstan. Only one in eight (12.5%) of the country's manufacturing firms were active in innovation in 2012, according to a survey by the UNESCO Institute for Statistics. Enterprises prefer to purchase technological solutions that are already embodied in imported machinery and equipment. Just 4% of firms purchase the license and patents that come with this technology. Nevertheless, there appears to be a growing demand for the products of research, since enterprises spent 4.5 times more on scientific and technological services in 2008 than in 1997.
Kazakhstan and Uzbekistan count the highest researcher density in Central Asia. The number of researchers per million population is close to the world average (1,083 in 2013) in Kazakhstan (1,046) and higher than the world average in Uzbekistan (1,097).

Kazakhstan is the only Central Asian country where the business enterprise and private non-profit sectors make any significant contribution to research and development. Uzbekistan is in a particularly vulnerable position, with its heavy reliance on higher education: three-quarters of researchers were employed by the university sector in 2013 and just 6% in the business enterprise sector. With most Uzbek university researchers nearing retirement, this imbalance imperils Uzbekistan's research future. Almost all holders of a Candidate of Science, Doctor of Science or PhD are more than 40 years old and half are aged over 60; more than one in three researchers (38.4%) holds a PhD degree, or its equivalent, the remainder holding a bachelor's or master's degree.

Kazakhstan, Kyrgyzstan and Uzbekistan have all maintained a share of women researchers above 40% since the fall of the Soviet Union. Kazakhstan has even achieved gender parity, with Kazakh women dominating medical and health research and representing some 45–55% of engineering and technology researchers in 2013. In Tajikistan, however, only one in three scientists (34%) was a woman in 2013, down from 40% in 2002. Although policies are in place to give Tajik women equal rights and opportunities, these are underfunded and poorly understood. Turkmenistan has offered a state guarantee of equality for women since a law adopted in 2007 but the lack of available data makes it impossible to draw any conclusions as to the law's impact on research. As for Turkmenistan, it does not make data available on higher education, research expenditure or researchers.

Table: PhDs obtained in science and engineering in Central Asia, 2013 or closest year
Source: "UNESCO Science Report: towards 2030" (2015), Table 14.1

"Note: PhD graduates in science cover life sciences, physical sciences, mathematics and statistics, and computing; PhDs in engineering also cover manufacturing and construction. For Central Asia, the generic term of PhD also encompasses Candidate of Science and Doctor of Science degrees. Data are unavailable for Turkmenistan."

Table: Central Asian researchers by field of science and gender, 2013 or closest year
Source: "UNESCO Science Report: towards 2030" (2015), Table 14.1

The number of scientific papers published in Central Asia grew by almost 50% between 2005 and 2014, driven by Kazakhstan, which overtook Uzbekistan over this period to become the region's most prolific scientific publisher, according to Thomson Reuters' Web of Science (Science Citation Index Expanded). Between 2005 and 2014, Kazakhstan's share of scientific papers from the region grew from 35% to 56%. Although two-thirds of papers from the region have a foreign co-author, the main partners tend to come from beyond Central Asia, namely the Russian Federation, USA, German, United Kingdom and Japan.

Five Kazakh patents were registered at the US Patent and Trademark Office between 2008 and 2013, compared to three for Uzbek inventors and none at all for the other three Central Asian republics, Kyrgyzstan, Tajikistan and Turkmenistan.
Kazakhstan is Central Asia's main trader in high-tech products. Kazakh imports nearly doubled between 2008 and 2013, from US$2.7 billion to US$5.1 billion. There has been a surge in imports of computers, electronics and telecommunications; these products represented an investment of US$744 million in 2008 and US$2.6 billion five years later. The growth in exports was more gradual – from US$2.3 billion to US$3.1 billion – and dominated by chemical products (other than pharmaceuticals), which represented two-thirds of exports in 2008 (US$1.5 billion) and 83% (US$2.6 billion) in 2013.

The five Central Asian republics belong to several international bodies, including the Organization for Security and Co-operation in Europe, the Economic Cooperation Organization and the Shanghai Cooperation Organisation. They are also members of the Central Asia Regional Economic Cooperation (CAREC) Programme, which also includes Afghanistan, Azerbaijan, China, Mongolia and Pakistan. In November 2011, the 10 member countries adopted the "CAREC 2020 Strategy", a blueprint for furthering regional co-operation. Over the decade to 2020, US$50 billion is being invested in priority projects in transport, trade and energy to improve members' competitiveness. The landlocked Central Asian republics are conscious of the need to co-operate in order to maintain and develop their transport networks and energy, communication and irrigation systems. Only Kazakhstan and Turkmenistan border the Caspian Sea and none of the republics has direct access to an ocean, complicating the transportation of hydrocarbons, in particular, to world markets.

Kazakhstan is also one of the three founding members of the Eurasian Economic Union in 2014, along with Belarus and the Russian Federation. Armenia and Kyrgyzstan have since joined this body. As co-operation among the member states in science and technology is already considerable and well-codified in legal texts, the Eurasian Economic Union is expected to have a limited additional impact on co-operation among public laboratories or academia but it should encourage business ties and scientific mobility, since it includes provision for the free circulation of labour and unified patent regulations.

Kazakhstan and Tajikistan participated in the Innovative Biotechnologies Programme (2011–2015) launched by the Eurasian Economic Community, the predecessor of the Eurasian Economic Union, The programme also involved Belarus and the Russian Federation. Within this programme, prizes were awarded at an annual bio-industry exhibition and conference. In 2012, 86 Russian organizations participated, plus three from Belarus, one from Kazakhstan and three from Tajikistan, as well as two scientific research groups from Germany. At the time, Vladimir Debabov, Scientific Director of the Genetika State Research Institute for Genetics and the Selection of Industrial Micro-organisms in the Russian Federation, stressed the paramount importance of developing bio-industry. 'In the world today, there is a strong tendency to switch from petrochemicals to renewable biological sources,' he said. 'Biotechnology is developing two to three times faster than chemicals.'

Kazakhstan also participated in a second project of the Eurasian Economic Community, the establishment of the Centre for Innovative Technologies on 4 April 2013, with the signing of an agreement between the Russian Venture Company (a government fund of funds), the Kazakh JSC National Agency and the Belarusian Innovative Foundation. Each of the selected projects is entitled to funding of US$3–90 million and is implemented within a public–private partnership. The first few approved projects focused on supercomputers, space technologies, medicine, petroleum recycling, nanotechnologies and the ecological use of natural resources. Once these initial projects have spawned viable commercial products, the venture company plans to reinvest the profits in new projects. This venture company is not a purely economic structure; it has also been designed to promote a common economic space among the three participating countries.

Four of the five Central Asian republics have also been involved in a project launched by the European Union in September 2013, IncoNet CA. The aim of this project is to encourage Central Asian countries to participate in research projects within Horizon 2020, the European Union's eighth research and innovation funding programme. The focus of this research projects is on three societal challenges considered as being of mutual interest to both the European Union and Central Asia, namely: climate change, energy and health. IncoNet CA builds on the experience of earlier projects which involved other regions, such as Eastern Europe, the South Caucasus and the Western Balkans. IncoNet CA focuses on twinning research facilities in Central Asia and Europe. It involves a consortium of partner institutions from Austria, the Czech Republic, Estonia, Germany, Hungary, Kazakhstan, Kyrgyzstan, Poland, Portugal, Tajikistan, Turkey and Uzbekistan. In May 2014, the European Union launched a 24-month call for project applications from twinned institutions – universities, companies and research institutes – for funding of up to €10, 000 to enable them to visit one another's facilities to discuss project ideas or prepare joint events like workshops.

The International Science and Technology Center (ISTC) was established in 1992 by the European Union, Japan, the Russian Federation and the US to engage weapons scientists in civilian research projects and to foster technology transfer. ISTC branches have been set up in the following countries party to the agreement: Armenia, Belarus, Georgia, Kazakhstan, Kyrgyzstan and Tajikistan. The headquarters of ISTC were moved to Nazarbayev University in Kazakhstan in June 2014, three years after the Russian Federation announced its withdrawal from the centre.

Kyrgyzstan, Tajikistan and Kazakhstan have been members of the World Trade Organization since 1998, 2013 and 2015 respectively.

By a broad definition including Mongolia and Afghanistan, more than 90 million people live in Central Asia, about 2% of Asia's total population. Of the regions of Asia, only North Asia has fewer people. It has a population density of 9 people per km, vastly less than the 80.5 people per km of the continent as a whole.

Russian, as well as being spoken by around six million ethnic Russians and Ukrainians of Central Asia, is the de facto lingua franca throughout the former Soviet Central Asian Republics. Mandarin Chinese has an equally dominant presence in Inner Mongolia, Qinghai and Xinjiang.

The languages of the majority of the inhabitants of the former Soviet Central Asian Republics belong to the Turkic language group. Turkmen, is mainly spoken in Turkmenistan, and as a minority language in Afghanistan, Russia, Iran and Turkey. Kazakh and Kyrgyz are related languages of the Kypchak group of Turkic languages and are spoken throughout Kazakhstan, Kyrgyzstan, and as a minority language in Tajikistan, Afghanistan and Xinjiang. Uzbek and Uyghur are spoken in Uzbekistan, Tajikistan, Kyrgyzstan, Afghanistan and Xinjiang.

The Turkic languages may belong to a larger, but controversial, Altaic language family, which includes Mongolian. Mongolian is spoken throughout Mongolia and into Buryatia, Kalmyk, Tuva, Inner Mongolia, and Xinjiang.

Middle Iranian languages were once spoken throughout Central Asia, such as the once prominent Sogdian, Khwarezmian, Bactrian and Scythian, which are now extinct and belonged to the Eastern Iranian family. The Eastern Iranian Pashto language is still spoken in Afghanistan and northwestern Pakistan. Other minor Eastern Iranian languages such as Shughni, Munji, Ishkashimi, Sarikoli, Wakhi, Yaghnobi and Ossetic are also spoken at various places in Central Asia. Varieties of Persian are also spoken as a major language in the region, locally known as Dari (in Afghanistan), Tajik (in Tajikistan and Uzbekistan), and Bukhori (by the Bukharan Jews of Central Asia).

Tocharian, another Indo-European language group, which was once predominant in oases on the northern edge of the Tarim Basin of Xinjiang, is now extinct.

Other language groups include the Tibetic languages, spoken by around six million people across the Tibetan Plateau and into Qinghai, Sichuan, Ladakh and Baltistan, and the Nuristani languages of northeastern Afghanistan. Dardic languages, such as Shina, Kashmiri, Pashayi and Khowar, are also spoken in eastern Afghanistan, the Gilgit-Baltistan and Khyber Pakhtunkhwa of Pakistan and the disputed territory of Kashmir. Korean is spoken by the Koryo-saram minority, mainly in Kazakhstan and Uzbekistan.

Islam is the religion most common in the Central Asian Republics, Afghanistan, Xinjiang and the peripheral western regions, such as Bashkortostan. Most Central Asian Muslims are Sunni, although there are sizable Shia minorities in Afghanistan and Tajikistan.

Buddhism and Zoroastrianism were the major faiths in Central Asia prior to the arrival of Islam. Zoroastrian influence is still felt today in such celebrations as Nowruz, held in all five of the Central Asian states.

Buddhism was a prominent religion in Central Asia prior to the arrival of Islam, and the transmission of Buddhism along the Silk Road eventually brought the religion to China. Amongst the Turkic peoples, Tengrianism was the popular religion before arrival of Islam. Tibetan Buddhism is most common in Tibet, Mongolia, Ladakh and the southern Russian regions of Siberia.

The form of Christianity most practiced in the region in previous centuries was Nestorianism, but now the largest denomination is the Russian Orthodox Church, with many members in Kazakhstan where about 25% of the population of 19 million identify as Christian, 17% in Uzbekistan and 5% in Kyrgyzstan.

The Bukharan Jews were once a sizable community in Uzbekistan and Tajikistan, but nearly all have emigrated since the dissolution of the Soviet Union.

In Siberia, Shamanism is practiced, including forms of divination, such as Kumalak.

Contact and migration with Han people from China has brought Confucianism, Daoism, Mahayana Buddhism, and other Chinese folk beliefs into the region.

Central Asia has long been a strategic location merely because of its proximity to several great powers on the Eurasian landmass. The region itself never held a dominant stationary population nor was able to make use of natural resources. Thus, it has rarely throughout history become the seat of power for an empire or influential state. Central Asia has been divided, redivided, conquered out of existence, and fragmented time and time again. Central Asia has served more as the battleground for outside powers than as a power in its own right.

Central Asia had both the advantage and disadvantage of a central location between four historical seats of power. From its central location, it has access to trade routes to and from all the regional powers. On the other hand, it has been continuously vulnerable to attack from all sides throughout its history, resulting in political fragmentation or outright power vacuum, as it is successively dominated.

In the post–Cold War era, Central Asia is an ethnic cauldron, prone to instability and conflicts, without a sense of national identity, but rather a mess of historical cultural influences, tribal and clan loyalties, and religious fervor. Projecting influence into the area is no longer just Russia, but also Turkey, Iran, China, Pakistan, India and the United States:

Russian historian Lev Gumilev wrote that Xiongnu, Mongols (Mongol Empire, Zunghar Khanate) and Turkic peoples (Turkic Khaganate, Uyghur Khaganate) played a role to stop Chinese aggression to the north. The Turkic Khaganate had special policy against Chinese assimilation policy. Another interesting theoretical analysis on the historical-geopolitics of the Central Asia was made through the reinterpretation of Orkhun Inscripts.

The region, along with Russia, is also part of "the great pivot" as per the Heartland Theory of Halford Mackinder, which says that the power which controls Central Asia—richly endowed with natural resources—shall ultimately be the "empire of the world".

In the context of the United States' War on Terror, Central Asia has once again become the center of geostrategic calculations. Pakistan's status has been upgraded by the U.S. government to Major non-NATO ally because of its central role in serving as a staging point for the invasion of Afghanistan, providing intelligence on Al-Qaeda operations in the region, and leading the hunt on Osama bin Laden.

Afghanistan, which had served as a haven and source of support for Al-Qaeda under the protection of Mullah Omar and the Taliban, was the target of a U.S. invasion in 2001 and ongoing reconstruction and drug-eradication efforts. U.S. military bases have also been established in Uzbekistan and Kyrgyzstan, causing both Russia and the People's Republic of China to voice their concern over a permanent U.S. military presence in the region.

Western governments have accused Russia, China and the former Soviet republics of justifying the suppression of separatist movements, and the associated ethnics and religion with the War on Terror.





</doc>
<doc id="6744" url="https://en.wikipedia.org/wiki?curid=6744" title="Constantine II">
Constantine II

Constantine II may refer to:




</doc>
<doc id="6745" url="https://en.wikipedia.org/wiki?curid=6745" title="Couscous">
Couscous

Couscous (Berber : ⵙⴽⵙⵓ "seksu," "") is originally a Maghrebi dish of small (about diameter) steamed balls of crushed durum wheat semolina that is traditionally served with a stew spooned on top. Pearl millet and sorghum especially in the Sahel and other cereals can be cooked in a similar way and the resulting dishes are also sometimes called couscous.

Couscous is a staple food throughout the North African cuisines of Algeria, Morocco, Tunisia, Mauritania, and Libya, as well as in Israel, due to the large population of Jews of North African origin. In Western supermarkets, it is sometimes sold in instant form with a flavor packet, and may be served as a side or on its own as a main dish.

The original name may be derived from the Arabic word "Kaskasa", meaning "to pound small" or the Berber "Seksu", meaning "well rolled", "well formed", or "rounded".

Numerous different names and pronunciations for couscous exist around the world. Couscous is or in the United Kingdom and only the latter in the United States. It is sometimes pronounced "kuskusi" () in Arabic, while it is also known in Morocco as "seksu"; "kesksu" or () ; in Algeria as "kosksi" or as "ṭa`ām" (, literally meaning "food"); in Tunisia and Libya "kosksi" or "kuseksi", in Egypt "kuskusi" (), in Israel it is known as (), or "kuskus", in Sicily "cuscusu" and "keskes" in Tuareg.

The origin of couscous appears to be in the region from eastern to northern Africa where Berbers used it as early as the 7th century. Recognized as a traditional North African delicacy, it is a common cuisine component among Maghreb countries.

Ibn Battuta (b. Morocco, 1304–1368? AD) stated in his "Rihlah" (Travels), indicating what may be the earliest mention of couscous (kuskusu) in West Africa from the early 1350s:

Couscous was traditionally made from the hard part of the durum, the part of the grain that resisted the grinding of the millstone. The semolina is sprinkled with water and rolled with the hands to form small pellets, sprinkled with dry flour to keep them separate, and then sieved. Any pellets that are too small to be finished granules of couscous fall through the sieve and are again rolled and sprinkled with dry semolina and rolled into pellets. This labor-intensive process continues until all the semolina has been formed into tiny granules of couscous. In the traditional method of preparing couscous, groups of women came together to make large batches over several days, which were then dried in the sun and used for several months. Handmade couscous may need to be rehydrated as it is prepared; this is achieved by a process of moistening and steaming over stew until the couscous reaches the desired light and fluffy consistency.

In some regions couscous is made from Farina or coarsely ground barley or pearl millet. In Brazil, the traditional couscous is made from cornmeal.
In modern times, couscous production is largely mechanized, and the product is sold in markets around the world. This couscous can be sauteed before it is cooked in water or another liquid. Properly cooked couscous is light and fluffy, not gummy or gritty. Traditionally, North Africans use a food steamer (called a"Taseksut" in Berber, a "kiskas" in Arabic or a "couscoussier" in French). The base is a tall metal pot shaped rather like an oil jar in which the meat and vegetables are cooked as a stew. On top of the base, a steamer sits where the couscous is cooked, absorbing the flavours from the stew. The lid to the steamer has holes around its edge so steam can escape. It is also possible to use a pot with a steamer insert. If the holes are too big, the steamer can be lined with damp cheesecloth. There is little archaeological evidence of early diets including couscous, possibly because the original "couscoussier" was probably made from organic materials that could not survive extended exposure to the elements.

The couscous that is sold in most Western supermarkets has been pre-steamed and dried. It is typically prepared by adding 1.5 measures of boiling water or stock to each measure of couscous then leaving covered tightly for about five minutes. Pre-steamed couscous takes less time to prepare than regular couscous, most dried pasta, or dried grains (such as rice).

In Tunisia, Algeria, Morocco, and Libya, couscous is generally served with vegetables (carrots, potatoes, and turnips) cooked in a spicy or mild broth or stew, and some meat (generally, chicken, lamb or mutton). In Algeria and Morocco it may be served at the end of a meal or by itself as a delicacy called ""sfouff"". The couscous is usually steamed several times until it is fluffy and pale in color. It is then sprinkled with almonds, cinnamon and sugar. Traditionally, this dessert is served with milk perfumed with orange flower water, or it can be served plain with buttermilk in a bowl as a cold light soup for supper. Algerian couscous includes tomatoes and a variety of legumes and vegetables, and Moroccan couscous uses saffron. Saharan couscous is served without legumes and without broth.

In Tunisia, it is made mostly spicy with harissa sauce and served commonly with any dish, including lamb, fish, seafood, beef and sometimes in southern regions, camel. Fish couscous is a Tunisian specialty and can also be made with octopus, squid or other seafood in hot, red, spicy sauce.

In Libya, it is mostly served with meat, specifically mostly lamb, but also camel, and rarely beef, in Tripoli and the western parts of Libya, but not during official ceremonies or weddings. Another way to eat couscous is as a dessert; it is prepared with dates, sesame, and pure honey, and locally referred to as "maghrood".

In Israel, and among members of Sephardic Jewish communities in the diaspora, couscous is a common food and it is frequently consumed. Couscous is not indigenous to the Eastern Mediterranean, and arrived in the region with the migration of Jews from North Africa to Israel in the 20th century. Since then it has become a very popular dish in the country, and it is a staple about the Sephardic community and people of all backgrounds.

In addition, Israelis of all backgrounds commonly eat ptitim, also known as Israeli couscous, or pearl couscous, which is similar to regular couscous except it is larger like the Ashkenazi farfel or the Levantine maftoul (though ptitim does not contain bulgur unlike maftoul). Ptitim is a staple food and it very popular, especially with children, and is commonly served with butter or perhaps cooked with vegetables or chicken broth. However, it is prepared more similarly to pasta and it only boiled for a few minutes, and it is not steamed and fluffed like North African couscous. There are other shapes of ptitim, including a shape which resembled rice, which is also known as Ben Gurion’s rice, which was created in the 1950's after Israel's independence, as there were rations and food shortages and rice was unavailable in the country.

In addition to ptitim, there are many varieties of couscous that can be found throughout Israel. Among Israelis of Sephardic origin, (whose families moved to Israel from North African counties such as Morocco, Algeria, Tunisia, and Libya), couscous is a very common food and is served at almost every meal, especially on holidays, specials ocassions and celebrations, as well as on Shabbat (Jewish sabbath), for their Friday night dinners. Many people make their own couscous by hand, but it is a very labor intensive process. It is also common to buy instant couscous, and there are a large variety of brands and varieties for sale in Israel.

Different communities have different styles and sizes of couscous, similar to the differences in size and style between the couscous of the different cuisines of the Maghreb. Moroccan Jewish couscous is larger, and is frequently prepared with aromatic spices and served with meat. Algerian Jewish couscous is smaller. The smallest is Tunisian Jewish couscous, which is not much larger than grains of coarse sand. Tunisian Jewish couscous is often served with harissa or shkug, or cooked with vegetables such as carrots, zucchini, or potatoes and served with chamin, a North African Jewish beef stew similar to cholent, that is often served for Shabbat. Couscous is also be prepared with cinnamon sticks, orange and other citrus peel, and aromatic spices for special ocassions and celebrations. It is not common to find sweet couscous or dessert couscous in Israel, as in Egypt and other countries.

In Egypt, couscous is eaten more as a dessert. It is prepared with butter, sugar, cinnamon, raisins, and nuts and topped with cream.

In the Palestinian community, North African style couscous is not consumed. The Palestinians instead prepare a dish called maftoul, which is also consumed in Lebanon, Syria and Jordan and is called mograhbieh. Maftoul can be considered to be a special type of couscous but made from different ingredients and a different shape. It is significantly larger than North African couscous, and is similar in size to Israeli couscous, but has a different preparation. Maffoul is similarly steamed as North African couscous and often served on special occasions in a chicken broth with garbanzo beans and tender pieces of chicken taken off the bone. Maftoul is an Arabic word derived from the root "fa-ta-la", which means to roll or to twist, which is exactly describing the method used to make maftoul by hand rolling bulgur with wheat flour.< Couscous may be used to make a breakfast tabbouleh salad. Though usually cooked it water, it can also be cooking in another liquid, like apple juice, and served with dried fruit and honey.

In the Levant (excluding Israel and the Palestinian territories) they consume a large type of couscous with bulgur at the center, similar to the Palestinian maftoul called mograhbieh, which is commonly served in Lebanon, Syria and Jordan as part of a stew or cooked in chicken broth with cinnamon, caraway and chickpeas.

Couscous is also consumed in France, where it is considered a traditional dish, and has also become common in Spain, Portugal, Italy, and Greece. Indeed, many polls have indicated that it is often a favorite dish. In France, Spain and Italy, the word "couscous" ("cuscús" in Spanish and Italian; "cuscuz" in Portuguese) usually refers to couscous together with the stew. Packaged sets containing a box of quick-preparation couscous and a can of vegetables and, generally, meat are sold in French, Spanish and Italian grocery stores and supermarkets. In France, it is generally served with harissa sauce, a style inherited from the Tunisian cuisine. Indeed, couscous was voted as the third-favourite dish of French people in 2011 in a study by TNS Sofres for magazine "Vie Pratique Gourmand", and the first in the east of France.

In the Sahelian countries of West Africa, such as Mali and Senegal, pearl millet is pounded or milled to the size and consistency necessary for the couscous.

In a one cup reference amount, wheat couscous provides 6 grams of protein, 36 grams of carbohydrates, and negligible fat.

Couscous is distinct from pasta, even pasta such as orzo and risoni of similar size, in that it is made from crushed durum wheat semolina, while pasta is made from ground wheat. Couscous and pasta have similar nutritional value, although pasta is usually more refined. Pasta is cooked by boiling and couscous is steamed. Burghul or bulgur is a kind of parboiled dried cracked wheat of similar size to couscous, cooked by adding boiling water and leaving for a few minutes to soften.



</doc>
<doc id="6746" url="https://en.wikipedia.org/wiki?curid=6746" title="Constantius II">
Constantius II

Constantius II (; ; 7 August 317 – 3 November 361) was Roman Emperor from 337 to 361. His reign saw constant warfare on the borders against the Sasanian Empire and Germanic peoples, while internally the Roman Empire went through repeated civil wars and usurpations, culminating in Constantius' overthrow as emperor by his cousin Julian. His religious policies inflamed domestic conflicts that would continue after his death.

The second son of Constantine I and Fausta, Constantius was made "Caesar" by his father in 324. He led the Roman army in war against the Sasanian Empire in 336. A year later, Constantine I died, and Constantius became "Augustus" with his brothers Constantine II and Constans. He promptly oversaw the massacre of eight of his relatives, consolidating his hold on power. The brothers divided the empire among themselves, with Constantius receiving the eastern provinces. In 340, his brothers Constantine and Constans clashed over the western provinces of the empire. The resulting conflict left Constantine dead and Constans as ruler of the west. The war against the Sasanians continued, with Constantius losing a major battle at Singara in 344. Constans was overthrown and assassinated in 350 by the usurper Magnentius. 

Unwilling to accept Magnentius as co-ruler, Constantius waged a civil war against the usurper, defeating him at the battles of Mursa Major in 351 and Mons Seleucus in 353. Magnentius committed suicide after the latter battle, leaving Constantius as sole ruler of the empire. In 351, Constantius elevated his cousin Constantius Gallus to the subordinate rank of "Caesar" to rule in the east, but had him executed three years later after receiving scathing reports of his violent and corrupt nature. Shortly thereafter, in 355, Constantius promoted his last surviving cousin, Gallus' younger half-brother Julian, to the rank of "Caesar".

As emperor, Constantius promoted Arian Christianity, persecuted pagans by banning sacrifices and closing pagan temples and issued laws discriminating against Jews. His military campaigns against Germanic tribes were successful: he defeated the Alamanni in 354 and campaigned across the Danube against the Quadi and Sarmatians in 357. The war against the Sasanians, which had been in a lull since 350, erupted with renewed intensity in 359 and Constantius traveled to the east in 360 to restore stability after the loss of several border fortresses to the Sasanians. However, Julian claimed the rank of Augustus in 360, leading to war between the two after Constantius' attempts to convince Julian to back down failed. No battle was fought, as Constantius became ill and died of fever on 3 November 361 in Mopsuestia, naming Julian as his rightful successor before his death.

Constantius was born in 317 at Sirmium, Pannonia. He was the third son of Constantine the Great, and second by his second wife Fausta, the daughter of Maximian. Constantius was made Caesar by his father on 13 November 324. 
In 336, religious unrest in Armenia and tense relations between Constantine and king Shapur II caused war to break out between Rome and Sassanid Persia. Though he made initial preparations for the war, Constantine fell ill and sent Constantius east to take command of the eastern frontier. Before Constantius arrived, the Persian general Narses, who was possibly the king's brother, overran Mesopotamia and captured Amida. Constantius promptly attacked Narses, and after suffering minor setbacks defeated and killed Narses at the Battle of Narasara. Constantius captured Amida and initiated a major refortification of the city, enhancing the city's circuit walls and constructing large towers. He also built a new stronghold in the hinterland nearby, naming it "Antinopolis".

In early 337, Constantius hurried to Constantinople after receiving news that his father was near death. After Constantine died, Constantius buried him with lavish ceremony in the Church of the Holy Apostles. Soon after his father's death Constantius supposedly ordered a massacre of his relatives descended from the second marriage of his paternal grandfather Constantius Chlorus, though the details are unclear. Eutropius, writing between 350 and 370, states that Constantius merely sanctioned “"the act, rather than commanding it"”. The massacre killed two of Constantius' uncles and six of his cousins, including Hannibalianus and Dalmatius, rulers of Pontus and Moesia respectively. The massacre left Constantius, his older brother Constantine II, his younger brother Constans, and three cousins Gallus, Julian and Nepotianus as the only surviving male relatives of Constantine the Great.

Soon after, Constantius met his brothers in Pannonia at Sirmium to formalize the partition of the empire. Constantius received the eastern provinces, including Constantinople, Thrace, Asia Minor, Syria, Egypt, and Cyrenaica; Constantine received Britannia, Gaul, Hispania, and Mauretania; and Constans, initially under the supervision of Constantine II, received Italy, Africa, Illyricum, Pannonia, Macedonia, and Achaea.

Constantius then hurried east to Antioch to resume the war with Persia. While Constantius was away from the eastern frontier in early 337, King Shapur II assembled a large army, which included war elephants, and launched an attack on Roman territory, laying waste to Mesopotamia and putting the city of Nisibis under siege. Despite initial success, Shapur lifted his siege after his army missed an opportunity to exploit a collapsed wall. When Constantius learned of Shapur's withdrawal from Roman territory, he prepared his army for a counter-attack.

Constantius repeatedly defended the eastern border against invasions by the aggressive Sassanid Empire under Shapur. These conflicts were mainly limited to Sassanid sieges of the major fortresses of Roman Mesopotamia, including Nisibis (Nusaybin), Singara, and Amida (Diyarbakir). Although Shapur seems to have been victorious in most of these confrontations, the Sassanids were able to achieve little. However, the Romans won a decisive victory at the Battle of Narasara, killing Shapur's brother, Narses. Ultimately, Constantius was able to push back the invasion, and Shapur failed to make any significant gains.

Meanwhile, Constantine II desired to retain control of Constans' realm, leading the brothers into open conflict. Constantine was killed in 340 near Aquileia during an ambush. As a result, Constans took control of his deceased brother's realms and became sole ruler of the Western two-thirds of the empire. This division lasted until 350, when Constans was assassinated by forces loyal to the usurper Magnentius.

As the only surviving son of Constantine the Great, Constantius felt that the position of emperor was his alone, and he determined to march west to fight the usurper, Magnentius. However, feeling that the east still required some sort of imperial presence, he elevated his cousin Constantius Gallus to Caesar of the eastern provinces. As an extra measure to ensure the loyalty of his cousin, he married the elder of his two sisters, Constantina, to him.

Before facing Magnentius, Constantius first came to terms with Vetranio, a loyal general in Illyricum who had recently been acclaimed emperor by his soldiers. Vetranio immediately sent letters to Constantius pledging his loyalty, which Constantius may have accepted simply in order to stop Magnentius from gaining more support. These events may have been spurred by the action of Constantina, who had since traveled east to marry Gallus. Constantius subsequently sent Vetranio the imperial diadem and acknowledged the general‘s new position as "Augustus". However, when Constantius arrived, Vetranio willingly resigned his position and accepted Constantius’ offer of a comfortable retirement in Bithynia.

In 351, Constantius clashed with Magnentius in Pannonia with a large army. The ensuing Battle of Mursa Major was one of the largest and bloodiest battles ever between two Roman armies. The result was a victory for Constantius, but a costly one. Magnentius survived the battle and, determined to fight on, withdrew into northern Italy. Rather than pursuing his opponent, however, Constantius turned his attention to securing the Danubian border, where he spent the early months of 352 campaigning against the Sarmatians along the middle Danube. After achieving his aims, Constantius advanced on Magnentius in Italy. This action led the cities of Italy to switch their allegiance to him and eject the usurper's garrisons. Again, Magnentius withdrew, this time to southern Gaul.

In 353, Constantius and Magnentius met for the final time at the Battle of Mons Seleucus in southern Gaul, and again Constantius emerged the victor. Magnentius, realizing the futility of continuing his position, committed suicide on 10 August 353.

Constantius spent much of the rest of 353 and early 354 on campaign against the Alamanni on the Danube frontier. The campaign was successful and raiding by the Alamanni ceased temporarily. In the meantime, Constantius had been receiving disturbing reports regarding the actions of his cousin Gallus. Possibly as a result of these reports, Constantius concluded a peace with the Alamanni and traveled to Mediolanum (Milan).

In Mediolanum, Constantius first summoned Ursicinus, Gallus’ "magister equitum", for reasons that remain unclear. Constantius then summoned Gallus and Constantina. Although Gallus and Constantina complied with the order at first, when Constantina died in Bithynia, Gallus began to hesitate. However, after some convincing by one of Constantius’ agents, Gallus continued his journey west, passing through Constantinople and Thrace to Poetovio (Ptuj) in Pannonia.

In Poetovio, Gallus was arrested by the soldiers of Constantius under the command of Barbatio. Gallus was then moved to Pola and interrogated. Gallus claimed that it was Constantina who was to blame for all the trouble while he was in charge of the eastern provinces. This angered Constantius so greatly that he immediately ordered Gallus' execution. He soon changed his mind, however, and recanted the order. Unfortunately for Gallus, this second order was delayed by Eusebius, one of Constantius' eunuchs, and Gallus was executed.

In spite of some of the edicts issued by Constantius, he never made any attempt to disband the various Roman priestly colleges or the Vestal Virgins, he never acted against the various pagan schools, and, at times, he actually made some effort to protect paganism. In fact, he even ordered the election of a priest for Africa. Also, he remained pontifex maximus and was deified by the Roman Senate after his death. His relative moderation toward paganism is reflected by the fact that it was over twenty years after his death, during the reign of Gratian, that any pagan senator protested his treatment of their religion.

Although often considered an Arian, Constantius ultimately preferred a third, compromise version that lay somewhere in between Arianism and the Nicene Creed, retrospectively called Semi-Arianism. During his reign he attempted to mold the Christian church to follow this compromise position, convening several Christian councils. The most notable of these were the Council of Rimini and its twin at Seleucia, which met in 359 and 360 respectively. "Unfortunately for his memory the theologians whose advice he took were ultimately discredited and the malcontents whom he pressed to conform emerged victorious," writes the historian A.H.M. Jones. "The great councils of 359–60 are therefore not reckoned ecumenical in the tradition of the church, and Constantius II is not remembered as a restorer of unity, but as a heretic who arbitrarily imposed his will on the church."

Christian-related edicts issued by Constantius (by himself or with others) included:

Judaism faced some severe restrictions under Constantius, who seems to have followed an anti-Jewish policy in line with that of his father. Early in his reign, Constantius issued a double edict in concert with his brothers limiting the ownership of slaves by Jewish people and banning marriages between Jews and Christian women. A later edict issued by Constantius after becoming sole emperor decreed that a person who was proven to have converted from Christianity to Judaism would have all of his property confiscated by the state. However, Constantius' actions in this regard may not have been so much to do with Jewish religion as with Jewish business—apparently, privately owned Jewish businesses were often in competition with state-owned businesses. As a result, Constantius may have sought to provide an advantage to state-owned businesses by limiting the skilled workers and slaves available to Jewish businesses.

Jew-related edicts issued by Constantius (by himself or with others) included:

On 11 August 355, the magister militum Claudius Silvanus revolted in Gaul. Silvanus had surrendered to Constantius after the Battle of Mursa Major. Constantius had made him magister militum in 353 with the purpose of blocking the German threats, a feat that Silvanus achieved by bribing the German tribes with the money he had collected. A plot organized by members of Constantius' court led the emperor to recall Silvanus. After Silvanus revolted, he received a letter from Constantius recalling him to Milan, but which made no reference to the revolt. Ursicinus, who was meant to replace Silvanus, bribed some troops, and Silvanus was killed.

Constantius realised that too many threats still faced the Empire, however, and he could not possibly handle all of them by himself. So on 6 November 355, he elevated his last remaining male relative, Julian, to the rank of Caesar. A few days later, Julian was married to Helena, the last surviving sister of Constantius. Constantius soon sent Julian off to Gaul.

Constantius spent the next few years overseeing affairs in the western part of the empire primarily from his base at Mediolanum. In 357 he visited Rome for the only time in his life. The same year, he forced Sarmatian and Quadi invaders out of Pannonia and Moesia Inferior, then led a successful counter-attack across the Danube.

In the winter of 357–58, Constantius received ambassadors from Shapur II who demanded that Rome restore the lands surrendered by Narseh. Despite rejecting these terms, Constantius tried to avert war with the Sassanid Empire by sending two embassies to Shapur II. Shapur II nevertheless launched another invasion of Roman Mesopotamia. In 360, when news reached Constantius that Shapur II had destroyed Singara, and taken Kiphas (Hasankeyf), Amida, and Ad Tigris (Cizre), he decided to travel east to face the re-emergent threat.

In the meantime, Julian had won some victories against the Alamanni, who had once again invaded Roman Gaul. However, when Constantius requested reinforcements from Julian's army for the eastern campaign, the Gallic legions revolted and proclaimed Julian "Augustus".

On account of the immediate Sassanid threat, Constantius was unable to directly respond to his cousin's usurpation, other than by sending missives in which he tried to convince Julian to resign the title of "Augustus" and be satisfied with that of "Caesar". By 361, Constantius saw no alternative but to face the usurper with force, and yet the threat of the Sassanids remained. Constantius had already spent part of early 361 unsuccessfully attempting to re-take the fortress of Ad Tigris. After a time he had withdrawn to Antioch to regroup and prepare for a confrontation with Shapur II. The campaigns of the previous year had inflicted heavy losses on the Sassanids, however, and they did not attempt another round of campaigns that year. This temporary respite in hostilities allowed Constantius to turn his full attention to facing Julian.

Constantius immediately gathered his forces and set off west. However, by the time he reached Mopsuestia in Cilicia, it was clear that he was fatally ill and would not survive to face Julian. Apparently, realising his death was near, Constantius had himself baptised by Euzoius, the Semi-Arian bishop of Antioch, and then declared that Julian was his rightful successor. Constantius II died of fever on 3 November 361.

Constantius II was married three times:

First to a daughter of his half-uncle Julius Constantius, whose name is unknown. She was a full-sister of Gallus and a half-sister of Julian. She died c. 352/3.

Second, to Eusebia, a woman of Macedonian origin, originally from the city of Thessaloniki, whom Constantius married before his defeat of Magnentius in 353. She died in 360.

Third and lastly, in 360, to Faustina, who gave birth to Constantius' only child, a posthumous daughter named Flavia Maxima Constantia, who later married Emperor Gratian.

Constantius II is a particularly difficult figure to judge properly due to the hostility of most sources toward him. A. H. M. Jones writes that Constantius "appears in the pages of Ammianus as a conscientious emperor but a vain and stupid man, an easy prey to flatterers. He was timid and suspicious, and interested persons could easily play on his fears for their own advantage." However, Kent and M. and A. Hirmer suggest that Constantius "has suffered at the hands of unsympathetic authors, ecclesiastical and civil alike. To orthodox churchmen he was a bigoted supporter of the Arian heresy, to Julian the Apostate and the many who have subsequently taken his part he was a murderer, a tyrant and inept as a ruler". They go on to add, "Most contemporaries seem in fact to have held him in high esteem, and he certainly inspired loyalty in a way his brother could not".




</doc>
<doc id="6747" url="https://en.wikipedia.org/wiki?curid=6747" title="Constans">
Constans

Constans (; ; c. 323 – 350) or Constans I was Roman Emperor from 337 to 350. He defeated his brother Constantine II in 340, but anger in the army over his personal life (homosexuality) and favouritism towards his barbarian bodyguards led the general Magnentius to rebel, resulting in the assassination of Constans in 350.

Constans was the third and youngest son of Constantine the Great and Fausta, his father's second wife. He was educated at the court of his father at Constantinople under the tutelage of the poet Aemilius Magnus Arborius.

On 25 December 333, Constantine I elevated Constans to the rank of "Caesar" at Constantinople. Constans became engaged to Olympias, the daughter of the Praetorian Prefect Ablabius, but the marriage never came to pass. With Constantine’s death in 337, Constans and his two brothers, Constantine II and Constantius II, divided the Roman world among themselves and disposed of virtually all relatives who could possibly have a claim to the throne. The army proclaimed them "Augusti" on September 9, 337. Almost immediately, Constans was required to deal with a Sarmatian invasion in late 337, in which he won a resounding victory.
Constans was initially under the guardianship of Constantine II. The original settlement assigned Constans the praetorian prefecture of Italy, which included Northern Africa. Constans was unhappy with this division, so the brothers met at Viminacium in 338 to revise the boundaries. Constans managed to extract the prefecture of Illyricum and the diocese of Thrace, provinces that were originally to be ruled by his cousin Dalmatius, as per Constantine I’s proposed division after his death. Constantine II soon complained that he had not received the amount of territory that was his due as the eldest son.

Annoyed that Constans had received Thrace and Macedonia after the death of Dalmatius, Constantine demanded that Constans hand over the African provinces, which he agreed to do in order to maintain a fragile peace. Soon, however, they began quarreling over which parts of the African provinces belonged to Carthage, and thus Constantine, and which belonged to Italy, and therefore Constans. This led to growing tensions between the two brothers, which were only heightened by Constans finally coming of age and Constantine refusing to give up his guardianship. In 340 Constantine II invaded Italy. Constans, at that time in Dacia, detached and sent a select and disciplined body of his Illyrian troops, stating that he would follow them in person with the remainder of his forces. Constantine was eventually trapped at Aquileia, where he died, leaving Constans to inherit all of his brother’s former territories – Hispania, Britannia and Gaul.

Constans began his reign in an energetic fashion. In 341-42, he led a successful campaign against the Franks, and in the early months of 343 he visited Britain. The source for this visit, Julius Firmicus Maternus, does not provide a reason, but the quick movement and the danger involved in crossing the channel in the winter months suggests it was in response to a military emergency, possibly to repel the Picts and Scots.

Regarding religion, Constans was tolerant of Judaism and promulgated an edict banning pagan sacrifices in 341. He suppressed Donatism in Africa and supported Nicene orthodoxy against Arianism, which was championed by his brother Constantius. Although Constans called the Council of Serdica in 343 to settle the conflict, it was a complete failure, and by 346 the two emperors were on the point of open warfare over the dispute. The conflict was only resolved by an interim agreement which allowed each emperor to support their preferred clergy within their own spheres of influence.

The Roman historian Eutropius says Constans "indulged in great vices," in reference to his homosexuality, and Aurelius Victor stated that Constans had a reputation for scandalous behaviour with "handsome barbarian hostages." Nevertheless, Constans did sponsor a decree alongside Constantius II that ruled that marriage based on "unnatural" sex should be punished meticulously. Boswell argues that the decree outlawed homosexual marriages only, rather than homosexual activity more generally. However, it was likely the case that Constans promulgated the legislation under pressure from the growing band of Christian leaders, in an attempt to placate public outrage at his own perceived indecencies.

In the final years of his reign, Constans developed a reputation for cruelty and misrule. Dominated by favourites and openly preferring his select bodyguard, he lost the support of the legions. In 350, the general Magnentius declared himself emperor at Augustodunum with the support of the troops on the Rhine frontier and, later, the western provinces of the Empire. Constans was enjoying himself nearby when he was notified of the elevation of Magnentius. Lacking any support beyond his immediate household, he was forced to flee for his life. As he was trying to reach Hispania, supporters of Magnentius cornered him in a fortification in "Helena" (now Elne) in the eastern Pyrenees of southwestern Gaul, where he was killed after seeking sanctuary in a temple. An alleged prophecy at his birth had said Constans would die "in the arms of his grandmother". His place of death happens to have been named after Helena, mother of Constantine and his own grandmother, thus realizing the prophecy.





</doc>
<doc id="6749" url="https://en.wikipedia.org/wiki?curid=6749" title="Cheerleading">
Cheerleading

Cheerleading is an activity in which the participants (called "cheerleaders") cheer for their team as a form of encouragement. It can range from chanting slogans to intense physical activity. It can be performed to motivate sports teams, to entertain the audience, or for competition. Competitive routines typically range anywhere from one to three minutes, and contain components of tumbling, dance, jumps, cheers, and stunting.

Cheerleading originated in the United States, and remains predominantly in America, with an estimated 1.5 million participants in all-star cheerleading. The global presentation of cheerleading was led by the 1997 broadcast of ESPN's International cheerleading competition, and the worldwide release of the 2000 film "Bring It On". Due in part to this recent exposure, there are now an estimated 100,000 participants scattered around the globe in Australia, Canada, China, Colombia, Finland, France, Germany, Japan, the Netherlands, New Zealand, and the United Kingdom.

Cheerleading began during the late 18th century with the rebellion of male students. After the American Revolutionary War, students experienced harsh treatment from teachers. In response to faculty's abuse, college students violently acted out. The undergraduates began to riot, burn down buildings located on their college campuses, and assault faculty members. As a more subtle way to gain independence, however, students invented and organized their own extracurricular activities outside their professors' control. This brought about American sports, beginning first with collegiate teams.

In the 1860s, students from Great Britain began to cheer and chant in unison for their favorite athletes at sporting events. Soon, that gesture of support crossed overseas to America.

On November 6, 1869, the United States witnessed its first intercollegiate football game. It took place between Princeton and Rutgers University, and marked the day the original "Sis Boom Rah!" cheer was shouted out by student fans.

Organized cheerleading started as an all-male activity. As early as 1877, Princeton University had a "Princeton Cheer", documented in the February 22, 1877, March 12, 1880, and November 4, 1881, issues of "The Daily Princetonian". This cheer was yelled from the stands by students attending games, as well as by the athletes themselves. The cheer, "Hurrah! Hurrah! Hurrah! Tiger! S-s-s-t! Boom! A-h-h-h!" remains in use with slight modifications today, where it is now referred to as the "Locomotive".

Princeton class of 1882 graduate Thomas Peebles moved to Minnesota in 1884. He transplanted the idea of organized crowds cheering at football games to the University of Minnesota. The term "Cheer Leader" had been used as early as 1897, with Princeton's football officials having named three students as "Cheer Leaders:" Thomas, Easton, and Guerin from Princeton's classes of 1897, 1898, and 1899, respectively, on October 26, 1897. These students would cheer for the team also at football practices, and special cheering sections were designated in the stands for the games themselves for both the home and visiting teams.

It was not until 1898 that University of Minnesota student Johnny Campbell directed a crowd in cheering "Rah, Rah, Rah! Ski-u-mah, Hoo-Rah! Hoo-Rah! Varsity! Varsity! Varsity, Minn-e-So-Tah!", making Campbell the very first cheerleader.

November 2, 1898 is the official birth date of organized cheerleading. Soon after, the University of Minnesota organized a "yell leader" squad of six male students, who still use Campbell's original cheer today. In 1903, the first cheerleading fraternity, Gamma Sigma, was founded.

In 1923, at the University of Minnesota, women were permitted to participate in cheerleading. However, it took time for other schools to follow. In the late 1920s, many school manuals and newspapers that were published still referred to cheerleaders as "chap," "fellow," and "man". Women cheerleaders were overlooked until the 1940s. In the 1940s, collegiate men were drafted for World War II, creating the opportunity for more women to make their way onto sporting event sidelines. As noted by Kieran Scott in "Ultimate Cheerleading": "Girls really took over for the first time." An overview written on behalf of cheerleading in 1955 explained that in larger schools, "occasionally boys as well as girls are included,", and in smaller schools, "boys can usually find their place in the athletic program, and cheerleading is likely to remain solely a feminine occupation." During the 1950s, cheerleading in America also increased in popularity. By the 1960s, some began to consider cheerleading a feminine extracurricular for boys, and by the 1970s, girls primarily cheered at public school games. However, this did not stop its growth. Cheerleading could be found at almost every school level across the country, even pee wee and youth leagues began to appear.

In 1975, it was estimated by a man named Randy Neil that over 500,000 students actively participated in American cheerleading from elementary school to the collegiate level. He also approximated that ninety-five percent of cheerleaders within America were female. Since 1973, cheerleaders have started to attend female basketball and other all-female sports as well.

As of 2005, overall statistics show around 97% of all modern cheerleading participants are female, although at the collegiate level, cheerleading is co-ed with about 50% of participants being male.

In 1948, Lawrence "Herkie" Herkimer, of Dallas, Texas, a former cheerleader at Southern Methodist University, formed the National Cheerleaders Association (NCA) in order to hold clinics for cheerleading. In 1949, The NCA held its first clinic in Huntsville, Texas, with 52 girls in attendance. Herkimer contributed many firsts to cheerleading: the founding of the Cheerleader & Danz Team cheerleading uniform supply company, inventing the herkie jump (where one leg is bent towards the ground as if kneeling and the other is out to the side as high as it will stretch in toe-touch position), and creating the "Spirit Stick". By the 1960s, college cheerleaders began hosting workshops across the nation, teaching fundamental cheer skills to high-school-age girls. In 1965, Fred Gastoff invented the vinyl pom-pom, which was introduced into competitions by the International Cheerleading Foundation (ICF, now the World Cheerleading Association, or WCA). Organized cheerleading competitions began to pop up with the first ranking of the "Top Ten College Cheerleading Squads" and "Cheerleader All America" awards given out by the ICF in 1967. In 1978, America was introduced to competitive cheerleading by the first broadcast of Collegiate Cheerleading Championships on CBS.

In the 1950s, the formation of professional cheerleading started. The first recorded cheer squad in National Football League (NFL) history was for the Baltimore Colts. Professional cheerleaders put a new perspective on American cheerleading. Women were selected for two reasons: visual sex appeal, and the ability to dance. Women were exclusively chosen because men were the targeted marketing group. The Dallas Cowboys Cheerleaders soon gained the spotlight with their revealing outfits and sophisticated dance moves, debuting in the 1972–1973 season, but were first widely seen in Super Bowl X (1976). These pro squads of the 1970s established cheerleaders as "American icons of wholesome sex appeal." By 1981, a total of seventeen Nation Football League teams had their own cheerleaders. The only teams without NFL cheerleaders at this time were New Orleans, New York, Detroit, Cleveland, Denver, Minnesota, Pittsburg, San Francisco, and San Diego. Professional cheerleading eventually spread to soccer and basketball teams as well.

The 1980s saw the beginning of modern cheerleading, adding difficult stunt sequences and gymnastics into routines. All-star teams popped up, and with them, the creation of the United States All-Star Federation (USASF). ESPN first broadcast the National High School Cheerleading Competition nationwide in 1983. Cheerleading organizations such as the American Association of Cheerleading Coaches and Advisors (AACCA), founded in 1987, started applying universal safety standards to decrease the number of injuries and prevent dangerous stunts, pyramids, and tumbling passes from being included in the cheerleading routines. In 2003, the National Council for Spirit Safety and Education (NCSSE) was formed to offer safety training for youth, school, all-star, and college coaches. The NCAA requires college cheer coaches to successfully complete a nationally recognized safety-training program. The NCSSE or AACCA certification programs are both recognized by the NCAA.

Even with its athletic and competitive development, cheerleading at the school level has retained its ties to its spirit leading traditions. Cheerleaders are quite often seen as ambassadors for their schools, and leaders among the student body. At the college level, cheerleaders are often invited to help at university fundraisers and events.

Cheerleading is very closely associated with American football and basketball. Sports such as association football (soccer), ice hockey, volleyball, baseball, and wrestling will sometimes sponsor cheerleading squads. The ICC Twenty20 Cricket World Cup in South Africa in 2007 was the first international cricket event to have cheerleaders. The Florida Marlins were the first Major League Baseball team to have a cheerleading team. Debuting in 2003, the "Marlin Mermaids" gained national exposure, and have influenced other MLB teams to develop their own cheer/dance squads.

Competitive cheerleading is scored subjectively based on components including, but not limited to, the cheer itself, dance/choreography, pyramids, stunting, and tumbling. In order to prevent injuries, there are certain rules that cheerleading teams have to follow according to their level (high school, all-star, or college). According to the Encyclopedia of Sports Medicine, there are two purposes of cheerleading - to cheer on the sidelines for other athletes, and to be a "highly skilled competing athlete."

Along with this evolution to the sport's structure, there have been significant advancements made to the typical cheerleading uniform. What began as the classic sweater and mid-calf pleated skirt uniform has now come to incorporate materials that allow for stretch and flexibility. Uniform changes are a result of the changing culture since the 1930s.

Cheerleading may seem like a light-hearted activity to some, but injuries that can come from practice or a competition can be severe if the athlete is not properly trained. There have been many catastrophic injuries from cheer, especially from tumbling and stunting. Because of the lack of studies on injuries in competitive cheerleading, many injuries that happen could be avoided. Most studies in sports medicine pertaining to cheerleading are focused on whether it is a sport or not.

Most American middle schools, high schools, and colleges have organized cheerleading squads. Many colleges offer cheerleading scholarships for students. A cheerleading team may compete locally, regionally, or nationally, as well as cheer for sporting events and encourage audience participation. Cheerleading is quickly becoming a year-round activity, starting with tryouts during the spring semester of the preceding school year. Teams may attend organized summer cheerleading camps and practices to improve skills and create routines for competition.

Student cheerleaders compete with recreational-style routine at competitions year-round. Teams practice intensely for competition and perform a routine no longer than 2 minutes and 30 seconds. Like other school-level athletes, teams compete to win league titles, and move on to bigger competitions with the hopes of reaching a national competition. The advantages to a school squad versus an all-star squad is cheering at various sporting events.

The tryout process can sometimes take place over a multiple day period. The cheerleading coach will arrange for a cheerleading clinic, during which basic materials are taught or reviewed before the final day of tryouts. The clinic gives returning cheerleaders and new cheerleaders an equal chance of becoming familiar with the material. Skills that are necessary to be a cheerleader include jumps, tumbling, motions, and dance ability. Tryouts often take place during the spring, so that the coach has the squad chosen in time to attend summer camp as a team.

Middle school cheerleading evolved shortly after high school squads were created. In middle school, cheerleading squads serve the same purpose, and follow the same rules as high school squads. Squads cheer for basketball teams, football teams, and other sports teams in their school. Squads also perform at pep rallies and compete against other local schools from the area. Cheerleading in middle school sometimes can be a two-season activity: fall and winter. However, many middle school cheer squads will go year-round like high school squads. Middle school cheerleaders use the same cheerleading movements as their older counterparts, yet they perform less extreme stunts. These stunts range from preps, thigh stands, and extensions, to harder one-legged stunts.

In high school, there are usually two squads per school: varsity and a junior varsity. High school cheerleading contains aspects of school spirit as well as competition. These squads have become part of a year-round cycle. Starting with tryouts in the spring, year-round practice, cheering on teams in the fall and winter, and participating in cheerleading competitions. Most squads practice at least three days a week for about two hours each practice during the summer. Many teams also attend separate tumbling sessions outside of practice. During the school year, cheerleading is usually practiced five- to six-days-a-week. During competition season, it often becomes seven days with practice twice a day sometimes. The school spirit aspect of cheerleading involves cheering, supporting, and "pumping up" the crowd at football games, basketball games, and even at wrestling meets. Along with this, they perform at pep rallies, and bring school spirit to other students. In May 2009, the National Federation of State High School Associations released the results of their first true high school participation study. They estimated that the number of high school cheerleaders from public high schools is around 394,700.

There are different cheerleading organizations that put on competitions; some of the major ones include state and regional competitions. Many high schools will often host cheerleading competitions, bringing in IHSA judges. The regional competitions are qualifiers for national competitions, such as the UCA (Universal Cheerleaders Association) in Orlando, Florida every year. The competition aspect of cheerleading can be very enduring; styles and rules change every year, making it important and difficult to find the newest and hottest routines. Most teams have a professional choreographer that choreographs their routine in order to ensure they are not breaking rules or regulations. For a list of rules, visit AACCA (American Association of Cheerleading Coaches and Administrators). All high school coaches are required to attend an IHSA rules meeting at the beginning of the season. This ensures their knowledge of changed rules and their compliance with these rules.

Most American universities have a cheerleading squad to cheer for football, basketball, volleyball, and soccer. Most college squads tend to be large coed although in recent years; all-girl and small coed college squads have increased rapidly.

College squads perform more difficult stunts which include pyramids, as well as flipping and twisting basket tosses.

Not only do college cheerleaders cheer on the other sports at their university, many teams at universities compete with other schools at either UCA College Nationals or NCA College Nationals. This requires the teams to make a 2 minute 30 seconds that is full of tumbling, stunting, basket tosses, and pyramids. Winning these competitions are very prestigious accomplishments and is seen as another national title for most schools.

Organizations that sponsor youth cheer teams usually sponsor either youth league football or basketball teams as well. This allows for the two, under the same sponsor, to be intermingled. Both teams have the same mascot name and the cheerleaders will perform at their football or basketball games. Examples of such sponsors include Pop Warner and Pasco Police Athletic League (PPAL). The YMCA (Young Men's Christian Association) is also a well-known sponsor for youth cheerleading leagues.

During the early 1980s, cheerleading squads not associated with a school or sports leagues, whose main objective was competition, began to emerge. The first organization to call themselves all-stars and go to competitions were the Q94 Rockers from Richmond, Virginia, founded in 1982. All-star teams competing prior to 1987 were placed into the same divisions as teams that represented schools and sports leagues. In 1986, the National Cheerleaders Association (NCA) addressed this situation by creating a separate division for teams lacking a sponsoring school or athletic association, calling it the All-Star Division and debuting it at their 1987 competitions. As the popularity of this type of team grew, more and more of them were formed, attending competitions sponsored by many different organizations and companies, each using its own set of rules, regulations, and divisions. This situation became a concern to gym owners because the inconsistencies caused coaches to keep their routines in a constant state of flux, detracting from time that could be better utilized for developing skills and providing personal attention to their athletes. More importantly, because the various companies were constantly vying for a competitive edge, safety standards had become more and more lax. In some cases, unqualified coaches and inexperienced squads were attempting dangerous stunts as a result of these expanded sets of rules.

The USASF was formed in 2003 by the competition companies to act as the national governing body for all star cheerleading and to create a standard set of rules and judging standards to be followed by all competitions sanctioned by the Federation, ultimately leading to the Cheerleading Worlds. The USASF hosted the first Cheerleading Worlds on April 24, 2004. In 2009, the first All-Level Worlds was held. It included teams from all levels, with each winner continuing to the online championships, where teams from across the nation competed to win the Worlds Title. At the same time, cheerleading coaches from all over the country organized themselves for the same rule making purpose, calling themselves the National All Star Cheerleading Coaches Congress (NACCC). In 2005, the NACCC was absorbed by the USASF to become their rule making body. In late 2006, the USASF facilitated the creation of the International All-Star Federation (IASF).

, all-star cheerleading as sanctioned by the USASF involves a squad of 6–36 females and/or males. All-star differs from sideline cheerleading because all-star focuses on performing , while sideline cheers for others sport such as football or basketball. All-star is competitive teams that perform a routine for the purpose of entertainment against other teams, typically in the same divisions, to try to win. The squad prepares year-round for many different competition appearances, but they actually perform only for up to 2½ minutes during their team's routines. The numbers of competitions a team participates in varies from team to team, but generally, most teams tend to participate in eight to twelve competitions a year. These competitions include locals, which are normally taken place in school gymnasiums or local venues, nationals, hosted in big venues all around the U.S. with national champions, and the Cheerleading Worlds, taken place at Disney World in Orlando, Florida. During a competition routine, a squad performs carefully choreographed stunting, tumbling, jumping, and dancing to their own custom music. Teams create their routines to an eight-count system and apply that to the music so that the team members execute the elements with precise timing and synchronization.

There are many different organizations that host their own state and national competitions. Some major companies include: Universal Spirit, AmeriCheer, Cheersport, Planet Spirit, Eastern Cheer and Dance Association, and The JAM Brands. This means that many gyms within the same area could be state and national champions for the same year and never have competed against each other. Currently, there is no system in place that awards only one state or national title.

Judges at the competition watch closely for illegal moves from the group or any individual member. Here, an illegal move is something that is not allowed in that division due to difficulty and/or safety restrictions. They look out for deductions, or things that go wrong, such as a dropped stunt. They also look for touch downs in tumbling for deductions. More generally, judges look at the difficulty and execution of jumps, stunts and tumbling, synchronization, creativity, the sharpness of the motions, showmanship, and overall routine execution.

All-star cheerleaders are placed into divisions, which are grouped based upon age, size of the team, gender of participants, and ability level. The age levels vary from under 4 year of age to 18 years and over. The divisions used by the USASF/IASF are currently Tiny, Mini, Youth, Junior, Junior International, Junior Coed, Senior, Senior Coed, Special Needs, and Open International. It originally began with "all girl" teams and later co-ed teams began to gain popularity. That being said, the all-girl squad remains the most prevalent.

If a team places high enough at selected USASF/IASF sanctioned national competitions, they could be included in the Cheerleading Worlds and compete against teams from all over the world, as well as receive money for placing. Each team receives a bid from another cheerleading company and goes in the name of that company. One must get a bid from a company in order to compete at the Cheerleading Worlds. For example, a team could get a bid from Cheersport, and they compete as a team representing that company. Cheerleading companies give out three types of bids to go to Cheerleading Worlds, Full Paid Bid, Partial Bid, or an Un-paid bid. The Cheerleading Worlds are only for teams that are level 5 and up.

Professional cheerleaders and dancers cheer for sports such as football, basketball, baseball, wrestling, or hockey. There are only a small handful of professional cheerleading leagues around the world; some professional leagues include the NBA Cheerleading League, the NFL Cheerleading League, the CFL Cheerleading League, the MLS Cheerleading League, the MLB Cheerleading League, and the NHL Ice Dancers. Although professional cheerleading leagues exist in multiple countries, there are no Olympic teams.

In addition to cheering at games and competing, professional cheerleaders also, as teams, can often do a lot of philanthropy and charity work, modeling, motivational speaking, television performances, and advertising.

Americheer: Americheer was founded in 1987 by Elizabeth Rossetti. It is the parent company to Ameridance and Eastern Cheer and Dance Association. In 2005, Americheer became one of the founding members of the NLCC. This means that Americheer events offer bids to The U.S. Finals: The Final Destination. AmeriCheer InterNational Championship competition is held every March at the Walt Disney World Resort in Orlando, Florida.

International Cheer Union (ICU): Established on April 26, 2004, the ICU is recognized by the SportAccord as the world governing body of cheerleading and the authority on all matters with relation to it. Including participation from its 105-member national federations reaching 3.5 million athletes globally, the ICU continues to serve as the unified voice for those dedicated to cheerleading's positive development around the world.

Following a positive vote by the SportAccord General Assembly on May 31, 2013, in Saint Petersburg, the International Cheer Union (ICU) became SportAccord's 109th member, and SportAccord's 93rd international sports federation to join the international sports family. In accordance with the SportAccord statutes, the ICU is recognized as the world governing body of cheerleading and the authority on all matters related to it.

As of the 2016–17 season, the ICU has introduced a Junior aged team (12-16) to compete at the Cheerleading Worlds, because cheerleading is now in provisional status to become a sport in the Olympics. For cheerleading to one day be in the Olympics, there must be a junior and senior team that competes at the world championships. The first junior cheerleading team that was selected to become the junior national team was Eastside Middle School, located in Mount Washington Kentucky and will represent the United States in the inaugural junior division at the world championships.

The ICU holds training seminars for judges and coaches, global events and the World Cheerleading Championships. The ICU is also fully applied to the International Olympic Committee (IOC) and is compliant under the code set by the World Anti-Doping Agency (WADA).

International Federation of Cheerleading (IFC): Established on July 5, 1998, the International Federation of Cheerleading (IFC) is a non-profit federation based in Tokyo, Japan, and is the world governing body of cheerleading. The IFC objectives are to promote cheerleading worldwide, to spread knowledge of cheerleading, and to develop friendly relations among the member associations and federations.

National Cheerleaders Association: The NCA was founded in 1948 by Lawrence Herkimer. Every year, the NCA hosts the NCA High School Cheerleading Nationals and the NCA All-Star Cheerleading Nationals in Dallas, Texas. They also host the NCA/NDA Collegiate Cheer & Dance Championship in Daytona Beach, Florida.

United Spirit Association: In 1950, Robert Olmstead directed his first summer training camp, and USA later sprouted from this. USA's focus is on the game day experience as a way to enhance audience entertainment. This focus led to the first American football half-time shows to reach adolescences from around the world and expose them to American style cheerleading. USA has choreographed material for professional and competitive cheerleaders alike. USA provides competitions for cheerleading squads without prior qualifications needed in order to participate. The organization also allows the opportunity for cheerleaders to become an All-American, participate in the Macy's Thanksgiving Day Parade, and partake in London's New Year's Day Parade and other special events much like UCA and NCA allow participants to do.

Universal Cheerleaders Association: Universal Cheerleaders Association was founded in 1974 by Jeff Webb. Since 1980, UCA has hosted the National High School Cheerleading Championship in Walt Disney World Resort. They also host the National All-Star Cheerleading Championship, and the College Cheerleading National Championship at Walt Disney World Resort. To qualify for these events, all teams must submit a video. All of these events air on ESPN.

Asian Thailand Cheerleading Invitational (ATCI): Organised by the Cheerleading Association of Thailand (CAT) in accordance with the rules and regulations of the International Federation of Cheerleading (IFC). The ATCI is held every year since 2009. At the ATCI, many teams from all over Thailand compete, joining them are many invited neighbouring nations who also send cheer squads.

Cheerleading Asia International Open Championships (CAIOC): Hosted by the Foundation of Japan Cheerleading Association (FJCA) in accordance with the rules and regulations of the IFC. The CAIOC has been a yearly event since 2007. Every year, many teams from all over Asia converge in Tokyo to compete.

Cheerleading World Championships (CWC): Organised by the IFC. The IFC is a non-profit organisation founded in 1998 and based in Tokyo, Japan.
The CWC has been held every two years since 2001, and to date, the competition has been held in Japan, the United Kingdom, Finland, Germany, and Hong Kong. The 6th CWC was held at the Hong Kong Coliseum on November 26–27, 2011.

ICU World Championships: The International Cheer Union currently encompasses 105 National Federations from countries across the globe. Every year, the ICU host the World Cheerleading Championship. Unlike the USASF Worlds, this competition uses Level 6/ Collegiate style rules. Countries assemble and send only one team to represent them.

National Cheerleading Championships (NCC): The NCC is the annual IFC-sanctioned national cheerleading competition in Indonesia organised by the Indonesian Cheerleading Community (ICC). Since NCC 2010, the event is now open to international competition, representing a significant step forward for the ICC. Teams from many countries such as Japan, Thailand, the Philippines, and Singapore participated in the ground breaking event.

NLCC Final Destination: Nation's Leading Cheer Companies is a multi brand company, partnered with other companies such as: Americheer/Ameridance, American Cheer & Dance Academy, Eastern Cheer & Dance Association, and Spirit Unlimited. Every year, starting in 2006, the NLCC hosts The US Finals: The Final Destination of Cheerleading and Dance. Every team that attends must qualify and receive a bid at a partner company's competition. In May 2008, the NLCC and The JAM Brands announced a partnership to produce The U.S. Finals - Final Destination. There are nine Final Destination locations across the country. After the regional events, videos of all the teams that competed are sent to a new panel of judges and rescored to rank teams against those against whom they may never have had a chance to compete.

Pan-American Cheerleading Championships (PCC): The PCC was held for the first time in 2009 in the city of Latacunga, Ecuador and is the continental championship organised by the Pan-American Federation of Cheerleading (PFC). The PFC, operating under the umbrella of the IFC, is the non-profit continental body of cheerleading whose aim it is to promote and develop cheerleading in the Americas. The PCC is a biennial event, and was held for the second time in Lima, Peru, in November 2010.

The JAM Brands: The JAM Brands, headquartered in Louisville, Kentucky, provides products and services for the cheerleading and dance industry. It is made up of approximately 12 different brands that produce everything from competitions to camps to uniforms to merchandise and apparel. JAMfest, the original brand of The JAM Brands, has been around since 1996 and was founded by Aaron Flaker and Emmitt Tyler. Dan Kessler has since become a co-owner of The JAM Brands along with Flaker and Tyler.

USASF/IASF Worlds: Many United States cheerleading organizations form and register the not-for-profit entity the United States All Star Federation (USASF) and also the International All Star Federation (IASF) to support international club cheerleading and the World Cheerleading Club Championships. The first World Cheerleading Championships, or Cheerleading Worlds, were hosted by the USASF/IASF at the Walt Disney World Resort and taped for an ESPN global broadcast in 2004. This competition is only for All-Star/Club cheer. Only levels Junior 5, Senior 5, Senior Open 5, International 5, International Open 5, International 6, and International Open 6 may attend. Teams must receive a bid from a partner company to attend.

Varsity: Partnered with the UCA, Varsity created the National High School Cheerleading Championship in 1980. Varsity All-Star owns or partners with many of the largest cheerleading events in the country.

There is a large debate on whether or not cheerleading should be considered a sport for Title IX (a portion of the United States Education Amendments of 1972 forbidding discrimination under any education program on the basis of sex) purposes. Supporters consider cheerleading, as a whole, a sport, citing the heavy use of athletic talents while critics see it as a physical activity because a "sport" implies a competition among all squads and not all squads compete, along with subjectivity of competitions where—as with gymnastics, diving, and figure skating—scores are assessed based on human judgment and not an objective goal or measurement of time.

On January 27, 2009, in a lawsuit involving an accidental injury sustained during a cheerleading practice, the Wisconsin Supreme Court ruled that cheerleading is a full-contact sport in that state, not allowing any participants to be sued for accidental injury. In contrast, on July 21, 2010, in a lawsuit involving whether college cheerleading qualified as a sport for purposes of Title IX, a federal court, citing a current lack of program development and organization, ruled that it is not a sport at all.

Cheerleading carries the highest rate of catastrophic injuries to girl athletes in sports. The risks of cheerleading were highlighted when Kristi Yamaoka, a cheerleader for Southern Illinois University, suffered a fractured vertebra when she hit her head after falling from a human pyramid. She also suffered from a concussion, and a bruised lung. The fall occurred when Yamaoka lost her balance during a basketball game between Southern Illinois University and Bradley University at the Savvis Center in St. Louis on March 5, 2006. The fall gained "national attention", because Yamaoka continued to perform from a stretcher as she was moved away from the game. Yamaoka has since made a full recovery.

The accident caused the Missouri Valley Conference to ban its member schools from allowing cheerleaders to be "launched or tossed and from taking part in formations higher than two levels" for one week during a women's basketball conference tournament, and also resulted in a recommendation by the NCAA that conferences and tournaments do not allow pyramids two and one half levels high or higher, and a stunt known as basket tosses, during the rest of the men's and women's basketball season. On July 11, 2006, the bans were made permanent by the AACCA rules committee:

The committee unanimously voted for sweeping revisions to cheerleading safety rules, the most major of which restricts specific upper-level skills during basketball games. Basket tosses, 2½ high pyramids, one-arm stunts, stunts that involve twisting or flipping, and twisting tumbling skills may be performed only during halftime and post-game on a matted surface and are prohibited during game play or time-outs.
Another major cheerleading accident was the death of Lauren Chang. Chang died on April 14, 2008 after competing in a competition where her teammate had kicked her so hard in the chest that her lungs collapsed.

Of the United States' 2.9 million female high school athletes, only 3% are cheerleaders, yet cheerleading accounts for nearly 65% of all catastrophic injuries in girls' high school athletics. The NCAA does not recognize cheerleading as a collegiate sport; there are no solid numbers on college cheerleading, yet when it comes to injuries, 67% of female athlete injuries at the college level are due to cheerleading mishaps. Another study found that between 1982 and 2007, there were 103 fatal, disabling, or serious injuries recorded among female high school athletes, with the vast majority (67) occurring in cheerleading.

In the early 2000s, cheerleading was considered one of the most dangerous school activities. The main source of injuries comes from stunting, also known as pyramids. These stunts are performed at games and pep rallies, as well as competitions. Sometimes competition routines are focused solely around the use of difficult and risky stunts. These stunts usually include a flyer (the person on top), along with one or two bases (the people on the bottom), and one or two spotters in the front and back on the bottom. The most common cheerleading related injury is a concussion. 96% of those concussions are stunt related. Others injuries are: sprained ankles, sprained wrists, back injuries, head injuries (sometimes concussions), broken arms, elbow injuries, knee injuries, broken noses, and broken collarbones. Sometimes, however, injuries can be as serious as whiplash, broken necks, broken vertebrae, and death.

The journal "Pediatrics" has reportedly said that the number of cheerleaders suffering from broken bones, concussions, and sprains has increased by over 100 percent between the years of 1990 and 2002, and that in 2001, there were 25,000 hospital visits reported for cheerleading injuries dealing with the shoulder, ankle, head, and neck. Meanwhile, in the USA, cheerleading accounted for 65.1% of all major physical injuries to high school females, and to 66.7% of major injuries to college students due to physical activity from 1982 to 2007, with 22,900 minors being admitted to hospital with cheerleading-related injuries in 2002.

In October 2009, the American Association of Cheerleading Coaches and Advisors (AACCA), a subsidiary of Varsity Brands, released a study that analyzed the data from emergency room visits of all high school athletes. The study asserted that contrary to many perceptions, cheerleading injuries are in line with female sports.

Cheerleading (for both girls and boys) was one of the sports studied in the Pediatric Injury Prevention, Education and Research Program of the Colorado School of Public Health in 2009/10–2012/13. Data on cheerleading injuries is included in the report for 2012–13.

The revamped and provocative Dallas Cowboys Cheerleaders of the 1970s—and the many imitators that followed—firmly established the cheerleader as an American icon of wholesome sex appeal. In response, a new subgenre of exploitation films suddenly sprang up with titles such as "The Cheerleaders" (1972), "The Swinging Cheerleaders" (1974), "Revenge of the Cheerleaders" (1975), "The Pom Pom Girls" (1976), "Satan's Cheerleaders" (1977), "Cheerleaders Beach Party" (1978), "Cheerleaders's Wild Weekend" (1979), and "Gimme an 'F'" (1984). In addition to R-rated sex comedies and horror films, cheerleaders became a staple of the adult film industry, starting with "Debbie Does Dallas" (1978) and its four sequels.

On television, the made-for-TV movie "The Dallas Cowboys Cheerleaders" (which aired January 14, 1979) starring Jane Seymour was a highly rated success, spawning the 1980 sequel "The Dallas Cowboys Cheerleaders II".

The Dallas squad was in high demand during the late 1970s with frequent appearances on network specials, awards shows, variety programs, commercials, the game show "Family Feud" and TV series such as "The Love Boat". The sci-fi sitcom "Mork & Mindy" also based a 1979 episode around the Denver Broncos cheerleaders with Mork (Robin Williams) trying out for the squad.

"The Positively True Adventures of the Alleged Texas Cheerleader-Murdering Mom" (1993) is a TV movie which dramatized the true story of Wanda Holloway, the Texas mother whose obsession with her daughter's cheerleading career made headline news. Another lurid TV movie based on a true story, "" was produced in 2008.

Cheerleading's increasing popularity in recent decades has made it a prominent feature in high-school themed movies and television shows. The 2000 film "Bring It On", about a San Diego high school cheerleading squad called "The Toros", starred real-life former cheerleader Kirsten Dunst. "Bring It On" was a surprise hit and earned nearly $70 million domestically. It spawned five direct-to-video sequels: "Bring It On Again" (2004), "" (2006), "" (2007), "" (2009) and "" (2017). The first "Bring It On" was followed by the cheerleader caper-comedy, "Sugar & Spice" (2001) and a string of campy horror/action films such as "Cheerleader Ninjas" (2002), "Cheerleader Autopsy", "Cheerleader Massacre" (both 2003), "Chainsaw Cheerleaders", and "Ninja Cheerleaders" (both 2008).

In 2006, Hayden Panettiere, star of "Bring It On: All or Nothing", took another cheerleading role as Claire Bennet, the cheerleader with an accelerated healing factor on NBC's hit sci-fi TV series "Heroes", launching cheerleading back into the limelight of pop culture. Claire was the main focus of the show's first story arc, featuring the popular catchphrase, "Save the cheerleader, save the world". Her prominent, protagonist role in "Heroes" was supported by a strong fan-base and provided a positive image for high school cheerleading.

In 2009, Panettiere starred again as a cheerleader, this time as Beth Cooper in the film adaptation of the novel "I Love You, Beth Cooper".

In 2006, the reality show "Cheerleader Nation" was featured on the Lifetime television channel. "Cheerleader Nation" is a 60-minute television series based on the Paul Laurence Dunbar High School cheerleading team's ups and downs on the way to nationals, of which they are the three-time champions. The show also believes that cheerleading is tough. The show takes place in Lexington, Kentucky.

The 2007 series "" shows the process of getting on the pro squad of the Dallas Cowboys Cheerleaders. Everything from initial tryouts to workout routines and the difficulties involved is shown.

"Fired Up!", a teen comedy about cheerleading camp, was released by Screen Gems in 2009. In the supernatural horror-comedy "Jennifer's Body" (2009), Megan Fox plays a demonically possessed high school cheerleader. Also that year, Universal Pictures signed music video and film director Bille Woodruff ("Barbershop", "Honey") to direct the fifth film in the "Bring It On" series titled "". The film stars Christina Milian (who previously played cheerleaders in "Love Don't Cost a Thing" and "Man of the House") and Rachelle Brook Smith, and was released directly to DVD and Blu-ray on September 1, 2009.

The television series "Glee" (2009-2015) featured Dianna Agron as Quinn Fabray, the captain of her high school cheerleading squad, the Cheerios. Quinn becomes pregnant, leading to her expulsion from the squad, but two of the other Cheerios, Santana Lopez and Brittany Pierce also feature heavily in the show. In the episode "The Power of Madonna", Kurt Hummel joins the Cheerios along with Mercedes Jones.

The CW Television Network created the short-lived "Hellcats" series (2010–11). This drama was about the ups and downs of being a college cheerleader. It starred Aly Michalka as Marty (a former gymnast forced to become a cheerleader after her academic scholarship is canceled) and Ashley Tisdale from "High School Musical".

More recent films include the teen sex-comedy "#1 Cheerleader Camp" (2010) and the horror-comedy "All Cheerleaders Die" (2013), a remake of an earlier 2001 film of the same name.

Nintendo has released a pair of video games in Japan for the Nintendo DS, "Osu! Tatakae! Ouendan" and its sequel "Moero! Nekketsu Rhythm Damashii" that star teams of male cheer squads, or Ouendan that practice a form of cheerleading. Each of the games' most difficult modes replaces the male characters with female cheer squads that dress in western cheerleading uniforms. The games task the cheer squads with assisting people in desperate need of help by cheering them on and giving them the motivation to succeed. There are also a "All Star Cheerleader" and "We Cheer" for the Wii in which one does routines at competitions with the Wiimote & Nunchuck. "All Star Cheerleader" is also available for Nintendo DS.

Cheerleading in Canada is rising in popularity among the youth in co-curricular programs. Cheerleading has grown from the sidelines to a competitive activity throughout the world and in particular Canada. Cheerleading has a few streams in Canadian sports culture. It is available at the middle-school, high-school, collegiate, and best known for all-star. There are multiple regional, provincial, and national championship opportunities for all athletes participating in cheerleading. Canada does not have provincial teams, just a national program referred to as CCU or Team Canada. Their first year as a national team was in 2009 when they represented Canada at the International Cheer Union World Cheerleading Championships International Cheer Union (ICU).

There is no official governing body for Canadian cheerleading. The rules and guidelines for cheerleading used in Canada are the ones set out by the USASF. However, there are many organizations in Canada that put on competitions and have separate and individual rules and scoresheets for each competition. Cheer Evolution is the largest cheerleading and dance organization for Canada. They hold many competitions as well as provide a competition for bids to Worlds. There are other organizations such as the Ontario Cheerleading Federation (Ontario), Power Cheerleading Association (Ontario), Kicks Athletics (Quebec), and the International Cheer Alliance (Vancouver). There are over forty recognized competitive gym clubs with numerous teams that compete at competitions across Canada.

There are two world championship competitions that Canada participates in. There is the ICU World Championships where the national teams compete against each other and then there are the club team world championships. These club teams are referred to as "all-star" teams who compete at the USASF World Championships of Cheerleading. This is where teams must have earned a bid from their own country to attend. National team members who compete at the ICU Worlds can also compete with their "all-star club" teams. Although athletes can compete in both International Cheer Union (ICU) and USASF, crossovers between teams at each individual competition are not permitted. Teams compete against the other teams from their countries on the first day of competition and the top three teams from each country in each division continue to finals. At the end of finals, the top team scoring the highest for their country earns the "Nations Cup". Canada has multiple teams across their country that compete in the USASF Cheerleading Worlds Championship.

The International Cheer Union (ICU) is built of 103 countries that compete against each other in four divisions; Coed Premier, All-girl Premier, Coed Elite, and All-girl Elite. Canada has a national team ran by the Canadian Cheer Union (CCU). Their Coed Elite Level 5 Team and their All-girl Elite Level 5 team are 4-time world champions. They are found from all over the country. In 2013, they added two more teams to their roster. A new division that will compete head-to-head with the United States: in both the All-girl and Coed Premier Level 6 divisions. Members tryout and are selected on the basis of their skills and potential to succeed. Athletes are selected from all over. Canada's national program has grown to be one of the most successful programs.



</doc>
<doc id="6751" url="https://en.wikipedia.org/wiki?curid=6751" title="Cottingley Fairies">
Cottingley Fairies

The Cottingley Fairies appear in a series of five photographs taken by Elsie Wright (1901–1988) and Frances Griffiths (1907–1986), two young cousins who lived in Cottingley, near Bradford in England. In 1917, when the first two photographs were taken, Elsie was 16 years old and Frances was 9. The pictures came to the attention of writer Sir Arthur Conan Doyle, who used them to illustrate an article on fairies he had been commissioned to write for the Christmas 1920 edition of "The Strand Magazine". Doyle, as a spiritualist, was enthusiastic about the photographs, and interpreted them as clear and visible evidence of psychic phenomena. Public reaction was mixed; some accepted the images as genuine, others believed that they had been faked.

Interest in the Cottingley Fairies gradually declined after 1921. Both girls married and lived abroad for a time after they grew up, yet the photographs continued to hold the public imagination. In 1966 a reporter from the "Daily Express" newspaper traced Elsie, who had by then returned to the UK. Elsie left open the possibility that she believed she had photographed her thoughts, and the media once again became interested in the story.

In the early 1980s Elsie and Frances admitted that the photographs were faked, using cardboard cutouts of fairies copied from a popular children's book of the time, but Frances maintained that the fifth and final photograph was genuine. The photographs and two of the cameras used are on display in the National Science and Media Museum in Bradford, England.

In mid-1917 nine-year-old Frances Griffiths and her mother—both newly arrived in the UK from South Africa—were staying with Frances' aunt, Elsie Wright's mother, in the village of Cottingley in West Yorkshire; Elsie was then 16 years old. The two girls often played together beside the beck (stream) at the bottom of the garden, much to their mothers' annoyance, because they frequently came back with wet feet and clothes. Frances and Elsie said they only went to the beck to see the fairies, and to prove it, Elsie borrowed her father's camera, a Midg quarter-plate. The girls returned about 30 minutes later, "triumphant".

Elsie's father, Arthur, was a keen amateur photographer, and had set up his own darkroom. The picture on the photographic plate he developed showed Frances behind a bush in the foreground, on which four fairies appeared to be dancing. Knowing his daughter's artistic ability, and that she had spent some time working in a photographer's studio, he dismissed the figures as cardboard cutouts. Two months later the girls borrowed his camera again, and this time returned with a photograph of Elsie sitting on the lawn holding out her hand to a gnome. Exasperated by what he believed to be "nothing but a prank", and convinced that the girls must have tampered with his camera in some way, Arthur Wright refused to lend it to them again. His wife Polly, however, believed the photographs to be authentic.

Towards the end of 1918, Frances sent a letter to Johanna Parvin, a friend in Cape Town, South Africa, where Frances had lived for most of her life, enclosing the photograph of herself with the fairies. On the back she wrote "It is funny, I never used to see them in Africa. It must be too hot for them there."

The photographs became public in mid-1919, after Elsie's mother attended a meeting of the Theosophical Society in Bradford. The lecture that evening was on "fairy life", and at the end of the meeting Polly Wright showed the two fairy photographs taken by her daughter and niece to the speaker. As a result, the photographs were displayed at the society's annual conference in Harrogate, held a few months later. There they came to the attention of a leading member of the society, Edward Gardner. One of the central beliefs of theosophy is that humanity is undergoing a cycle of evolution, towards increasing "perfection", and Gardner recognised the potential significance of the photographs for the movement:
Gardner sent the prints along with the original glass-plate negatives to Harold Snelling, a photography expert. Snelling's opinion was that "the two negatives are entirely genuine, unfaked photographs ... [with] no trace whatsoever of studio work involving card or paper models". He did not go so far as to say that the photographs showed fairies, stating only that "these are straight forward photographs of whatever was in front of the camera at the time". Gardner had the prints "clarified" by Snelling, and new negatives produced, "more conducive to printing", for use in the illustrated lectures he gave around the UK. Snelling supplied the photographic prints which were available for sale at Gardner's lectures.
Author and prominent spiritualist Sir Arthur Conan Doyle learned of the photographs from the editor of the spiritualist publication "Light". Doyle had been commissioned by "The Strand Magazine" to write an article on fairies for their Christmas issue, and the fairy photographs "must have seemed like a godsend" according to broadcaster and historian Magnus Magnusson. Doyle contacted Gardner in June 1920 to determine the background to the photographs, and wrote to Elsie and her father to request permission from the latter to use the prints in his article. Arthur Wright was "obviously impressed" that Doyle was involved, and gave his permission for publication, but he refused payment on the grounds that, if genuine, the images should not be "soiled" by money.

Gardner and Doyle sought a second expert opinion from the photographic company Kodak. Several of the company's technicians examined the enhanced prints, and although they agreed with Snelling that the pictures "showed no signs of being faked", they concluded that "this could not be taken as conclusive evidence ... that they were authentic photographs of fairies". Kodak declined to issue a certificate of authenticity. Gardner believed that the Kodak technicians might not have examined the photographs entirely objectively, observing that one had commented "after all, as fairies couldn't be true, the photographs must have been faked somehow". The prints were also examined by another photographic company, Ilford, who reported unequivocally that there was "some evidence of faking". Gardner and Doyle, perhaps rather optimistically, interpreted the results of the three expert evaluations as two in favour of the photographs' authenticity and one against.

Doyle also showed the photographs to the physicist and pioneering psychical researcher Sir Oliver Lodge, who believed the photographs to be fake. He suggested that a troupe of dancers had masqueraded as fairies, and expressed doubt as to their "distinctly 'Parisienne hairstyles.

On October 4, 2018 the first two of the photographs, "Alice and the Fairies" and "Iris and the Gnome," were to be sold by Dominic Winter Auctioneers, in Gloucestershire. The prints, suspected to have been made in 1920 to sell at theosophical lectures, were expected to bring £700-£1000 each. As it turned out, 'Iris with the Gnome' sold for a hammer price of £5,400 (plus 24% buyer's premium incl. VAT), while 'Alice and the Fairies' sold for a hammer price of £15,000 (plus 24% buyer's premium incl. VAT).

Doyle was preoccupied with organising an imminent lecture tour of Australia, and in July 1920, sent Gardner to meet the Wright family. Frances was by then living with her parents in Scarborough, but Elsie's father told Gardner that he had been so certain the photographs were fakes that while the girls were away he searched their bedroom and the area around the beck (stream), looking for scraps of pictures or cutouts, but found nothing "incriminating".
Gardner believed the Wright family to be honest and respectable. To place the matter of the photographs' authenticity beyond doubt, he returned to Cottingley at the end of July with two Kodak Cameo cameras and 24 secretly marked photographic plates. Frances was invited to stay with the Wright family during the school summer holiday so that she and Elsie could take more pictures of the fairies. Gardner described his briefing in his 1945 "Fairies: A Book of Real Fairies":

Until 19 August the weather was unsuitable for photography. Because Frances and Elsie insisted that the fairies would not show themselves if others were watching, Elsie's mother was persuaded to visit her sister's for tea, leaving the girls alone. In her absence the girls took several photographs, two of which appeared to show fairies. In the first, "Frances and the Leaping Fairy", Frances is shown in profile with a winged fairy close by her nose. The second, "Fairy offering Posy of Harebells to Elsie", shows a fairy either hovering or tiptoeing on a branch, and offering Elsie a flower. Two days later the girls took the last picture, "Fairies and Their Sun-Bath".

The plates were packed in cotton wool and returned to Gardner in London, who sent an "ecstatic" telegram to Doyle, by then in Melbourne. Doyle wrote back:

Doyle's article in the December 1920 issue of "The Strand" contained two higher-resolution prints of the 1917 photographs, and sold out within days of publication. To protect the girls' anonymity, Frances and Elsie were called Alice and Iris respectively, and the Wright family was referred to as the "Carpenters". An enthusiastic and committed spiritualist, Doyle hoped that if the photographs convinced the public of the existence of fairies then they might more readily accept other psychic phenomena. He ended his article with the words:

Early press coverage was "mixed", generally a combination of "embarrassment and puzzlement". The historical novelist and poet Maurice Hewlett published a series of articles in the literary journal "John O' London's Weekly", in which he concluded: "And knowing children, and knowing that Sir Arthur Conan Doyle has legs, I decide that the Miss Carpenters have pulled one of them." The Sydney newspaper "Truth" on 5 January 1921 expressed a similar view; "For the true explanation of these fairy photographs what is wanted is not a knowledge of occult phenomena but a knowledge of children." Some public figures were more sympathetic. Margaret McMillan, the educational and social reformer, wrote: "How wonderful that to these dear children such a wonderful gift has been vouchsafed." The novelist Henry De Vere Stacpoole decided to take the fairy photographs and the girls at face value. In a letter to Gardner he wrote: "Look at Alice's [Frances'] face. Look at Iris's [Elsie's] face. There is an extraordinary thing called Truth which has 10 million faces and forms – it is God's currency and the cleverest coiner or forger can't imitate it."

Major John Hall-Edwards, a keen photographer and pioneer of medical X-ray treatments in Britain, was a particularly vigorous critic:

Doyle used the later photographs in 1921 to illustrate a second article in "The Strand", in which he described other accounts of fairy sightings. The article formed the foundation for his 1922 book "The Coming of the Fairies". As before, the photographs were received with mixed credulity. Sceptics noted that the fairies "looked suspiciously like the traditional fairies of nursery tales" and that they had "very fashionable hairstyles".

Gardner made a final visit to Cottingley in August 1921. He again brought cameras and photographic plates for Frances and Elsie, but was accompanied by the clairvoyant Geoffrey Hodson. Although neither of the girls claimed to see any fairies, and there were no more photographs, "on the contrary, he [Hodson] saw them [fairies] everywhere" and wrote voluminous notes on his observations.

By now Elsie and Frances were tired of the whole fairy business. Years later Elsie looked at a photograph of herself and Frances taken with Hodson and said: "Look at that, fed up with fairies." Both Elsie and Frances later admitted that they "played along" with Hodson "out of mischief", and that they considered him "a fake".

Public interest in the Cottingley Fairies gradually subsided after 1921. Elsie and Frances eventually married and lived abroad for many years. In 1966, a reporter from the "Daily Express" newspaper traced Elsie, who was by then back in England. She admitted in an interview given that year that the fairies might have been "figments of my imagination", but left open the possibility she believed that she had somehow managed to photograph her thoughts. The media subsequently became interested in Frances and Elsie's photographs once again. BBC television's "Nationwide" programme investigated the case in 1971, but Elsie stuck to her story: "I've told you that they're photographs of figments of our imagination, and that's what I'm sticking to".

Elsie and Frances were interviewed by journalist Austin Mitchell in September 1976, for a programme broadcast on Yorkshire Television. When pressed, both women agreed that "a rational person doesn't see fairies", but they denied having fabricated the photographs. In 1978 the magician and scientific sceptic James Randi and a team from the Committee for the Scientific Investigation of Claims of the Paranormal examined the photographs, using a "computer enhancement process". They concluded that the photographs were fakes, and that strings could be seen supporting the fairies. Geoffrey Crawley, editor of the "British Journal of Photography", undertook a "major scientific investigation of the photographs and the events surrounding them", published between 1982 and 1983, "the first major postwar analysis of the affair". He also concluded that the pictures were fakes.

In 1983, the cousins admitted in an article published in the magazine "The Unexplained" that the photographs had been faked, although both maintained that they really had seen fairies. Elsie had copied illustrations of dancing girls from a popular children's book of the time, "Princess Mary's Gift Book", published in 1914, and drew wings on them. They said they had then cut out the cardboard figures and supported them with hatpins, disposing of their props in the beck once the photograph had been taken. But the cousins disagreed about the fifth and final photograph, which Doyle in his "The Coming of the Fairies" described in this way:

Elsie maintained it was a fake, just like all the others, but Frances insisted that it was genuine. In an interview given in the early 1980s Frances said:

Both Frances and Elsie claimed to have taken the fifth photograph. In a letter published in "The Times" newspaper on 9 April 1983, Geoffrey Crawley explained the discrepancy by suggesting that the photograph was "an unintended double exposure of fairy cutouts in the grass", and thus "both ladies can be quite sincere in believing that they each took it".

In a 1985 interview on Yorkshire Television's "Arthur C. Clarke's World of Strange Powers", Elsie said that she and Frances were too embarrassed to admit the truth after fooling Doyle, the author of Sherlock Holmes: "Two village kids and a brilliant man like Conan Doyle – well, we could only keep quiet." In the same interview Frances said: "I never even thought of it as being a fraud – it was just Elsie and I having a bit of fun and I can't understand to this day why they were taken in – they wanted to be taken in."

Frances died in 1986, and Elsie in 1988. Prints of their photographs of the fairies, along with a few other items including a first edition of Doyle's book "The Coming of the Fairies", were sold at auction in London for £21,620 in 1998. That same year, Geoffrey Crawley sold his Cottingley Fairy material to the National Museum of Film, Photography and Television in Bradford (now the National Science and Media Museum), where it is on display. The collection included prints of the photographs, two of the cameras used by the girls, watercolours of fairies painted by Elsie, and a nine-page letter from Elsie admitting to the hoax.
The glass photographic plates were bought for £6,000 by an unnamed buyer at a London auction held in 2001.

Frances' daughter, Christine Lynch, appeared in an episode of the television programme "Antiques Roadshow" in Belfast, broadcast on BBC One in January 2009, with the photographs and one of the cameras given to the girls by Doyle. Christine told the expert, Paul Atterbury, that she believed, as her mother had done, that the fairies in the fifth photograph were genuine. Atterbury estimated the value of the items at between £25,000 and £30,000. The first edition of Frances' memoirs was published a few months later, under the title "Reflections on the Cottingley Fairies". The book contains correspondence, sometimes "bitter", between Elsie and Frances. In one letter, dated 1983, Frances wrote:
The 1997 films "" and "Photographing Fairies" were inspired by the events surrounding the Cottingley Fairies. The photographs were parodied in a 1994 book written by Terry Jones and Brian Froud, "Lady Cottington's Pressed Fairy Book".

In 2017 a further two fairy photographs were presented as evidence that the girls' parents were part of the conspiracy. Dating from 1917 and 1918, both photographs are poorly executed copies of two of the original fairy photographs. One was published in 1918 in "The Sphere" newspaper, which was before the originals had been seen by anyone outside the girls' immediate family.




</doc>
<doc id="6752" url="https://en.wikipedia.org/wiki?curid=6752" title="Cheka">
Cheka

The All-Russian Extraordinary Commission (), abbreviated as VChK (, "Ve-Che-Ka") and commonly known as Cheka (from the initialism ChK - ), was the first of a succession of Soviet secret-police organizations. Established on December 5 (Old Style) 1917 by the Sovnarkom, it came under the leadership of Felix Dzerzhinsky, a Polish aristocrat-turned-communist.
By late 1918 hundreds of Cheka committees had sprung up in the RSFSR at the oblast, guberniya, raion, uyezd, and volost levels.

The official designation was All-Russian Extraordinary (or Emergency) Commission for Combating Counter-Revolution and Sabotage under the Council of People's Commissars of the RSFSR (, "Vserossiyskaya chrezvychaynaya komissiya po borbe s kontrrevolyutsiyey i sabotazhem pri Sovete narodnykh komisarov RSFSR").

In 1918 its name was changed, becoming All-Russian Extraordinary Commission for Combating Counter-Revolution, Profiteering and Corruption.

A member of Cheka was called a "chekist". Also, the term "chekist" often referred to Soviet secret police throughout the Soviet period, despite official name changes over time. In "The Gulag Archipelago", Alexander Solzhenitsyn recalls that "zeks" in the labor camps used "old chekist" as a mark of special esteem for particularly experienced camp administrators. The term is still found in use in Russia today (for example, President Vladimir Putin has been referred to in the Russian media as a "chekist" due to his career in the KGB and as head of the KGB's successor, FSB).

The chekists commonly dressed in black leather, including long flowing coats, reportedly after being issued such distinctive coats early in their existence. Western communists adopted this clothing fashion. The Chekists also often carried with them Greek-style worry beads made of amber, which had become "fashionable among high officials during the time of the 'cleansing'".

In 1921, the "Troops for the Internal Defense of the Republic" (a branch of the Cheka) numbered at least 200,000. These troops policed labor camps, ran the Gulag system, conducted requisitions of food, and subjected political opponents to secret arrest, detention, torture and summary execution. They also put down rebellions and riots by workers or peasants, and mutinies in the desertion-plagued Red Army.

After 1922 Cheka groups underwent the first of a series of reorganizations; however the theme of a government dominated by "the organs" persisted indefinitely afterward, and Soviet citizens continued to refer to members of the various organs as Chekists.

In the first month and half after the October Revolution (1917), the duty of "extinguishing the resistance of exploiters" was assigned to the Petrograd Military Revolutionary Committee (or PVRK). It represented a temporary body working under directives of the Council of People's Commissars (Sovnarkom) and Central Committee of RDSRP(b). The VRK created new bodies of government, organized food delivery to cities and the Army, requisitioned products from "bourgeoisie", and sent its emissaries and agitators into provinces. One of its most important functions was the security of "revolutionary order", and the fight against "counterrevolutionary" activity (see: Anti-Soviet agitation).

On December 1, 1917, the All-Russian Central Executive Committee (VTsIK or TsIK) reviewed a proposed reorganization of the VRK, and possible replacement of it. On December 5, the Petrograd VRK published an announcement of dissolution and transferred its functions to the department of TsIK for the fight against "counterrevolutionaries". On December 6, the Council of People's Commissars (Sovnarkom) strategized how to persuade government workers to strike across Russia. They decided that a special commission was needed to implement the "most energetically revolutionary" measures. Felix Dzerzhinsky (the Iron Felix) was appointed as Director and invited the participation of the following individuals: V. K. Averin, V. N. Vasilevsky, D. G. Yevseyev, N. A. Zhydelev, I. K. Ksenofontov, G. K. Ordjonikidze, Ya. Kh. Peters, K. A. Peterson, V. A. Trifonov.

On December 7, 1917, all invited except Zhydelev and Vasilevsky gathered in the Smolny Institute to discuss the competence and structure of the commission to combat counterrevolution and sabotage. The obligations of the commission were: "to liquidate to the root all of the counterrevolutionary and sabotage activities and all attempts to them in all of Russia, to hand over counter-revolutionaries and saboteurs to the revolutionary tribunals, develop measures to combat them and relentlessly apply them in real world applications. The commission should only conduct a preliminary investigation". The commission should also observe the press and counterrevolutionary parties, sabotaging officials and other criminals. 

Three sections were created: informational, organizational, and a unit to combat counter-revolution and sabotage. Upon the end of the meeting, Dzerzhinsky reported to the Sovnarkom with the requested information. The commission was allowed to apply such measures of repression as 'confiscation, deprivation of ration cards, publication of lists of enemies of the people etc.'". That day, Sovnarkom officially confirmed the creation of VCheKa. The commission was created not under the VTsIK as was previously anticipated, but rather under the Council of the People's Commissars.

On December 8, 1917, some of the original members of the VCheka were replaced. Averin, Ordzhonikidze, and Trifonov were replaced by V. V. Fomin, S. E. Shchukin, Ilyin, and Chernov. On the meeting of December 8, the presidium of VChK was elected of five members, and chaired by Dzerzhinsky. The issue of "speculation" was raised at the same meeting, which was assigned to Peters to address and report with results to one of the next meetings of the commission. A circular, published on , gave the address of VCheka's first headquarters as "Petrograd, Gorokhovaya 2, 4th floor". On December 11, Fomin was ordered to organize a section to suppress "speculation." And in the same day, VCheKa offered Shchukin to conduct arrests of counterfeiters.

In January 1918, a subsection of the anti-counterrevolutionary effort was created to police bank officials. The structure of VCheKa was changing repeatedly. By March 1918, when the organization came to Moscow, it contained the following sections: against counterrevolution, speculation, non-residents, and information gathering. By the end of 1918–1919, some new units were created: secretly operative, investigatory, of transportation, military (special), operative, and instructional. By 1921, it changed once again, forming the following sections: directory of affairs, administrative-organizational, secretly operative, economical, and foreign affairs.

In the first months of its existence, VCheKa consisted of only 40 officials. It commanded a team of soldiers, the Sveaborgesky regiment, as well as a group of Red Guardsmen. On January 14, 1918, Sovnarkom ordered Dzerzhinsky to organize teams of "energetic and ideological" sailors to combat speculation. By the spring of 1918, the commission had several teams: in addition to the Sveaborge team, it had an intelligence team, a team of sailors, and a strike team. Through the winter of 1917–1918, all activities of VCheKa were centralized mainly in the city of Petrograd. It was one of several other commissions in the country which fought against counterrevolution, speculation, banditry, and other activities perceived as crimes. Other organizations included: the Bureau of Military Commissars, and an Army-Navy investigatory commission to attack the counterrevolutionary element in the Red Army, plus the Central Requisite and Unloading Commission to fight speculation. The investigation of counterrevolutionary or major criminal offenses was conducted by the Investigatory Commission of Revtribunal. The functions of VCheKa were closely intertwined with the Commission of V. D. Bonch-Bruyevich, which beside the fight against wine pogroms was engaged in the investigation of most major political offenses (see: Bonch-Bruyevich Commission).

All results of its activities, VCheKa had either to transfer to the Investigatory Commission of Revtribunal, or to dismiss. The control of the commission's activity was provided by the People's Commissariat for Justice (Narkomjust, at that time headed by Isidor Steinberg) and Internal Affairs (NKVD, at that time headed by Grigory Petrovsky). Although the VCheKa was officially an independent organization from the NKVD, its chief members such as Dzerzhinsky, Latsis, Unszlicht, and Uritsky (all main chekists), since November 1917 composed the collegiate of NKVD headed by Petrovsky. In November 1918, Petrovsky was appointed as head of the All-Ukrainian Central Military Revolutionary Committee during VCheKa's expansion to provinces and front-lines. At the time of political competition between Bolsheviks and SRs (January 1918), Left SRs attempted to curb the rights of VCheKa and establish through the Narkomiust their control over its work. Having failed in attempts to subordinate the VCheKa to Narkomiust, the Left SRs tried to gain control of the Extraordinary Commission in a different way: they requested that the Central Committee of the party was granted the right to directly enter their representatives into the VCheKa. Sovnarkom recognized the desirability of including five representatives of the Left Socialist-Revolutionary faction of VTsIK. Left SRs were granted the post of a companion (deputy) chairman of VCheKa. However, Sovnarkom, in which the majority belonged to the representatives of RSDLP(b) retained the right to approve members of the collegium of the VCheKa.

Originally, members of the Cheka were exclusively Bolshevik; however, in January 1918, Left SRs also joined the organization The Left SRs were expelled or arrested later in 1918, following the attempted assassination of Lenin by an SR, Fanni Kaplan.

By the end of January 1918, the Investigatory Commission of Petrograd Soviet (probably same as of Revtribunal) petitioned Sovnarkom to delineate the role of detection and judicial-investigatory organs. It offered to leave, for the VCheKa and the Commission of Bonch-Bruyevich, only the functions of detection and suppression, while investigative functions entirely transferred to it. The Investigatory Commission prevailed. On January 31, 1918, Sovnarkom ordered to relieve VCheKa of the investigative functions, leaving for the commission only the functions of detection, suppression, and prevention of anti revolutionary crimes. At the meeting of the Council of People's Commissars on January 31, 1918, a merger of VCheKa and the Commission of Bonch-Bruyevich was proposed. The existence of both commissions, VCheKa of Sovnarkom and the Commission of Bonch-Bruyevich of VTsIK, with almost the same functions and equal rights, became impractical. A decision followed two weeks later.

On February 23, 1918, VCheKa sent a radio telegram to all Soviets with a petition to immediately organize emergency commissions to combat counter-revolution, sabotage and speculation, if such commissions had not been yet organized. February 1918 saw the creation of local Extraordinary Commissions. One of the first founded was the Moscow Cheka. Sections and commissariats to combat counterrevolution were established in other cities. The Extraordinary Commissions arose, usually in the areas during the moments of the greatest aggravation of political situation. On February 25, 1918, as the counterrevolutionary organization "Union of Front-liners" was making advances, the executive committee of the Saratov Soviet formed a counter-revolutionary section. On March 7, 1918, because of the move from Petrograd to Moscow, the Petrograd Cheka was created. On March 9, a section for combating counterrevolution was created under the Omsk Soviet. Extraordinary commissions were also created in Penza, Perm, Novgorod, Cherepovets, Rostov, Taganrog. On March 18, VCheKa adopted a resolution, "The Work of VCheKa on the All-Russian Scale", foreseeing the formation everywhere of Extraordinary Commissions after the same model, and sent a letter that called for the widespread establishment of the Cheka in combating counterrevolution, speculation, and sabotage. Establishment of provincial Extraordinary Commissions was largely completed by August 1918. In the Soviet Republic, there were 38 gubernatorial Chekas (Gubcheks) by this time.

On June 12, 1918, the All-Russian Conference of Cheka adopted the "Basic Provisions on the Organization of Extraordinary Commissions". They set out to form Extraordinary Commissions not only at Oblast and Guberniya levels, but also at the large Uyezd Soviets. In August 1918, in the Soviet Republic had accounted for some 75 Uyezd-level Extraordinary Commissions. By the end of the year, 365 Uyezd-level Chekas were established. In 1918, the All-Russia Extraordinary Commission and the Soviets managed to establish a local Cheka apparatus. It included Oblast, Guberniya, Raion, Uyezd, and Volost Chekas, with Raion and Volost Extraordinary Commissioners. In addition, border security Chekas were included in the system of local Cheka bodies.

In the autumn of 1918, as consolidation of the political situation of the republic continued, a move toward elimination of Uyezd-, Raion-, and Volost-level Chekas, as well as the institution of Extraordinary Commissions was considered. On January 20, 1919, VTsIK adopted a resolution prepared by VCheKa, "On the abolition of Uyezd Extraordinary Commissions". On January 16 the presidium of VCheKa approved the draft on the establishment of the Politburo at Uyezd militsiya. This decision was approved by the Conference of the Extraordinary Commission IV, held in early February 1920.

On August 3, a VCheKa section for combating counterrevolution, speculation and sabotage on railways was created. On August 7, 1918, Sovnarkom adopted a decree on the organization of the railway section at VCheKa. Combating counterrevolution, speculation, and malfeasance on railroads was passed under the jurisdiction of the railway section of VCheKa and local Cheka. In August 1918, railway sections were formed under the Gubcheks. Formally, they were part of the non-resident sections, but in fact constituted a separate division, largely autonomous in their activities. The gubernatorial and oblast-type Chekas retained in relationship to the transportation sections only control and investigative functions.

The beginning of a systematic work of organs of VCheKa in RKKA refers to July 1918, the period of extreme tension of the civil war and class struggle in the country. On July 16, 1918, the Council of People's Commissars formed the Extraordinary Commission for combating counterrevolution at the Czechoslovak (Eastern) Front, led by M. I. Latsis. In the fall of 1918, Extraordinary Commissions to combat counterrevolution on the Southern (Ukraine) Front were formed. In late November, the Second All-Russian Conference of the Extraordinary Commissions accepted a decision after a report from I. N. Polukarov to establish at all frontlines, and army sections of the Cheka and granted them the right to appoint their commissioners in military units. On December 9, 1918, the collegiate (or presidium) of VCheKa had decided to form a military section, headed by M. S. Kedrov, to combat counterrevolution in the Army. In early 1919, the military control and the military section of VCheKa were merged into one body, the Special Section of the Republic. Kedrov was appointed as head. On January 1, he issued an order to establish the Special Section. The order instructed agencies everywhere to unite the Military control and the military sections of Chekas and to form special sections of frontlines, armies, military districts, and guberniyas.

In November 1920 the Soviet of Labor and Defense created a Special Section of VCheKa for the security of the state border. On February 6, 1922, after the Ninth All-Russian Soviet Congress, the Cheka was dissolved by VTsIK, "with expressions of gratitude for heroic work." It was replaced by the State Political Administration or OGPU, a section of the NKVD of the Russian Soviet Federative Socialist Republic (RSFSR). Dzerzhinsky remained as chief of the new organization.
Initially formed to fight against counter-revolutionaries and saboteurs, as well as financial speculators, the Cheka had its own classifications. Those counter-revolutionaries fell under these categories:


As its name implied, the Extraordinary Commission had virtually unlimited powers and could interpret them in any way it wished. No standard procedures were ever set up, except that the Commission was supposed to send the arrested to the Military-Revolutionary tribunals if outside of a war zone. This left an opportunity for a wide range of interpretations, as the whole country was in total chaos. At the direction of Lenin, the Cheka performed mass arrests, imprisonments, and executions of "enemies of the people". In this, the Cheka said that they targeted "class enemies" such as the bourgeoisie, and members of the clergy; the first organized mass repression began against the libertarians and socialists of Petrograd in April 1918. Over the next few months, 800 were arrested and shot without trial.

Within a month, the Cheka had extended its repression to all political opponents of the communist government, including anarchists and others on the left. On April 11/12, 1918, some 26 anarchist political centres in Moscow were attacked. Forty anarchists were killed by Cheka forces, and about 500 were arrested and jailed after a pitched battle took place between the two groups. In response to the anarchists' resistance, the Cheka orchestrated a massive retaliatory campaign of repression, executions, and arrests against all opponents of the Bolshevik government, in what came to be known as "Red Terror". The "Red Terror", implemented by Dzerzhinsky on September 5, 1918, was vividly described by the Red Army journal "Krasnaya Gazeta":

Without mercy, without sparing, we will kill our enemies in scores of hundreds. Let them be thousands, let them drown themselves in their own blood. For the blood of Lenin and Uritsky … let there be floods of blood of the bourgeoisie – more blood, as much as possible..."

An early Bolshevik, Victor Serge described in his book "Memoirs of a Revolutionary":

The Cheka was also used against the armed anarchist Black Army of Nestor Makhno in the Ukraine. After the Black Army had served its purpose in aiding the Red Army to stop the Whites under Denikin, the Soviet communist government decided to eliminate the anarchist forces. In May 1919, two Cheka agents sent to assassinate Makhno were caught and executed.

Many victims of Cheka repression were "bourgeois hostages" rounded up and held in readiness for summary execution in reprisal for any alleged counter-revolutionary act. Wholesale, indiscriminate arrests became an integral part of the system. The Cheka used trucks disguised as delivery trucks, called "Black Marias", for the secret arrest and transport of prisoners.

It was during the Red Terror that the Cheka, hoping to avoid the bloody aftermath of having half-dead victims writhing on the floor, developed a technique for execution known later by the German words ""Nackenschuss" or ""Genickschuss", a shot to the nape of the neck, which caused minimal blood loss and instant death. The victim's head was bent forward, and the executioner fired slightly downward at point blank range. This had become the standard method used later by the NKVD to liquidate Joseph Stalin's purge victims and others.

It is believed that there were more than three million deserters from the Red Army in 1919 and 1920. Approximately 500,000 deserters were arrested in 1919 and close to 800,000 in 1920, by troops of the 'Special Punitive Department' of the Cheka, created to punish desertions. These troops were used to forcibly repatriate deserters, taking and shooting hostages to force compliance or to set an example. Throughout the course of the civil war, several thousand deserters were shot – a number comparable to that of belligerents during World War I.

In September 1918, according to "The Black Book of Communism", in only twelve provinces of Russia, 48,735 deserters and 7,325 "bandits" were arrested, 1,826 were killed and 2,230 were executed. The exact identity of these individuals is confused by the fact that the Soviet Bolshevik government used the term 'bandit' to cover ordinary criminals as well as armed and unarmed political opponents, such as the anarchists.

Estimates on Cheka executions vary widely. The lowest figures ("disputed below") are provided by Dzerzhinsky's lieutenant Martyn Latsis, limited to RSFSR over the period 1918–1920:




Experts generally agree these semi-official figures are vastly understated. Pioneering historian of the Red Terror Sergei Melgunov claims that this was done deliberately in an attempt to demonstrate the government's humanity. For example, he refutes the claim made by Latsis that only 22 executions were carried out in the first six months of the Cheka's existence by providing evidence that the true number was 884 executions. W. H. Chamberlin claims, "It is simply impossible to believe that the Cheka only put to death 12,733 people in all of Russia up to the end of the civil war." Donald Rayfield concurs, noting that, "Plausible evidence reveals that the actual numbers . . . vastly exceeded the official figures." Chamberlin provides the "reasonable and probably moderate" estimate of 50,000, while others provide estimates ranging up to 500,000. Several scholars put the number of executions at about 250,000. Some believe it is possible more people were murdered by the Cheka than died in battle. Historian James Ryan gives a modest estimate of 28,000 executions per year from December 1917 to February 1922.

Lenin himself seemed unfazed by the killings. On 12 January 1920, while addressing trade union leaders, he said: "We did not hesitate to shoot thousands of people, and we shall not hesitate, and we shall save the . On 14 May 1921, the Politburo, chaired by Lenin, passed a motion "broadening the rights of the [Cheka] in relation to the use of the [death penalty]."

The Cheka is reported to have practiced torture. Depending on Cheka committees in various cities, the methods included: being skinned alive, scalped, "crowned" with barbed wire, impaled, crucified, hanged, stoned to death, tied to planks and pushed slowly into furnaces or tanks of boiling water, or rolled around naked in internally nail-studded barrels. Chekists reportedly poured water on naked prisoners in the winter-bound streets until they became living ice statues. Others reportedly beheaded their victims by twisting their necks until their heads could be torn off. The Cheka detachments stationed in Kiev reportedly would attach an iron tube to the torso of a bound victim and insert a rat in the tube closed off with wire netting, while the tube was held over a flame until the rat began gnawing through the victim's guts in an effort to escape. Anton Denikin's investigation discovered corpses whose lungs, throats, and mouths had been packed with earth.

Women and children were also victims of Cheka terror. Women would sometimes be tortured and raped before being shot. Children between the ages of 8 and 13 were imprisoned and occasionally executed.

All of these atrocities were published on numerous occasions in "Pravda" and "Izvestiya": January 26, 1919 "Izvestiya" #18 article "Is it really a medieval imprisonment?" («Неужели средневековый застенок?»); February 22, 1919 "Pravda" #12 publishes details of the Vladimir Cheka's tortures, September 21, 1922 "Socialist Herald" publishes details of series of tortures conducted by the Stavropol Cheka (hot basement, cold basement, skull measuring etc.).

The Chekists were also supplemented by the militarized Units of Special Purpose (the Party's Spetsnaz or ).

Cheka was actively and openly utilizing kidnapping methods. With kidnapping methods Cheka was able to extinguish numerous cases of discontent especially among the rural population. Among the notorious ones was the Tambov rebellion.

Villages were bombarded to complete annihilation like in the case of Tretyaki, Novokhopersk uyezd, Voronezh Governorate. 

As a result of this relentless violence more than a few Chekists ended up with psychopathic disorders, which Nikolai Bukharin said were "an occupational hazard of the Chekist profession." Many hardened themselves to the executions by heavy drinking and drug use. Some developed a gangster-like slang for the verb to kill in an attempt to distance themselves from the killings, such as 'shooting partridges', of 'sealing' a victim, or giving him a "natsokal" (onomatopoeia of the trigger action).

On November 30, 1992, by the initiative of the President of the Russian Federation the Constitutional Court of the Russian Federation recognized the Red Terror as unlawful, which in turn led to suspension of the Communist Party of the RSFSR.

Cheka departments were organized not only in big cities and guberniya seats, but also in each uyezd, at any front-lines and military formations. Nothing is known on what resources they were created. Many who were hired to head those departments were so-called "nestlings of Alexander Keren".


Konstantin Preobrazhenskiy criticised the continuing celebration of the professional holiday of the old and the modern Russian security services on the anniversary of the creation of the Cheka, with the assent of the Presidents of Russia. (Vladimir Putin, former KGB officer, chose not to change the date to another): "The successors of the KGB still haven't renounced anything; they even celebrate their professional holiday the same day, as during repression, on the 20th of December. It is as if the present intelligence and counterespionage services of Germany celebrated Gestapo Day. I can imagine how indignant our press would be!"




</doc>
<doc id="6753" url="https://en.wikipedia.org/wiki?curid=6753" title="Clitic">
Clitic

A clitic (, backformed from Greek "leaning" or "enclitic") is a morpheme in morphology and syntax that has syntactic characteristics of a word, but depends phonologically on another word or phrase. In this sense, it is syntactically independent but phonologically dependent—always attached to a host. The term derives from the Greek for "leaning". A clitic is pronounced like an affix, but plays a syntactic role at the phrase level. In other words, clitics have the "form" of affixes, but the distribution of function words. For example, the contracted forms of the auxiliary verbs in "I'm" and "we've" are clitics.

Clitics can belong to any grammatical category, although they are commonly pronouns, determiners, or adpositions. Note that orthography is not always a good guide for distinguishing clitics from affixes: clitics may be written as separate words, but sometimes they are joined to the word they depend on (like the Latin clitic "-que", meaning "and"), or separated by special characters such as hyphens or apostrophes (like the English clitic "’s" in "it's" for "it has" or "it is").

Clitics fall into various categories depending on their position in relation to the word they connect to.

A proclitic appears before its host. It is common in Romance languages. For example, in French, there is "il s'est réveillé" ("he woke up"), or "je t'aime" ("I love you").

An enclitic appears after its host.

A mesoclitic appears between the stem of the host and other affixes. For example, in Portuguese, "conquistar-se-á" ("it will be conquered"), "dá-lo-ei" ("I will give it"), "matá-la-ia" ("he/she/it would kill her"). These are found much more often in writing than in speech. It is even possible to use two pronouns inside the verb, as in "dar-no-lo-á" ("he/she/it will give it to us"), or "dar-ta-ei" ("ta" = "te" + "a", "I will give it/her to you"). As in other Romance languages, the Portuguese synthetic future tense comes from the merging of the infinitive and the corresponding finite forms of the verb "haver" (from Latin "habēre"), which explains the possibility of separating it from the infinitive.

The endoclitic splits apart the root and is inserted between the two pieces. Endoclitics defy the Lexical Integrity Hypothesis (or Lexicalist hypothesis) and so were long thought impossible. However, evidence from the Udi language suggests that they exist. Endoclitics are also found in Pashto and are reported to exist in Degema.

One important distinction divides the broad term 'clitics' into two categories, simple clitics and special clitics. This distinction is, however, disputed.

Simple clitics are free morphemes, meaning they can stand alone in a phrase or sentence. They are unaccented and thus phonologically dependent upon a nearby word. They only derive meaning from this "host".

Special clitics are morphemes that are bound to the word they are dependent upon, meaning they exist as a part of their host. This form, which is unaccented, represents a variant of a free form that does carry stress. While the two variants carry similar meaning and phonological makeup, the special clitic is bound to a host word and unaccented.

Some clitics can be understood as elements undergoing a historical process of grammaticalization:

lexical item → clitic → affix

According to this model from Judith Klavans, an autonomous lexical item in a particular context loses the properties of a fully independent word over time and acquires the properties of a morphological affix (prefix, suffix, infix, etc.). At any intermediate stage of this evolutionary process, the element in question can be described as a "clitic". As a result, this term ends up being applied to a highly heterogeneous class of elements, presenting different combinations of word-like and affix-like properties.

One characteristic shared by many clitics is a lack of prosodic independence. A clitic attaches to an adjacent word, known as its "host". Orthographic conventions treat clitics in different ways: Some are written as separate words, some are written as one word with their hosts, and some are attached to their hosts, but set off by punctuation (a hyphen or an apostrophe, for example).

Although the term "clitic" can be used descriptively to refer to any element whose grammatical status is somewhere in between a typical word and a typical affix, linguists have proposed various definitions of "clitic" as a technical term. One common approach is to treat clitics as words that are prosodically deficient: they cannot appear without a host, and they can only form an accentual unit in combination with their host. The term "postlexical clitic" is used for this narrower sense of the term.

Given this basic definition, further criteria are needed to establish a dividing line between postlexical clitics and morphological affixes, since both are characterized by a lack of prosodic autonomy. There is no natural, clear-cut boundary between the two categories (since from a historical point of view, a given form can move gradually from one to the other by morphologization). However, by identifying clusters of observable properties that are associated with core examples of clitics on the one hand, and core examples of affixes on the other, one can pick out a battery of tests that provide an empirical foundation for a clitic/affix distinction.

An affix syntactically and phonologically attaches to a base morpheme of a limited part of speech, such as a verb, to form a new word. A clitic syntactically functions above the word level, on the phrase or clause level, and attaches only phonetically to the first, last, or only word in the phrase or clause, whichever part of speech the word belongs to.
The results of applying these criteria sometimes reveal that elements that have traditionally been called "clitics" actually have the status of affixes (e.g., the Romance pronominal clitics discussed below).

Zwicky and Pullum postulated five characteristics that distinguish clitics from affixes:
An example of differing analyses by different linguists is the discussion of the possessive ('s) in English, some linguists treating it as an affix, while others treat it as a special clitic. 

Similar to the discussion above, clitics must be distinguishable from words. Linguists have proposed a number of tests to differentiate between the two categories. Some tests, specifically, are based upon the understanding that when comparing the two, clitics resemble affixes, while words resemble syntactic phrases. Clitics and words resemble different categories, in the sense that they share certain properties. Six such tests are described below. These, of course, are not the only ways to differentiate between words and clitics.


Clitics do not always appear next to the word or phrase that they are associated with grammatically. They may be subject to global word order constraints that act on the entire sentence. Many Indo-European languages, for example, obey Wackernagel's law (named after Jacob Wackernagel), which requires clitics to appear in "second position", after the first syntactic phrase or the first stressed word in a clause:

English enclitics include the contracted versions of auxiliary verbs, as in "I'm" and "we've". Some also regard the possessive marker, as in "The Queen of England's crown" as an enclitic, rather than a (phrasal) genitival inflection.

Some consider the English articles "a, an, the" and the infinitive marker "to" proclitics.

The negative marker "n’t" as in "couldn’t" etc. is typically considered a clitic that developed from the lexical item "not". Linguists Arnold Zwicky and Geoffrey Pullum argue, however, that the form has the properties of an affix rather than a syntactically independent clitic.


In Romance languages, some feel the object personal pronoun forms are clitics. Others consider them affixes, as they only attach to the verb they are the object of. There is no general agreement on the issue. For the Spanish object pronouns, for example:


Colloquial European Portuguese allows object suffixes before the conditional and future suffixes of the verbs:


Colloquial Portuguese of Brazil and Portugal and Spanish of the former Gran Colombia allow ser to be conjugated as a verbal clitic adverbial adjunct to emphasize the importance of the phrase compared to its context, or with the meaning of "really" or "in truth":


Note that this clitic form is only for the verb ser and is restricted to only third-person singular conjugations. It is not used as a verb in the grammar of the sentence but introduces prepositional phrases and adds emphasis. It does not need to concord with the tense of the main verb, as in the second example, and can be usually removed from the sentence without affecting the simple meaning.

In the Indo-European languages, some clitics can be traced back to Proto-Indo-European: for example, *"" is the original form of Sanskrit "च" ("-ca"), Greek "τε" ("-te"), and Latin "-que".


In Croatian these clitics follow the first stressed word in the sentence or clause in most cases, which may have been inherited from Proto-Indo-European (see Wackernagel's Law), even though many of the modern clitics became cliticised much more recently in the language (e.g. auxiliary verbs or the accusative forms of pronouns). In subordinate clauses and questions, they follow the connector and/or the question word respectively. 

Examples (clitics - "sam" "I am", "biste" "you would (pl.)", "mi" "to me", "vam" "to you (pl.)", "ih" "them"): 


In certain rural dialects this rule is (or was until recently) very strict, whereas elsewhere various exceptions occur. These include phrases containing conjunctions (e. g. "Ivan i Ana" "Ivan and Ana"), nouns with a genitival attribute (e. g. "vrh brda" "the top of the hill"), proper names and titles and the like (e. g. "(gospođa) Ivana Marić" "(Mrs) Ivana Marić", "grad Zagreb" "the city (of) Zagreb"), and in many local varieties clitics are hardly ever inserted into any phrases (e. g. "moj najbolji prijatelj" "my best friend", "sutra ujutro" "tomorrow morning"). In cases like these, clitics normally follow the initial phrase, although some Standard grammar handbooks recommend that they should be placed immediately after the verb (many native speakers find this unnatural). 

Examples: 


Clitics are however never inserted after the negative particle "ne", which always precedes the verb in Croatian, or after prefixes (earlier preverbs), and the interrogative particle "li" always immediately follows the verb. Colloquial interrogative particles such as "da li", "dal", "jel" appear in sentence-initial position and are followed by clitics (if there are any). 

Examples: 




</doc>
<doc id="6759" url="https://en.wikipedia.org/wiki?curid=6759" title="Context-free grammar">
Context-free grammar

In formal language theory, a context-free grammar (CFG) is a certain type of formal grammar: a set of production rules that describe all possible strings in a given formal language. Production rules are simple replacements. For example, the rule

formula_1

replaces formula_2 with formula_3. There can be multiple replacement rules for any given value. For example,

formula_1

formula_5

means that formula_2 can be replaced with either formula_3 or formula_8.

In context-free grammars, all rules are one-to-one, one-to-many, or one-to-none. These rules can be applied regardless of context. The left-hand side of the production rule is always a nonterminal symbol. This means that the symbol does not appear in the resulting formal language. So in our case, our language contains the letters formula_3 and formula_8 but not formula_11

Rules can also be applied in reverse to check whether a string is grammatically correct according to the grammar.

Here is an example context-free grammar that describes all two-letter strings containing the letters formula_3 or formula_8.

formula_14

formula_1

formula_5

If we start with the nonterminal symbol formula_17 then we can use the rule
formula_14 to turn formula_17 into formula_20. We can then apply one of the two later rules. For example, if we apply formula_5 to the first formula_2 we get formula_23. If we then apply formula_1 to the second formula_2 we get formula_26. Since both formula_3 and formula_8 are terminal symbols, and in context-free grammars terminal symbols never appear on the left hand side of a production rule, there are no more rules that can be applied. This same process can be used, applying the last two rules in different orders in order to get all possible strings within our simple context-free grammar.

Languages generated by context-free grammars are known as context-free languages (CFL). Different context-free grammars can generate the same context-free language. It is important to distinguish the properties of the language (intrinsic properties) from the properties of a particular grammar (extrinsic properties). The language equality question (do two given context-free grammars generate the same language?) is undecidable.

Context-free grammars arise in linguistics where they are used to describe the structure of sentences and words in a natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose, but have not really lived up to their original expectation. By contrast, in computer science, as the use of recursively-defined concepts increased, they were used more and more. In an early application, grammars are used to describe the structure of programming languages. In a newer application, they are used in an essential part of the Extensible Markup Language (XML) called the "Document Type Definition".

In linguistics, some authors use the term phrase structure grammar to refer to context-free grammars, whereby phrase-structure grammars are distinct from dependency grammars. In computer science, a popular notation for context-free grammars is Backus–Naur form, or "BNF".

Since the time of Pāṇini, at least, linguists have described the grammars of languages in terms of their block structure, and described how sentences are recursively built up from smaller phrases, and eventually individual words or word elements. An essential property of these block structures is that logical units never overlap. For example, the sentence:
can be logically parenthesized as follows:
A context-free grammar provides a simple and mathematically precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks, capturing the "block structure" of sentences in a natural way. Its simplicity makes the formalism amenable to rigorous mathematical study. Important features of natural language syntax such as agreement and reference are not part of the context-free grammar, but the basic recursive structure of sentences, the way in which clauses nest inside other clauses, and the way in which lists of adjectives and adverbs are swallowed by nouns and verbs, is described exactly.

Context-free grammars are a special form of Semi-Thue systems that in their general form date back to the work of Axel Thue.

The formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky, and also their classification as a special type of formal grammar (which he called phrase-structure grammars). What Chomsky called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency grammars. In Chomsky's generative grammar framework, the syntax of natural language was described by context-free rules combined with transformation rules.

Block structure was introduced into computer programming languages by the Algol project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting Algol syntax. This became a standard feature of computer languages, and the notation for grammars used in concrete descriptions of computer languages came to be known as Backus–Naur form, after two members of the Algol language design committee. The "block structure" aspect that context-free grammars capture is so fundamental to grammar that the terms syntax and grammar are often identified with context-free grammar rules, especially in computer science. Formal constraints not captured by the grammar are then considered to be part of the "semantics" of the language.

Context-free grammars are simple enough to allow the construction of efficient parsing algorithms that, for a given string, determine whether and how it can be generated from the grammar. An Earley parser is an example of such an algorithm, while the widely used LR and LL parsers are simpler algorithms that deal only with more restrictive subsets of context-free grammars.

A context-free grammar is defined by the 4-tuple:

formula_29
where

A production rule in is formalized mathematically as a pair formula_32, where formula_33 is a nonterminal and formula_34 is a string of variables and/or terminals; rather than using ordered pair notation, production rules are usually written using an arrow operator with as its left hand side and as its right hand side:
formula_35.

It is allowed for to be the empty string, and in this case it is customary to denote it by ε. The form formula_36 is called an -production.

It is common to list all right-hand sides for the same left-hand side on the same line, using | (the pipe symbol) to separate them. Rules formula_37 and formula_38 can hence be written as formula_39. In this case, formula_40 and formula_41 is called the first and second alternative, respectively.

For any strings formula_42, we say directly yields , written as formula_43, if formula_44 with formula_33 and formula_46 such that formula_47 and formula_48. Thus, is a result of applying the rule formula_49 to .

For any strings formula_50 we say yields , written as formula_51 (or formula_52 in some textbooks), if formula_53 such that formula_54. In this case, if formula_55 (i.e., formula_56), the relation formula_57 holds. In other words, formula_58 and formula_59 are the reflexive transitive closure (allowing a word to yield itself) and the transitive closure (requiring at least one step) of formula_60, respectively.

The language of a grammar formula_29 is the set

A language is said to be a context-free language (CFL), if there exists a CFG , such that formula_63.

Non-deterministic pushdown automata recognize exactly the context-free languages.

A context-free grammar is said to be "proper", if it has

Every context-free grammar can be effectively transformed into a weakly equivalent one without unreachable symbols, a weakly equivalent one without unproductive symbols, and a weakly equivalent one without cycles.
Every context-free grammar not producing ε can be effectively transformed into a weakly equivalent one without ε-productions; altogether, every such grammar can be effectively transformed into a weakly equivalent proper CFG.

The grammar formula_68, with productions

is context-free. It is not proper since it includes an ε-production. A typical derivation in this grammar is
This makes it clear that 
formula_69. 
The language is context-free, however, it can be proved that it is not regular.

If the productions

are added, a context-free grammar for the set of all palindromes over the alphabet is obtained.

The canonical example of a context-free grammar is parenthesis matching, which is representative of the general case. There are two terminal symbols "(" and ")" and one nonterminal symbol S. The production rules are

The first rule allows the S symbol to multiply; the second rule allows the S symbol to become enclosed by matching parentheses; and the third rule terminates the recursion.

A second canonical example is two different kinds of matching nested parentheses, described by the productions:

with terminal symbols [ ] ( ) and nonterminal S.

The following sequence can be derived in that grammar:

In a context-free grammar, we can pair up characters the way we do with brackets. The simplest example:

This grammar generates the language formula_70, which is not regular (according to the pumping lemma for regular languages).

The special character ε stands for the empty string. By changing the above grammar to
we obtain a grammar generating the language formula_71 instead. This differs only in that it contains the empty string while the original grammar did not.

A context-free grammar for the language consisting of all strings over {a,b} containing an unequal number of a's and b's:
Here, the nonterminal T can generate all strings with the same number of a's as b's, the nonterminal U generates all strings with more a's than b's and the nonterminal V generates all strings with fewer a's than b's. Omitting the third alternative in the rule for U and V doesn't restrict the grammar's language.

Another example of a non-regular language is formula_72. It is context-free as it can be generated by the following context-free grammar:

The formation rules for the terms and formulas of formal logic fit the definition of context-free grammar, except that the set of symbols may be infinite and there may be more than one start symbol.

In contrast to well-formed nested parentheses and square brackets in the previous section, there is no context-free grammar for generating all sequences of two different types of parentheses, each separately balanced "disregarding the other", where the two types need not nest inside one another, for example:

or

The fact that this language is not context free can be proven using Pumping lemma for context-free languages and a proof by contradiction, observing that all words of the form 
formula_73
should belong to the language. This language belongs instead to a more general class and can be described by a conjunctive grammar, which in turn also includes other non-context-free languages, such as the language of all words of the form
formula_74.

Every regular grammar is context-free, but not all context-free grammars are regular. The following context-free grammar, however, is also regular.

The terminals here are "a" and "b", while the only nonterminal is S.
The language described is all nonempty strings of formula_75s and formula_76s that end in formula_75.

This grammar is regular: no rule has more than one nonterminal in its right-hand side, and each of these nonterminals is at the same end of the right-hand side.

Every regular grammar corresponds directly to a nondeterministic finite automaton, so we know that this is a regular language.

Using pipe symbols, the grammar above can be described more tersely as follows:

A "derivation" of a string for a grammar is a sequence of grammar rule applications that transform the start symbol into the string.
A derivation proves that the string belongs to the grammar's language.

A derivation is fully determined by giving, for each step:
For clarity, the intermediate string is usually given as well.

For instance, with the grammar:

the string

can be derived with the derivation:

Often, a strategy is followed that deterministically determines the next nonterminal to rewrite:
Given such a strategy, a derivation is completely determined by the sequence of rules applied. For instance, the leftmost derivation

can be summarized as

The distinction between leftmost derivation and rightmost derivation is important because in most parsers the transformation of the input is defined by giving a piece of code for every grammar rule that is executed whenever the rule is applied. Therefore, it is important to know whether the parser determines a leftmost or a rightmost derivation because this determines the order in which the pieces of code will be executed. See for an example LL parsers and LR parsers.

A derivation also imposes in some sense a hierarchical structure on the string that is derived. For example, if the string "1 + 1 + a" is derived according to the leftmost derivation:

the structure of the string would be:

where { ... } indicates a substring recognized as belonging to S. This hierarchy can also be seen as a tree:

This tree is called a "parse tree" or "concrete syntax tree" of the string, by contrast with the abstract syntax tree. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another (rightmost) derivation of the same string

and this defines the following parse tree:

Note however that both parse trees can be obtained by both leftmost and rightmost derivations. For example, the last tree can be obtained with the leftmost derivation as follows:

If a string in the language of the grammar has more than one parsing tree, then the grammar is said to be an "ambiguous grammar". Such grammars are usually hard to parse because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called "inherently ambiguous languages".

Here is a context-free grammar for syntactically correct infix algebraic expressions in the variables x, y and z:

This grammar can, for example, generate the string

as follows:

Note that many choices were made underway as to which rewrite was going to be performed next.
These choices look quite arbitrary. As a matter of fact, they are, in the sense that the string finally generated is always the same. For example, the second and third rewrites

could be done in the opposite order:

Also, many choices were made on which rule to apply to each selected codice_1.
Changing the choices made and not only the order they were made in usually affects which terminal string comes out at the end.

Let's look at this in more detail. Consider the parse tree of this derivation:

Starting at the top, step by step, an S in the tree is expanded, until no more unexpanded codice_1es (nonterminals) remain.
Picking a different order of expansion will produce a different derivation, but the same parse tree.
The parse tree will only change if we pick a different rule to apply at some position in the tree.

But can a different parse tree still produce the same terminal string,
which is codice_3 in this case?
Yes, for this particular grammar, this is possible.
Grammars with this property are called ambiguous.

For example, codice_4 can be produced with these two different parse trees:

However, the "language" described by this grammar is not inherently ambiguous:
an alternative, unambiguous grammar can be given for the language, for example:

(once again picking codice_1 as the start symbol). This alternative grammar will produce codice_4 with a parse tree similar to the left one above, i.e. implicitly assuming the association codice_7, which is not according to standard operator precedence. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired operator precedence and associativity rules.

Every context-free grammar that does not generate the empty string can be transformed into one in which there is no ε-production (that is, a rule that has the empty string as a product). If a grammar does generate the empty string, it will be necessary to include the rule formula_78, but there need be no other ε-rule. Every context-free grammar with no ε-production has an equivalent grammar in Chomsky normal form, and a grammar in Greibach normal form. "Equivalent" here means that the two grammars generate the same language.

The especially simple form of production rules in Chomsky normal form grammars has both theoretical and practical implications. For instance, given a context-free grammar, one can use the Chomsky normal form to construct a polynomial-time algorithm that decides whether a given string is in the language represented by that grammar or not (the CYK algorithm).

Context-free languages are closed under the various operations, that is, if the languages "K" and "L" are 
context-free, so is the result of the following operations:


They are not closed under general intersection (hence neither under complementation) and set difference.

The following are some decidable problems about context-free grammars.

The parsing problem, checking whether a given word belongs to the language given by a context-free grammar, is decidable, using one of the general-purpose parsing algorithms:
Context-free parsing for Chomsky normal form grammars was shown by Leslie G. Valiant to be reducible to boolean matrix multiplication, thus inheriting its complexity upper bound of "O"("n"). Conversely, Lillian Lee has shown "O"("n") boolean matrix multiplication to be reducible to "O"("n") CFG parsing, thus establishing some kind of lower bound for the latter.

It is decidable whether a given non-terminal of a context-free grammar is reachable, whether it is productive, and whether it is nullable (that is, it can derive the empty string).

It is decidable whether a given "grammar" is a regular grammar, as well as whether it is an LL("k") grammar for a given "k"≥0. If "k" is not given, the latter problem is undecidable.

Given a context-free "language", it is neither decidable whether it is regular, nor whether it is an LL("k") language for a given "k".

There are algorithms to decide whether a language of a given context-free language is empty, as well as whether it is finite.

Some questions that are undecidable for wider classes of grammars become decidable for context-free grammars; e.g. the emptiness problem (whether the grammar generates any terminal strings at all), is undecidable for context-sensitive grammars, but decidable for context-free grammars.

However, many problems are undecidable even for context-free grammars. Examples are:

Given a CFG, does it generate the language of all strings over the alphabet of terminal symbols used in its rules?

A reduction can be demonstrated to this problem from the well-known undecidable problem of determining whether a Turing machine accepts a particular input (the halting problem). The reduction uses the concept of a "computation history", a string describing an entire computation of a Turing machine. A CFG can be constructed that generates all strings that are not accepting computation histories for a particular Turing machine on a particular input, and thus it will accept all strings only if the machine doesn't accept that input.

Given two CFGs, do they generate the same language?

The undecidability of this problem is a direct consequence of the previous: it is impossible to even decide whether a CFG is equivalent to the trivial CFG defining the language of all strings.

Given two CFGs, can the first one generate all strings that the second one can generate?

If this problem was decidable, then language equality could be decided too: two CFGs G1 and G2 generate the same language if L(G1) is a subset of L(G2) and L(G2) is a subset of L(G1).

Using Greibach's theorem, it can be shown that the two following problems are undecidable:


Given a CFG, is it ambiguous?

The undecidability of this problem follows from the fact that if an algorithm to determine ambiguity existed, the Post correspondence problem could be decided, which is known to be undecidable.

Given two CFGs, is there any string derivable from both grammars?

If this problem was decidable, the undecidable Post correspondence problem could be decided, too: given strings formula_79 over some alphabet formula_80, let the grammar consist of the rule
where formula_82 denotes the reversed string formula_83 and formula_76 doesn't occur among the formula_85; and let grammar consist of the rule
Then the Post problem given by formula_79 has a solution if and only if and share a derivable string.

An obvious way to extend the context-free grammar formalism is to allow nonterminals to have arguments, the values of which are passed along within the rules. This allows natural language features such as agreement and reference, and programming language analogs such as the correct use and definition of identifiers, to be expressed in a natural way. E.g. we can now easily express that in English sentences, the subject and verb must agree in number. In computer science, examples of this approach include affix grammars, attribute grammars, indexed grammars, and Van Wijngaarden two-level grammars. Similar extensions exist in linguistics.

An extended context-free grammar (or regular right part grammar) is one in which the right-hand side of the production rules is allowed to be a regular expression over the grammar's terminals and nonterminals. Extended context-free grammars describe exactly the context-free languages.

Another extension is to allow additional terminal symbols to appear at the left-hand side of rules, constraining their application. This produces the formalism of context-sensitive grammars.

There are a number of important subclasses of the context-free grammars:


LR parsing extends LL parsing to support a larger range of grammars; in turn, generalized LR parsing extends LR parsing to support arbitrary context-free grammars. On LL grammars and LR grammars, it essentially performs LL parsing and LR parsing, respectively, while on nondeterministic grammars, it is as efficient as can be expected. Although GLR parsing was developed in the 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR parsing up to the present day.

Chomsky initially hoped to overcome the limitations of context-free grammars by adding transformation rules.

Such rules are another standard device in traditional linguistics; e.g. passivization in English. Much of generative grammar has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations does not meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).

Chomsky's general position regarding the non-context-freeness of natural language has held up since then, although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved.
Gerald Gazdar and Geoffrey Pullum have argued that despite a few non-context-free constructions in natural language (such as cross-serial dependencies in Swiss German and reduplication in Bambara), the vast majority of forms in natural language are indeed context-free.





</doc>
<doc id="6760" url="https://en.wikipedia.org/wiki?curid=6760" title="Cryonics">
Cryonics

Cryonics (from Greek κρύος "kryos" meaning 'cold') is the low-temperature freezing (usually at ) of a human corpse or severed head, with the hope that resuscitation may be possible in the future. It is regarded with skepticism within the mainstream scientific community and has been widely characterized as quackery.

Cryonics procedures can begin only after clinical death, and cryonics "patients" are legally dead. Cryonics procedures ideally begin within minutes of death, and use cryoprotectants to prevent ice formation during cryopreservation. It is unlikely that a corpse could be reanimated after undergoing vitrification, which causes damage to the brain including its neural networks. The first corpse to be frozen was that of Dr. James Bedford in 1967. As of 2014, about 250 bodies were cryopreserved in the United States, and 1,500 people had made arrangements for cryopreservation after their legal death.

Cryonic proponents go further than the mainstream consensus in saying that the brain does not have to be continuously active to survive or retain memory. Cryonics controversially states that a human survives even within an inactive brain that has been badly damaged, provided that original encoding of memory and personality can, in theory, be adequately inferred and reconstituted from structure that remains. Cryonicists argue that as long as brain structure remains intact, there is no fundamental barrier, given our current understanding of physical law, to recovering its information content. The cryonics' argument that death does not occur as long as brain structure remains intact and theoretically repairable has received some mainstream medical discussion in the context of the ethical concept of brain death and organ donation.

Cryonics uses temperatures below −130 °C, called cryopreservation, in an attempt to preserve enough brain information to permit future revival of the cryopreserved person. Cryopreservation may be accomplished by freezing, freezing with cryoprotectant to reduce ice damage, or by vitrification to avoid ice damage. Even using the best methods, cryopreservation of whole bodies or brains is very damaging and irreversible with current technology.

Cryonics requires unknown future technology to repair or regenerate tissue that is diseased, damaged, or missing. Brain repairs in particular will require analysis at the molecular level. This far-future technology is usually assumed to be nanomedicine based on molecular nanotechnology. Biological repair methods or mind uploading have also been proposed.

Costs can include payment for medical personnel to be on call for death, vitrification, transportation in dry ice to a preservation facility, and payment into a trust fund intended to cover indefinite storage in liquid nitrogen and future revival costs. As of 2011, U.S. cryopreservation costs can range from $28,000 to $200,000, and are often financed via life insurance. KrioRus, which stores bodies communally in large dewars, charges $12,000 to $36,000 for the procedure. Some patients opt to have only their brain cryopreserved, rather than their whole body.

As of 2014, about 250 corpses have been cryogenically preserved in the U.S., and around 1,500 people have signed up to have their remains preserved. As of 2016, four facilities exist in the world to retain cryopreserved bodies: three in the U.S. and one in Russia.

Long-term preservation of biological tissue can be achieved by cooling to temperatures below . Immersion in liquid nitrogen at a temperature of is often used for convenience. Low temperature preservation of tissue is called cryopreservation. Contrary to popular belief, water that freezes during cryopreservation is usually water outside cells, not water inside cells. Cells don't burst during freezing, but instead become dehydrated and compressed between ice crystals that surround them. Intracellular ice formation occurs only if the rate of freezing is faster than the rate of osmotic loss of water to the extracellular space.

Without cryoprotectants, cell shrinkage and high salt concentrations during freezing usually prevent frozen cells from functioning again after thawing. In tissues and organs, ice crystals can also disrupt connections between cells that are necessary for organs to function. The difficulties of recovering large animals and their individual organs from a frozen state have been long known. Attempts to recover frozen mammals by simply rewarming them were abandoned by 1957. At present, only cells, tissues, and some small organs can be reversibly cryopreserved.

When used at high concentrations, cryoprotectants can stop ice formation completely. Cooling and solidification without crystal formation is called vitrification. The first cryoprotectant solutions able to vitrify at very slow cooling rates while still being compatible with whole organ survival were developed in the late 1990s by cryobiologists Gregory Fahy and Brian Wowk for the purpose of banking transplantable organs. This has allowed animal brains to be vitrified, warmed back up, and examined for ice damage using light and electron microscopy. No ice crystal damage was found; remaining cellular damage was due to dehydration and toxicity of the cryoprotectant solutions. Large vitrified organs tend to develop fractures during cooling, a problem worsened by the large tissue masses and very low temperatures of cryonics.

The use of vitrification rather than freezing for cryonics was anticipated in 1986, when K. Eric Drexler proposed a technique called "fixation and vitrification", anticipating reversal by molecular nanotechnology. In 2016, Robert L. McIntyre and Gregory Fahy at the cryobiology research company 21st Century Medicine, Inc. won the Small Animal Brain Preservation Prize of the Brain Preservation Foundation by demonstrating to the satisfaction of neuroscientist judges that a particular implementation of fixation and vitrification called "aldehyde-stabilized cryopreservation" could preserve a rabbit brain in "near perfect" condition at −135 °C, with the cell membranes, synapses, and intracellular structures intact in electron micrographs. Brain Preservation Foundation President, Ken Hayworth, said, "This result directly answers a main skeptical and scientific criticism against cryonics—that it does not provably preserve the delicate synaptic circuitry of the brain.” However the price paid for perfect preservation as seen by microscopy was tying up all protein molecules with chemical crosslinks, completely eliminating biological viability. Actual cryonics organizations use vitrification without a chemical fixation step, sacrificing some structural preservation quality for less damage at the molecular level. Some scientists, like Joao Pedro Magalhaes, have questioned whether using a deadly chemical for fixation eliminates the possibility of biological revival, making chemical fixation unsuitable for cryonics.

While preservation of both structure and function has been possible for brain slices using vitrification, this goal remains elusive for whole brains. In absence of a revived brain, or brain simulation from somehow scanning a preserved brain, the adequacy of present vitrification technology (with or without fixation) for preserving the anatomical and molecular basis of long-term memory as required by cryonics is still unproven.

Outside the cryonics community, many scientists have a blanket skepticism toward existing preservation methods. Cryobiologist Dayong Gao states that "we simply don't know if (subjects have) been damaged to the point where they've 'died' during vitrification because the subjects are now inside liquid nitrogen canisters." Biochemist Ken Storey argues (based on experience with organ transplants), that "even if you only wanted to preserve the brain, it has dozens of different areas, which would need to be cryopreserved using different protocols."

Those who believe that revival may someday be possible generally look toward advanced bioengineering, molecular nanotechnology, or nanomedicine as key technologies. Revival would require repairing damage from lack of oxygen, cryoprotectant toxicity, thermal stress (fracturing), freezing in tissues that do not successfully vitrify, and reversing the cause of death. In many cases extensive tissue regeneration would be necessary.

According to Cryonics Institute president Ben Best, cryonics revival may be similar to a last in, first out process. People cryopreserved in the future, with better technology, may require less advanced technology to be revived because they will have been cryopreserved with better technology that caused less damage to tissue. In this view, preservation methods would get progressively better until eventually they are demonstrably reversible, after which medicine would begin to reach back and revive people cryopreserved by more primitive methods.

Historically, a person had little control regarding how their body was treated after death as religion had jurisdiction over the disposal of the body. However, secular courts began to exercise jurisdiction over the body and use discretion in carrying out of the wishes of the deceased person. Most countries legally treat preserved individuals as deceased persons because of laws that forbid vitrifying someone who is medically alive. In France, cryonics is not considered a legal mode of body disposal; only burial, cremation, and formal donation to science are allowed. However, bodies may legally be shipped to other countries for cryonic freezing. As of 2015, the Canadian province of British Columbia prohibits the sale of arrangements for body preservation based on cryonics. In Russia, cryonics falls outside both the medical industry and the funeral services industry, making it easier in Russia than in the U.S. to get hospitals and morgues to release cryonics candidates.

In London in 2016, the English High Court ruled in favor of a mother's right to seek cryopreservation of her terminally ill 14-year-old daughter, as the girl wanted, contrary to the father's wishes. The decision was made on the basis that the case represented a conventional dispute over the disposal of the girl's body, although the judge urged ministers to seek "proper regulation" for the future of cryonic preservation following concerns raised by the hospital about the competence and professionalism of the team that conducted the preservation procedures. In Alcor Life Extension Foundation v. Richardson, the Iowa Court of Appeals ordered for the disinterment of Richardson, who was buried against his wishes for cryopreservation.

A detailed legal examination by Jochen Taupitz concludes that cryonic storage is legal in Germany for an indefinite period of time.

In 2009, writing in "Bioethics", David Shaw examines the ethical status of cryonics. The arguments against it include changing the concept of death, the expense of preservation and revival, lack of scientific advancement to permit revival, temptation to use premature euthanasia, and failure due to catastrophe. Arguments in favor of cryonics include the potential benefit to society, the prospect of immortality, and the benefits associated with avoiding death. Shaw explores the expense and the potential payoff, and applies an adapted version of Pascal's Wager to the question.

In 2016, Charles Tandy wrote in favor of cryonics, arguing that honoring someone's last wishes is seen as a benevolent duty in American and many other cultures.

Cryopreservation was applied to human cells beginning in 1954 with frozen sperm, which was thawed and used to inseminate three women. The freezing of humans was first scientifically proposed by Michigan professor Robert Ettinger when he wrote "The Prospect of Immortality" (1962). In April 1966, the first human body was frozen—though it had been embalmed for two months—by being placed in liquid nitrogen and stored at just above freezing. The middle-aged woman from Los Angeles, whose name is unknown, was soon thawed out and buried by relatives.

The first body to be frozen with the hope of future revival was James Bedford's, a few hours after his cancer-caused death in 1967. His body was frozen by Robert Nelson, a former TV repairman with no scientific background, before the body was turned over to Bedford's relatives. Bedford's corpse is the only one frozen before 1974 still preserved today. In 1976, Ettinger founded the Cryonics Institute; his corpse was cryopreserved in 2011. Nelson was sued in 1981 for allowing nine bodies to thaw and decompose in the 1970s; in his defense, he claimed that the Cryonics Society of California had run out of money. This led to the lowered reputation of cryonics in the U.S.

In 2018, a Y-Combinator startup called Nectome was recognized for developing a method of preserving brains with chemicals rather than by freezing. The method is fatal, performed as euthanasia under general anethesia, but the hope is that future technology would allow the brain to be physically scanned into a computer simulation, neuron by neuron.

According to "The New York Times", cryonicists are predominantly nonreligious white males, outnumbering women by about three to one. According to "The Guardian", as of 2008, while most cryonicists used to be young, male and "geeky" recent demographics have shifted slightly towards whole families.

In 2015 Du Hong, a 61-year-old female writer of children's literature, became the first known Chinese national to have their head cryopreserved.

Scientists have expressed skepticism about cryonics in media sources, and the number of peer-reviewed papers on cryonics is limited because its speculative aspects place it outside of the focus of most academic fields. While some neuroscientists contend that all the subtleties of a human mind are contained in its anatomical structure, few neuroscientists will comment directly upon the topic of cryonics due to its speculative nature. Individuals who intend to be frozen are often "looked at as a bunch of kooks".

William T. Jarvis has written that "Cryonics might be a suitable subject for scientific research, but marketing an unproven method to the public is quackery".

According to cryonicist Aschwin de Wolf and others, cryonics can often produce intense hostility from spouses who are not cryonicists. James Hughes, the executive director of the pro-life-extension Institute for Ethics and Emerging Technologies, chooses not to personally sign up for cryonics, calling it a worthy experiment but stating laconically that "I value my relationship with my wife."

Cryobiologist Dayong Gao states that "People can always have hope that things will change in the future, but there is no scientific foundation supporting cryonics at this time." As well, while it is universally agreed that "personal identity" is uninterrupted when brain activity temporarily ceases during incidents of accidental drowning (where people have been restored to normal functioning after being completely submerged in cold water for up to 66 minutes), some people express concern that a centuries-long cryopreservation might interrupt their conception of personal identity, such that the revived person would "not be you".

Many people say there would be no point in being revived in the far future if their friends and families are dead.

Suspended animation is a popular subject in science fiction and fantasy settings. It is often the means by which a character is transported into the future.

A survey in Germany found that about half of the respondents were familiar with cryonics, and about half of those familiar with cryonics had learned of the subject from films or television.

Among the cryopreserved are L. Stephen Coles (in 2014), Hal Finney (in 2014), and Ted Williams.

The urban legend suggesting Walt Disney was cryopreserved is false; he was cremated and interred at Forest Lawn Memorial Park Cemetery. Robert A. Heinlein, who wrote enthusiastically of the concept in "The Door into Summer" (serialized in 1956), was cremated and had his ashes distributed over the Pacific Ocean. Timothy Leary was a long-time cryonics advocate and signed up with a major cryonics provider, but he changed his mind shortly before his death, and was not cryopreserved.

Footnotes
Citations


</doc>
<doc id="6761" url="https://en.wikipedia.org/wiki?curid=6761" title="Unitary patent">
Unitary patent

The European patent with unitary effect (EPUE), more commonly known as the unitary patent, is a new type of European patent in advanced stage of adoption which would be valid in participating member states of the European Union. Unitary effect can be registered for a European patent upon grant, replacing validation of the European patent in the individual countries concerned. The unitary effect means a single renewal fee, a single ownership, a single object of property, a single court (the Unified Patent Court) and uniform protection—which means that revocation as well as infringement proceedings are to be decided for the unitary patent as a whole rather than for each country individually. Licensing is however to remain possible for part of the unitary territory.

On 17 December 2012, agreement was reached between the European Council and European Parliament on the two EU regulations that made the unitary patent possible through enhanced cooperation at EU level. The legality of the two regulations was challenged by Spain and Italy, but all their claims were rejected by the European Court of Justice. Italy subsequently joined the unitary patent regulation in September 2015, so that all EU member states except Spain and Croatia now participate in the enhanced cooperation for a unitary patent. Unitary effect of newly granted European patents can be requested from the date when the related Unified Patent Court Agreement enters into force for the first group of ratifiers, and will extend to those participating member states for which the UPC Agreement had entered into force at the time of registration of the unitary patent. Previously granted unitary patents will not automatically get their unitary effect extended to the territory of participating states which ratify the UPC agreement at a later date. 

The negotiations which resulted in the unitary patent can be traced back to various initiatives dating to the 1970s. At different times, the project, or very similar projects, have been referred to as the "European Union patent" (the name used in the EU treaties, which serve as the legal basis for EU competency), "EU patent", "Community patent", "European Community Patent", "EC patent" and "COMPAT".

By not requiring translations into a language of each contracting state, and by requiring the payment of only a single renewal fee for the group of contracting states, the unitary patent aims to be cheaper than European patents. Instead, unitary patents will be accepted in English, French, or German with no further translation required after grant. Machine translations will be provided, but will be, in the words of the regulation, "for information purposes only and should not have any legal effect". The maintenance fees, with a single fee for the whole area, are also expected to be lower compared to renewal fees for the whole area but the fees have yet to be announced.

The proposed unitary patent will be a particular type of European patent, granted under the European Patent Convention. A European patent, once granted, becomes a "bundle of nationally enforceable patents", in the states which are designated by the applicant, and the unitary effect would effectively create a single enforceable region in a subgroup of those 38 states, which may coexist with nationally enforceable patents ("classical" patents) in the remaining states. "Classical", non-unitary European patents hold exclusively for single countries and require the filing of a translation in some contracting states, in accordance with .

January 18, 2019, Kluwer Patent Blog wrote, "a recurring theme for some years has been that ‘the UPC will start next year’". Then Brexit and German constitutional court challenge by Dr Stjerna were considered as the main objects. The decision is expected early 2019. If Germany can then ratify the UPC Agreement, only one other country must do it, which is expected to happen soon, for the provisional application phase to start. It was also considered that Brexit will be postponed, thus allowing the UK participate in the unitary patent. The UK ratified the agreement in April 2018 and intends to remain in the UPC even after Brexit. Therefore, it seemed possible that the start would be late 2019 and even more likely in 2020.

In 2009, three draft documents were published regarding a community patent: a European patent in which the European Community was designated:


Based on those documents, the European Council requested on 6 July 2009 an opinion from the Court of Justice of the European Union, regarding the compatibility of the envisioned Agreement with EU law: "‘Is the envisaged agreement creating a Unified Patent Litigation System (currently named European and Community Patents Court) compatible with the provisions of the Treaty establishing the European Community?’"

In December 2010, the use of the enhanced co-operation procedure, under which of the Treaty on the Functioning of the European Union provides that a group of member states of the European Union can choose to co-operate on a specific topic, was proposed by twelve Member States to set up a unitary patent applicable in all participating European Union Member States. The use of this procedure had only been used once in the past, for harmonising rules regarding the applicable law in divorce across several EU Member States.

In early 2011, the procedure leading to the enhanced co-operation was reported to be progressing. Twenty-five Member States had written to the European Commission requesting to participate, with Spain and Italy remaining outside, primarily on the basis of ongoing concerns over translation issues. On 15 February, the European Parliament approved the use of the enhanced co-operation procedure for unitary patent protection by a vote of 471 to 160. and on 10 March 2011 the Council gave their authorisation. Two days earlier, on 8 March 2011, the Court of Justice of the European Union had issued its opinion, stating that the draft Agreement creating the European and Community Patent Court would be incompatible with EU law. The same day, the Hungarian Presidency of the Council insisted that this opinion would not affect the enhanced co-operation procedure.

In November 2011, negotiations on the enhanced co-operation system were reportedly advancing rapidly—too fast, in some views. It was announced that implementation required an enabling European Regulation, and a Court agreement between the states that elect to take part. The European Parliament approved the continuation of negotiations in September. A draft of the agreement was issued on 11 November 2011 and was open to all member states of the European Union, but not to other European Patent Convention states. However, serious criticisms of the proposal remained mostly unresolved. A meeting of the Competitiveness Council on 5 December failed to agree on the final text. In particular, there was no agreement on where the Central Division of a Unified Patent Court should be located, "with London, Munich and Paris the candidate cities."

The Polish Presidency acknowledged on 16 December 2011 the failure to reach an agreement "on the question of the location of the seat of the central division." The Danish Presidency therefore inherited the issue. According to the President of the European Commission in January 2012, the only question remaining to be settled was the location of the Central Division of the Court. However, evidence presented to the UK House of Commons European Scrutiny Committee in February suggested that the position was more complicated. At an EU summit at the end of January 2012, participants agreed to press on and finalise the system by June. On 26 April, Herman Van Rompuy, President of the European Council, wrote to members of the Council, saying "This important file has been discussed for many years and we are now very close to a final deal... This deal is needed now, because this is an issue of crucial importance for innovation and growth. I very much hope that the last outstanding issue will be sorted out at the May Competitiveness Council. If not, I will take it up at the June European Council." The Competitiveness Council met on 30 May and failed to reach agreement.

A compromise agreement on the seat(s) of the unified court was eventually reached at the June European Council (28–29 June 2012), splitting the central division according to technology between Paris (the main seat), London and Munich. However, on 2 July 2012, the European Parliament decided to postpone the vote following a move by the European Council to modify the arrangements previously approved by MEPs in negotiations with the European Council. The modification was considered controversial and included the deletion of three key articles (6–8) of the legislation, seeking to reduce the competence of the European Union Court of Justice in unitary patent litigation. On 9 July 2012, the Committee on Legal Affairs of the European Parliament debated the patent package following the decisions adopted by the General Council on 28–29 June 2012 in camera in the presence of MEP Bernhard Rapkay. A later press release by Rapkay quoted from a legal opinion submitted by the Legal Service of the European Parliament, which affirmed the concerns of MEPs to approve the decision of a recent EU summit to delete said articles as it "nullifies central aspects of a substantive patent protection". A Europe-wide uniform protection of intellectual property would thus not exist with the consequence that the requirements of the corresponding EU treaty would not be met and that the European Court of Justice could therefore invalidate the legislation. By the end of 2012 a new compromise was reached between the European Parliament and the European Council, including a limited role for the European Court of Justice. The Unified Court will apply the Unified Patent Court Agreement, which is considered national patent law from an EU law point of view, but still is equal for each participant. [However the draft statutory instrument aimed at implementation of the Unified Court and UPC in the UK provides for different infringement laws for: European patents (unitary or not) litigated through the Unified Court; European patents (UK) litigated before UK courts; and national patents]. The legislation for the enhanced co-operation mechanism was approved by the European Parliament on 11 December 2012 and the regulations were signed by the European Council and European Parliament officials on 17 December 2012.

On 30 May 2011, Italy and Spain challenged the Council's authorisation of the use of enhanced co-operation to introduce the trilingual (English, French, German) system for the unitary patent, which they viewed as discriminatory to their languages, with the CJEU on the grounds that it did not comply with the EU treaties. In January 2013, Advocate General Yves Bot delivered his recommendation that the court reject the complaint. Suggestions by the Advocate General are advisory only, but are generally followed by the court. The case was dismissed by the court in April 2013, however Spain launched two new challenges with the EUCJ in March 2013 against the regulations implementing the unitary patent package. The court hearing for both cases was scheduled for 1 July 2014. Advocate-General Yves Bot published his opinion on 18 November 2014, suggesting that both actions be dismissed ( and ). The court handed down its decisions on 5 May 2015 as and fully dismissing the Spanish claims. Following a request by the government of Italy, it became a participant of the unitary patent regulations in September 2015.

European patents are granted in accordance with the provisions of the European Patent Convention, via a unified procedure before the European Patent Office. A single patent application, in one language, may be filed at the European Patent Office or at a national patent office of certain Contracting States. Patentable inventions, according to the EPC, are "any inventions, in all fields of technology, providing that they are new, involve an inventive step, and are susceptible of industrial application."

In contrast to the unified character of a European patent application, a granted European patent has, in effect, no unitary character, except for the centralized opposition procedure (which can be initiated within 9 months from grant, by somebody else than the patent proprietor), and the centralized limitation and revocation procedures (which can only be instituted by the patent proprietor). In other words, a European patent in one Contracting State, i.e. a "national" European patent, is effectively independent of the same European patent in each other Contracting State, except for the opposition, limitation and revocation procedures. The enforcement of a European patent is dealt with by national law. The abandonment, revocation or limitation of the European patent in one state does not affect the European patent in other states.

The EPC provides however the possibility for a group of member states to allow European patents to have a unitary character also after grant. Until now, only Liechtenstein and Switzerland have opted to create a unified protection area (see Unitary patent (Switzerland and Liechtenstein)).

Upon filing of a European patent application, all 38 Contracting States are automatically designated, unless, when filing the application, the applicant withdraws one or more designations. This may be important to avoid conflicts with national (non European patent) applications. Designations may also be withdrawn after filing, at any time before grant. Upon grant, a European patent has immediate effect in all designated States, but to remain effective, yearly renewal fees have to be paid, in each State, and in certain countries translation requirements have to be met.

Three instruments were proposed for the implementation of the unitary patent:


The system is based on EU law as well as the European Patent Convention (EPC). provides the legal basis for establishing a common system of patents for Parties to the EPC. Previously, only Liechtenstein and Switzerland had used this possibility to create a unified protection area (see Unitary patent (Switzerland and Liechtenstein)).

The first two regulations were approved by the European Parliament on 11 December 2012, with future application set for the 25 member states then participating in the enhanced cooperation for a unitary patent (all current EU member states except Croatia, Italy and Spain). The instruments were adopted as regulations EU 1257/2012 and 1260/2012 on 17 December 2012, and entered into force in January 2013. Following a request by the government of Italy, it became a participant of the unitary patent regulations in September 2015.

As of March 2017, neither of the two remaining non-participants in the unitary patent (Spain and Croatia) had requested the European Commission to participate.

Although formally the Regulations will apply to all 26 participating states from the moment the UPC Agreement enters into force for the first group of ratifiers, the unitary effect of newly granted unitary patents will only extend to those of the 26 states where the UPC Agreement has entered into force, while patent coverage for other participating states without UPC Agreement ratification will be covered by a coexisting normal European patent in each of those states. 

The unitary effect of unitary patents means a single renewal fee, a single ownership, a single object of property, a single court (the Unified Patent Court) and uniform protection, which means that revocation as well as infringement proceedings are to be decided for the unitary patent as a whole rather than for each country individually. Licensing is however to remain possible for part of the unitary territory.

Some administrative tasks relating to the European patents with unitary effect will be performed by the European Patent Office. These tasks include the collection of renewal fees and registration of unitary effect upon grant, exclusive licenses and statements that licenses are available to any person. Decisions of the European Patent Office regarding the unitary patent are open to appeal to the Unified Patent Court, rather than to the EPO Boards of Appeal.

For a unitary patent ultimately no translation will be required, which significantly reduces the cost for protection in the whole area. However, article 6 of EU regulation 1260/2012 provides that during a transition period of no more than twelve years one translation needs to be provided. A translation needs to be provided either into English if the application is in French or German, or into any EU official language if the application is in English. In addition, machine translations will be provided, which will be, in the words of the regulation, "for information purposes only and should not have any legal effect".

In several contracting states, for "national" European patents a translation has to be filed within a three-month time limit after the publication of grant in the European Patent Bulletin under , otherwise the patent is considered never to have existed (void ab initio) in that state. For the 21 parties to the London Agreement, this requirement has already been abolished or reduced (e.g. by dispensing with the requirement if the patent is available in English, and/or only requiring translation of the claims). Translation requirements for the participating states in the enhanced cooperation for a unitary patent are shown below:
Article 7 of Regulation 1257/2012 provides that, as an object of property, a European patent with unitary effect will be treated "in its entirety and in all participating Member States as a national patent of the participating Member State in which that patent has unitary effect and in which the applicant had her/his residence or principal place of business or, by default, had a place of business on the date of filing the application for the European patent." When the applicant had no domicile in a participating Member State, German law will apply. Ullrich has the criticized the system, which is similar to the Community Trademark and the Community Design, as being "in conflict with both the purpose of the creation of unitary patent protection and with primary EU law."

In January 2013, after the two regulations about the unitary patent had entered into force, but before the regulations applied, the participating member states in the unitary patent established (as member states of the European Patent Convention) a Select Committee of the Administrative Council of the European Patent Organisation in order to prepare the work for implementation of the provisions. The committee held its inaugural meeting on 20 March 2013. The work of the Select Committee has to proceed in parallel to the work of the Preparatory Committee for the creation of the Unified Patent Court. Implementation of the Unitary Patent—including the legal, administrative and financial measures—shall be completed in due time before the entry into operation of the Unified Patent Court. In May 2015, the communicated target date for completion of the remaining preparatory work of the Select Committee was 30 June 2015. The Committee reached agreement on the level of the renewal fees June 2015, while it stated that a decision on the distribution of those fees among member states was in Autumn 2015.

The Agreement on a Unified Patent Court provides the legal basis for the Unified Patent Court: a patent court for European patents (with and without unitary effect), with jurisdiction in those countries where the Agreement is in effect. In addition to regulations regarding the court structure, it also contains substantive provisions relating to the right to prevent use of an invention and allowed use by non patent proprietors (e.g. for private non-commercial use), preliminary and permanent injunctions.

The Agreement was signed on 19 February 2013 by 24 EU member states, including all states then participating in the enhanced co-operation measures except Bulgaria and Poland, while Italy, which did not originally join the enhanced co-operation measures but subsequently signed up, did sign the UPC agreement. The agreement remains open to accession for all remaining EU member states, and Bulgaria signed the agreement on 5 March. Meanwhile, Poland decided to wait to see how the new patent system works before joining due to concerns that it would harm their economy. States which do not participate in the unitary patent regulations can still become parties to the UPC agreement, which would allow the new court to handle European patents validated in the country. Entry into force for the UPC will take place after 13 states (including Germany, France and the United Kingdom as the three states with the most patents in force) have ratified the Agreement. As of November 2015, the agreement has been ratified by 8 states (including 1 of the required ratifiers: France).

The Unified Patent Court will have exclusive jurisdiction in infringement and revocation proceedings involving European patents with unitary effect, and during a transition period non-exclusive jurisdiction (that the patent holder can be opt out from) regarding European patents without unitary effect in the states where the Agreement applies. It furthermore has jurisdiction to hear cases against decisions of the European Patent Office regarding unitary patents. As a court of several member states of the European Union it may (Court of First Instance) or must (Court of Appeal) ask prejudicial questions to the European Court of Justice when the interpretation of EU law (including the two unitary patent regulations, but excluding the UPC Agreement) is not obvious.

The court would have two divisions: a court of first instance and a court of appeal. The court of appeal and the registry would have their seats in Luxembourg, while the central division of the court of first instance would have its seat in Paris. The central division would have thematic branches in London and Munich. The court of first instance may further have local and regional divisions in all member states that wish to set up such divisions.

While the regulations formally apply to all 26 member states participating in the enhanced cooperation for a unitary patent, from the date the UPC agreement has entered into force for the first group of ratifiers, unitary patents will only extend to the territory of those participating member states where the UPC Agreement had entered into force when the unitary effect was registered. If the unitary effect territory subsequently expands to additional participating member states for which the UPC Agreement later enters into force, this will be reflected for all subsequently registered unitary patents, but the territorial scope of the unitary effect of existing unitary patents will not be extended to these states.

Unitary effect can be requested up to one month after grant of the European patent, with retroactive effect from the date of grant. However, according to the "Draft Rules Relating to Unitary Patent Protection", unitary effect would be registered only if the European patent has been granted with the same set of claims for all the 26 participating member states in the regulations, whether the unitary effect applies to them or not. European patents automatically become a bundle of "national" European patents upon grant. Upon the grant of unitary effect, the "national" European patents will retroactively be considered to never have existed in the territories where the unitary patent has effect. The unitary effect does not affect "national" European patents in states where the unitary patent does not apply. Any "national" European patents applying outside the "unitary effect" zone will co-exist with the unitary patent.

As the unitary patent is introduced by an EU regulation, it is expected to not only be valid in the mainland territory of the participating member states that are party to the UPC, but also in those of their special territories that are part of the European Union. As of April 2014, this includes the following fourteen territories:


In addition to the territories above, the European Patent Convention has been extended by three member states participating in the enhanced cooperation for a unitary patent to cover some of their dependent territories outside the European Union:


Among the dependencies in the second list, the Isle of Man intends to apply the unitary patent. Draft Dutch implementation provision provide for the "Dutch European Patent" to exist in parallel with the unitary patent: while the unitary patent will cover the European territory, the Dutch EP will be valid in Caribbean Netherlands, Curacao and Sint Maarten and be subject to regular translation and renewal requirements.

The renewal fees of the unitary patent range from 32 Euro in the second year to 4855 in the 20th year as is based on the cumulative renewal fees of Germany, France, the UK and the Netherlands, the 4 states in which most European patents are in force.

Translation requirements as well as the requirement to pay yearly patent fees in all countries in which a European patent is designated, presently renders the European patent system costly in the European Union. In an impact assessment the European Commission estimated that the costs of obtaining a patent in all 27 EU countries would drop from over 32 000 euro (mainly due to translation costs) to 6 500 euro (for the combination of an EU, Spanish and Italian patent) due to introduction of the EU patent. Per capita costs of an EU patent were estimated at just 6 euro/million in the original 25 participating countries (and 12 euro/million in the 27 EU countries for protection with an EU, Italian and Spanish patent).

How the EU Commission has presented the expected cost savings has however been sharply criticized as exaggerated and based on unrealistic assumptions. The EU Commission has notably considered the costs for validating a European patent in 27 countries while in reality only about 1% of all granted European patents are currently validated in all 27 contracting states. Based on more realistic assumptions, the cost savings are expected to be much lower than actually claimed by the Commission.

Work on a Community patent started in the 1970s, but the resulting Community Patent Convention (CPC) was a failure.

The "Luxembourg Conference on the Community Patent" took place in 1975 and the Convention for the European Patent for the common market, or (Luxembourg) Community Patent Convention (CPC), was signed at Luxembourg on 15 December 1975, by the 9 member states of the European Economic Community at that time. However, the CPC never entered into force. It was not ratified by enough countries.

Fourteen years later, the Agreement relating to Community patents was made at Luxembourg on 15 December 1989. It attempted to revive the CPC project, but also failed. This Agreement consisted of an amended version of the original Community Patent Convention. Twelve states signed the Agreement: Belgium, Denmark, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, and United Kingdom. All of those states would need to have ratified the Agreement to cause it to enter into force, but only seven did so: Denmark, France, Germany, Greece, Luxembourg, the Netherlands, and United Kingdom.

Nevertheless, a majority of member states of the EEC at that time introduced some harmonisation into their national patent laws in anticipation of the entry in force of the CPC. A more substantive harmonisation took place at around the same time to take account of the European Patent Convention and the Strasbourg Convention.

In 2000, renewed efforts from the European Union resulted in a Community Patent Regulation proposal, sometimes abbreviated as CPR. It provides that the patent, once it has been granted by the European Patent Office (EPO) in one of its procedural languages (English, German or French) and published in that language, with a translation of the claims into the two other procedural languages, will be valid without any further translation. This proposal is aimed to achieve a considerable reduction in translation costs.

Nevertheless, additional translations could become necessary in legal proceedings against a suspected infringer. In such a situation, a suspected infringer who has been unable to consult the text of the patent in the official language of the Member State in which he is domiciled, is presumed, until proven otherwise, not to have knowingly infringed the patent. To protect a suspected infringer who, in such a situation, has not acted in a deliberate manner, it is provided that the proprietor of the patent will not be able to obtain damages in respect of the period prior to the translation of the patent being notified to the infringer.

The proposed Community Patent Regulation should also establish a court holding exclusive jurisdiction to invalidate issued patents; thus, a Community Patent's validity will be the same in all EU member states. This court will be attached to the present European Court of Justice and Court of First Instance through use of provisions in the Treaty of Nice.

Discussion regarding the Community patent had made clear progress in 2003 when a political agreement was reached on 3 March 2003. However, one year later in March 2004 under the Irish presidency, the Competitiveness Council failed to agree on the details of the Regulation. In particular the time delays for translating the claims and the authentic text of the claims in case of an infringement remained problematic issues throughout discussions and in the end proved insoluble.

In view of the difficulties in reaching an agreement on the community patent, other legal agreements have been proposed outside the European Union legal framework to reduce the cost of translation (of patents when granted) and litigation, namely the London Agreement, which entered into force on 1 May 2008—and which has reduced the number of countries requiring translation of European patents granted nowadays under the European Patent Convention, and the corresponding costs to obtain a European patent—and the European Patent Litigation Agreement (EPLA), a proposal that has now lapsed.

After the council in March 2004, EU Commissioner Frits Bolkestein said that "The failure to agree on the Community Patent I am afraid undermines the credibility of the whole enterprise to make Europe the most competitive economy in the world by 2010." Adding:

Jonathan Todd, Commission's Internal Market spokesman, declared:
European Commission President Romano Prodi, asked to evaluate his five-year term, cites as his weak point the failure of many EU governments to implement the "Lisbon Agenda", agreed in 2001. In particular, he cited the failure to agree on a Europewide patent, or even the languages to be used for such a patent, "because member states did not accept a change in the rules; they were not coherent".

There is support for the Community patent from various quarters. From the point of view of the European Commission the Community Patent is an essential step towards creating a level playing field for trade within the European Union. For smaller businesses, if the Community patent achieves its aim of providing a relatively inexpensive way of obtaining patent protection across a wide trading area, then there is also support. 

For larger businesses, however, other issues come into play, which have tended to dilute overall support. In general, these businesses recognise that the current European Patent system provides the best possible protection given the need to satisfy national sovereignty requirements such as regarding translation and enforcement. The Community Patent proposal was generally supported if it would do away with both of these issues, but there was some concern about the level of competence of the proposed European Patent Court. A business would be reluctant to obtain a Europe-wide patent if it ran the risk of being revoked by an inexperienced judge. Also, the question of translations would not go away – unless the users of the system could see significant change in the position of some of the countries holding out for more of a patent specification to be translated on grant or before enforcement, it was understood that larger businesses (the bulk of the users of the patent system) would be unlikely to move away from the tried and tested European Patent.

Thus, in 2005, the Community patent looked unlikely to be implemented in the near future. However, on 16 January 2006 the European Commission "launched a public consultation on how future action in patent policy to create an EU-wide system of protection can best take account of stakeholders' needs." The Community patent was one of the issues the consultation focused on. More than 2500 replies were received. According to the European Commission, the consultation showed that there is widespread support for the Community patent but not at any cost, and "in particular not on the basis of the Common Political Approach reached by EU Ministers in 2003".

In February 2007, EU Commissioner Charlie McCreevy was quoted as saying:
The European Commission released a white paper in April 2007 seeking to "improve the patent system in Europe and revitalise the debate on this issue." On 18 April 2007, at the European Patent Forum in Munich, Germany, Günter Verheugen, Vice-President of the European Commission, said that his proposal to support the European economy was "to have the London Agreement ratified by all member states, and to have a European patent judiciary set up, in order to achieve rapid implementation of the Community patent, which is indispensable". He further said that he believed this could be done within five years.

In October 2007, the Portuguese presidency of the Council of the European Union proposed an EU patent jurisdiction, "borrowing heavily from the rejected draft European Patent Litigation Agreement (EPLA)". In November 2007, EU ministers were reported to have made some progress towards a community patent legal system, with "some specific results" expected in 2008.

In 2008, the idea of using machine translations to translate patents was proposed to solve the language issue, which is partially responsible for blocking progress on the community patent. Meanwhile, European Commissioner for Enterprise and Industry Günter Verheugen declared at the European Patent Forum in May 2008 that there was an "urgent need" for a community patent.

In December 2009, it was reported that the Swedish EU presidency had achieved a breakthrough in negotiations concerning the community patent. The breakthrough was reported to involve setting up a single patent court for the EU, however ministers conceded much work remained to be done before the community patent would become a reality.

According to the agreed plan, the EU would accede to the European Patent Convention as a contracting state, and patents granted by the European Patent Office will, when validated for the EU, have unitary effect in the territory of the European Union. On 10 November 2010, it was announced that no agreement had been reached and that, "in spite of the progress made, [the Competitiveness Council of the European Union had] fallen short of unanimity by a small margin," with commentators reporting that the Spanish representative, citing the aim to avoid any discrimination, had "re-iterated at length the stubborn rejection of the Madrid Government of taking the 'Munich' three languages regime (English, German, French) of the European Patent Convention (EPC) as a basis for a future EU Patent."





</doc>
<doc id="6763" url="https://en.wikipedia.org/wiki?curid=6763" title="Cistron">
Cistron

A cistron is an alternative term to a gene. The word cistron is used to emphasize that genes exhibit a specific behavior in a cis-trans test; distinct positions (or loci) within a genome are cistronic.

The words "cistron" and "gene" were coined before the advancing state of biology made it clear that the concepts they refer to are practically equivalent. The same historical naming practices are responsible for many of the synonyms in the life sciences.

The term cistron was coined by Seymour Benzer in an article entitled "The elementary units of heredity". The cistron was defined by an operational test applicable to most organisms that is sometimes referred to as a cis-trans test, but more often as a complementation test.

For example, suppose a mutation at a chromosome position formula_1 is responsible for a change in recessive trait in a diploid organism (where chromosomes come in pairs). We say that the mutation is recessive because the organism will exhibit the wild type phenotype (ordinary trait) unless both chromosomes of a pair have the mutation (homozygous mutation). Similarly, suppose a mutation at another position, formula_2, is responsible for the same recessive trait. The positions formula_1 and formula_2 are said to be within the same cistron when an organism that has the mutation at formula_1 on one chromosome and has the mutation at position formula_2 on the paired chromosome exhibits the recessive trait even though the organism is not homozygous for either mutation. When instead the wild type trait is expressed, the positions are said to belong to distinct cistrons / genes. Or simply put, mutations on the same cistrons will not complement; as opposed to mutations on different cistrons may complement (see Benzer's T4 bacteriophage experiments T4 rII system).

For example, an operon is a stretch of DNA that is transcribed to create a contiguous segment of RNA, but contains more than one cistron / gene. The operon is said to be polycistronic, whereas ordinary genes are said to be monocistronic.


</doc>
<doc id="6766" url="https://en.wikipedia.org/wiki?curid=6766" title="Commonwealth">
Commonwealth

Commonwealth is a traditional English term for a political community founded for the common good. Historically it has sometimes been synonymous with "republic". The noun "commonwealth", meaning "public welfare general good or advantage" dates from the 15th century. Originally a phrase (the common-wealth or the common weal – echoed in the modern synonym "public weal") it comes from the old meaning of "wealth", which is "well-being", and is itself a loose translation of the Latin res publica (republic). The term literally meant "common well-being". In the 17th century, the definition of "commonwealth" expanded from its original sense of "public welfare" or "commonweal" to mean "a state in which the supreme power is vested in the people; a republic or democratic state".

The term evolved to become a title to a number of political entities. Three countries – Australia, the Bahamas, and Dominica – have the official title "Commonwealth", as do four U.S. states and two U.S. territories. Since the early 20th century, the term has been used to name some fraternal associations of nations, most notably the Commonwealth of Nations, an organization primarily of former territories of the British Empire, which is often referred to as simply "the Commonwealth".

Translations of Roman writers' works to English have on occasion translated ""Res publica"", and variants thereof, to "the commonwealth", a term referring to the Roman state as a whole.

The Commonwealth of England was the official name of the political unit ("de facto" military rule in the name of parliamentary supremacy) that replaced the Kingdom of England (after the English Civil War) from 1649–53 and 1659–60, under the rule of Oliver Cromwell and his son and successor Richard. From 1653 to 1659, although still legally known as a Commonwealth, the republic, united with the former Kingdom of Scotland, operated under different institutions (at times as a "de facto" monarchy) and is known by historians as the Protectorate. In a British context, it is sometimes referred to as the "Old Commonwealth".

The Icelandic Commonwealth or the Icelandic Free State () was the state existing in Iceland between the establishment of the Althing in 930 and the pledge of fealty to the Norwegian king in 1262. It was initially established by a public consisting largely of recent immigrants from Norway who had fled the unification of that country under King Harald Fairhair.

The Commonwealth of the Philippines was the administrative body that governed the Philippines from 1935 to 1946, aside from a period of exile in the Second World War from 1942 to 1945 when Japan occupied the country. It replaced the Insular Government, a United States territorial government, and was established by the Tydings–McDuffie Act. The Commonwealth was designed as a transitional administration in preparation for the country's full achievement of independence, which was achieved in 1946. The Commonwealth of the Philippines was a founding member of the United Nations.

"Republic" is still an alternative translation of the traditional name of the Polish–Lithuanian Commonwealth. Wincenty Kadłubek (Vincent Kadlubo, 1160–1223) used for the first time the original Latin term "res publica" in the context of Poland in his "Chronicles of the Kings and Princes of Poland". The name was used officially for the confederal country formed by Poland and Lithuania 1569–1795.

It is also often referred as "Nobles' Commonwealth" (1505–1795, i.e., before the union). In the contemporary political doctrine of the Polish–Lithuanian Commonwealth, "our state is a Republic (or Commonwealth) under the presidency of the King". The Commonwealth introduced a doctrine of religious tolerance called Warsaw Confederation, had its own parliament "Sejm" (although elections were restricted to nobility and elected kings, who were bound to certain contracts "Pacta conventa" from the beginning of the reign).

"A commonwealth of good counsaile" was the title of the 1607 English translation of the work of Wawrzyniec Grzymała Goślicki "De optimo senatore" that presented to English readers many of the ideas present in the political system of the Polish–Lithuanian Commonwealth.

Between 1914 and 1925, Catalonia was an autonomous region of Spain. Its government during that time was given the title "mancomunidad" (Catalan: "mancomunitat"), which is translated into English as "commonwealth". The Commonwealth of Catalonia had limited powers and was formed as a federation of the four Catalan provinces. A number of Catalan-language institutions were created during its existence.

Between 1838 and 1847, Liberia was officially known as the "Commonwealth of Liberia". It changed its name to the "Republic of Liberia" when it declared independence (and adopted a new constitution) in 1847.

"Commonwealth" was first proposed as a term for a federation of the six Australian crown colonies at the 1891 constitutional convention in Sydney. Its adoption was initially controversial, as it was associated by some with the republicanism of Oliver Cromwell (see above), but it was retained in all subsequent drafts of the constitution. The term was finally incorporated into law in the "Commonwealth of Australia Constitution Act 1901", which established the federation. Australia operates under a federal system, in which power is divided between the federal (national) government and the state governments (the successors of the six colonies). So, in an Australian context, the term "Commonwealth" (capitalized) refers to the federal government, and "Commonwealth of Australia" is the official name of the country.

The Bahamas uses the official style "Commonwealth of The Bahamas".

The small Caribbean republic of Dominica has used the official style "Commonwealth of Dominica" since 1978.

Four states of the United States officially designate themselves as "commonwealths". All four were part of Great Britain's possessions along the Atlantic coast of North America prior to the formation of the United States of America in 1776. As such, they share a strong influence of English common law in some of their laws and institutions. The four are:

Two organized but unincorporated U.S. territories are called commonwealths. The two are:

The Commonwealth of Nations—formerly the British Commonwealth—is a voluntary association or confederation of 53 independent sovereign states, most of which were once part of the British Empire. The Commonwealth's membership includes both republics and monarchies. The Head of the Commonwealth is Queen Elizabeth II, who also reigns as monarch directly in the 16 member states known as Commonwealth realms. 

The Commonwealth of Independent States (CIS) is a loose alliance or confederation consisting of 10 of the 15 former Soviet Republics, the exceptions being Turkmenistan (a CIS associate member), Lithuania, Latvia, Estonia, Ukraine, and Georgia. Georgia left the CIS in August 2008 after a clash with Russia over South Ossetia. Its creation signalled the dissolution of the Soviet Union, its purpose being to "allow a civilised divorce" between the Soviet Republics. The CIS has developed as a forum by which the member-states can co-operate in economics, defence, and foreign policy.

Labour MP Tony Benn sponsored a "Commonwealth of Britain Bill" several times between 1991 and 2001, intended to abolish the monarchy and establish a British republic. It never reached second reading.




</doc>
<doc id="6767" url="https://en.wikipedia.org/wiki?curid=6767" title="Commodore 1541">
Commodore 1541

The Commodore 1541 (also known as the CBM 1541 and VIC-1541) is a floppy disk drive which was made by Commodore International for the Commodore 64 (C64), Commodore's most popular home computer. The best-known floppy disk drive for the C64, the 1541 is a single-sided 170-kilobyte drive for 5¼" disks. The 1541 directly followed the Commodore 1540 (meant for the VIC-20).

The disk drive uses group coded recording (GCR) and contains a MOS Technology 6502 microprocessor, doubling as a disk controller and on-board disk operating system processor. The number of sectors per track varies from 17 to 21 (an early implementation of zone bit recording). The drive's built-in disk operating system is CBM DOS 2.6.

The 1541 was priced at under at its introduction. A C64 plus a 1541 cost about $900, while an Apple II with no disk drive cost $1295. The first 1541 drives produced in 1982 have a label on the front reading VIC-1541 and have an off-white case to match the VIC-20. In 1983, the 1541 was switched to having the familiar beige case and a front label reading simply "1541" along with rainbow stripes to match the Commodore 64.

By 1983 a 1541 sold for $300 or less. After a brutal home-computer price war that Commodore began, the C64 and 1541 together cost under $500. The drive became very popular, and became difficult to find. The company claimed that the shortage occurred because 90% of C64 owners bought the 1541 compared to its 30% expectation, but the press discussed what "Creative Computing" described as "an absolutely alarming return rate" because of defects. The magazine reported in March 1984 that it received three defective drives in two weeks, and "Compute!'s Gazette" reported in December 1983 that four of the magazine's seven drives had failed; "COMPUTE! Publications sorely needs additional 1541s for in-house use, yet we can't find any to buy. After numerous phone calls over several days, we were able to locate only two units in the entire continental United States", reportedly because of Commodore's attempt to resolve a manufacturing issue that caused the high failures.

The early (1982 to 1983) 1541s have a spring-eject mechanism (Alps drive), and the disks often fail to release. This style of drive has the popular nickname "Toaster Drive", because it requires the use of a knife or other hard thin object to pry out the stuck media just like a piece of toast stuck in an actual toaster (though this is inadvisable with actual toasters). This was fixed later when Commodore changed the vendor of the drive mechanism (Mitsumi) and adopted the flip-lever Newtronics mechanism, greatly improving reliability. In addition, Commodore made the drive's controller board smaller and reduced its chip count compared to the early 1541s (which had a large PCB running the length of the case, with dozens of TTL chips). The beige-case Newtronics 1541 was produced from 1984 to 1986.

All but the very earliest non-II model 1541s can use either the Alps or Newtronics mechanism. Visually, the first models, of the "VIC-1541" denomination, have an off-white color like the VIC-20 and VIC-1540. Then, to match the look of the C64, CBM changed the drive's color to brown-beige and the name to "Commodore 1541".

The 1541's numerous shortcomings opened a market for a number of third-party clones of the disk drive, a situation that continued for the lifetime of the C64. Well-known clones are the "Oceanic OC-118" a.k.a. "Excelerator+", the MSD Super Disk single and dual drives, the "Enhancer 2000", the "Indus GT", and "CMD" 's "FD-2000" and "FD-4000". Nevertheless, the 1541 became the first disk drive to see widespread use in the home and Commodore sold millions of the units.

In 1986, Commodore released the 1541C, a revised version that offered quieter and slightly more reliable operation and a light beige case matching the color scheme of the Commodore 64C. It was replaced in 1988 by the 1541-II, which uses an external power supply to provide cooler operation and allows the drive to have a smaller desktop footprint (the power supply "brick" being placed elsewhere, typically on the floor). Later ROM revisions fixed assorted problems, including a software bug that caused the save-and-replace command to corrupt data.

The Commodore 1570 is an upgrade from the 1541 for use with the Commodore 128, available in Europe. It offers MFM capability for accessing CP/M disks, improved speed, and somewhat quieter operation, but was only manufactured until Commodore got its production lines going with the 1571, the double-sided drive. Finally, the small, external-power-supply-based, MFM-based Commodore 1581 3½" drive was made, giving 800 KB access to the C128 and C64.

The 1541 does not have DIP switches to change the device number. If a user added more than one drive to a system the user had to open the case and cut a trace in the circuit board to permanently change the drive's device number, or hand-wire an external switch to allow it to be changed externally. It was also possible to change the drive number via a software command, which was temporary and would be erased as soon as the drive was powered off.

1541 drives at power up always default to device #8. If multiple drives in a chain are used, then the startup procedure is to power on the first drive in the chain, alter its device number via a software command to the highest number in the chain (if three drives were used, then the first drive in the chain would be set to device #10), then power on the next drive, alter its device number to the next lowest, and repeat the procedure until the final drive at the end of the chain was powered on and left as device #8.

Unlike the Apple II, where support for two drives was normal, it was relatively uncommon for Commodore software to support this setup, and the CBM DOS copy file command was not even able to copy files between drives--a third party copy utility must be used instead.

The pre-II 1541s also have an internal power source, which generate much heat. The heat generation was a frequent source of humour. For example, "Compute!" stated in 1988 that "Commodore 64s used to be a favorite with amateur and professional chefs since they could compute and cook on top of their 1500-series disk drives at the same time". A series of humorous tips in "MikroBitti" in 1989 said "When programming late, coffee and kebab keep nicely warm on top of the 1541." The "MikroBitti" review of the 1541-II said that its external power source "should end the jokes about toasters".

The drive-head mechanism installed in the early production years is notoriously easy to misalign. The most common cause of the 1541's drive head knocking and subsequent misalignment is copy-protection schemes on commercial software. The main cause of the problem is that the disk drive itself does not feature any means of detecting when the read/write head reaches track zero. Accordingly, when a disk is not formatted or a disk error occurs, the unit tries to move the head 40 times in the direction of track zero (although the 1541 DOS only uses 35 tracks, the drive mechanism itself is a 40-track unit, so this ensured track zero would be reached no matter where the head was before). Once track zero is reached, every further attempt to move the head in that direction would cause it to be rammed against a solid stop: for example, if the head happened to be on track 18 (where the directory is located) before this procedure, the head would be actually moved 18 times, and then rammed against the stop 22 times. This ramming gives the characteristic "machine gun" noise and sooner or later throws the head out of alignment.

A defective head-alignment part likely caused many of the reliability issues in early 1541 drives; one dealer told "Compute!s Gazette" in 1983 that the part had caused all but three of several hundred drive failures that he had repaired. The drives were so unreliable that "Info" magazine joked, "Sometimes it seems as if one of the original design specs ... must have said 'Mean time between failure: 10 accesses.'". Users can realign the drive themselves with a software program and a calibration disk. What the user would do is remove the drive from its case and then loosen the screws holding the stepper motor that moved the head, then with the calibration disk in the drive gently turn the stepper motor back and forth until the program shows a good alignment. The screws are then tightened and the drive is put back into its case.

A third-party fix for the 1541 appeared in which the solid head stop was replaced by a sprung stop, giving the head a much easier life. The later 1571 drive (which is 1541-compatible) incorporates track-zero detection by photo-interrupter and is thus immune to the problem. Also, a software solution, which resides in the drive controller's ROM, prevents the rereads from occurring, though this could cause problems when genuine errors did occur.

Due to the alignment issues on the Alps drive mechanisms, Commodore switched suppliers to Newtronics in 1984. The Newtronics mechanism drives have a lever rather than a pull-down tab to close the drive door. Although the alignment issues were resolved after the switch, the Newtronics drives added a new reliability problem in that many of the read/write heads were improperly sealed, causing moisture to penetrate the head and short it out.

The 1541's PCB consists mainly of a 6502 CPU, two 6522 chips, and 2k of work RAM. Up to 48k of RAM can be added; this was mainly useful for defeating copy protection schemes since an entire disk track could be loaded into drive RAM, while the standard 2k only accommodated a few sectors (theoretically eight, but some of the RAM was used by CBM DOS as work space). Some Commodore users used 1541s with expanded RAM as an impromptu math coprocessor by uploading math-intensive code to the drive for background processing.

The 1541 uses a proprietary serialized derivative of the IEEE-488 parallel interface, which Commodore used on their previous disk drives for the PET/CBM range of personal and business computers, but when the VIC-20 was in development, a cheaper alternative to the expensive IEE-488 cables was sought. To ensure a ready supply of inexpensive cabling for its home computer peripherals, Commodore chose standard DIN connectors for the serial interface. Disk drives and other peripherals such as printers connected to the computer via a daisy chain setup, necessitating only a single connector on the computer itself.

"IEEE Spectrum" in 1985 stated that:

The C-64's designers blamed the 1541's slow speed on the marketing department's insistence that the computer be compatible with the 1540, which was slow because of a flaw in the 6522 VIA interface controller. Initially, Commodore intended to use a hardware shift register (one component of the 6522) to maintain fast drive speeds with the new serial interface. However, a hardware bug with this chip prevented the initial design from working as anticipated, and the ROM code was hastily rewritten to handle the entire operation in software. According to Jim Butterfield, this causes a speed reduction by a factor of five; had 1540 compatibility not been a requirement, the disk interface would have been much faster. In any case, the C64 normally could not work with a 1540 unless the VIC-II video output was disabled via a register write, which would increase the CPU speed enough to be able to access it.

As implemented on the VIC-20 and C64, Commodore DOS transfers 512 bytes per second, compared to the Atari 810's 1000 bytes per second, the Apple Disk II's 15,000 bytes per second, and the 300-baud data rate of the Commodore Datasette storage system. About 20 minutes are needed to copy one disk—10 minutes of reading time, and 10 minutes of writing time. However, since both the computer and the drive can easily be reprogrammed, third parties quickly wrote more efficient firmware that would speed up drive operations drastically. Without hardware modifications, some "fast loader" utilities (which replaced 1541's onboard ROM) managed to achieve speeds of up to 4 kB/s. The most common of these products are the Epyx FastLoad, the Final Cartridge, and the Action Replay plug-in ROM cartridges, which all have machine code monitor and disk editor software on board as well. The popular Commodore computer magazines of the era also entered the arena with type-in fast-load utilities, with "Compute!'s Gazette" publishing "TurboDisk" in 1985 and "RUN" publishing "Sizzle" in 1987.

Even though each 1541 has its own on-board disk controller and disk operating system, it is not normally possible for a user to command two 1541 drives to copy a disk (one drive reading and the other writing) as with older dual drives like the 4040 and 8050 that were often found with the PET computer, and which the 1541 is backward-compatible with (it can read 4040 disks but not write to them since its internal operating system is similar enough for reading but not for writing). Unfortunately, however, the routines in the previous disk operating system to enable disk copying were removed for the 1541 as it was intended to be a stand-alone unit. Originally, to copy from drive to drive, software running on the C64 was needed and it would first read from one drive into computer memory, then write out to the other. Only later when first, Fast Hack'em, then other disk backup programs, were released, was true drive-to-drive copying possible for a pair of 1541s. The user could then unplug the C64 from the drives (i.e. from the first drive in the daisy chain) and do something else with the computer as the drives proceeded to copy the entire disk. This is not a recommended practice as disconnecting the serial lead from a powered drive and/or computer can result in destruction of one or both of the port chips in the disk drive.

The 1541 drive uses standard 5.25" double-density floppy media; high-density media will not work due to its different magnetic coating requiring a higher magnetic coercivity. As the GCR encoding scheme does not use the index hole, the drive was also compatible with hard-sectored disks. The standard CBM DOS format is 170k with 35 tracks and 256-byte sectors. It is similar to the format used on the PET 4040 drives, but a minor difference in a header byte makes the 4040 and 1541 only read-compatible; disks formatted with one drive cannot be written to by the other.

The 4040 drives used Shugart SA-400s, which were 35-track units, thus the format there was due to physical limitations of the drive mechanism. The 1541 used 40 track mechanisms, but Commodore intentionally limited the CBM DOS format to 35 tracks because of reliability issues with the early units. It was possible via low-level programming to move the drive head to tracks 36-40 and write on them, this was sometimes done by commercial software for copy protection purposes or to get additional data on the disk.

However, one track is reserved by DOS for directory and file allocation information (the BAM, block availability map). And since for normal files, two bytes of each physical sector are used by DOS as a pointer to the next physical track and sector of the file, only 254 out of the 256 bytes of a block are used for file contents.

If the disk side was not otherwise prepared with a custom format, (e.g. for data disks), 664 blocks would be free after formatting, giving 664 × 254 = 168,656 bytes (or almost 165 kB) for user data.

By using custom formatting and load/save routines (sometimes included in third-party DOSes, see below), all of the mechanically possible 40 tracks can be used.

Owing to the drive's non-use of the index hole, it was also possible to make "flippy" disks by inserting the diskette upside-down and formatting the other side, and it was commonplace and normal for commercial software to be distributed on such disks.

Tracks 36-42 are non-standard. The bitrate is the raw one between the read/write head and signal circuitry so actual useful data rate is a factor 5/4 less due to GCR encoding.

The 1541 disk typically has 35 tracks. Track 18 is reserved; the remaining tracks are available for data storage. The header is on 18/0 (track 18, sector 0) along with the BAM (block availability map), and the directory starts on 18/1 (track 18, sector 1). The file interleave is 10 blocks, while the directory interleave is 3 blocks.

Header contents: The header is similar to other Commodore disk headers, the structural differences being the BAM offset ($04) and size, and the label+ID+type offset ($90).

Early copy protection schemes deliberately introduced read errors on the disk, the software refusing to load unless the correct error message is returned. The general idea was that simple disk-copy programs are incapable of copying the errors. When one of these errors is encountered, the disk drive (as do many floppy disk drives) will attempt one or more reread attempts after first resetting the head to track zero. Few of these schemes had much deterrent effect, as various software companies soon released "nibbler" utilities that enabled protected disks to be copied and, in some cases, the protection removed.

Commodore copy protection sometimes depends on specific hardware configurations. "Gunship", for example, does not load if a second disk drive or printer is connected to the computer.




</doc>
<doc id="6769" url="https://en.wikipedia.org/wiki?curid=6769" title="Commodore 1581">
Commodore 1581

The Commodore 1581 is a 3½-inch double-sided double-density floppy disk drive that was released by Commodore Business Machines (CBM) in 1987, primarily for its C64 and C128 home/personal computers. The drive stores 800 kilobytes using an MFM encoding but formats different from the MS-DOS (720 kB), Amiga (880 kB), and Mac Plus (800 kB) formats. With special software it's possible to read C1581 disks on an x86 PC system, and likewise, read MS-DOS and other formats of disks in the C1581 (using Big Blue Reader), provided that the PC or other floppy handles the size format. This capability was most frequently used to read MS-DOS disks. The drive was released in the summer of 1987 and quickly became popular with bulletin board system (BBS) operators and other users.

Like the 1541 and 1571, the 1581 has an onboard MOS Technology 6502 CPU with its own ROM and RAM, and uses a serial version of the IEEE-488 interface. Inexplicably, the drive's ROM contains commands for parallel use, although no parallel interface was available. Unlike the 1571, which is nearly 100% backward-compatible with the 1541, the 1581 is only compatible with previous Commodore drives at the DOS level and cannot utilize software that performs low-level disk access (as the vast majority of Commodore 64 games do).

The version of Commodore DOS built into the 1581 added support for partitions, which could also function as fixed-allocation subdirectories. PC-style subdirectories were rejected as being too difficult to work with in terms of block availability maps, then still much in vogue, and which for some time had been the traditional way of inquiring into block availability. The 1581 supports the C128's burst mode for fast disk access, but not when connected to an older Commodore machine like the Commodore 64. The 1581 provides a total of 3160 blocks free when formatted (a block being equal to 256 bytes). The number of permitted directory entries was also increased, to 296 entries. With a storage capacity of 800 kB, the 1581 is the highest-capacity serial-bus drive that was ever made by Commodore (the 1-MB SFD-1001 uses the parallel IEEE-488), and the only 3½" one. However, starting in 1991, Creative Micro Designs (CMD) made the FD-2000 high density (1.6 MB) and FD-4000 extra-high density (3.2 MB) 3½" drives, both of which offered not only a 1581-emulation mode but also 1541- and 1571-compatibility modes.

Like the 1541 and 1571, a nearly identical job queue is available to the user in zero page (except for job 0), providing for exceptional degrees of compatibility.

Unlike the cases of the 1541 and 1571, the low-level disk format used by the 1581 is similar enough to the MS-DOS format as the 1581 is built around a WD1770 FM/MFM floppy controller chip. The 1581 disk format consists of 80 tracks and ten 512 byte sectors per track, used as 20 logical sectors of 256 bytes each. Special software is required to read 1581 disks on a PC due to the different file system. An internal floppy drive and controller are required as well; USB floppy drives operate strictly at the file system level and do not allow low-level disk access. The WD1770 controller chip, however, was the seat of some early problems with 1581 drives when the first production runs were recalled due to a high failure rate; the problem was quickly corrected. Later versions of the 1581 drive have a smaller, more streamlined-looking external power supply provided with them.

The 1581 disk has 80 logical tracks, each with 40 logical sectors (the actual physical layout of the diskette is abstracted and managed by a hardware translation layer). The directory starts on 40/3 (track 40, sector 3). The disk header is on 40/0, and the BAM (block availability map) resides on 40/1 and 40/2.

Header Contents

BAM Contents, 40/1

BAM Contents, 40/2




</doc>
<doc id="6771" url="https://en.wikipedia.org/wiki?curid=6771" title="College football">
College football

College football is American football played by teams of student athletes fielded by American universities, colleges, and military academies, or Canadian football played by teams of student athletes fielded by Canadian universities. It was through college football play that American football rules first gained popularity in the United States.

Unlike most other sports in North America, no minor league farm organizations exist in American or Canadian football. Therefore, college football is generally considered to be the second tier of American football in the United States and Canadian football in Canada; one step ahead of high school competition, and one step below professional competition. However, in some areas of the country, college football is more popular than professional football, and for much of the early 20th century, college football was seen as more prestigious than professional football.

It is in college football where a player's performance directly impacts his chances of playing professional football. The best collegiate players will typically declare for the professional draft after three to four years of collegiate competition, with the NFL holding its annual draft every spring in which 256 players are selected annually. Those not selected can still attempt to land an NFL roster spot as an undrafted free agent.

Even after the emergence of the professional National Football League (NFL), college football remained extremely popular throughout the U.S. 
Although the college game has a much larger margin for talent than its pro counterpart, the sheer number of fans following major colleges provides a financial equalizer for the game, with Division I programs — the highest level — playing in huge stadiums, six of which have seating capacity exceeding 100,000 people. In many cases, college stadiums employ bench-style seating, as opposed to individual seats with backs and arm rests (although many stadiums do have a small number of chairback seats in addition to the bench seating). This allows them to seat more fans in a given amount of space than the typical professional stadium, which tends to have more features and comforts for fans. (Only three stadiums owned by U.S. colleges or universities — Cardinal Stadium at the University of Louisville, Georgia State Stadium at Georgia State University and FAU Stadium at Florida Atlantic University — consist entirely of chairback seating).

College athletes, unlike players in the NFL, are not permitted by the NCAA to be paid salaries. Colleges are only allowed to provide non-monetary compensation such as athletic scholarships that provide for tuition, housing, and books.

Modern North American football has its origins in various games, all known as "football", played at public schools in Great Britain in the mid-19th century. By the 1840s, students at Rugby School were playing a game in which players were able to pick up the ball and run with it, a sport later known as Rugby football. The game was taken to Canada by British soldiers stationed there and was soon being played at Canadian colleges.

The first documented gridiron football match was played at University College, a college of the University of Toronto, November 9, 1861. One of the participants in the game involving University of Toronto students was (Sir) William Mulock, later Chancellor of the school. A football club was formed at the university soon afterward, although its rules of play at this stage are unclear.

In 1864, at Trinity College, also a college of the University of Toronto, F. Barlow Cumberland and Frederick A. Bethune devised rules based on rugby football. Modern Canadian football is widely regarded as having originated with a game played in Montreal, in 1865, when British Army officers played local civilians. The game gradually gained a following, and the Montreal Football Club was formed in 1868, the first recorded non-university football club in Canada.

Early games appear to have had much in common with the traditional "mob football" played in Great Britain. The games remained largely unorganized until the 19th century, when intramural games of football began to be played on college campuses. Each school played its own variety of football. Princeton University students played a game called "ballown" as early as 1820. A Harvard tradition known as "Bloody Monday" began in 1827, which consisted of a mass ballgame between the freshman and sophomore classes. In 1860, both the town police and the college authorities agreed the Bloody Monday had to go. The Harvard students responded by going into mourning for a mock figure called "Football Fightum", for whom they conducted funeral rites. The authorities held firm and it was a dozen years before football was once again played at Harvard. Dartmouth played its own version called "Old division football", the rules of which were first published in 1871, though the game dates to at least the 1830s. All of these games, and others, shared certain commonalities. They remained largely "mob" style games, with huge numbers of players attempting to advance the ball into a goal area, often by any means necessary. Rules were simple, violence and injury were common. The violence of these mob-style games led to widespread protests and a decision to abandon them. Yale, under pressure from the city of New Haven, banned the play of all forms of football in 1860.

American football historian Parke H. Davis described the period between 1869 and 1875 as the 'Pioneer Period'; the years 1876–93 he called the 'Period of the American Intercollegiate Football Association'; and the years 1894–1933 he dubbed the 'Period of Rules Committees and Conferences'.

On November 6, 1869, Rutgers University faced Princeton University (then known as the College of New Jersey) in the first-ever game of intercollegiate football that resembled more the game of soccer than "football" as it is played today. It was played with a round ball and, like all early games, used a set of rules suggested by Rutgers captain William J. Leggett, based on The Football Association's first set of rules, which were an early attempt by the former pupils of England's public schools, to unify the rules of their public schools games and create a universal and standardized set of rules for the game of football and bore little resemblance to the American game which would be developed in the following decades. It is still usually regarded as the first game of college football. The game was played at a Rutgers field. Two teams of 25 players attempted to score by kicking the ball into the opposing team's goal. Throwing or carrying the ball was not allowed, but there was plenty of physical contact between players. The first team to reach six goals was declared the winner. Rutgers won by a score of six to four. A rematch was played at Princeton a week later under Princeton's own set of rules (one notable difference was the awarding of a "free kick" to any player that caught the ball on the fly, which was a feature adopted from The Football Association's rules; the fair catch kick rule has survived through to modern American game). Princeton won that game by a score of 8 – 0. Columbia joined the series in 1870, and by 1872 several schools were fielding intercollegiate teams, including Yale and Stevens Institute of Technology.

Columbia University was the third school to field a team. The Lions traveled from New York City to New Brunswick on November 12, 1870, and were defeated by Rutgers 6 to 3. The game suffered from disorganization and the players kicked and battled each other as much as the ball. Later in 1870, Princeton and Rutgers played again with Princeton defeating Rutgers 6-0. This game's violence caused such an outcry that no games at all were played in 1871. Football came back in 1872, when Columbia played Yale for the first time. The Yale team was coached and captained by David Schley Schaff, who had learned to play football while attending Rugby School. Schaff himself was injured and unable to the play the game, but Yale won the game 3-0 nonetheless. Later in 1872, Stevens Tech became the fifth school to field a team. Stevens lost to Columbia, but beat both New York University and City College of New York during the following year.

By 1873, the college students playing football had made significant efforts to standardize their fledgling game. Teams had been scaled down from 25 players to 20. The only way to score was still to bat or kick the ball through the opposing team's goal, and the game was played in two 45 minute halves on fields 140 yards long and 70 yards wide. On October 20, 1873, representatives from Yale, Columbia, Princeton, and Rutgers met at the Fifth Avenue Hotel in New York City to codify the first set of intercollegiate football rules. Before this meeting, each school had its own set of rules and games were usually played using the home team's own particular code. At this meeting, a list of rules, based more on the Football Association's rules than the rules of the recently founded Rugby Football Union, was drawn up for intercollegiate football games.

Old "Football Fightum" had been resurrected at Harvard in 1872, when Harvard resumed playing football. Harvard, however, preferred to play a rougher version of football called "the Boston Game" in which the kicking of a round ball was the most prominent feature though a player could run with the ball, pass it, or dribble it (known as "babying"). The man with the ball could be tackled, although hitting, tripping, "hacking" (shin-kicking) and other unnecessary roughness was prohibited. There was no limit to the number of players, but there were typically ten to fifteen per side. A player could carry the ball only when being pursued.

As a result of this, Harvard refused to attend the rules conference organized by Rutgers, Princeton and Columbia at the Fifth Avenue Hotel in New York City on October 20, 1873 to agree on a set of rules and regulations that would allow them to play a form of football that was essentially Association football; and continued to play under its own code. While Harvard's voluntary absence from the meeting made it hard for them to schedule games against other American universities, it agreed to a challenge to play the rugby team of McGill University, from Montreal, in a two-game series. It was agreed that two games would be played on Harvard's Jarvis baseball field in Cambridge, Massachusetts on May 14 and 15, 1874: one to be played under Harvard rules, another under the stricter rugby regulations of McGill. Jarvis Field was at the time a patch of land at the northern point of the Harvard campus, bordered by Everett and Jarvis Streets to the north and south, and Oxford Street and Massachusetts Avenue to the east and west. Harvard beat McGill in the "Boston Game" on the Thursday and held McGill to a 0-0 tie on the Friday. The Harvard students took to the rugby rules and adopted them as their own, The games featured a round ball instead of a rugby-style oblong ball. This series of games represents an important milestone in the development of the modern game of American football. In October 1874, the Harvard team once again traveled to Montreal to play McGill in rugby, where they won by three tries.

Inasmuch as Rugby football had been transplanted to Canada from England, the McGill team played under a set of rules which allowed a player to pick up the ball and run with it whenever he wished. Another rule, unique to McGill, was to count tries (the act of grounding the football past the opposing team's goal line; it is important to note that there was no end zone during this time), as well as goals, in the scoring. In the Rugby rules of the time, a try only provided the attempt to kick a free goal from the field. If the kick was missed, the try did not score any points itself.

Harvard quickly took a liking to the rugby game, and its use of the try which, until that time, was not used in American football. The try would later evolve into the score known as the touchdown. On June 4, 1875, Harvard faced Tufts University in the first game between two American colleges played under rules similar to the McGill/Harvard contest, which was won by Tufts. The rules included each side fielding 11 men at any given time, the ball was advanced by kicking or carrying it, and tackles of the ball carrier stopped play - - actions of which have carried over to the modern version of football played today 

Harvard later challenged its closest rival, Yale, to which the Bulldogs accepted. The two teams agreed to play under a set of rules called the "Concessionary Rules", which involved Harvard conceding something to Yale's soccer and Yale conceding a great deal to Harvard's rugby. They decided to play with 15 players on each team. On November 13, 1875, Yale and Harvard played each other for the first time ever, where Harvard won 4-0. At the first The Game (as the annual contest between Harvard and Yale came to be named) the future "father of American football" Walter Camp was among the 2000 spectators in attendance. Walter, who would enroll at Yale the next year, was torn between an admiration for Harvard's style of play and the misery of the Yale defeat, and became determined to avenge Yale's defeat. Spectators from Princeton also carried the game back home, where it quickly became the most popular version of football.

On November 23, 1876, representatives from Harvard, Yale, Princeton, and Columbia met at the Massasoit House in Springfield, Massachusetts to standardize a new code of rules based on the rugby game first introduced to Harvard by McGill University in 1874. Three of the schools—Harvard, Columbia, and Princeton—formed the Intercollegiate Football Association, as a result of the meeting. Yale initially refused to join this association because of a disagreement over the number of players to be allowed per team (relenting in 1879) and Rutgers were not invited to the meeting. The rules that they agreed upon were essentially those of rugby union at the time with the exception that points be awarded for scoring a try, not just the conversion afterwards (extra point). Incidentally, rugby was to make a similar change to its scoring system 10 years later.
Walter Camp is widely considered to be the most important figure in the development of American football. As a youth, he excelled in sports like track, baseball, and association football, and after enrolling at Yale in 1876, he earned varsity honors in every sport the school offered.

Following the introduction of rugby-style rules to American football, Camp became a fixture at the Massasoit House conventions where rules were debated and changed. Dissatisfied with what seemed to him to be a disorganized mob, he proposed his first rule change at the first meeting he attended in 1878: a reduction from fifteen players to eleven. The motion was rejected at that time but passed in 1880. The effect was to open up the game and emphasize speed over strength. Camp's most famous change, the establishment of the line of scrimmage and the snap from center to quarterback, was also passed in 1880. Originally, the snap was executed with the foot of the center. Later changes made it possible to snap the ball with the hands, either through the air or by a direct hand-to-hand pass. Rugby league followed Camp's example, and in 1906 introduced the play-the-ball rule, which greatly resembled Camp's early scrimmage and center-snap rules. In 1966, rugby league introduced a four-tackle rule (changed in 1972 to a six-tackle rule) based on Camp's early down-and-distance rules.

Camp's new scrimmage rules revolutionized the game, though not always as intended. Princeton, in particular, used scrimmage play to slow the game, making incremental progress towards the end zone during each down. Rather than increase scoring, which had been Camp's original intent, the rule was exploited to maintain control of the ball for the entire game, resulting in slow, unexciting contests. At the 1882 rules meeting, Camp proposed that a team be required to advance the ball a minimum of five yards within three downs. These down-and-distance rules, combined with the establishment of the line of scrimmage, transformed the game from a variation of rugby football into the distinct sport of American football.

Camp was central to several more significant rule changes that came to define American football. In 1881, the field was reduced in size to its modern dimensions of 120 by 53 yards (109.7 by 48.8 meters). Several times in 1883, Camp tinkered with the scoring rules, finally arriving at four points for a touchdown, two points for kicks after touchdowns, two points for safeties, and five for field goals. Camp's innovations in the area of point scoring influenced rugby union's move to point scoring in 1890. In 1887, game time was set at two halves of 45 minutes each. Also in 1887, two paid officials—a referee and an umpire—were mandated for each game. A year later, the rules were changed to allow tackling below the waist, and in 1889, the officials were given whistles and stopwatches.

After leaving Yale in 1882, Camp was employed by the New Haven Clock Company until his death in 1925. Though no longer a player, he remained a fixture at annual rules meetings for most of his life, and he personally selected an annual All-American team every year from 1889 through 1924. The Walter Camp Football Foundation continues to select All-American teams in his honor.
College football expanded greatly during the last two decades of the 19th century. Several major rivalries date from this time period.

November 1890 was an active time in the sport. In Baldwin City, Kansas, on November 22, 1890, college football was first played in the state of Kansas. Baker beat Kansas 22–9. On the 27th, Vanderbilt played Nashville (Peabody) at Athletic Park and won 40–0. It was the first time organized football played in the state of Tennessee. The 29th also saw the first instance of the Army–Navy Game. Navy won 24–0.

Rutgers was first to extend the reach of the game. An intercollegiate game was first played in the state of New York when Rutgers played Columbia on November 2, 1872. It was also the first scoreless tie in the history of the fledgling sport. Yale football starts the same year and has its first match against Columbia, the nearest college to play football. It took place at Hamilton Park in New Haven and was the first game in New England. The game was essentially soccer with 20-man sides, played on a field 400 by 250 feet. Yale wins 3-0, Tommy Sherman scoring the first goal and Lew Irwin the other two.

After the first game against Harvard, Tufts took its squad to Bates College in Lewiston, Maine for the first football game played in Maine. This occurred on November 6, 1875.

Penn's Athletic Association was looking to pick "a twenty" to play a game of football against Columbia. This "twenty" never played Columbia, but did play twice against Princeton. Princeton won both games 6 to 0. The first of these happened on November 11, 1876, in Philadelphia and was the first intercollegiate game in the state of Pennsylvania.

Brown enters the intercollegiate game in 1878.

The first game where one team scored over 100 points happened on October 25, 1884, when Yale routed Dartmouth 113–0. It was also the first time one team scored over 100 points and the opposing team was shut out. The next week, Princeton outscored Lafayette by 140 to 0.

The first intercollegiate game in the state of Vermont happened on November 6, 1886, between Dartmouth and Vermont at Burlington, Vermont. Dartmouth won 91 to 0.

Penn State played its first season in 1887, but had no head coach for their first five years, from 1887–1891. The teams played its home games on the Old Main lawn on campus in State College, Pennsylvania. They compiled a 12–8–1 record in these seasons, playing as an independent from 1887–1890.

In 1891, the Pennsylvania Intercollegiate Football Association (PIFA) was formed. It consisted of Bucknell (University of Lewisburg), Dickinson, Franklin & Marshall, Haverford, Penn State and Swarthmore. Lafayette and Lehigh were excluded because it was felt they would dominate the Association. Penn State won the championship with a 4–1–0 record. Bucknell's record was 3–1–1 (losing to Franklin & Marshall and tying Dickinson). The Association was dissolved prior to the 1892 season.

The first nighttime football game was played in Mansfield, Pennsylvania on September 28, 1892, between Mansfield State Normal and Wyoming Seminary and ended at halftime in a 0–0 tie. The Army–Navy game of 1893 saw the first documented use of a football helmet by a player in a game. Joseph M. Reeves had a crude leather helmet made by a shoemaker in Annapolis and wore it in the game after being warned by his doctor that he risked death if he continued to play football after suffering an earlier kick to the head.

In 1879, the University of Michigan became the first school west of Pennsylvania to establish a college football team. On May 30, 1879, Michigan beat Racine College 1–0 in a game played in Chicago. The "Chicago Daily Tribune" called it "the first rugby-football game to be played west of the Alleghenies." Other Midwestern schools soon followed suit, including the University of Chicago, Northwestern University, and the University of Minnesota. The first western team to travel east was the 1881 Michigan team, which played at Harvard, Yale and Princeton. The nation's first college football league, the Intercollegiate Conference of Faculty Representatives (also known as the Western Conference), a precursor to the Big Ten Conference, was founded in 1895.

Led by coach Fielding H. Yost, Michigan became the first "western" national power. From 1901 to 1905, Michigan had a 56-game undefeated streak that included a 1902 trip to play in the first college football bowl game, which later became the Rose Bowl Game. During this streak, Michigan scored 2,831 points while allowing only 40.

Organized intercollegiate football was first played in the state of Minnesota on September 30, 1882, when Hamline was convinced to play Minnesota. Minnesota won 2 to 0. It was the first game west of the Mississippi River.

November 30, 1905, saw Chicago defeat Michigan 2 to 0. Dubbed "The First Greatest Game of the Century", broke Michigan's 56-game unbeaten streak and marked the end of the "Point-a-Minute" years.

Organized intercollegiate football was first played in the state of Virginia and the south on November 2, 1873, in Lexington between Washington and Lee and VMI. Washington and Lee won 4–2. Some industrious students of the two schools organized a game for October 23, 1869, but it was rained out. Students of the University of Virginia were playing pickup games of the kicking-style of football as early as 1870, and some accounts even claim it organized a game against Washington and Lee College in 1871; but no record has been found of the score of this contest. Due to scantness of records of the prior matches some will claim Virginia v. Pantops Academy November 13, 1887, as the first game in Virginia.

On April 9, 1880, at Stoll Field, Transylvania University (then called Kentucky University) beat Centre College by the score of 13¾–0 in what is often considered the first recorded game played in the South. The first game of "scientific football" in the South was the first instance of the Victory Bell rivalry between North Carolina and Duke (then known as Trinity College) held on Thanksgiving Day, 1888, at the North Carolina State Fairgrounds in Raleigh, North Carolina.

On November 13, 1887 the Virginia Cavaliers and Pantops Academy fought to a scoreless tie in the first organized football game in the state of Virginia. Students at UVA were playing pickup games of the kicking-style of football as early as 1870, and some accounts even claim that some industrious ones organized a game against Washington and Lee College in 1871, just two years after Rutgers and Princeton's historic first game in 1869. But no record has been found of the score of this contest. Washington and Lee also claims a 4 to 2 win over VMI in 1873.

On October 18, 1888, the Wake Forest Demon Deacons defeated the North Carolina Tar Heels 6 to 4 in the first intercollegiate game in the state of North Carolina.

On December 14, 1889, Wofford defeated Furman 5 to 1 in the first intercollegiate game in the state of South Carolina. The game featured no uniforms, no positions, and the rules were formulated before the game.

January 30, 1892, saw the first football game played in the Deep South when the Georgia Bulldogs defeated Mercer 50–0 at Herty Field.

The beginnings of the contemporary Southeastern Conference and Atlantic Coast Conference start in 1894. The Southern Intercollegiate Athletic Association (SIAA) was founded on December 21, 1894, by William Dudley, a chemistry professor at Vanderbilt. The original members were Alabama, Auburn, Georgia, Georgia Tech, North Carolina, , and Vanderbilt. Clemson, Cumberland, Kentucky, LSU, Mercer, Mississippi, Mississippi A&M (Mississippi State), Southwestern Presbyterian University, Tennessee, Texas, Tulane, and the University of Nashville joined the following year in 1895 as invited charter members. The conference was originally formed for "the development and purification of college athletics throughout the South".

It is thought that the first forward pass in football occurred on October 26, 1895, in a game between Georgia and North Carolina when, out of desperation, the ball was thrown by the North Carolina back Joel Whitaker instead of punted and George Stephens caught the ball. On November 9, 1895, John Heisman executed a hidden ball trick utilizing quarterback Reynolds Tichenor to get Auburn's only touchdown in a 6 to 9 loss to Vanderbilt. It was the first game in the south decided by a field goal. Heisman later used the trick against Pop Warner's Georgia team. Warner picked up the trick and later used it at Cornell against Penn State in 1897. He then used it in 1903 at Carlisle against Harvard and garnered national attention.

The 1899 Sewanee Tigers are one of the all-time great teams of the early sport. The team went 12–0, outscoring opponents 322 to 10. Known as the "Iron Men", with just 13 men they had a six-day road trip with five shutout wins over Texas A&M; Texas; Tulane; LSU; and Ole Miss. It is recalled memorably with the phrase "... and on the seventh day they rested." Grantland Rice called them "the most durable football team I ever saw."

Organized intercollegiate football was first played in the state of Florida in 1901. A 7-game series between intramural teams from Stetson and Forbes occurred in 1894. The first intercollegiate game between official varsity teams was played on November 22, 1901. Stetson beat Florida Agricultural College at Lake City, one of the four forerunners of the University of Florida, 6-0, in a game played as part of the Jacksonville Fair.

On September 27, 1902, Georgetown beat Navy 4 to 0. It is claimed by Georgetown authorities as the game with the first ever "roving center" or linebacker when Percy Given stood up, in contrast to the usual tale of Germany Schulz. The first linebacker in the South is often considered to be Frank Juhan.

On Thanksgiving Day 1903, a game was scheduled in Montgomery, Alabama between the best teams from each region of the Southern Intercollegiate Athletic Association for an "SIAA championship game", pitting Cumberland against Heisman's Clemson. The game ended in an 11–11 tie causing many teams to claim the title. Heisman pressed hardest for Cumberland to get the claim of champion. It was his last game as Clemson head coach.

1904 saw big coaching hires in the south: Mike Donahue at Auburn, John Heisman at Georgia Tech, and Dan McGugin at Vanderbilt were all hired that year. Both Donahue and McGugin just came from the north that year, Donahue from Yale and McGugin from Michigan, and were among the initial inductees of the College Football Hall of Fame. The undefeated 1904 Vanderbilt team scored an average of 52.7 points per game, the most in college football that season, and allowed just four points.

The first college football game in Oklahoma Territory occurred on November 7, 1895, when the 'Oklahoma City Terrors' defeated the Oklahoma Sooners 34 to 0. The Terrors were a mix of Methodist college students and high schoolers. The Sooners did not manage a single first down. By next season, Oklahoma coach John A. Harts had left to prospect for gold in the Arctic. Organized football was first played in the territory on November 29, 1894, between the Oklahoma City Terrors and Oklahoma City High School. The high school won 24 to 0.

The University of Southern California first fielded an American football team in 1888. Playing its first game on November 14 of that year against the Alliance Athletic Club, in which USC gained a 16–0 victory. Frank Suffel and Henry H. Goddard were playing coaches for the first team which was put together by quarterback Arthur Carroll; who in turn volunteered to make the pants for the team and later became a tailor. USC faced its first collegiate opponent the following year in fall 1889, playing St. Vincent's College to a 40–0 victory. In 1893, USC joined the Intercollegiate Football Association of Southern California (the forerunner of the SCIAC), which was composed of USC, Occidental College, Throop Polytechnic Institute (Caltech), and Chaffey College. Pomona College was invited to enter, but declined to do so. An invitation was also extended to Los Angeles High School.
In 1891, the first Stanford football team was hastily organized and played a four-game season beginning in January 1892 with no official head coach. Following the season, Stanford captain John Whittemore wrote to Yale coach Walter Camp asking him to recommend a coach for Stanford. To Whittemore's surprise, Camp agreed to coach the team himself, on the condition that he finish the season at Yale first. As a result of Camp's late arrival, Stanford played just three official games, against San Francisco's Olympic Club and rival California. The team also played exhibition games against two Los Angeles area teams that Stanford does not include in official results. Camp returned to the East Coast following the season, then returned to coach Stanford in 1894 and 1895.

On December 25, 1894, Amos Alonzo Stagg's Chicago Maroons agreed to play Camp's Stanford football team in San Francisco in the first postseason intersectional contest, foreshadowing the modern bowl game. Future president Herbert Hoover was Stanford's student financial manager. Chicago won 24 to 4. Stanford won a rematch in Los Angeles on December 29 by 12 to 0.
The Big Game between Stanford and California is the oldest college football rivalry in the West. The first game was played on San Francisco's Haight Street Grounds on March 19, 1892, with Stanford winning 14–10. The term "Big Game" was first used in 1900, when it was played on Thanksgiving Day in San Francisco. During that game, a large group of men and boys, who were observing from the roof of the nearby S.F. and Pacific Glass Works, fell into the fiery interior of the building when the roof collapsed, resulting in 13 dead and 78 injured. On December 4, 1900, the last victim of the disaster (Fred Lilly) died, bringing the death toll to 22; and, to this day, the "Thanksgiving Day Disaster" remains the deadliest accident to kill spectators at a U.S. sporting event.

The University of Oregon began playing American football in 1894 and played its first game on March 24, 1894, defeating Albany College 44–3 under head coach Cal Young. Cal Young left after that first game and J.A. Church took over the coaching position in the fall for the rest of the season. Oregon finished the season with two additional losses and a tie, but went undefeated the following season, winning all four of its games under head coach Percy Benson. In 1899, the Oregon football team left the state for the first time, playing the California Golden Bears in Berkeley, California.

American football at Oregon State University started in 1893 shortly after athletics were initially authorized at the college. Athletics were banned at the school in May 1892, but when the strict school president, Benjamin Arnold, died, President John Bloss reversed the ban. Bloss's son William started the first team, on which he served as both coach and quarterback. The team's first game was an easy 63-0 defeat over the home team, Albany College.

In May 1900, Yost was hired as the football coach at Stanford University, and, after traveling home to West Virginia, he arrived in Palo Alto, California, on August 21, 1900. Yost led the 1900 Stanford team to a 7–2–1, outscoring opponents 154 to 20. The next year in 1901, Yost was hired by Charles A. Baird as the head football coach for the Michigan Wolverines football team. On January 1, 1902, Yost's dominating 1901 Michigan Wolverines football team agreed to play a 3–1–2 team from Stanford University in the inaugural "Tournament East-West football game" what is now known as the "Rose Bowl Game" by a score of 49–0 after Stanford captain Ralph Fisher requested to quit with eight minutes remaining.

The 1905 season marked the first meeting between Stanford and USC. Consequently, Stanford is USC's oldest existing rival. The Big Game between Stanford and Cal on November 11, 1905, was the first played at Stanford Field, with Stanford winning 12–5.

In 1906, citing concerns about the violence in American Football, universities on the West Coast, led by California and Stanford, replaced the sport with rugby union. At the time, the future of American football was very much in doubt and these schools believed that rugby union would eventually be adopted nationwide. Other schools followed suit and also made the switch included Nevada, St. Mary's, Santa Clara, and USC (in 1911). However, due to the perception that West Coast football was inferior to the game played on the East Coast anyway, East Coast and Midwest teams shrugged off the loss of the teams and continued playing American football. With no nationwide movement, the available pool of rugby teams to play remained small. The schools scheduled games against local club teams and reached out to rugby union powers in Australia, New Zealand, and especially, due to its proximity, Canada. The annual Big Game between Stanford and California continued as rugby, with the winner invited by the British Columbia Rugby Union to a tournament in Vancouver over the Christmas holidays, with the winner of that tournament receiving the Cooper Keith Trophy.

During 12 seasons of playing rugby union, Stanford was remarkably successful: the team had three undefeated seasons, three one-loss seasons, and an overall record of 94 wins, 20 losses, and 3 ties for a winning percentage of .816. However, after a few years, the school began to feel the isolation of its newly adopted sport, which was not spreading as many had hoped. Students and alumni began to clamor for a return to American football to allow wider intercollegiate competition. The pressure at rival California was stronger (especially as the school had not been as successful in the Big Game as they had hoped), and in 1915 California returned to American football. As reasons for the change, the school cited rule change back to American football, the overwhelming desire of students and supporters to play American football, interest in playing other East Coast and Midwest schools, and a patriotic desire to play an "American" game. California's return to American football increased the pressure on Stanford to also change back in order to maintain the rivalry. Stanford played its 1915, 1916, and 1917 "Big Games" as rugby union against Santa Clara and California's football "Big Game" in those years was against Washington, but both schools desired to restore the old traditions. The onset of American involvement in World War I gave Stanford an out: In 1918, the Stanford campus was designated as the Students' Army Training Corps headquarters for all of California, Nevada, and Utah, and the commanding officer Sam M. Parker decreed that American football was the appropriate athletic activity to train soldiers and rugby union was dropped.

The University of Colorado began playing American football in 1890. Colorado found much success in its early years, winning eight Colorado Football Association Championships (1894–97, 1901–08).

The following was taken from the "Silver & Gold" newspaper of December 16, 1898. It was a recollection of the birth of Colorado football written by one of CU's original gridders, John C. Nixon, also the school's second captain. It appears here in its original form:

In 1909, the Rocky Mountain Athletic Conference was founded, featuring four members: Colorado, Colorado College, Colorado School of Mines, and Colorado Agricultural College. The University of Denver and the University of Utah joined the RMAC in 1910. For its first thirty years, the RMAC was considered a major conference equivalent to today's Division I, before 7 larger members left and formed the Mountain States Conference (also called the Skyline Conference).

College football increased in popularity through the remainder of the 19th and early 20th century. It also became increasingly violent. Between 1890 and 1905, 330 college athletes died as a direct result of injuries sustained on the football field. These deaths could be attributed to the mass formations and gang tackling that characterized the sport in its early years.

The 1894 Harvard–Yale game, known as the "Hampden Park Blood Bath", resulted in crippling injuries for four players; the contest was suspended until 1897. The annual Army–Navy game was suspended from 1894 to 1898 for similar reasons. One of the major problems was the popularity of mass-formations like the flying wedge, in which a large number of offensive players charged as a unit against a similarly arranged defense. The resultant collisions often led to serious injuries and sometimes even death. Georgia fullback Richard Von Albade Gammon notably died on the field from concussions received against Virginia in 1897, causing Georgia, Georgia Tech, and Mercer to suspend their football programs.

The situation came to a head in 1905 when there were 19 fatalities nationwide. President Theodore Roosevelt reportedly threatened to shut down the game if drastic changes were not made. However, the threat by Roosevelt to eliminate football is disputed by sports historians. What is absolutely certain is that on October 9, 1905, Roosevelt held a meeting of football representatives from Harvard, Yale, and Princeton. Though he lectured on eliminating and reducing injuries, he never threatened to ban football. He also lacked the authority to abolish football and was, in fact, actually a fan of the sport and wanted to preserve it. The President's sons were also playing football at the college and secondary levels at the time.

Meanwhile, John H. Outland held an experimental game in Wichita, Kansas that reduced the number of scrimmage plays to earn a first down from four to three in an attempt to reduce injuries. The "Los Angeles Times" reported an increase in punts and considered the game much safer than regular play but that the new rule was not "conducive to the sport". In 1906, President Roosevelt organized a meeting among thirteen school leaders at the White House to find solutions to make the sport safer for the athletes. Because the college officials could not agree upon a change in rules, it was decided over the course of several subsequent meetings that an external governing body should be responsible. Finally, on December 28, 1905, 62 schools met in New York City to discuss rule changes to make the game safer. As a result of this meeting, the Intercollegiate Athletic Association of the United States was formed in 1906. The IAAUS was the original rule making body of college football, but would go on to sponsor championships in other sports. The IAAUS would get its current name of National Collegiate Athletic Association (NCAA) in 1910, and still sets rules governing the sport.

The rules committee considered widening the playing field to "open up" the game, but Harvard Stadium (the first large permanent football stadium) had recently been built at great expense; it would be rendered useless by a wider field. The rules committee legalized the forward pass instead. Though it was underutilized for years, this proved to be one of the most important rule changes in the establishment of the modern game. Another rule change banned "mass momentum" plays (many of which, like the infamous "flying wedge", were sometimes literally deadly).

As a result of the 1905–1906 reforms, mass formation plays became illegal and forward passes legal. Bradbury Robinson, playing for visionary coach Eddie Cochems at Saint Louis University, threw the first legal pass in a September 5, 1906, game against Carroll College at Waukesha. Other important changes, formally adopted in 1910, were the requirements that at least seven offensive players be on the line of scrimmage at the time of the snap, that there be no pushing or pulling, and that interlocking interference (arms linked or hands on belts and uniforms) was not allowed. These changes greatly reduced the potential for collision injuries. Several coaches emerged who took advantage of these sweeping changes. Amos Alonzo Stagg introduced such innovations as the huddle, the tackling dummy, and the pre-snap shift. Other coaches, such as Pop Warner and Knute Rockne, introduced new strategies that still remain part of the game.

Besides these coaching innovations, several rules changes during the first third of the 20th century had a profound impact on the game, mostly in opening up the passing game. In 1914, the first roughing-the-passer penalty was implemented. In 1918, the rules on eligible receivers were loosened to allow eligible players to catch the ball anywhere on the field—previously strict rules were in place allowing passes to only certain areas of the field. Scoring rules also changed during this time: field goals were lowered to three points in 1909 and touchdowns raised to six points in 1912.

Star players that emerged in the early 20th century include Jim Thorpe, Red Grange, and Bronko Nagurski; these three made the transition to the fledgling NFL and helped turn it into a successful league. Sportswriter Grantland Rice helped popularize the sport with his poetic descriptions of games and colorful nicknames for the game's biggest players, including Notre Dame's "Four Horsemen" backfield and Fordham University's linemen, known as the "Seven Blocks of Granite".

In 1907 at Champaign, Illinois Chicago and Illinois played in the first game to have a halftime show featuring a marching band. Chicago won 42–6. On November 25, 1911 Kansas and Missouri played the first homecoming football game. The game was "broadcast" play-by-play over telegraph to at least 1,000 fans in Lawrence, Kansas. It ended in a 3–3 tie. The game between West Virginia and Pittsburgh on October 8, 1921, saw the first live radio broadcast of a college football game when Harold W. Arlin announced that year's Backyard Brawl played at Forbes Field on KDKA. Pitt won 21–13. On October 28, 1922, Princeton and Chicago played the first game to be nationally broadcast on radio. Princeton won 21–18 in a hotly contested game which had Princeton dubbed the "Team of Destiny."

One publication claims "The first scouting done in the South was in 1905, when Dan McGugin and Captain Innis Brown, of Vanderbilt went to Atlanta to see Sewanee play Georgia Tech." Fuzzy Woodruff claims Davidson was the first in the south to throw a legal forward pass in 1906. The following season saw Vanderbilt execute a double pass play to set up the touchdown that beat Sewanee in a meeting of unbeatens for the SIAA championship. Grantland Rice cited this event as the greatest thrill he ever witnessed in his years of watching sports. Vanderbilt coach Dan McGugin in "Spalding's Football Guide"'s summation of the season in the SIAA wrote "The standing. First, Vanderbilt; second, Sewanee, a might good second;" and that Aubrey Lanier "came near winning the Vanderbilt game by his brilliant dashes after receiving punts." Bob Blake threw the final pass to center Stein Stone, catching it near the goal amongst defenders. Honus Craig then ran in the winning touchdown.

Utilizing the "jump shift" offense, John Heisman's Georgia Tech Golden Tornado won 222 to 0 over Cumberland on October 7, 1916, at Grant Field in the most lopsided victory in college football history. Tech went on a 33-game winning streak during this period. The 1917 team was the first national champion from the South, led by a powerful backfield. It also had the first two players from the Deep South selected first-team All-American in Walker Carpenter and Everett Strupper. Pop Warner's Pittsburgh Panthers were also undefeated, but declined a challenge by Heisman to a game. When Heisman left Tech after 1919, his shift was still employed by protege William Alexander.

In 1906, Vanderbilt defeated Carlisle 4 to 0, the result of a Bob Blake field goal. In 1907 Vanderbilt fought Navy to a 6 to 6 tie. In 1910 Vanderbilt held defending national champion Yale to a scoreless tie.

Helping Georgia Tech's claim to a title in 1917, the Auburn Tigers held undefeated, Chic Harley-led Big Ten champion Ohio State to a scoreless tie the week before Georgia Tech beat the Tigers 68 to 7. The next season, with many players gone due to World War I, a game was finally scheduled at Forbes Field with Pittsburgh. The Panthers, led by freshman Tom Davies, defeated Georgia Tech 32 to 0. Tech center Bum Day was the first player on a Southern team ever selected first-team All-American by Walter Camp.

1917 saw the rise of another Southern team in Centre of Danville, Kentucky. In 1921 Bo McMillin-led Centre upset defending national champion Harvard 6 to 0 in what is widely considered one of the greatest upsets in college football history. The next year Vanderbilt fought Michigan to a scoreless tie at the inaugural game at Dudley Field (now Vanderbilt Stadium), the first stadium in the South made exclusively for college football. Michigan coach Fielding Yost and Vanderbilt coach Dan McGugin were brothers-in-law, and the latter the protege of the former. The game featured the season's two best defenses and included a goal line stand by Vanderbilt to preserve the tie. Its result was "a great surprise to the sporting world." Commodore fans celebrated by throwing some 3,000 seat cushions onto the field. The game features prominently in Vanderbilt's history. That same year, Alabama upset Penn 9 to 7.

Vanderbilt's line coach then was Wallace Wade, who coached Alabama to the south's first Rose Bowl victory in 1925. This game is commonly referred to as "the game that changed the south." Wade followed up the next season with an undefeated record and Rose Bowl tie. Georgia's 1927 "dream and wonder team" defeated Yale for the first time. Georgia Tech, led by Heisman protege William Alexander, gave the dream and wonder team its only loss, and the next year were national and Rose Bowl champions. The Rose Bowl included Roy Riegels' wrong-way run. On October 12, 1929, Yale lost to Georgia in Sanford Stadium in its first trip to the south. Wade's Alabama again won a national championship and Rose Bowl in 1930.

Glenn "Pop" Warner coached at several schools throughout his career, including the University of Georgia, Cornell University, University of Pittsburgh, Stanford University, Iowa State University, and Temple University. One of his most famous stints was at the Carlisle Indian Industrial School, where he coached Jim Thorpe, who went on to become the first president of the National Football League, an Olympic Gold Medalist, and is widely considered one of the best overall athletes in history. Warner wrote one of the first important books of football strategy, "Football for Coaches and Players", published in 1927. Though the shift was invented by Stagg, Warner's single wing and double wing formations greatly improved upon it; for almost 40 years, these were among the most important formations in football. As part of his single and double wing formations, Warner was one of the first coaches to effectively utilize the forward pass. Among his other innovations are modern blocking schemes, the three-point stance, and the reverse play. The youth football league, Pop Warner Little Scholars, was named in his honor.

Knute Rockne rose to prominence in 1913 as an end for the University of Notre Dame, then a largely unknown Midwestern Catholic school. When Army scheduled Notre Dame as a warm-up game, they thought little of the small school. Rockne and quarterback Gus Dorais made innovative use of the forward pass, still at that point a relatively unused weapon, to defeat Army 35–13 and helped establish the school as a national power. Rockne returned to coach the team in 1918, and devised the powerful Notre Dame Box offense, based on Warner's single wing. He is credited with being the first major coach to emphasize offense over defense. Rockne is also credited with popularizing and perfecting the forward pass, a seldom used play at the time. The 1924 team featured the Four Horsemen backfield. In 1927, his complex shifts led directly to a rule change whereby all offensive players had to stop for a full second before the ball could be snapped. Rather than simply a regional team, Rockne's "Fighting Irish" became famous for barnstorming and played any team at any location. It was during Rockne's tenure that the annual Notre Dame-University of Southern California rivalry began. He led his team to an impressive 105–12–5 record before his premature death in a plane crash in 1931. He was so famous at that point that his funeral was broadcast nationally on radio.

In the early 1930s, the college game continued to grow, particularly in the South, bolstered by fierce rivalries such as the "South's Oldest Rivalry", between Virginia and North Carolina and the "Deep South's Oldest Rivalry", between Georgia and Auburn. Although before the mid-1920s most national powers came from the Northeast or the Midwest, the trend changed when several teams from the South and the West Coast achieved national success. Wallace William Wade's 1925 Alabama team won the 1926 Rose Bowl after receiving its first national title and William Alexander's 1928 Georgia Tech team defeated California in the 1929 Rose Bowl. College football quickly became the most popular spectator sport in the South.

Several major modern college football conferences rose to prominence during this time period. The Southwest Athletic Conference had been founded in 1915. Consisting mostly of schools from Texas, the conference saw back-to-back national champions with Texas Christian University (TCU) in 1938 and Texas A&M in 1939. The Pacific Coast Conference (PCC), a precursor to the Pac-12 Conference (Pac-12), had its own back-to-back champion in the University of Southern California which was awarded the title in 1931 and 1932. The Southeastern Conference (SEC) formed in 1932 and consisted mostly of schools in the Deep South. As in previous decades, the Big Ten continued to dominate in the 1930s and 1940s, with Minnesota winning 5 titles between 1934 and 1941, and Michigan (1933, 1947, and 1948) and Ohio State (1942) also winning titles.

As it grew beyond its regional affiliations in the 1930s, college football garnered increased national attention. Four new bowl games were created: the Orange Bowl, Sugar Bowl, the Sun Bowl in 1935, and the Cotton Bowl in 1937. In lieu of an actual national championship, these bowl games, along with the earlier Rose Bowl, provided a way to match up teams from distant regions of the country that did not otherwise play. In 1936, the Associated Press began its weekly poll of prominent sports writers, ranking all of the nation's college football teams. Since there was no national championship game, the final version of the AP poll was used to determine who 
was crowned the National Champion of college football.

The 1930s saw growth in the passing game. Though some coaches, such as General Robert Neyland at Tennessee, continued to eschew its use, several rules changes to the game had a profound effect on teams' ability to throw the ball. In 1934, the rules committee removed two major penalties—a loss of five yards for a second incomplete pass in any series of downs and a loss of possession for an incomplete pass in the end zone—and shrunk the circumference of the ball, making it easier to grip and throw. Players who became famous for taking advantage of the easier passing game included Alabama end Don Hutson and TCU passer "Slingin" Sammy Baugh.

In 1935, New York City's Downtown Athletic Club awarded the first Heisman Trophy to University of Chicago halfback Jay Berwanger, who was also the first ever NFL Draft pick in 1936. The trophy was designed by sculptor Frank Eliscu and modeled after New York University player Ed Smith. The trophy recognizes the nation's "most outstanding" college football player and has become one of the most coveted awards in all of American sports.

During World War II, college football players enlisted in the armed forces, some playing in Europe during the war. As most of these players had eligibility left on their college careers, some of them returned to college at West Point, bringing Army back-to-back national titles in 1944 and 1945 under coach Red Blaik. Doc Blanchard (known as "Mr. Inside") and Glenn Davis (known as "Mr. Outside") both won the Heisman Trophy, in 1945 and 1946. On the coaching staff of those 1944–1946 Army teams was future Pro Football Hall of Fame coach Vince Lombardi.

The 1950s saw the rise of yet more dynasties and power programs. Oklahoma, under coach Bud Wilkinson, won three national titles (1950, 1955, 1956) and all ten Big Eight Conference championships in the decade while building a record 47-game winning streak. Woody Hayes led Ohio State to two national titles, in 1954 and 1957, and won three Big Ten titles. The Michigan State Spartans were known as the "football factory" during the 1950s, where coaches Clarence Munn and Duffy Daugherty led the Spartans to two national titles and two Big Ten titles after joining the Big Ten athletically in 1953. Wilkinson and Hayes, along with Robert Neyland of Tennessee, oversaw a revival of the running game in the 1950s. Passing numbers dropped from an average of 18.9 attempts in 1951 to 13.6 attempts in 1955, while teams averaged just shy of 50 running plays per game. Nine out of ten Heisman Trophy winners in the 1950s were runners. Notre Dame, one of the biggest passing teams of the decade, saw a substantial decline in success; the 1950s were the only decade between 1920 and 1990 when the team did not win at least a share of the national title. Paul Hornung, Notre Dame quarterback, did, however, win the Heisman in 1956, becoming the only player from a losing team ever to do so.

Following the enormous success of the 1958 NFL Championship Game, college football no longer enjoyed the same popularity as the NFL, at least on a national level. While both games benefited from the advent of television, since the late 1950s, the NFL has become a nationally popular sport while college football has maintained strong regional ties.
As professional football became a national television phenomenon, college football did as well. In the 1950s, Notre Dame, which had a large national following, formed its own network to broadcast its games, but by and large the sport still retained a mostly regional following. In 1952, the NCAA claimed all television broadcasting rights for the games of its member institutions, and it alone negotiated television rights. This situation continued until 1984, when several schools brought a suit under the Sherman Antitrust Act; the Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals. ABC Sports began broadcasting a national Game of the Week in 1966, bringing key matchups and rivalries to a national audience for the first time.

New formations and play sets continued to be developed. Emory Bellard, an assistant coach under Darrell Royal at the University of Texas, developed a three-back option style offense known as the wishbone. The wishbone is a run-heavy offense that depends on the quarterback making last second decisions on when and to whom to hand or pitch the ball to. Royal went on to teach the offense to other coaches, including Bear Bryant at Alabama, Chuck Fairbanks at Oklahoma and Pepper Rodgers at UCLA; who all adapted and developed it to their own tastes. The strategic opposite of the wishbone is the spread offense, developed by professional and college coaches throughout the 1960s and 1970s. Though some schools play a run-based version of the spread, its most common use is as a passing offense designed to "spread" the field both horizontally and vertically. Some teams have managed to adapt with the times to keep winning consistently. In the rankings of the most victorious programs, Michigan, Texas, and Notre Dame are ranked first, second, and third in total wins.

In 1940, for the highest level of college football, there were only five bowl games (Rose, Orange, Sugar, Sun, and Cotton). By 1950, three more had joined that number and in 1970, there were still only eight major college bowl games. The number grew to eleven in 1976. At the birth of cable television and cable sports networks like ESPN, there were fifteen bowls in 1980. With more national venues and increased available revenue, the bowls saw an explosive growth throughout the 1980s and 1990s. In the thirty years from 1950 to 1980, seven bowl games were added to the schedule. From 1980 to 2008, an additional 20 bowl games were added to the schedule. Some have criticized this growth, claiming that the increased number of games has diluted the significance of playing in a bowl game. Yet others have countered that the increased number of games has increased exposure and revenue for a greater number of schools, and see it as a positive development.

With the growth of bowl games, it became difficult to determine a national champion in a fair and equitable manner. As conferences became contractually bound to certain bowl games (a situation known as a tie-in), match-ups that guaranteed a consensus national champion became increasingly rare. In 1992, seven conferences and independent Notre Dame formed the Bowl Coalition, which attempted to arrange an annual No.1 versus No.2 matchup based on the final AP poll standings. The Coalition lasted for three years; however, several scheduling issues prevented much success; tie-ins still took precedence in several cases. For example, the Big Eight and SEC champions could never meet, since they were contractually bound to different bowl games. The coalition also excluded the Rose Bowl, arguably the most prestigious game in the nation, and two major conferences—the Pac-10 and Big Ten—meaning that it had limited success. In 1995, the Coalition was replaced by the Bowl Alliance, which reduced the number of bowl games to host a national championship game to three—the Fiesta, Sugar, and Orange Bowls—and the participating conferences to five—the ACC, SEC, Southwest, Big Eight, and Big East. It was agreed that the No.1 and No.2 ranked teams gave up their prior bowl tie-ins and were guaranteed to meet in the national championship game, which rotated between the three participating bowls. The system still did not include the Big Ten, Pac-10, or the Rose Bowl, and thus still lacked the legitimacy of a true national championship. However, one positive side effect is that if there were three teams at the end of the season vying for a national title, but one of them was a Pac-10/Big Ten team bound to the Rose Bowl, then there would be no difficulty in deciding which teams to place in the Bowl Alliance "national championship" bowl; if the Pac-10 / Big Ten team won the Rose Bowl and finished with the same record as whichever team won the other bowl game, they could have a share of the national title. This happened in the final year of the Bowl Alliance, with Michigan winning the 1998 Rose Bowl and Nebraska winning the 1998 Orange Bowl. Without the Pac-10/Big Ten team bound to a bowl game, it would be difficult to decide which two teams should play for the national title.

In 1998, a new system was put into place called the Bowl Championship Series. For the first time, it included all major conferences (ACC, Big East, Big 12, Big Ten, Pac-10, and SEC) and four major bowl games (Rose, Orange, Sugar and Fiesta). The champions of these six conferences, along with two "at-large" selections, were invited to play in the four bowl games. Each year, one of the four bowl games served as a national championship game. Also, a complex system of human polls, computer rankings, and strength of schedule calculations was instituted to rank schools. Based on this ranking system, the No.1 and No.2 teams met each year in the national championship game. Traditional tie-ins were maintained for schools and bowls not part of the national championship. For example, in years when not a part of the national championship, the Rose Bowl still hosted the Big Ten and Pac-10 champions.

The system continued to change, as the formula for ranking teams was tweaked from year to year. At-large teams could be chosen from any of the Division I-A conferences, though only one selection—Utah in 2005—came from a BCS non-AQ conference. Starting with the 2006 season, a fifth game—simply called the BCS National Championship Game—was added to the schedule, to be played at the site of one of the four BCS bowl games on a rotating basis, one week after the regular bowl game. This opened up the BCS to two additional at-large teams. Also, rules were changed to add the champions of five additional conferences (Conference USA [C-USA], the Mid-American Conference [MAC], the Mountain West Conference [MW], the Sun Belt Conference and the Western Athletic Conference [WAC]), provided that said champion ranked in the top twelve in the final BCS rankings, or was within the top 16 of the BCS rankings and ranked higher than the champion of at least one of the BCS Automatic Qualifying (AQ) conferences. Several times since this rule change was implemented, schools from non-AQ conferences have played in BCS bowl games. In 2009, Boise State played TCU in the Fiesta Bowl, the first time two schools from non-AQ conferences played each other in a BCS bowl game. The last team from the non-AQ ranks to reach a BCS bowl game in the BCS era was Northern Illinois in 2012, which played in (and lost) the 2013 Orange Bowl.

The longtime resistance to a playoff system at the FBS level finally ended with the creation of the College Football Playoff (CFP) beginning with the 2014 season. The CFP is a Plus-One system, a concept that became popular as a BCS alternative following controversies in 2003 and 2004. The CFP is a four-team tournament whose participants are chosen and seeded by a 13-member selection committee. The semifinals are hosted by two of a group of six traditional bowl games often called the "New Year's Six", with semifinal hosting rotating annually among three pairs of games in the following order: Rose/Sugar, Orange/Cotton, and Fiesta/Peach. The two semifinal winners then advance to the College Football Playoff National Championship, whose host is determined by open bidding several years in advance.

The establishment of the CFP followed a tumultuous period of conference realignment in Division I. The WAC, after seeing all but two of its football members leave, dropped football after the 2012 season. The Big East split into two leagues in 2013; the schools that did not play FBS football reorganized as a new non-football Big East Conference, while the FBS member schools that remained in the original structure joined with several new members and became the American Athletic Conference. The American retained the Big East's automatic BCS bowl bid for the 2013 season, but lost this status in the CFP era.

The 10 FBS conferences are formally and popularly divided into two groups:

Although rules for the high school, college, and NFL games are generally consistent, there are several minor differences. The NCAA Football Rules Committee determines the playing rules for Division I (both Bowl and Championship Subdivisions), II, and III games (the National Association of Intercollegiate Athletics (NAIA) is a separate organization, but uses the NCAA rules).


College teams mostly play other similarly sized schools through the NCAA's divisional system. Division I generally consists of the major collegiate athletic powers with larger budgets, more elaborate facilities, and (with the exception of a few conferences such as the Pioneer Football League) more athletic scholarships. Division II primarily consists of smaller public and private institutions that offer fewer scholarships than those in Division I. Division III institutions also field teams, but do not offer any scholarships.

Football teams in Division I are further divided into the Bowl Subdivision (consisting of the largest programs) and the Championship Subdivision. The Bowl Subdivision has historically not used an organized tournament to determine its champion, and instead teams compete in post-season bowl games. That changed with the debut of the four-team College Football Playoff at the end of the 2014 season.

Teams in each of these four divisions are further divided into various regional conferences.

Several organizations operate college football programs outside the jurisdiction of the NCAA:


A college that fields a team in the NCAA is not restricted from fielding teams in club or sprint football, and several colleges field two teams, a varsity (NCAA) squad and a club or sprint squad (no schools, , field both club "and" sprint teams at the same time).


Started in the 2014 season, four Division I FBS teams are selected at the end of regular season to compete in a playoff for the FBS national championship. The inaugural champion was Ohio State University. The College Football Playoff replaced the Bowl Championship Series, which had been used as the selection method to determine the national championship game participants since in the 1998 season. Clemson won the 2019 national championship.

At the Division I FCS level, the teams participate in a 24-team playoff (most recently expanded from 20 teams in 2013) to determine the national championship. Under the current playoff structure, the top eight teams are all seeded, and receive a bye week in the first round. The highest seed receives automatic home field advantage. Starting in 2013, non-seeded teams can only host a playoff game if both teams involved are unseeded; in such a matchup, the schools must bid for the right to host the game. Selection for the playoffs is determined by a selection committee, although usually a team must have an 8-4 record to even be considered. Losses to an FBS team count against their playoff eligibility, while wins against a Division II opponent do not count towards playoff consideration. Thus, only Division I wins (whether FBS, FCS, or FCS non-scholarship) are considered for playoff selection. The Division I National Championship game is held in Frisco, Texas.

Division II and Division III of the NCAA also participate in their own respective playoffs, crowning national champions at the end of the season. The National Association of Intercollegiate Athletics also holds a playoff.

Unlike other college football divisions and most other sports—collegiate or professional—the Football Bowl Subdivision, formerly known as Division I-A college football, has historically not employed a playoff system to determine a champion. Instead, it has a series of postseason "bowl games". The annual National Champion in the Football Bowl Subdivision is then instead traditionally determined by a vote of sports writers and other non-players.

This system has been challenged often, beginning with an NCAA committee proposal in 1979 to have a four-team playoff following the bowl games. However, little headway was made in instituting a playoff tournament until 2014, given the entrenched vested economic interests in the various bowls. Although the NCAA publishes lists of claimed FBS-level national champions in its official publications, it has never recognized an official FBS national championship; this policy continues even after the establishment of the College Football Playoff (which is not directly run by the NCAA) in 2014. As a result, the official Division I National Champion is the winner of the Football Championship Subdivision, as it is the highest level of football with an NCAA-administered championship tournament.

The first bowl game was the 1902 Rose Bowl, played between Michigan and Stanford; Michigan won 49-0. It ended when Stanford requested and Michigan agreed to end it with 8 minutes on the clock. That game was so lopsided that the game was not played annually until 1916, when the Tournament of Roses decided to reattempt the postseason game. The term "bowl" originates from the shape of the Rose Bowl stadium in Pasadena, California, which was built in 1923 and resembled the Yale Bowl, built in 1915. This is where the name came into use, as it became known as the Rose Bowl Game. Other games came along and used the term "bowl", whether the stadium was shaped like a bowl or not.

At the Division I FBS level, teams must earn the right to be bowl eligible by winning at least 6 games during the season (teams that play 13 games in a season, which is allowed for Hawaii and any of its home opponents, must win 7 games). They are then invited to a bowl game based on their conference ranking and the tie-ins that the conference has to each bowl game. For the 2009 season, there were 34 bowl games, so 68 of the 120 Division I FBS teams were invited to play at a bowl. These games are played from mid-December to early January and most of the later bowl games are typically considered more prestigious.

After the Bowl Championship Series, additional all-star bowl games round out the post-season schedule through the beginning of February.

Partly as a compromise between both bowl game and playoff supporters, the NCAA created the Bowl Championship Series (BCS) in 1998 in order to create a definitive national championship game for college football. The series included the four most prominent bowl games (Rose Bowl, Orange Bowl, Sugar Bowl, Fiesta Bowl), while the national championship game rotated each year between one of these venues. The BCS system was slightly adjusted in 2006, as the NCAA added a fifth game to the series, called the National Championship Game. This allowed the four other BCS bowls to use their normal selection process to select the teams in their games while the top two teams in the BCS rankings would play in the new National Championship Game.

The BCS selection committee used a complicated, and often controversial, computer system to rank all Division I-FBS teams and the top two teams at the end of the season played for the national championship. This computer system, which factored in newspaper polls, online polls, coaches' polls, strength of schedule, and various other factors of a team's season, led to much dispute over whether the two best teams in the country were being selected to play in the National Championship Game.

The BCS ended after the 2013 season and, since the 2014 season, the FBS national champion has been determined by a four-team tournament known as the College Football Playoff (CFP). A selection committee of college football experts decides the participating teams. Six major bowl games (the Rose, Sugar, Cotton, Orange, Peach, and Fiesta) rotate on a three-year cycle as semifinal games, with the winners advancing to the College Football Playoff National Championship. This arrangement is contractually locked in until the 2026 season.

College football is a controversial institution within American higher education, where the amount of money involved—what people will pay for the entertainment provided—is a corrupting factor within universities that they are usually ill-equipped to deal with. According to William E. Kirwan, chancellor of the University of Maryland System and co-director of the Knight Commission on Intercollegiate Athletics, "We've reached a point where big-time intercollegiate athletics is undermining the integrity of our institutions, diverting presidents and institutions from their main purpose." Football coaches often make more than the presidents of their universities which employ them. Athletes are alleged to receive preferential treatment both in academics and when they run afoul of the law. Although in theory football is an extra-curricular activity engaged in as a sideline by students, it is widely believed to turn a substantial profit, from which the athletes receive no direct benefit. There has been serious discussion about making student-athletes university employees to allow them to be paid. In reality, the majority of major collegiate football programs operated at a financial loss in 2014.

Canadian football, which parallels American football, is played by university teams in Canada under the auspices of U Sports. (Unlike in the United States, no junior colleges play football in Canada, and the sanctioning body for junior college athletics in Canada, CCAA, does not sanction the sport.) However, amateur football outside of colleges is played in Canada, such as in the Canadian Junior Football League. Organized competition in American football also exists at the collegiate level in Mexico (ONEFA), the UK (British Universities American Football League), Japan (Japan American Football Association, Koshien Bowl), and South Korea (Korea American Football Association).









</doc>
<doc id="6773" url="https://en.wikipedia.org/wiki?curid=6773" title="Ciprofloxacin">
Ciprofloxacin

Ciprofloxacin is an antibiotic used to treat a number of bacterial infections. This includes bone and joint infections, intra abdominal infections, certain type of infectious diarrhea, respiratory tract infections, skin infections, typhoid fever, and urinary tract infections, among others. For some infections it is used in addition to other antibiotics. It can be taken by mouth, as eye drops, as ear drops, or intravenously.
Common side effects include nausea, vomiting, diarrhea and rash. Ciprofloxacin increases the risk of tendon rupture. In people with myasthenia gravis, there is worsening muscle weakness. Rates of side effects appear to be higher than some groups of antibiotics such as cephalosporins but lower than others such as clindamycin. Studies in other animals raise concerns regarding use in pregnancy. No problems were identified, however, in the children of a small number of women who took the medication. It appears to be safe during breastfeeding. It is a second-generation fluoroquinolone with a broad spectrum of activity that usually results in the death of the bacteria.
Ciprofloxacin was patented in 1980 and introduced in 1987. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. It is available as a generic medication and is not very expensive. The wholesale cost in the developing world is between 0.03 and 0.13 a dose. In the United States it is sold for about 0.40 per dose. In 2016 it was the 102nd most prescribed medication in the United States with more than 7 million prescriptions.

Ciprofloxacin is used to treat a wide variety of infections, including infections of bones and joints, endocarditis, gastroenteritis, malignant otitis externa, respiratory tract infections, cellulitis, urinary tract infections, prostatitis, anthrax, and chancroid.

Ciprofloxacin only treats bacterial infections; it does not treat viral infections such as the common cold. For certain uses including acute sinusitis, lower respiratory tract infections and uncomplicated gonorrhea, ciprofloxacin is not considered a first-line agent.

Ciprofloxacin occupies an important role in treatment guidelines issued by major medical societies for the treatment of serious infections, especially those likely to be caused by Gram-negative bacteria, including "Pseudomonas aeruginosa". For example, ciprofloxacin in combination with metronidazole is one of several first-line antibiotic regimens recommended by the Infectious Diseases Society of America for the treatment of community-acquired abdominal infections in adults. It also features prominently in treatment guidelines for acute pyelonephritis, complicated or hospital-acquired urinary tract infection, acute or chronic prostatitis, certain types of endocarditis, certain skin infections, and prosthetic joint infections.

In other cases, treatment guidelines are more restrictive, recommending in most cases that older, narrower-spectrum drugs be used as first-line therapy for less severe infections to minimize fluoroquinolone-resistance development. For example, the Infectious Diseases Society of America recommends the use of ciprofloxacin and other fluoroquinolones in urinary tract infections be reserved to cases of proven or expected resistance to narrower-spectrum drugs such as nitrofurantoin or trimethoprim/sulfamethoxazole. The European Association of Urology recommends ciprofloxacin as an alternative regimen for the treatment of uncomplicated urinary tract infections, but cautions that the potential for "adverse events have to be considered".

Although approved by regulatory authorities for the treatment of respiratory infections, ciprofloxacin is not recommended for respiratory infections by most treatment guidelines due in part to its modest activity against the common respiratory pathogen "Streptococcus pneumoniae". "Respiratory quinolones" such as levofloxacin, having greater activity against this pathogen, are recommended as first line agents for the treatment of community-acquired pneumonia in patients with important co-morbidities and in patients requiring hospitalization (Infectious Diseases Society of America 2007). Similarly, ciprofloxacin is not recommended as a first-line treatment for acute sinusitis.

Ciprofloxacin is approved for the treatment of gonorrhea in many countries, but this recommendation is widely regarded as obsolete due to resistance development.

In the United States ciprofloxacin is pregnancy category C. This category includes drugs for which no adequate and well-controlled studies in human pregnancy exist, and for which animal studies have suggested the potential for harm to the fetus, but potential benefits may warrant use of the drug in pregnant women
despite potential risks. An expert review of published data on experiences with ciprofloxacin use during pregnancy by the Teratogen Information System concluded therapeutic doses during
pregnancy are unlikely to pose a substantial teratogenic risk (quantity and quality of data=fair), but the data are insufficient to state no risk exists. Exposure to quinolones, including levofloxacin, during the first-trimester is not associated with an increased risk of stillbirths, premature births, birth defects, or low birth weight.

Two small post-marketing epidemiology studies of mostly short-term, first-trimester exposure found that fluoroquinolones did not increase risk of major malformations, spontaneous abortions, premature birth, or low birth weight. The label notes, however, that these studies are insufficient to reliably evaluate the definitive safety or risk of less common defects by ciprofloxacin in pregnant women and their developing fetuses.

Fluoroquinolones have been reported as present in a mother's milk and thus passed on to the nursing child. The U.S. FDA recommends that because of the risk of serious adverse reactions (including articular damage) in infants nursing from mothers taking ciprofloxacin, a decision should be made whether to discontinue nursing or discontinue the drug, taking into account the importance of the drug to the mother.

Oral and intravenous ciprofloxacin are approved by the FDA for use in children for only two indications due to the risk of permanent injury to the musculoskeletal system:

1) Inhalational anthrax (postexposure)

2) Complicated urinary tract infections and pyelonephritis due to "Escherichia coli", but never as first-line agents. Current recommendations by the American Academy of Pediatrics note the systemic use of ciprofloxacin in children should be restricted to infections caused by multidrug-resistant pathogens or when no safe or effective alternatives are available.

Its spectrum of activity includes most strains of bacterial pathogens responsible for community-acquired pneumonias, bronchitis, urinary tract infections, and gastroenteritis. Ciprofloxacin is particularly effective against Gram-negative bacteria (such as "Escherichia coli", "Haemophilus influenzae", "Klebsiella pneumoniae", "Legionella pneumophila", "Moraxella catarrhalis", "Proteus mirabilis", and "Pseudomonas aeruginosa"), but is less effective against Gram-positive bacteria (such as methicillin-sensitive "Staphylococcus aureus", "Streptococcus pneumoniae", and "Enterococcus faecalis") than newer fluoroquinolones.

As a result of its widespread use to treat minor infections readily treatable with older, narrower spectrum antibiotics, many bacteria have developed resistance to this drug in recent years, leaving it significantly less effective than it would have been otherwise.

Resistance to ciprofloxacin and other fluoroquinolones may evolve rapidly, even during a course of treatment. Numerous pathogens, including enterococci, "Streptococcus pyogenes" and "Klebsiella pneumoniae" (quinolone-resistant) now exhibit resistance. Widespread veterinary usage of the fluoroquinolones, particularly in Europe, has been implicated. Meanwhile, some "Burkholderia cepacia", "Clostridium innocuum" and "Enterococcus faecium" strains have developed resistance to ciprofloxacin to varying degrees.

Fluoroquinolones had become the class of antibiotics most commonly prescribed to adults in 2002. Nearly half (42%) of those prescriptions in the U.S. were for conditions not approved by the FDA, such as acute bronchitis, otitis media, and acute upper respiratory tract infection, according to a study supported in part by the Agency for Healthcare Research and Quality. Additionally, they were commonly prescribed for medical conditions that were not even bacterial to begin with, such as viral infections, or those to which no proven benefit existed.

Contraindications include:

Ciprofloxacin is also considered to be contraindicated in children (except for the indications outlined above), in pregnancy, to nursing mothers, and in people with epilepsy or other seizure disorders.

Adverse effects can involve the tendons, muscles, joints, nerves, and the central nervous system.

Rates of adverse effects appear to be higher than with some groups of antibiotics such as cephalosporins but lower than with others such as clindamycin. Compared to other antibiotics some studies find a higher rate of adverse effects while others find no difference.

In clinical trials most of the adverse events were described as mild or moderate in severity, abated soon after the drug was discontinued, and required no treatment. Some adverse effects may be permanent. Ciprofloxacin was stopped because of an adverse event in 1% of people treated with the medication by mouth. The most frequently reported drug-related events, from trials of all formulations, all dosages, all drug-therapy durations, and for all indications, were nausea (2.5%), diarrhea (1.6%), abnormal liver function tests (1.3%), vomiting (1%), and rash (1%). Other adverse events occurred at rates of <1%.

Ciprofloxacin includes a black box warning in the United States of an increased risk of tendinitis and tendon rupture, especially in people who are older than 60 years, people who also use corticosteroids, and people with kidney, lung, or heart transplants. Tendon rupture can occur during therapy or even months after discontinuation of the medication. One study found that fluoroquinolone use was associated with a 1.9-fold increase in tendon problems. The risk increased to 3.2 in those over 60 years of age and to 6.2 in those over the age of 60 who were also taking corticosteroids. Among the 46,766 quinolone users in the study, 38 (0.08%) cases of Achilles tendon rupture were identified.

The fluoroquinolones, including ciprofloxacin, are associated with an increased risk of cardiac toxicity, including QT interval prolongation, torsades de pointes, ventricular arrhythmia, and sudden death. 

The 2013 FDA label warns of nervous system effects. Ciprofloxacin, like other fluoroquinolones, is known to trigger seizures or lower the seizure threshold, and may cause other central nervous system adverse effects. Headache, dizziness, and insomnia have been reported as occurring fairly commonly in postapproval review articles, along with a much lower incidence of serious CNS adverse effects such as tremors, psychosis, anxiety, hallucinations, paranoia, and suicide attempts, especially at higher doses. Like other fluoroquinolones, it is also known to cause peripheral neuropathy that may be irreversible, such as weakness, burning pain, tingling or numbness.

Ciprofloxacin is active in six of eight "in vitro" assays used as rapid screens for the detection of genotoxic effects, but is not active in "in vivo" assays of genotoxicity. Long-term carcinogenicity studies in rats and mice resulted in no carcinogenic or tumorigenic effects due to ciprofloxacin at daily oral dose levels up to 250 and 750 mg/kg to rats and mice, respectively (about 1.7 and 2.5 times the highest recommended therapeutic dose based upon mg/m). Results from photo co-carcinogenicity testing indicate ciprofloxacin does not reduce the time to appearance of UV-induced skin tumors as compared to vehicle control.

The other black box warning is that ciprofloxacin should not be used in people with myasthenia gravis due to possible exacerbation of muscle weakness which may lead to breathing problems resulting in death or ventilator support. Fluoroquinolones are known to block neuromuscular transmission. There are concerns that fluoroquinolones including ciprofloxacin can affect cartilage in young children.

"Clostridium difficile"-associated diarrhea is a serious adverse effect of ciprofloxacin and other fluoroquinolones; it is unclear whether the risk is higher than with other broad-spectrum antibiotics.

A wide range of rare but potentially fatal adverse effects reported to the U.S. FDA or the subject of case reports includes aortic dissection, toxic epidermal necrolysis, Stevens-Johnson syndrome, low blood pressure, allergic pneumonitis, bone marrow suppression, hepatitis or liver failure, and sensitivity to light. The medication should be discontinued if a rash, jaundice, or other sign of hypersensitivity occurs.

Children and the elderly are at a much greater risk of experiencing adverse reactions.

Overdose of ciprofloxacin may result in reversible renal toxicity. Treatment of overdose includes emptying of the stomach by induced vomiting or gastric lavage, as well as administration of antacids containing magnesium, aluminum, or calcium to reduce drug absorption. Renal function and urinary pH should be monitored. Important support includes adequate hydration and urine acidification if necessary to prevent crystalluria. Hemodialysis or peritoneal dialysis can only remove less than 10% of ciprofloxacin. Ciprofloxacin may be quantified in plasma or serum to monitor for drug accumulation in patients with hepatic dysfunction or to confirm a diagnosis of poisoning in acute overdose victims.

Ciprofloxacin interacts with certain foods and several other drugs leading to undesirable increases or decreases in the serum levels or distribution of one or both drugs.

Ciprofloxacin should not be taken with antacids containing magnesium or aluminum, highly buffered drugs (sevelamer, lanthanum carbonate, sucralfate, didanosine), or with supplements containing calcium, iron, or zinc. It should be taken two hours before or six hours after these products. Magnesium or aluminum antacids turn ciprofloxacin into insoluble salts that are not readily absorbed by the intestinal tract, reducing peak serum concentrations by 90% or more, leading to therapeutic failure. Additionally, it should not be taken with dairy products or calcium-fortified juices alone, as peak serum concentration and the area under the serum concentration-time curve can be reduced up to 40%. However, ciprofloxacin may be taken with dairy products or calcium-fortified juices as part of a meal.

Ciprofloxacin inhibits the drug-metabolizing enzyme CYP1A2 and thereby can reduce the clearance of drugs metabolized by that enzyme. CYP1A2 substrates that exhibit increased serum levels in ciprofloxacin-treated patients include tizanidine, theophylline, caffeine, methylxanthines, clozapine, olanzapine, and ropinirole. Co-administration of ciprofloxacin with the CYP1A2 substrate tizanidine (Zanaflex) is contraindicated due to a 583% increase in the peak serum concentrations of tizanidine when administered with ciprofloxacin as compared to administration of tizanidine alone. Use of ciprofloxacin is cautioned in patients on theophylline due to its narrow therapeutic index. The authors of one review recommended that patients being treated with ciprofloxacin reduce their caffeine intake. Evidence for significant interactions with several other CYP1A2 substrates such as cyclosporine is equivocal or conflicting.

The Committee on Safety of Medicines and the FDA warn that central nervous system adverse effects, including seizure risk, may be increased when NSAIDs are combined with quinolones. The mechanism for this interaction may involve a synergistic increased antagonism of GABA neurotransmission.

Altered serum levels of the antiepileptic drugs phenytoin and carbamazepine (increased and decreased) have been reported in patients receiving concomitant ciprofloxacin.

Ciprofloxacin is a potent inhibitor of CYP1A2, CYP2D6, and CYP3A4.

Ciprofloxacin is a broad-spectrum antibiotic of the fluoroquinolone class. It is active against both Gram-positive and Gram-negative bacteria. It functions by inhibiting DNA gyrase, and a type II topoisomerase, topoisomerase IV, necessary to separate bacterial DNA, thereby inhibiting cell division.

Ciprofloxacin for systemic administration is available as immediate-release tablets, extended-release tablets, an oral suspension, and as a solution for intravenous administration. 
When administered over one hour as an intravenous infusion, ciprofloxacin rapidly distributes into the tissues, with levels in some tissues exceeding those in the serum. Penetration into the central nervous system is relatively modest, with cerebrospinal fluid levels normally less than 10% of peak serum concentrations. The serum half-life of ciprofloxacin is about 4–6 hours, with 50-70% of an administered dose being excreted in the urine as unmetabolized drug. An additional 10% is excreted in urine as metabolites. Urinary excretion is virtually complete 24 hours after administration. Dose adjustment is required in the elderly and in those with renal impairment.

Ciprofloxacin is weakly bound to serum proteins (20-40%), but is an inhibitor of the drug-metabolizing enzyme cytochrome P450 1A2, which leads to the potential for clinically important drug interactions with drugs metabolized by that enzyme.

Ciprofloxacin is about 70% orally available when administered orally, so a slightly higher dose is needed to achieve the same exposure when switching from IV to oral administration

The extended release oral tablets allow once-daily administration by releasing the drug more slowly in the gastrointestinal tract. These tablets contain 35% of the administered dose in an immediate-release form and 65% in a slow-release matrix. Maximum serum concentrations are achieved between 1 and 4 hours after administration. Compared to the 250- and 500-mg immediate-release tablets, the 500-mg and 1000-mg XR tablets provide higher C, but the 24‑hour AUCs are equivalent.

Ciprofloxacin immediate-release tablets contain ciprofloxacin as the hydrochloride salt, and the XR tablets contain a mixture of the hydrochloride salt as the free base.

Ciprofloxacin is 1-cyclopropyl-6-fluoro-1,4-dihydro-4-oxo-7-(1-piperazinyl)-3-quinolinecarboxylic acid. Its empirical formula is CHFNO and its molecular weight is 331.4 g/mol. It is a faintly yellowish to light yellow crystalline substance.

Ciprofloxacin hydrochloride (USP) is the monohydrochloride monohydrate salt of ciprofloxacin. It is a faintly yellowish to light yellow crystalline substance with a molecular weight of 385.8 g/mol. Its empirical formula is CHFNOHCl•HO.

Ciprofloxacin is the most widely used of the second-generation quinolones. In 2010, over 20 million prescriptions were written, making it the 35th-most commonly prescribed generic drug and the 5th-most commonly prescribed antibacterial in the U.S.

The first members of the quinolone antibacterial class were relatively low-potency drugs such as nalidixic acid, used mainly in the treatment of urinary tract infections owing to their renal excretion and propensity to be concentrated in urine. In 1979, the publication of a patent filed by the pharmaceutical arm of Kyorin Seiyaku Kabushiki Kaisha disclosed the discovery of norfloxacin, and the demonstration that certain structural modifications including the attachment of a fluorine atom to the quinolone ring leads to dramatically enhanced antibacterial potency. In the aftermath of this disclosure, several other pharmaceutical companies initiated research and development programs with the goal of discovering additional antibacterial agents of the fluoroquinolone class.

The fluoroquinolone program at Bayer focused on examining the effects of very minor changes to the norfloxacin structure. In 1983, the company published "in vitro" potency data for ciprofloxacin, a fluoroquinolone antibacterial having a chemical structure differing from that of norfloxacin by the presence of a single carbon atom. This small change led to a two- to 10-fold increase in potency against most strains of Gram-negative bacteria. Importantly, this structural change led to a four-fold improvement in activity against the important Gram-negative pathogen "Pseudomonas aeruginosa", making ciprofloxacin one of the most potent known drugs for the treatment of this intrinsically antibiotic-resistant pathogen.

The oral tablet form of ciprofloxacin was approved in October 1987, just one year after the approval of norfloxacin. In 1991, the intravenous formulation was introduced. Ciprofloxacin sales reached a peak of about 2 billion euros in 2001, before Bayer's patent expired in 2004, after which annual sales have averaged around €200 million.

The name probably originates from the International Scientific Nomenclature: ci- (alteration of cycl-) + propyl + fluor- + ox- + az- + -mycin.

It is available as a generic medication and not very expensive. Wholesale it costs between 0.03 and 0.13 a dose. In the United States it is sold for about 0.40 per dose.

On 24 October 2001, the Prescription Access Litigation (PAL) project filed suit to dissolve an agreement between Bayer and three of its competitors which produced generic versions of drugs (Barr Laboratories, Rugby Laboratories, and Hoechst-Marion-Roussel) that PAL claimed was blocking access to adequate supplies and cheaper, generic versions of ciprofloxacin. The plaintiffs charged that Bayer Corporation, a unit of Bayer AG, had unlawfully paid the three competing companies a total of $200 million to prevent cheaper, generic versions of ciprofloxacin from being brought to the market, as well as manipulating its price and supply. Numerous other consumer advocacy groups joined the lawsuit. On 15 October 2008, five years after Bayer's patent had expired, the United States District Court for the Eastern District of New York granted Bayer's and the other defendants' motion for summary judgment, holding that any anticompetitive effects caused by the settlement agreements between Bayer and its codefendants were within the exclusionary zone of the patent and thus could not be redressed by federal antitrust law, in effect upholding Bayer's agreement with its competitors.

Ciprofloxacin for systemic administration is available as immediate-release tablets, as extended-release tablets, as an oral suspension, and as a solution for intravenous infusion. It is also available for local administration as eye drops and ear drops.

A class action was filed against Bayer AG on behalf of employees of the Brentwood Post Office in Washington, D.C., and workers at the U.S. Capitol, along with employees of American Media, Inc. in Florida and postal workers in general who alleged they suffered serious adverse effects from taking ciprofloxacin in the aftermath of the anthrax attacks in 2001. The action alleged Bayer failed to warn class members of the potential side effects of the drug, thereby violating the Pennsylvania Unfair Trade Practices and Consumer Protection Laws. The class action was defeated and the litigation abandoned by the plaintiffs. A similar action was filed in 2003 in New Jersey by four New Jersey postal workers but was withdrawn for lack of grounds, as workers had been informed of the risks of ciprofloxacin when they were given the option of taking the drug.

As resistance to ciprofloxacin has grown, research has been conducted to discover and develop analogs that can be effective against resistant bacteria; some have been looked at in antiviral models as well.



</doc>
<doc id="6774" url="https://en.wikipedia.org/wiki?curid=6774" title="Consubstantiation">
Consubstantiation

Consubstantiation is a Christian theological doctrine that (like transubstantiation) describes the real presence in the Eucharist. It holds that during the sacrament, the substance of the body and blood of Christ are present "alongside" the substance of the bread and wine, which remain present. 
It was part of the doctrines of Lollardy and considered a heresy by the Roman Catholic Church.

In the early church it was common for the bread to be regarded as bread yet also as the Body of Christ, and the wine regarded as wine yet also as the Blood of Christ; indeed, some argued that to deny that both bread and wine, and Body and Blood, were present was to deny the Incarnation:

In about 150, Justin Martyr, referring to the Eucharist, wrote: "Not as common bread and common drink do we receive these; but in like manner as Jesus Christ our Savior, having been made flesh by the Word of God, had both flesh and blood for our salvation, so likewise have we been taught that the food which is blessed by the prayer of His word, and from which our blood and flesh by transmutation are nourished, is the flesh and blood of that Jesus who was made flesh."

Justin Martyr wrote, in the "Dialogue with Trypho", ch 70: "Now it is evident, that in this prophecy [allusion is made] to the bread which our Christ gave us to eat, in remembrance of His being made flesh for the sake of His believers, for whom also He suffered; and to the cup which He gave us to drink, in remembrance of His own blood, with giving of thanks."

Irenaeus of Lyons wrote, "But what consistency is there in those who hold that the bread over which thanks have been given is the body of their Lord, and the cup his blood, if they do not acknowledge that He is the Son of the Creator… How can they say that the flesh which has been nourished by the Body of the Lord and by his blood gives way to corruption and does not partake of life? …For as the bread from the earth, receiving the invocation of God, is no longer common bread but the Eucharist, consisting of two elements, earthly and heavenly…”

The doctrine of consubstantiation is often held in contrast to the doctrine of transubstantiation. While some Lutherans use the term "consubstantiation" to describe their doctrine, many reject it as not accurately reflecting the eucharistic doctrine of Martin Luther, the sacramental union. They reject the concept of consubstantiation because it replaces what they believe to be the biblical doctrine with a philosophical construct, denotes a mixing of substances (bread and wine with body and blood), and denotes a "gross, Capernaitic, carnal" presence of the body and blood of Christ.

In England in the late 14th century, there was a political and religious movement known as Lollardy. Among much broader goals, the Lollards affirmed a form of consubstantiation—that the Eucharist remained physically bread and wine, while becoming spiritually the body and blood of Christ. Lollardy survived up until the time of the English Reformation.

Literary critic Kenneth Burke's dramatism takes this concept and utilizes it in secular rhetorical theory to look at the dialectic of unity and difference within the context of logology.



</doc>
<doc id="6775" url="https://en.wikipedia.org/wiki?curid=6775" title="Chlorophyta">
Chlorophyta

Chlorophyta or Prasinophyta is a taxon of green algae informally called chlorophytes. The name is used in two very different senses, so care is needed to determine the use by a particular author. In older classification systems, it refers to a highly paraphyletic group of "all" the green algae within the green plants (Viridiplantae) and thus includes about 7,000 species of mostly aquatic photosynthetic eukaryotic organisms. In newer classifications, it refers to the sister of the streptophytes/charophytes. The clade Streptophyta consists of the Charophyta in which the Embryophyta emerged. In this sense the Chlorophyta includes only about 4,300 species. About 90% of all known species live in freshwater.
Like the land plants (bryophytes and tracheophytes), green algae contain chlorophyll a and chlorophyll b and store food as starch in their plastids.

With the exception of Palmophyllophyceae, Trebouxiophyceae, Ulvophyceae and Chlorophyceae, which show various degrees of multicellularity, all the Chlorophyta lineages are unicellular. Some members of the group form symbiotic relationships with protozoa, sponges, and cnidarians. Others form symbiotic relationships with fungi to form lichens, but the majority of species are free-living. Some conduct sexual reproduction, which is oogamous or isogamous. All members of the clade have motile flagellated swimming cells. While most species live in freshwater habitats and a large number in marine habitats, other species are adapted to a wide range of land environments. For example, "Chlamydomonas nivalis", which causes Watermelon snow, lives on summer alpine snowfields. Others, such as "Trentepohlia" species, live attached to rocks or woody parts of trees. "Monostroma kuroshiense", an edible green alga cultivated worldwide and most expensive among green algae, belongs to this group.

Species of Chlorophyta (treated as what is now considered one of the two main clades of Viridiplantae) are common inhabitants of marine, freshwater and terrestrial environments. Several species have adapted to specialised and extreme environments, such as deserts, arctic environments, hypersaline habitats, marine deep waters, deep-sea hydrothermal vents and habitats that experiences extreme changes in temperature, light and salinity. Some groups, such as the Trentepohliales are exclusively found on land. Several species of Chlorophyta live in symbiosis with a diverse range of eukaryotes, including fungi (to form lichens), ciliates, forams, cnidarians and molluscs.

Characteristics used for the classification of Chlorophyta are: type of zoid, mitosis (karyokynesis), cytokinesis, organization level, life cycle, type of gametes, cell wall polysaccharides and more recently genetic data.

A newer proposed classification follows Leliaert et al. 2011 and modified with Silar 2016, Leliaert 2016 and Lopes dos Santos et al. 2017 for the green algae clades and Novíkov & Barabaš-Krasni 2015 for the land plants clade. Sánchez-Baracaldo et al. is followed for the basal clades.

Simplified phylogeny of the Chlorophyta, according to Leliaert "et al". 2012. Note that many algae previously classified in Chlorophyta are placed here in Streptophyta.

A possible classification when Chlorophyta refers to one of the two clades of the Viridiplantae is shown below.


Classification of the Chlorophyta, treated as all green algae, according to Hoek, Mann and Jahns 1995.

In a note added in proof, an alternative classification is presented for the algae of the class Chlorophyceae:

Classification of the Chlorophyta and Charophyta according to Bold and Wynne 1985.


Classification of the Chlorophyta according to Mattox & Stewart 1984:


Classification of the Chlorophyta according to Fott 1971.

Classification of the Chlorophyta and related algae according to Round 1971.


Classification of the Chlorophyta according to Smith 1938:



</doc>
<doc id="6776" url="https://en.wikipedia.org/wiki?curid=6776" title="Capybara">
Capybara

The capybara ("Hydrochoerus hydrochaeris") is a mammal native to South America. It is the largest living rodent in the world. Also called chigüire, chigüiro (in Colombia and Venezuela) and carpincho, it is a member of the genus "Hydrochoerus", of which the only other extant member is the lesser capybara ("Hydrochoerus isthmius"). Its close relatives include guinea pigs and rock cavies, and it is more distantly related to the agouti, the chinchilla, and the coypu. The capybara inhabits savannas and dense forests and lives near bodies of water. It is a highly social species and can be found in groups as large as 100 individuals, but usually lives in groups of 10–20 individuals. The capybara is not a threatened species but it is hunted for its meat and hide and also for grease from its thick fatty skin, which is used in the pharmaceutical trade.

Its common name is derived from Tupi ', a complex agglutination of ' (leaf) + ' (slender) + ' (eat) + "" (a suffix for agent nouns), meaning "one who eats slender leaves", or "grass-eater". Capybaras were called several times "Cuartins" in Colombia in 2018: in Eje Cafetero (Alcalà) and near Barranquilla where the meat was offered as "Cuartin Asado".

The scientific name, both "hydrochoerus" and "hydrochaeris", comes from Greek (' "water") and (' "pig, hog").

The capybara and the lesser capybara belong to the subfamily Hydrochoerinae along with the rock cavies. The living capybaras and their extinct relatives were previously classified in their own family Hydrochoeridae. Since 2002, molecular phylogenetic studies have recognized a close relationship between "Hydrochoerus" and "Kerodon", the rock cavies, supporting placement of both genera in a subfamily of Caviidae.

Paleontological classifications previously used Hydrochoeridae for all capybaras, while using Hydrochoerinae for the living genus and its closest fossil relatives, such as "Neochoerus", but more recently have adopted the classification of Hydrochoerinae within Caviidae. The taxonomy of fossil hydrochoerines is also in a state of flux. In recent years, the diversity of fossil hydrochoerines has been substantially reduced. This is largely due to the recognition that capybara molar teeth show strong variation in shape over the life of an individual. In one instance, material once referred to four genera and seven species on the basis of differences in molar shape is now thought to represent differently aged individuals of a single species, "Cardiatherium paranense".
Among fossil species, the name "capybara" can refer to the many species of Hydrochoerinae that are more closely related to the modern "Hydrochoerus" than to the "cardiomyine" rodents like "Cardiomys". The fossil genera "Cardiatherium", "Phugatherium", "Hydrochoeropsis", and "Neochoerus" are all capybaras under that concept.

The capybara has a heavy, barrel-shaped body and short head, with reddish-brown fur on the upper part of its body that turns yellowish-brown underneath. Its sweat glands can be found in the surface of the hairy portions of its skin, an unusual trait among rodents. The animal lacks down hair, and its guard hair differs little from over hair.

Adult capybaras grow to in length, stand tall at the withers, and typically weigh , with an average in the Venezuelan llanos of . Females are slightly heavier than males. The top recorded weights are for a wild female from Brazil and for a wild male from Uruguay. Also an 81 kg individual was reported in São Paulo in 2001 or 2002. The dental formula is . Capybaras have slightly webbed feet and vestigial tails. Their hind legs are slightly longer than their forelegs; they have three toes on their rear feet and four toes on their front feet. Their muzzles are blunt, with nostrils, and the eyes and ears are near the top of their heads.

Its karyotype has 2n = 66 and FN = 102.

Capybaras are semiaquatic mammals found throughout almost all countries of South America except Chile. They live in densely forested areas near bodies of water, such as lakes, rivers, swamps, ponds, and marshes, as well as flooded savannah and along rivers in the tropical rainforest. They are superb swimmers and can hold their breath underwater for up to five minutes at a time. Capybara have flourished in cattle ranches. They roam in home ranges averaging 10 hectares (25 acres) in high-density populations.

Many escapees from captivity can also be found in similar watery habitats around the world. Sightings are fairly common in Florida, although a breeding population has not yet been confirmed. In 2011, one specimen was spotted on the Central Coast of California.

Capybaras are herbivores, grazing mainly on grasses and aquatic plants, as well as fruit and tree bark. They are very selective feeders and feed on the leaves of one species and disregard other species surrounding it. They eat a greater variety of plants during the dry season, as fewer plants are available. While they eat grass during the wet season, they have to switch to more abundant reeds during the dry season. Plants that capybaras eat during the summer lose their nutritional value in the winter, so are not consumed at that time. The capybara's jaw hinge is not perpendicular, so they chew food by grinding back-and-forth rather than side-to-side. Capybaras are autocoprophagous, meaning they eat their own feces as a source of bacterial gut flora, to help digest the cellulose in the grass that forms their normal diet, and to extract the maximum protein and vitamins from their food. They may also regurgitate food to masticate again, similar to cud-chewing by cattle. As is the case with other rodents, the front teeth of capybaras grow continually to compensate for the constant wear from eating grasses; their cheek teeth also grow continuously.

Like its relative the guinea pig, the capybara does not have the capacity to synthesize vitamin C, and capybaras not supplemented with vitamin C in captivity have been reported to develop gum disease as a sign of scurvy.

They can have a lifespan of 8–10 years, but live less than four years in the wild, because they are "a favourite food of jaguar, puma, ocelot, eagle, and caiman". The capybara is also the preferred prey of the anaconda.

Capybaras are known to be gregarious. While they sometimes live solitarily, they are more commonly found in groups of around 10–20 individuals, with two to four adult males, four to seven adult females, and the remainder juveniles. Capybara groups can consist of as many as 50 or 100 individuals during the dry season when the animals gather around available water sources. Males establish social bonds, dominance, or general group consensus. They can make dog-like barks when threatened or when females are herding young.

Capybaras have two types of scent glands; a morillo (), located on the snout, and anal glands. Both sexes have these glands, but males have much larger morillos and use their anal glands more frequently. The anal glands of males are also lined with detachable hairs. A crystalline form of scent secretion is coated on these hairs and is released when in contact with objects such as plants. These hairs have a longer-lasting scent mark and are tasted by other capybaras. Capybaras scent-mark by rubbing their morillos on objects, or by walking over scrub and marking it with their anal glands. Capybaras can spread their scent further by urinating; however, females usually mark without urinating and scent-mark less frequently than males overall. Females mark more often during the wet season when they are in estrus. In addition to objects, males also scent-mark females.

When in estrus, the female's scent changes subtly and nearby males begin pursuit. In addition, a female alerts males she is in estrus by whistling through her nose. During mating, the female has the advantage and mating choice. Capybaras mate only in water, and if a female does not want to mate with a certain male, she either submerges or leaves the water. Dominant males are highly protective of the females, but they usually cannot prevent some of the subordinates from copulating. The larger the group, the harder it is for the male to watch all the females. Dominant males secure significantly more matings than each subordinate, but subordinate males, as a class, are responsible for more matings than each dominant male. The lifespan of the capybara's sperm is longer than that of other rodents.

Capybara gestation is 130–150 days, and produces a litter of four capybara young on average, but may produce between one and eight in a single litter. Birth is on land and the female rejoins the group within a few hours of delivering the newborn capybaras, which join the group as soon as they are mobile. Within a week, the young can eat grass, but continue to suckle—from any female in the group—until weaned around 16 weeks. The young form a group within the main group. Alloparenting has been observed in this species. Breeding peaks between April and May in Venezuela and between October and November in Mato Grosso, Brazil.

Though quite agile on land (capable of running as fast as a horse), capybaras are equally at home in the water. They are excellent swimmers, and can remain completely submerged for up to five minutes, an ability they use to evade predators. Capybaras can sleep in water, keeping only their noses out of the water. As temperatures increase during the day, they wallow in water and then graze during the late afternoon and early evening. They also spend time wallowing in mud. They rest around midnight and then continue to graze before dawn.

Capybaras are not considered a threatened species; their population is stable throughout most of their South American range, though in some areas hunting has reduced their numbers.

Capybaras are hunted for their meat and pelts in some areas, and otherwise killed by humans who see their grazing as competition for livestock. In some areas, they are farmed, which has the effect of ensuring the wetland habitats are protected. Their survival is aided by their ability to breed rapidly.

Capybaras have adapted well to urbanization in South America. They can be found in many areas in zoos and parks, and may live for 12 years in captivity. Capybaras are gentle and usually allow humans to pet and hand-feed them, but physical contact is normally discouraged, as their ticks can be vectors to Rocky Mountain spotted fever.

The European Association of Zoos and Aquaria asked Drusillas Park in Alfriston, Sussex, England to keep the studbook for capybaras, to monitor captive populations in Europe. The studbook includes information about all births, deaths and movements of capybaras, as well as how they are related.

Capybaras are farmed for meat and skins in South America. The meat is considered unsuitable to eat in some areas, while in other areas it is considered an important source of protein. In parts of South America, especially in Venezuela, capybara meat is popular during Lent and Holy Week as the Catholic Church previously issued special dispensation to allow it to be eaten while other meats are generally forbidden.

Although it is illegal in some states, capybaras are occasionally kept as pets in the United States.

The image of a capybara features on the 2-peso coin of Uruguay.

In Japan, following the lead of Izu Shaboten Park in 1982, multiple establishments in Japan that raise capybaras have adopted the practice of having them relax in onsen during the winter.




</doc>
<doc id="6777" url="https://en.wikipedia.org/wiki?curid=6777" title="Computer animation">
Computer animation

Computer animation is the process used for digitally generating animated images. The more general term computer-generated imagery (CGI) encompasses both static scenes and dynamic images, while computer animation "only" refers to the moving images. Modern computer animation usually uses 3D computer graphics, although 2D computer graphics are still used for stylistic, low bandwidth, and faster real-time renderings. Sometimes, the target of the animation is the computer itself, but sometimes film as well.

Computer animation is essentially a digital successor to the stop motion techniques using 3D models, and traditional animation techniques using frame-by-frame animation of 2D illustrations. Computer-generated animations are more controllable than other more physically based processes, constructing miniatures for effects shots or hiring extras for crowd scenes, and because it allows the creation of images that would not be feasible using any other technology. It can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it, but advanced slightly in time (usually at a rate of 24, 25 or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures.

For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. For 2D figure animations, separate objects (illustrations) and separate transparent layers are used with or without that virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.

For 3D animations, all frames must be rendered after the modeling is complete. For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use software on the end-users computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.

To trick the eye and the brain into thinking they are seeing a smoothly moving object, the pictures should be drawn at around 12 frames per second or faster. (A frame is one complete image.) With rates above 75-120 frames per second, no improvement in realism or smoothness is perceivable due to the way the eye and the brain both process images. At rates below 12 frames per second, most people can detect jerkiness associated with the drawing of new images that detracts from the illusion of realistic movement. Conventional hand-drawn cartoon animation often uses 15 frames per second in order to save on the number of drawings needed, but this is usually accepted because of the stylized nature of cartoons. To produce more realistic imagery, computer animation demands higher frame rates.

Films seen in theaters in the United States run at 24 frames per second, which is sufficient to create the illusion of continuous movement. For high resolution, adapters are used.

Early digital computer animation was developed at Bell Telephone Laboratories in the 1960s by Edward E. Zajac, Frank W. Sinden, Kenneth C. Knowlton, and A. Michael Noll. Other digital animation was also practiced at the Lawrence Livermore National Laboratory.

In 1967, a computer animation named "Hummingbird" was created by Charles Csuri and James Shaffer. 

In 1968, a computer animation called "" was created with BESM-4 by Nikolai Konstantinov, depicting a cat moving around. 

In 1971, a computer animation called "Metadata" was created, showing various shapes. 

An early step in the history of computer animation was the sequel to the 1973 film "Westworld," a science-fiction film about a society in which robots live and work among humans. The sequel, "Futureworld" (1976), used the 3D wire-frame imagery, which featured a computer-animated hand and face both created by University of Utah graduates Edwin Catmull and Fred Parke. This imagery originally appeared in their student film "A Computer Animated Hand", which they completed in 1972.

Developments in CGI technologies are reported each year at SIGGRAPH, an annual conference on computer graphics and interactive techniques that is attended by thousands of computer professionals each year. Developers of computer games and 3D video cards strive to achieve the same visual quality on personal computers in real-time as is possible for CGI films and animation. With the rapid advancement of real-time rendering quality, artists began to use game engines to render non-interactive movies, which led to the art form Machinima.

The very first full length computer animated television series was "ReBoot", which debuted in September 1994; the series followed the adventures of characters who lived inside a computer. The first feature-length computer animated film was "Toy Story" (1995), which was made by Pixar. It followed an adventure centered around toys and their owners. This groundbreaking film was also the first of many fully computer-animated movies.

In most 3D computer animation systems, an animator creates a simplified representation of a character's anatomy, which is analogous to a skeleton or stick figure. They are by default arranged into a default position known as a bind pose. The position of each segment of the skeletal model is defined by animation variables, or Avars for short. In human and animal characters, many parts of the skeletal model correspond to the actual bones, but skeletal animation is also used to animate other things, with facial features (though other methods for facial animation exist). The character "Woody" in "Toy Story", for example, uses 700 Avars (100 in the face alone). The computer doesn't usually render the skeletal model directly (it is invisible), but it does use the skeletal model to compute the exact position and orientation of that certain character, which is eventually rendered into an image. Thus by changing the values of Avars over time, the animator creates motion by making the character move from frame to frame.

There are several methods for generating the Avar values to obtain realistic motion. Traditionally, animators manipulate the Avars directly. Rather than set Avars for every frame, they usually set Avars at strategic points (frames) in time and let the computer interpolate or tween between them in a process called "keyframing". Keyframing puts control in the hands of the animator and has roots in hand-drawn traditional animation.

In contrast, a newer method called "motion capture" makes use of live action footage. When computer animation is driven by motion capture, a real performer acts out the scene as if they were the character to be animated. His/her motion is recorded to a computer using video cameras and markers and that performance is then applied to the animated character.

Each method has its advantages and as of 2007, games and films are using either or both of these methods in productions. Keyframe animation can produce motions that would be difficult or impossible to act out, while motion capture can reproduce the subtleties of a particular actor. For example, in the 2006 film "", Bill Nighy provided the performance for the character Davy Jones. Even though Nighy doesn't appear in the movie himself, the movie benefited from his performance by recording the nuances of his body language, posture, facial expressions, etc. Thus motion capture is appropriate in situations where believable, realistic behavior and action is required, but the types of characters required exceed what can be done throughout the conventional costuming.

3D computer animation combines 3D models of objects and programmed or hand "keyframed" movement. These models are constructed out of geometrical vertices, faces, and edges in a 3D coordinate system. Objects are sculpted much like real clay or plaster, working from general forms to specific details with various sculpting tools. Unless a 3D model is intended to be a solid color, it must be painted with "textures" for realism. A bone/joint animation system is set up to deform the CGI model (e.g., to make a humanoid model walk). In a process known as "rigging", the virtual marionette is given various controllers and handles for controlling movement. Animation data can be created using motion capture, or keyframing by a human animator, or a combination of the two.

3D models rigged for animation may contain thousands of control points — for example, "Woody" from "Toy Story" uses 700 specialized animation controllers. Rhythm and Hues Studios labored for two years to create Aslan in the movie "", which had about 1,851 controllers (742 in the face alone). In the 2004 film "The Day After Tomorrow", designers had to design forces of extreme weather with the help of video references and accurate meteorological facts. For the 2005 remake of "King Kong", actor Andy Serkis was used to help designers pinpoint the gorilla's prime location in the shots and used his expressions to model "human" characteristics onto the creature. Serkis had earlier provided the voice and performance for Gollum in J. R. R. Tolkien's "The Lord of the Rings" trilogy.

Computer animation can be created with a computer and an animation software. Some impressive animation can be achieved even with basic programs; however, the rendering can take a lot of time on an ordinary home computer. Professional animators of movies, television and video games could make photorealistic animation with high detail. This level of quality for movie animation would take hundreds of years to create on a home computer. Instead, many powerful workstation computers are used. Graphics workstation computers use two to four processors, and they are a lot more powerful than an actual home computer and are specialized for rendering. A large number of workstations (known as a ""render farm"") are networked together to effectively act as a giant computer. The result is a computer-animated movie that can be completed in about one to five years (however, this process is not composed solely of rendering). A workstation typically costs $2,000-16,000 with the more expensive stations being able to render much faster due to the more technologically-advanced hardware that they contain. Professionals also use digital movie cameras, motion/performance capture, bluescreens, film editing software, props, and other tools used for movie animation. Programs like Blendr allow for people who cant afford expensive animation and rendering software to be able to work in a similar manner to those who use the commercial grade equipment. 

The realistic modeling of human facial features is both one of the most challenging and sought after elements in computer-generated imagery. Computer facial animation is a highly complex field where models typically include a very large number of animation variables. Historically speaking, the first SIGGRAPH tutorials on "State of the art in Facial Animation" in 1989 and 1990 proved to be a turning point in the field by bringing together and consolidating multiple research elements and sparked interest among a number of researchers.

The Facial Action Coding System (with 46 "action units", "lip bite" or "squint"), which had been developed in 1976, became a popular basis for many systems. As early as 2001, MPEG-4 included 68 Face Animation Parameters (FAPs) for lips, jaws, etc., and the field has made significant progress since then and the use of facial microexpression has increased.

In some cases, an affective space, the PAD emotional state model, can be used to assign specific emotions to the faces of avatars. In this approach, the PAD model is used as a high level emotional space and the lower level space is the MPEG-4 Facial Animation Parameters (FAP). A mid-level Partial Expression Parameters (PEP) space is then used to in a two-level structure – the PAD-PEP mapping and the PEP-FAP translation model.

Realism in computer animation can mean making each frame look photorealistic, in the sense that the scene is rendered to resemble a photograph or make the characters' animation believable and lifelike. Computer animation can also be realistic with or without the photorealistic rendering.

One of the greatest challenges in computer animation has been creating human characters that look and move with the highest degree of realism. Part of the difficulty in making pleasing, realistic human characters is the uncanny valley, the concept where the human audience (up to a point) tends to have an increasingly negative, emotional response as a human replica looks and acts more and more human. Films that have attempted photorealistic human characters, such as "The Polar Express", "Beowulf", and "A Christmas Carol"
have been criticized as "creepy" and "disconcerting".

The goal of computer animation is not always to emulate live action as closely as possible, so many animated films instead feature characters who are anthropomorphic animals, fantasy creatures and characters, superheroes, or otherwise have non-realistic, cartoon-like proportions. Computer animation can also be tailored to mimic or substitute for other kinds of animation, like traditional stop-motion animation (as shown in "Flushed Away" or "The Lego Movie"). Some of the long-standing basic principles of animation, like squash & stretch, call for movement that is not strictly realistic, and such principles still see widespread application in computer animation.

CGI short films have been produced as independent animation since 1976. An early example of an animated feature film to incorporate CGI animation was the 1983 Japanese anime film "". The popularity of computer animation (especially in the field of special effects) skyrocketed during the modern era of U.S. animation. The first completely computer-animated movie was "Toy Story" (1995), but "VeggieTales" is the first American fully 3D computer animated series sold directly (made in 1993); its success inspired other animation series, such as "ReBoot" in 1994. While other films like Avatar used CGI for a majority of the movie while still incorporating human actors into the mix.

Some notable producers of computer-animated feature films include:


The popularity of websites that allow members to upload their own movies for others to view has created a growing community of amateur computer animators. With utilities and programs often included free with modern operating systems, many users can make their own animated movies and shorts. Several free and open-source animation software applications exist as well. The ease at which these animations can be distributed has attracted professional animation talent also. Companies such as PowToon and GoAnimate attempt to bridged the gap by giving amateurs access to professional animations as clip art.

The oldest (most backward compatible) web-based animations are in the animated GIF format, which can be uploaded and seen on the web easily. However, the raster graphics format of GIF animations slows the download and frame rate, especially with larger screen sizes. The growing demand for higher quality web-based animations was met by a vector graphics alternative that relied on the use of a plugin. For decades, Flash animations were the most popular format, until the web development community abandoned support for the Flash player plugin. Web browsers on mobile devices and mobile operating systems never fully supported the Flash plugin.

By this time, internet bandwidth and download speeds increased, making raster graphic animations more convenient. Some of the more complex vector graphic animations had a slower frame rate due to complex rendering than some of the raster graphic alternatives. Many of the GIF and Flash animations were already converted to digital video formats, which were compatible with mobile devices and reduced file sizes via video compression technology. However, compatibility was still problematic as some of the popular video formats such as Apple's QuickTime and Microsoft Silverlight required plugins. YouTube, the most popular video viewing website, was also relying on the Flash plugin to deliver digital video in the Flash Video format.

The latest alternatives are HTML5 compatible animations. Technologies such as JavaScript and CSS animations made sequencing the movement of images in HTML5 web pages more convenient. SVG animations offered a vector graphic alternative to the original Flash graphic format, SmartSketch. YouTube offers an HTML5 alternative for digital video. APNG (Animated PNG) offered a raster graphic alternative to animated GIF files that enables multi-level transparency not available in GIFs

In 2D computer animation, moving objects are often referred to as "sprites." A sprite is an image that has a location associated with it. The location of the sprite is changed slightly, between each displayed frame, to make the sprite appear to move. The following pseudocode makes a sprite move from left to right:

Computer animation uses different techniques to produce animations. Most frequently, sophisticated mathematics is used to manipulate complex three-dimensional polygons, apply "textures", lighting and other effects to the polygons and finally rendering the complete image. A sophisticated graphical user interface may be used to create the animation and arrange its choreography. Another technique called constructive solid geometry defines objects by conducting boolean operations on regular shapes, and has the advantage that animations may be accurately produced at any resolution.

"To animate means, figuratively, to "give life to". There are two basic methods that animators commonly use to accomplish this."

Computer-assisted animation is usually classed as two-dimensional (2D) animation. Drawings are either hand drawn (pencil to paper) or interactively drawn (on the computer) using different assisting appliances and are positioned into specific software packages. Within the software package, the creator places drawings into different key frames which fundamentally create an outline of the most important movements. The computer then fills in the "in-between frames", a process commonly known as Tweening. Computer-assisted animation employs new technologies to produce content faster than is possible with traditional animation, while still retaining the stylistic elements of traditionally drawn characters or objects.

Examples of films produced using computer-assisted animation are "The Little Mermaid", "The Rescuers Down Under", "Beauty and the Beast", "Aladdin", "The Lion King", "Pocahontas", "The Hunchback of Notre Dame", "Hercules", "Mulan", "The Road to El Dorado" and "Tarzan".

Computer-generated animation is known as three-dimensional (3D) animation. Creators design an object or character with an X, a Y and a Z axis. No pencil-to-paper drawings create the way computer generated animation works. The object or character created will then be taken into a software, key framing and tweening are also carried out in computer generated animation but are also a lot of techniques used that do not relate to traditional animation. Animators can break physical laws by using mathematical algorithms to cheat mass, force and gravity rulings. Fundamentally, time scale and quality could be said to be a preferred way to produce animation as they are two major things that are enhanced by using computer generated animation. Another positive aspect of CGA is the fact one can create a flock of creatures to act independently when created as a group. An animal's fur can be programmed to wave in the wind and lie flat when it rains instead of programming each strand of hair separately.

A few examples of computer-generated animation movies are "Toy Story", "Frozen", and "Shrek". 




</doc>
<doc id="6778" url="https://en.wikipedia.org/wiki?curid=6778" title="Ceawlin of Wessex">
Ceawlin of Wessex

Ceawlin (also spelled Ceaulin and Caelin, died "ca." 593) was a King of Wessex. He may have been the son of Cynric of Wessex and the grandson of Cerdic of Wessex, whom the "Anglo-Saxon Chronicle" represents as the leader of the first group of Saxons to come to the land which later became Wessex. Ceawlin was active during the last years of the Anglo-Saxon expansion, with little of southern England remaining in the control of the native Britons by the time of his death.

The chronology of Ceawlin's life is highly uncertain. The historical accuracy and dating of many of the events in the later "Anglo-Saxon Chronicle" have been called into question, and his reign is variously listed as lasting seven, seventeen, or thirty-two years. The "Chronicle" records several battles of Ceawlin's between the years 556 and 592, including the first record of a battle between different groups of Anglo-Saxons, and indicates that under Ceawlin Wessex acquired significant territory, some of which was later to be lost to other Anglo-Saxon kingdoms. Ceawlin is also named as one of the eight ""bretwaldas"", a title given in the "Chronicle" to eight rulers who had overlordship over southern Britain, although the extent of Ceawlin's control is not known.

Ceawlin died in 593, having been deposed the year before, possibly by his successor, Ceol. He is recorded in various sources as having two sons, Cutha and Cuthwine, but the genealogies in which this information is found are known to be unreliable.

The history of the sub-Roman period in Britain is poorly sourced and the subject of a number of important disagreements among historians. It appears, however, that in the fifth century raids on Britain by continental peoples developed into migrations. The newcomers included Angles, Saxons, Jutes, and Frisians. These peoples captured territory in the east and south of England, but at about the end of the fifth century, a British victory at the battle of Mons Badonicus halted the Anglo-Saxon advance for fifty years. Near the year 550, however, the British began to lose ground once more, and within twenty-five years, it appears that control of almost all of southern England was in the hands of the invaders.

The peace following the battle of Mons Badonicus is attested partly by Gildas, a monk, who wrote "De Excidio et Conquestu Britanniae" or "On the Ruin and Conquest of Britain" during the middle of the sixth century. This essay is a polemic against corruption and Gildas provides little in the way of names and dates. He appears, however, to state that peace had lasted from the year of his birth to the time he was writing. The "Anglo-Saxon Chronicle" is the other main source that bears on this period, in particular in an entry for the year 827 that records a list of the kings who bore the title ""bretwalda"", or "Britain-ruler". That list shows a gap in the early sixth century that matches Gildas's version of events.

Ceawlin's reign belongs to the period of Anglo-Saxon expansion at the end of the sixth century. Though there are many unanswered questions about the chronology and activities of the early West Saxon rulers, it is clear that Ceawlin was one of the key figures in the final Anglo-Saxon conquest of southern Britain.

The two main written sources for early West Saxon history are the "Anglo-Saxon Chronicle" and the West Saxon Genealogical Regnal List. The "Chronicle" is a set of annals which were compiled near the year 890, during the reign of King Alfred the Great of Wessex. They record earlier material for the older entries, which were assembled from earlier annals that no longer survive, as well as from saga material that might have been transmitted orally. The "Chronicle" dates the arrival of the future "West Saxons" in Britain to 495, when Cerdic and his son, Cynric, land at "Cerdices ora", or Cerdic's shore. Almost twenty annals describing Cerdic's campaigns and those of his descendants appear interspersed through the next hundred years of entries in the "Chronicle". Although these annals provide most of what is known about Ceawlin, the historicity of many of the entries is uncertain.

The West Saxon Genealogical Regnal List is a list of rulers of Wessex, including the lengths of their reigns. It survives in several forms, including as a preface to the [B] manuscript of the "Chronicle". As with the "Chronicle", the list was compiled during the reign of Alfred the Great, and both the list and the "Chronicle" are influenced by the desire of their writers to use a single line of descent to trace the lineage of the Kings of Wessex through Cerdic to Gewis, the legendary eponymous ancestor of the West Saxons, who is made to descend from Woden. The result served the political purposes of the scribe, but is riddled with contradictions for historians.

The contradictions may be seen clearly by calculating dates by different methods from the various sources. The first event in West Saxon history, the date of which can be regarded as reasonably certain, is the baptism of Cynegils, which occurred in the late 630s, perhaps as late as 640. The "Chronicle" dates Cerdic's arrival to 495, but adding up the lengths of the reigns as given in the West Saxon Genealogical Regnal List leads to the conclusion that Cerdic's reign might have started in 532, a difference of 37 years. Neither 495 nor 532 may be treated as reliable, however--the latter date relies on the presumption that the Regnal List is correct in presenting the Kings of Wessex as having succeeded one another, with no omitted kings, and no joint kingships, and that the durations of the reigns are correct as given. None of these presumptions may be made safely.

The sources also are inconsistent on the length of Ceawlin's reign. The "Chronicle" gives it as thirty-two years, from 560 to 592, but the Regnal Lists disagree: different versions give it as seven or seventeen years. A recent detailed study of the Regnal List dates the arrival of the West Saxons in England to 538, and favours seven years as the most likely length of Ceawlin's reign, with dates of 581–588 proposed. The sources do agree that Ceawlin is the son of Cynric and he usually is named as the father of Cuthwine. There is one discrepancy in this case: the entry for 685 in the [A] version of the "Chronicle" assigns Ceawlin a son, Cutha, but in the 855 entry in the same manuscript, Cutha is listed as the son of Cuthwine. Cutha also is named as Ceawlin's brother in the [E] and [F] versions of the "Chronicle", in the 571 and 568 entries, respectively.

Whether Ceawlin is a descendant of Cerdic is a matter of debate. Subgroupings of different West Saxon lineages give the impression of separate groups, of which Ceawlin's line is one. Some of the problems in the Wessex genealogies may have come about because of efforts to integrate Ceawlin's line with the other lineages: it was very important to the West Saxons to be able to trace their ancestors back to Cerdic. Another reason for doubting the literal nature of these early genealogies is that the etymology of the names of several early members of the dynasty do not appear to be Germanic, as would be expected in the names of leaders of an apparently Anglo-Saxon dynasty. The name Ceawlin is one of the names that do not have convincing Anglo-Saxon etymologies; it seems more likely to be of native British origin.

The earliest sources do not use the term "West Saxon". According to Bede's "Ecclesiastical History of the English People", the term is interchangeable with the Gewisse. The term "West Saxon" appears only in the late seventh century, after the reign of Cædwalla.

Ultimately, the kingdom of Wessex occupied the southwest of England, but the initial stages in this expansion are not apparent from the sources. Cerdic's landing, whenever it is to be dated, seems to have been near the Isle of Wight, and the annals record the conquest of the island in 530. In 534, according to the "Chronicle", Cerdic died and his son Cynric took the throne; the "Chronicle" adds that "they gave the Isle of Wight to their nephews, Stuf and Wihtgar". These records are in direct conflict with Bede, who states that the Isle of Wight was settled by Jutes, not Saxons; the archaeological record is somewhat in favour of Bede on this.

Subsequent entries in the "Chronicle" give details of some of the battles by which the West Saxons won their kingdom. Ceawlin's campaigns are not given as near the coast. They range along the Thames valley and beyond, as far as Surrey in the east and the mouth of the Severn in the west. Ceawlin clearly is part of the West Saxon expansion, but the military history of the period is difficult to understand. In what follows the dates are as given in the "Chronicle", although, as noted above, these are earlier than now thought accurate.

The first record of a battle fought by Ceawlin is in 556, when he and his father, Cynric, fought the native Britons at "Beran byrg", or Bera's Stronghold. This now is identified as Barbury Castle, an Iron Age hill fort in Wiltshire, near Swindon. Cynric would have been king of Wessex at this time.

The first battle Ceawlin fought as king is dated by the "Chronicle" to 568, when he and Cutha fought with Æthelberht, the king of Kent. The entry says "Here Ceawlin and Cutha fought against Aethelberht and drove him into Kent; and they killed two ealdormen, Oslaf and Cnebba, on Wibbandun." The location of "Wibbandun", which can be translated as "Wibba's Mount", has not been identified definitely; it was at one time thought to be Wimbledon, but this now is known to be incorrect. This battle is notable as the first recorded conflict between the invading peoples: previous battles recorded in the "Chronicle" are between the Anglo-Saxons and the native Britons.

There are multiple examples of joint kingship in Anglo-Saxon history, and this may be another: it is not clear what Cutha's relationship to Ceawlin is, but it certainly is possible he was also a king. The annal for 577, below, is another possible example.

The annal for 571 reads: "Here Cuthwulf fought against the Britons at Bedcanford, and took four settlements: Limbury and Aylesbury, Benson and Eynsham; and in the same year he passed away." Cuthwulf's relationship with Ceawlin is unknown, but the alliteration common to Anglo-Saxon royal families suggests Cuthwulf may be part of the West Saxon royal line. The location of the battle itself is unidentified. It has been suggested that it was Bedford, but what is known of the early history of Bedford's names does not support this. This battle is of interest because it is surprising that an area so far east should still be in Briton hands this late: there is ample archaeological evidence of early Saxon and Anglian presence in the Midlands, and historians generally have interpreted Gildas's "De Excidio" as implying that the Britons had lost control of this area by the mid-sixth century. One possible explanation is that this annal records a reconquest of land that was lost to the Britons in the campaigns ending in the battle of Mons Badonicus.

The annal for 577 reads "Here Cuthwine and Ceawlin fought against the Britons, and they killed three kings, Coinmail and Condidan and Farinmail, in the place which is called Dyrham, and took three cities: Gloucester and Cirencester and Bath." This entry is all that is known of these Briton kings; their names are in an archaic form that makes it very likely that this annal derives from a much older written source. The battle itself has long been regarded as a key moment in the Saxon advance, since in reaching the Bristol Channel, the West Saxons divided the Britons west of the Severn from land communication with those in the peninsula to the south of the Channel. Wessex almost certainly lost this territory to Penda of Mercia in 628, when the "Chronicle" records that "Cynegils and Cwichelm fought against Penda at Cirencester and then came to an agreement."

It is possible that when Ceawlin and Cuthwine took Bath, they found the Roman baths still operating to some extent. Nennius, a ninth-century historian, mentions a "Hot Lake" in the land of the Hwicce, which was along the Severn, and adds "It is surrounded by a wall, made of brick and stone, and men may go there to bathe at any time, and every man can have the kind of bath he likes. If he wants, it will be a cold bath; and if he wants a hot bath, it will be hot". Bede also describes hot baths in the geographical introduction to the "Ecclesiastical History" in terms very similar to those of Nennius.

Wansdyke, an early medieval defensive linear earthwork, runs from south of Bristol to near Marlborough, Wiltshire, passing not far from Bath. It probably was built in the fifth or sixth centuries, perhaps by Ceawlin.

Ceawlin's last recorded victory is in 584. The entry reads "Here Ceawlin and Cutha fought against the Britons at the place which is named Fethan leag, and Cutha was killed; and Ceawlin took many towns and countless war-loot, and in anger he turned back to his own [territory]." There is a wood named "Fethelée" mentioned in a twelfth-century document that relates to Stoke Lyne, in Oxfordshire, and it now is thought that the battle of Fethan leag must have been fought in this area.

The phrase "in anger he turned back to his own" probably indicates that this annal is drawn from saga material, as perhaps are all of the early Wessex annals. It also has been used to argue that perhaps, Ceawlin did not win the battle and that the chronicler chose not to record the outcome fully – a king does not usually come home "in anger" after taking "many towns and countless war-loot". It may be that Ceawlin's overlordship of the southern Britons came to an end with this battle.

About 731, Bede, a Northumbrian monk and chronicler, wrote a work called the "Ecclesiastical History of the English People". The work was not primarily a secular history, but Bede provides much information about the history of the Anglo-Saxons, including a list early in the history of seven kings who, he said, held "imperium" over the other kingdoms south of the Humber. The usual translation for "imperium" is "overlordship". Bede names Ceawlin as the second on the list, although he spells it "Caelin", and adds that he was "known in the speech of his own people as Ceaulin". Bede also makes it clear that Ceawlin was not a Christian—Bede mentions a later king, Æthelberht of Kent, as "the first to enter the kingdom of heaven".

The "Anglo-Saxon Chronicle," in an entry for the year 827, repeats Bede's list, adds Egbert of Wessex, and also mentions that they were known as "bretwalda", or "Britain-ruler". A great deal of scholarly attention has been given to the meaning of this word. It has been described as a term "of encomiastic poetry", but there also is evidence that it implied a definite role of military leadership.

Bede says that these kings had authority "south of the Humber", but the span of control, at least of the earlier bretwaldas, likely was less than this. In Ceawlin's case the range of control is hard to determine accurately, but Bede's inclusion of Ceawlin in the list of kings who held "imperium", and the list of battles he is recorded as having won, indicate an energetic and successful leader who, from a base in the upper Thames valley, dominated much of the surrounding area and held overlordship over the southern Britons for some period. Despite Ceawlin's military successes, the northern conquests he made could not always be retained: Mercia took much of the upper Thames valley, and the north-eastern towns won in 571 were among territory subsequently under the control of Kent and Mercia at different times.

Bede's concept of the power of these overlords also must be regarded as the product of his eighth-century viewpoint. When the "Ecclesiastical History" was written, Æthelbald of Mercia dominated the English south of the Humber, and Bede's view of the earlier kings was doubtless strongly coloured by the state of England at that time. For the earlier "bretwaldas", such as Ælle and Ceawlin, there must be some element of anachronism in Bede's description. It also is possible that Bede only meant to refer to power over Anglo-Saxon kingdoms, not the native Britons.

Ceawlin is the second king in Bede's list. All the subsequent bretwaldas followed more or less consecutively, but there is a long gap, perhaps fifty years, between Ælle of Sussex, the first bretwalda, and Ceawlin. The lack of gaps between the overlordships of the later bretwaldas has been used to make an argument for Ceawlin's dates matching the later entries in the "Chronicle" with reasonable accuracy. According to this analysis, the next bretwalda, Æthelberht of Kent, must have been already a dominant king by the time Pope Gregory the Great wrote to him in 601, since Gregory would have not written to an underking. Ceawlin defeated Æthelberht in 568 according to the "Chronicle". Æthelberht's dates are a matter of debate, but recent scholarly consensus has his reign starting no earlier than 580. The 568 date for the battle at Wibbandun is thought to be unlikely because of the assertion in various versions of the West Saxon Genealogical Regnal List that Ceawlin's reign lasted either seven or seventeen years. If this battle is placed near the year 590, before Æthelberht had established himself as a powerful king, then the subsequent annals relating to Ceawlin's defeat and death may be reasonably close to the correct date. In any case, the battle with Æthelberht is unlikely to have been more than a few years on either side of 590. The gap between Ælle and Ceawlin, on the other hand, has been taken as supporting evidence for the story told by Gildas in "De Excidio" of a peace lasting a generation or more following a Briton victory at Mons Badonicus.

Æthelberht of Kent succeeds Ceawlin on the list of bretwaldas, but the reigns may overlap somewhat: recent evaluations give Ceawlin a likely reign of 581–588, and place Æthelberht's accession near to the year 589, but these analyses are no more than scholarly guesses. Ceawlin's eclipse in 592, probably by Ceol, may have been the occasion for Æthelberht to rise to prominence; Æthelberht very likely was the dominant Anglo-Saxon king by 597. Æthelberht's rise may have been earlier: the 584 annal, even if it records a victory, is the last victory of Ceawlin's in the "Chronicle", and the period after that may have been one of Æthelberht's ascent and Ceawlin's decline.

Ceawlin lost the throne of Wessex in 592. The annal for that year reads, in part: "Here there was great slaughter at Woden's Barrow, and Ceawlin was driven out." Woden's Barrow is a tumulus, now called Adam's Grave, at Alton Priors, Wiltshire. No details of his opponent are given. The medieval chronicler William of Malmesbury, writing in about 1120, says that it was "the Angles and the British conspiring together". Alternatively, it may have been Ceol, who is supposed to have been the next king of Wessex, ruling for six years according to the West Saxon Genealogical Regnal List. According to the "Anglo-Saxon Chronicle", Ceawlin died the following year. The relevant part of the annal reads: "Here Ceawlin and Cwichelm and Crida perished." Nothing more is known of Cwichelm and Crida, although they may have been members of the Wessex royal house – their names fit the alliterative pattern common to royal houses of the time.

According to the Regnal List, Ceol was a son of Cutha, who was a son of Cynric; and Ceolwulf, his brother, reigned for seventeen years after him. It is possible that some fragmentation of control among the West Saxons occurred at Ceawlin's death: Ceol and Ceolwulf may have been based in Wiltshire, as opposed to the upper Thames valley. This split also may have contributed to Æthelberht's ability to rise to dominance in southern England. The West Saxons remained influential in military terms, however: the "Chronicle" and Bede record continued military activity against Essex and Sussex within twenty or thirty years of Ceawlin's death.


Primary sources

Secondary sources


 


</doc>
<doc id="6779" url="https://en.wikipedia.org/wiki?curid=6779" title="Christchurch (disambiguation)">
Christchurch (disambiguation)

Christchurch is the largest city in the South Island of New Zealand. 
Christchurch may also refer to:






</doc>
<doc id="6780" url="https://en.wikipedia.org/wiki?curid=6780" title="CD-R">
CD-R

CD-R (Compact Disc-Recordable) is a digital optical disc storage format. A CD-R disc is a compact disc that can be written once and read arbitrarily many times.

CD-R discs (CD-Rs) are readable by most plain CD readers, i.e., CD readers manufactured prior to the introduction of CD-R. This is an advantage over CD-RW, which can be re-written but cannot be played on many plain CD readers.

Originally named CD Write-Once (WO), the CD-R specification was first published in 1988 by Philips and Sony in the 'Orange Book'. The Orange Book consists of several parts, furnishing details of the CD-WO, CD-MO (Magneto-Optic), and CD-RW (ReWritable). The latest editions have abandoned the use of the term "CD-WO" in favor of "CD-R", while "CD-MO" were used very little. Written CD-Rs and CD-RWs are, in the aspect of low-level encoding and data format, fully compatible with the audio CD ("Red Book" CD-DA) and data CD ("Yellow Book" CD-ROM) standards. (Note that the Yellow Book standard for CD-ROM only specifies a high-level data format and refers to the Red Book for all physical format and low-level code details, such as track pitch, linear bit density, and bitstream encoding.) This means they use Eight-to-Fourteen Modulation, CIRC error correction, and, for CD-ROM, the third error correction layer defined in the Yellow Book. Properly written CD-R discs on blanks of less than 80 minutes length are fully compatible with the audio CD and CD-ROM standards in all details including physical specifications. 80 minute CD-R discs marginally violate the Red Book physical format specifications, and longer discs are noncompliant. CD-RW discs have lower reflectivity than CD-R or pressed (non-writable) CDs and for this reason cannot meet the Red Book standard (or come close). Some hardware compatible with Red Book CDs may have difficulty reading CD-Rs and, because of their lower reflectivity, especially CD-RWs. To the extent that CD hardware can read extended-length discs or CD-RW discs, it is because that hardware has capability beyond the minimum required by the Red Book and Yellow Book standards (the hardware is more capable than it needs to be to bear the Compact Disc logo).

CD-R recording systems available in 1990 were similar to the washing machine-sized Meridian CD Publisher, based on the two-piece rack mount Yamaha PDS audio recorder costing $35,000, not including the required external ECC circuitry for data encoding, SCSI hard drive subsystem, and MS-DOS control computer. By 1992, the cost of typical recorders was down to $10,000–12,000, and in September 1995, Hewlett-Packard introduced its model 4020i manufactured by Philips, which, at $995, was the first recorder to cost less than $1000.

The dye materials developed by Taiyo Yuden made it possible for CD-R discs to be compatible with Audio CD and CD-ROM discs.

Initially, in the United States, there was a market separation between "music" CD-Rs and "data" CD-Rs, the former being several times more expensive than the latter due to industry copyright arrangements with the RIAA. Physically, there is no difference between the discs save for the Disc Application Flag that identifies their type: standalone audio recorders will only accept "music" CD-Rs to enforce the RIAA arrangement, while computer CD-R drives can use either type of media to burn either type of content.

A standard CD-R is a thick disc made of polycarbonate about 120 mm in diameter. The 120 mm disc has a storage capacity of 74 minutes of audio or 650 Megabytes of data. CD-R/RWs are available with capacities of 80 minutes of audio or 737,280,000 bytes (700 MiB), which they achieve by molding the disc at the tightest allowable tolerances specified in the Orange Book CD-R/CD-RW standards. The engineering margin that was reserved for manufacturing tolerance has been used for data capacity instead, leaving no tolerance for manufacturing; for these discs to be truly compliant with the Orange Book standard, the manufacturing process must be perfect .

Despite the foregoing, most CD-Rs on the market have an 80-minute capacity. There are also 90 minute/790 MiB and 99 minute/870 MiB discs, although they are less common (and depart from the Orange Book standard outright). Also, due to the limitations of the data structures in the ATIP (see below), 90 and 99 minute blanks will identify as 80 minute ones. (As the ATIP is part of the Orange Book standard, it is natural that its design does not support some nonstandard disc configurations.) Therefore, in order to use the additional capacity, these discs have to be burned using "overburn" options in the CD recording software. (Overburning itself is so named because it is outside the written standards, but, due to market demand, it has nonetheless become a de facto standard function in most CD writing drives and software for them.)

Some drives use special techniques, such as Plextor's GigaRec or Sanyo's HD-BURN, to write more data onto a given disc; these techniques are inherently deviations from the Compact Disc (Red, Yellow, and/or Orange Book) standards, making the recorded discs proprietary-formatted and not fully compatible with standard CD players and drives. However, in certain applications where discs will not be distributed or exchanged outside a private group and will not be archived for a long time, a proprietary format may be an acceptable way to obtain greater capacity (up to 1.2 GiB with GigaRec or 1.8 GiB with HD-BURN on 99 minute media). The greatest risk in using such a proprietary data storage format, assuming that it works reliably as designed, is that it may be difficult or impossible to repair or replace the hardware used to read the media if it fails, is damaged, or is lost after its original vendor discontinues it.

Nothing in the Red, Yellow or Orange Book standards prohibits disc reading/writing devices from having the capacity to read or write discs beyond the Compact Disc standards. The standards do require discs to meet precise requirements in order to be called Compact Discs, but the other discs may be called by other names; if this were not true, no DVD drive could legally bear the Compact Disc logo. While disc players and drives may have capabilities beyond the standards, enabling them to read and write nonstandard discs, there is no assurance, in the absence of explicit additional manufacturer specifications beyond normal Compact Disc logo certification, that any particular player or drive will perform beyond the standards at all or consistently. Furthermore, if the same device with no explicit performance specs beyond the Compact Disc logo initially handles nonstandard discs reliably, there is no assurance that it will not later stop doing so, and in that case, there is no assurance that it can be made to do so again by service or adjustment. Therefore, discs with capacities larger than 650 MB, and especially those larger than 700 MB, are less interchangeable among players/drives than standard discs and are not very suitable for archival use, as their readability on future equipment, or even on the same equipment at a future time, is not assured, even under the assumption that the discs will not degrade at all.
The polycarbonate disc contains a spiral groove, called the "pregroove" (because it is molded in before data are written to the disc), to guide the laser beam upon writing and reading information. The pregroove is molded into the top side of the polycarbonate disc, where the pits and lands would be molded if it were a pressed (nonrecordable) Red Book CD; the bottom side, which faces the laser beam in the player or drive, is flat and smooth. The polycarbonate disc is coated on the pregroove side with a very thin layer of organic dye. Then, on top of the dye is coated a thin, reflecting layer of silver, a silver alloy, or gold. Finally, a protective coating of a photo-polymerizable lacquer is applied on top of the metal reflector and cured with UV-light.

A blank CD-R is not "empty"; the pregroove has a wobble (the ATIP), which helps the writing laser to stay on track and to write the data to the disc at a constant rate. Maintaining a constant rate is essential to ensure proper size and spacing of the pits and lands burned into the dye layer. As well as providing timing information, the ATIP (absolute time in pregroove) is also a data track containing information about the CD-R manufacturer, the dye used and media information (disc length and so on). The pregroove is not destroyed when the data are written to the CD-R, a point which some copy protection schemes use to distinguish copies from an original CD.
There are three basic formulations of dye used in CD-Rs:

There are many hybrid variations of the dye formulations, such as Formazan by Kodak (a hybrid of cyanine and phthalocyanine).

Unfortunately, many manufacturers have added additional coloring to disguise their unstable cyanine CD-Rs in the past, so the formulation of a disc cannot be determined based purely on its color. Similarly, a gold reflective layer does not guarantee use of phthalocyanine dye. The quality of the disc is also not only dependent on the dye used, it is also influenced by sealing, the top layer, the reflective layer, and the polycarbonate. Simply choosing a disc based on its dye type may be problematic. Furthermore, correct power calibration of the laser in the writer, as well as correct timing of the laser pulses, stable disc speed, and so on, is critical to not only the immediate readability but the longevity of the recorded disc, so for archiving it is important to have not only a high quality disc but a high quality writer. In fact, a high quality writer may produce adequate results with medium quality media, but high quality media cannot compensate for a mediocre writer, and discs written by such a writer cannot achieve their maximum potential archival lifetime.

These times only include the actual optical writing pass over the disc. For most disc recording operations, additional time is used for overhead processes, such as organizing the files and tracks, which adds to the theoretical minimum total time required to produce a disc. (An exception might be making a disc from a prepared ISO image, for which the overhead would likely be trivial.) At the lowest write speeds, this overhead takes so much less time than the actual disc writing pass that it may be negligible, but at higher write speeds, the overhead time becomes a larger proportion of the overall time taken to produce a finished disc and may add significantly to it.

Also, above 20× speed, drives use a Zoned-CLV or CAV strategy, where the advertised maximum speed is only reached near the outer rim of the disc. This is not taken into account by the above table. (If this were not done, the faster rotation that would be required at the inner tracks could cause the disc to fracture and/or could cause excessive vibration which would make accurate and successful writing impossible.)

The blank disc has a pre-groove track onto which the data are written. The pre-groove track, which also contains timing information, ensures that the recorder follows the same spiral path as a conventional CD. A CD recorder writes data to a CD-R disc by pulsing its laser to heat areas of the organic dye layer. The writing process does not produce indentations (pits); instead, the heat permanently changes the optical properties of the dye, changing the reflectivity of those areas. Using a low laser power, so as not to further alter the dye, the disc is read back in the same way as a CD-ROM. However, the reflected light is modulated not by pits, but by the alternating regions of heated and unaltered dye. The change of the intensity of the reflected laser radiation is transformed into an electrical signal, from which the digital information is recovered ("decoded"). Once a section of a CD-R is written, it cannot be erased or rewritten, unlike a CD-RW. A CD-R can be recorded in multiple sessions.
A CD recorder can write to a CD-R using several methods including:

With careful examination, the written and unwritten areas can be distinguished by the naked eye. CD-Rs are written from the center outwards, so the written area appears as an inner band with slightly different shading.

Real-life (not accelerated aging) tests have revealed that some CD-Rs degrade quickly even if stored normally. The quality of a CD-R disc has a large and direct influence on longevity—low quality discs should not be expected to last very long. According to research conducted by J. Perdereau, CD-Rs are expected to have an average life expectancy of 10 years. Branding isn't a reliable guide to quality, because many brands (major as well as no name) do not manufacture their own discs. Instead they are sourced from different manufacturers of varying quality. For best results, the actual manufacturer and material components of each batch of discs should be verified.

Burned CD-Rs suffer from material degradation, just like most writable media. CD-R media have an internal layer of dye used to store data. In a CD-RW disc, the recording layer is made of an alloy of silver and other metals—indium, antimony, and tellurium. In CD-R media, the dye itself can degrade, causing data to become unreadable.

As well as degradation of the dye, failure of a CD-R can be due to the reflective surface. While silver is less expensive and more widely used, it is more prone to oxidation resulting in a non-reflecting surface. Gold on the other hand, although more expensive and no longer widely used, is an inert material, so gold-based CD-Rs do not suffer from this problem. Manufacturers have estimated the longevity of gold-based CD-Rs to be as high as 100 years.

It is recommended if using adhesive-backed paper labels that the labels be specially made for CD-Rs. A balanced CD vibrates only slightly when rotated at high speed. Bad or improperly made labels, or labels applied off-center, unbalance the CD and can cause it to vibrate when it spins, which causes read errors and even risks damaging the drive.

A professional alternative to CD labels is pre-printed CDs using a 5-color silkscreen or offset press. Using a permanent marker pen is also a common practice. However, solvents from such pens can affect the dye layer.

Since CD-Rs in general cannot be logically erased to any degree, the disposal of CD-Rs presents a possible security issue if they contain sensitive / private data. Destroying the data requires physically destroying the disc or data layer. Heating the disc in a microwave oven for 10–15 seconds effectively destroys the data layer by causing arcing in the metal reflective layer, but this same arcing may cause damage or excessive wear to the microwave oven. Many office paper shredders are also designed to shred CDs.

Some recent burners (Plextor, LiteOn) support erase operations on -R media, by "overwriting" the stored data with strong laser power, although the erased area cannot be overwritten with new data.

The polycarbonate material and possible gold or silver in the reflective layer would make CD-Rs highly recyclable. However, the polycarbonate is of very little value and the quantity of precious metals is so small that it is not profitable to recover them. Consequently, recyclers that accept CD-Rs typically do not offer compensation for donating or transporting the materials.




</doc>
