<doc id="10775" url="https://en.wikipedia.org/wiki?curid=10775" title="Film editing">
Film editing

Film editing is both a creative and a tech part of the post-production process of filmmaking. The term is derived from the traditional process of working with film which increasingly involves the use of digital technology.

The film editor works with the raw footage, selecting shots and combines them into sequences which create a finished motion picture. Film editing is described as an art or skill, the only art that is unique to cinema, separating filmmaking from other art forms that preceded it, although there are close parallels to the editing process in other art forms such as poetry and novel writing. Film editing is often referred to as the "invisible art" because when it is well-practiced, the viewer can become so engaged that he or she is not aware of the editor's work.

On its most fundamental level, film editing is the art, technique and practice of assembling shots into a coherent sequence. The job of an editor is not simply to mechanically put pieces of a film together, cut off film slates or edit dialogue scenes. A film editor must creatively work with the layers of images, story, dialogue, music, pacing, as well as the actors' performances to effectively "re-imagine" and even rewrite the film to craft a cohesive whole. Editors usually play a dynamic role in the making of a film. Sometimes, auteurist film directors edit their own films, for example, Akira Kurosawa, Bahram Beyzai and the Coen brothers.

With the advent of digital editing, film editors and their assistants have become responsible for many areas of filmmaking that used to be the responsibility of others. For instance, in past years, picture editors dealt only with just that—picture. Sound, music, and (more recently) visual effects editors dealt with the practicalities of other aspects of the editing process, usually under the direction of the picture editor and director. However, digital systems have increasingly put these responsibilities on the picture editor. It is common, especially on lower budget films, for the editor to sometimes cut in temporary music, mock up visual effects and add temporary sound effects or other sound replacements. These temporary elements are usually replaced with more refined final elements produced by the sound, music and visual effects teams hired to complete the picture.

Early films were short films that were one long, static, and locked-down shot. Motion in the shot was all that was necessary to amuse an audience, so the first films simply showed activity such as traffic moving along a city street. There was no story and no editing. Each film ran as long as there was film in the camera.

The use of film editing to establish continuity, involving action moving from one sequence into another, is attributed to British film pioneer Robert W. Paul's "Come Along, Do!", made in 1898 and one of the first films to feature more than one shot. In the first shot, an elderly couple is outside an art exhibition having lunch and then follow other people inside through the door. The second shot shows what they do inside. Paul's 'Cinematograph Camera No. 1' of 1896 was the first camera to feature reverse-cranking, which allowed the same film footage to be exposed several times and thereby to create super-positions and multiple exposures. One of the first films to use this technique, Georges Méliès's "The Four Troublesome Heads" from 1898, was produced with Paul's camera.

The further development of action continuity in multi-shot films continued in 1899-1900 at the Brighton School in England, where it was definitively established by George Albert Smith and James Williamson. In that year, Smith made "As Seen Through a Telescope", in which the main shot shows street scene with a young man tying the shoelace and then caressing the foot of his girlfriend, while an old man observes this through a telescope. There is then a cut to close shot of the hands on the girl's foot shown inside a black circular mask, and then a cut back to the continuation of the original scene.

Even more remarkable was James Williamson's "Attack on a China Mission Station", made around the same time in 1900. The first shot shows the gate to the mission station from the outside being attacked and broken open by Chinese Boxer rebels, then there is a cut to the garden of the mission station where a pitched battle ensues. An armed party of British sailors arrived to defeat the Boxers and rescue the missionary's family. The film used the first "reverse angle" cut in film history.

James Williamson concentrated on making films taking action from one place shown in one shot to the next shown in another shot in films like "Stop Thief!" and "Fire!", made in 1901, and many others. He also experimented with the close-up, and made perhaps the most extreme one of all in "The Big Swallow", when his character approaches the camera and appears to swallow it. These two filmmakers of the Brighton School also pioneered the editing of the film; they tinted their work with color and used trick photography to enhance the narrative. By 1900, their films were extended scenes of up to 5 minutes long.
Other filmmakers then took up all these ideas including the American Edwin S. Porter, who started making films for the Edison Company in 1901. Porter worked on a number of minor films before making "Life of an American Fireman" in 1903. The film was the first American film with a plot, featuring action, and even a closeup of a hand pulling a fire alarm. The film comprised a continuous narrative over seven scenes, rendered in a total of nine shots. He put a dissolve between every shot, just as Georges Méliès was already doing, and he frequently had the same action repeated across the dissolves. His film, "The Great Train Robbery" (1903), had a running time of twelve minutes, with twenty separate shots and ten different indoor and outdoor locations. He used cross-cutting editing method to show simultaneous action in different places.

These early film directors discovered important aspects of motion picture language: that the screen image does not need to show a complete person from head to toe and that splicing together two shots creates in the viewer's mind a contextual relationship. These were the key discoveries that made all non-live or non live-on-videotape narrative motion pictures and television possible—that shots (in this case, whole scenes since each shot is a complete scene) can be photographed at widely different locations over a period of time (hours, days or even months) and combined into a narrative whole. That is, "The Great Train Robbery" contains scenes shot on sets of a telegraph station, a railroad car interior, and a dance hall, with outdoor scenes at a railroad water tower, on the train itself, at a point along the track, and in the woods. But when the robbers leave the telegraph station interior (set) and emerge at the water tower, the audience believes they went immediately from one to the other. Or that when they climb on the train in one shot and enter the baggage car (a set) in the next, the audience believes they are on the same train.

Sometime around 1918, Russian director Lev Kuleshov did an experiment that proves this point. (See Kuleshov Experiment) He took an old film clip of a headshot of a noted Russian actor and intercut the shot with a shot of a bowl of soup, then with a child playing with a teddy bear, then with a shot an elderly woman in a casket. When he showed the film to people they praised the actor's acting—the hunger in his face when he saw the soup, the delight in the child, and the grief when looking at the dead woman. Of course, the shot of the actor was years before the other shots and he never "saw" any of the items. The simple act of juxtaposing the shots in a sequence made the relationship.

Before the widespread use of digital non-linear editing systems, the initial editing of all films was done with a positive copy of the film negative called a film workprint (cutting copy in UK) by physically cutting and pasting together pieces of film. Strips of footage would be hand cut and attached together with tape and then later in time, glue. Editors were very precise; if they made a wrong cut or needed a fresh positive print, it cost the production money and time for the lab to reprint the footage. With the invention of a splicer and threading the machine with a viewer such as a Moviola, or "flatbed" machine such as a K.-E.-M. or Steenbeck, the editing process sped up a little bit and cuts came out cleaner and more precise. The Moviola editing practice is non-linear, allowing the editor to make choices faster, a great advantage to editing episodic films for television which have very short timelines to complete the work. All film studios and production companies who produced films for television provided this tool for their editors. Flatbed editing machines were used for playback and refinement of cuts, particularly in feature films and films made for television because they were less noisy and cleaner to work with. 
They were used extensively for documentary and drama production within the BBC's Film Department. Operated by a team of two, an editor and assistant editor, this tactile process required significant skill but allowed for editors to work extremely efficiently.
Today, most films are edited digitally (on systems such as Avid, Final Cut Pro or Premiere Pro) and bypass the film positive workprint altogether. In the past, the use of a film positive (not the original negative) allowed the editor to do as much experimenting as he or she wished, without the risk of damaging the original. With digital editing, editors can experiment just as much as before except with the footage completely transferred to a computer hard drive.

When the film workprint had been cut to a satisfactory state, it was then used to make an edit decision list (EDL). The negative cutter referred to this list while processing the negative, splitting the shots into rolls, which were then contact printed to produce the final film print or answer print. Today, production companies have the option of bypassing negative cutting altogether. With the advent of digital intermediate ("DI"), the physical negative does not necessarily need to be physically cut and hot spliced together; rather the negative is optically scanned into the computer(s) and a cut list is confirmed by a DI editor.

In the early years of film, editing was considered a technical job; editors were expected to "cut out the bad bits" and string the film together. Indeed, when the Motion Picture Editors Guild was formed, they chose to be "below the line", that is, not a creative guild, but a technical one. Women were not usually able to break into the "creative" positions; directors, cinematographers, producers, and executives were almost always men. Editing afforded creative women a place to assert their mark on the filmmaking process. The history of film has included many women editors such as Dede Allen, Anne Bauchens, Margaret Booth, Barbara McLean, Anne V. Coates, Adrienne Fazan, Verna Fields, Blanche Sewell and Eda Warren.

Post-production editing may be summarized by three distinct phases commonly referred to as the editor's cut, the director's cut, and the final cut.

There are several editing stages and the editor's cut is the first. An editor's cut (sometimes referred to as the "Assembly edit" or "Rough cut") is normally the first pass of what the final film will be when it reaches picture lock. The film editor usually starts working while principal photography starts. Sometimes, prior to cutting, the editor and director will have seen and discussed "dailies" (raw footage shot each day) as shooting progresses. As production schedules have shortened over the years, this co-viewing happens less often. Screening dailies give the editor a general idea of the director's intentions. Because it is the first pass, the editor's cut might be longer than the final film. The editor continues to refine the cut while shooting continues, and often the entire editing process goes on for many months and sometimes more than a year, depending on the film.

When shooting is finished, the director can then turn his or her full attention to collaborating with the editor and further refining the cut of the film. This is the time that is set aside where the film editor's first cut is molded to fit the director's vision. In the United States, under the rules of the Directors Guild of America, directors receive a minimum of ten weeks after completion of principal photography to prepare their first cut. While collaborating on what is referred to as the "director's cut", the director and the editor go over the entire movie in great detail; scenes and shots are re-ordered, removed, shortened and otherwise tweaked. Often it is discovered that there are plot holes, missing shots or even missing segments which might require that new scenes be filmed. Because of this time working closely and collaborating – a period that is normally far longer and more intricately detailed than the entire preceding film production – many directors and editors form a unique artistic bond.

Often after the director has had their chance to oversee a cut, the subsequent cuts are supervised by one or more producers, who represent the production company or movie studio. There have been several conflicts in the past between the director and the studio, sometimes leading to the use of the "Alan Smithee" credit signifying when a director no longer wants to be associated with the final release.

In motion picture terminology, a montage (from the French for "putting together" or "assembly") is a film editing technique.

There are at least three senses of the term:

Although film director D.W. Griffith was not part of the montage school, he was one of the early proponents of the power of editing — mastering cross-cutting to show parallel action in different locations, and codifying film grammar in other ways as well. Griffith's work in the teens was highly regarded by Lev Kuleshov and other Soviet filmmakers and greatly influenced their understanding of editing.

Kuleshov was among the very first to theorize about the relatively young medium of the cinema in the 1920s. For him, the unique essence of the cinema — that which could be duplicated in no other medium — is editing. He argues that editing a film is like constructing a building. Brick-by-brick (shot-by-shot) the building (film) is erected. His often-cited Kuleshov Experiment established that montage can lead the viewer to reach certain conclusions about the action in a film. Montage works because viewers infer meaning based on context. Sergei Eisenstein was briefly a student of Kuleshov's, but the two parted ways because they had different ideas of montage. Eisenstein regarded montage as a dialectical means of creating meaning. By contrasting unrelated shots he tried to provoke associations in the viewer, which were induced by shocks.

A montage sequence consists of a series of short shots that are edited into a sequence to condense narrative. It is usually used to advance the story as a whole (often to suggest the passage of time), rather than to create symbolic meaning. In many cases, a song plays in the background to enhance the mood or reinforce the message being conveyed. One famous example of montage was seen in the 1968 film "", depicting the start of man's first development from apes to humans. Another example that is employed in many films is the sports montage. The sports montage shows the star athlete training over a period of time, each shot having more improvement than the last. Classic examples include Rocky and the Karate Kid.

Continuity is a term for the consistency of on-screen elements over the course of a scene or film, such as whether an actor's costume remains the same from one scene to the next, or whether a glass of milk held by a character is full or empty throughout the scene. Because films are typically shot out of sequence, the script supervisor will keep a record of continuity and provide that to the film editor for reference. The editor may try to maintain continuity of elements, or may intentionally create a discontinuous sequence for stylistic or narrative effect.

The technique of continuity editing, part of the classical Hollywood style, was developed by early European and American directors, in particular, D.W. Griffith in his films such as "The Birth of a Nation" and "Intolerance". The classical style embraces temporal and spatial continuity as a way of advancing the narrative, using such techniques as the 180 degree rule, Establishing shot, and Shot reverse shot. Often, continuity editing means finding a balance between literal continuity and perceived continuity. For instance, editors may condense action across cuts in a non-distracting way. A character walking from one place to another may "skip" a section of floor from one side of a cut to the other, but the cut is constructed to appear continuous so as not to distract the viewer.

Early Russian filmmakers such as Lev Kuleshov (already mentioned) further explored and theorized about editing and its ideological nature. Sergei Eisenstein developed a system of editing that was unconcerned with the rules of the continuity system of classical Hollywood that he called Intellectual montage.

Alternatives to traditional editing were also explored by early surrealist and Dada filmmakers such as Luis Buñuel (director of the 1929 "Un Chien Andalou") and René Clair (director of 1924's "Entr'acte" which starred famous Dada artists Marcel Duchamp and Man Ray).

The French New Wave filmmakers such as Jean-Luc Godard and François Truffaut and their American counterparts such as Andy Warhol and John Cassavetes also pushed the limits of editing technique during the late 1950s and throughout the 1960s. French New Wave films and the non-narrative films of the 1960s used a carefree editing style and did not conform to the traditional editing etiquette of Hollywood films. Like its Dada and surrealist predecessors, French New Wave editing often drew attention to itself by its lack of continuity, its demystifying self-reflexive nature (reminding the audience that they were watching a film), and by the overt use of jump cuts or the insertion of material not often related to any narrative.

Since the late 20th century Post-classical editing has seen faster editing styles with nonlinear, discontinuous action.

Vsevolod Pudovkin noted that the editing process is the one phase of production that is truly unique to motion pictures. Every other aspect of filmmaking originated in a different medium than film (photography, art direction, writing, sound recording), but editing is the one process that is unique to film. Filmmaker Stanley Kubrick was quoted as saying: "I love editing. I think I like it more than any other phase of filmmaking. If I wanted to be frivolous, I might say that everything that precedes editing is merely a way of producing a film to edit."

According to writer-director Preston Sturges: [T]here is a law of natural cutting and that this replicates what an audience in a legitimate theater does for itself. The more nearly the film cutter approaches this law of natural interest, the more invisible will be his cutting. If the camera moves from one person to another at the exact moment that one in the legitimate theatre would have turned his head, one will not be conscious of a cut. If the camera misses by a quarter of a second, one will get a jolt. There is one other requirement: the two shots must be approximate of the same tone value. If one cuts from black to white, it is jarring. At any given moment, the camera must point at the exact spot the audience wishes to look at. To find that spot is absurdly easy: one has only to remember where one was looking at the time the scene was made.

Assistant editors aid the editor and director in collecting and organizing all the elements needed to edit the film. The Motion Picture Editors Guild defines an assistant editor as "a person who is assigned to assist an Editor. His [or her] duties shall be such as are assigned and performed under the immediate direction, supervision, and responsibility of the editor." When editing is finished, they oversee the various lists and instructions necessary to put the film into its final form. Editors of large budget features will usually have a team of assistants working for them. The first assistant editor is in charge of this team and may do a small bit of picture editing as well, if necessary. Often assistant editors will perform temporary sound, music, and visual effects work. The other assistants will have set tasks, usually helping each other when necessary to complete the many time-sensitive tasks at hand. In addition, an apprentice editor may be on hand to help the assistants. An apprentice is usually someone who is learning the ropes of assisting.

Television shows typically have one assistant per editor. This assistant is responsible for every task required to bring the show to the final form. Lower budget features and documentaries will also commonly have only one assistant.

The organizational aspects job could best be compared to database management. When a film is shot, every piece of picture or sound is coded with numbers and timecode. It is the assistant's job to keep track of these numbers in a database, which, in non-linear editing, is linked to the computer program. The editor and director cut the film using digital copies of the original film and sound, commonly referred to as an "offline" edit. When the cut is finished, it is the assistant's job to bring the film or television show "online". They create lists and instructions that tell the picture and sound finishers how to put the edit back together with the high-quality original elements. Assistant editing can be seen as a career path to eventually becoming an editor. Many assistants, however, do not choose to pursue advancement to the editor, and are very happy at the assistant level, working long and rewarding careers on many films and television shows.


Notes
Bibliography

Further reading


Wikibooks

Wikiversity


</doc>
<doc id="10776" url="https://en.wikipedia.org/wiki?curid=10776" title="Freestyle">
Freestyle

Freestyle may refer to:










</doc>
<doc id="10777" url="https://en.wikipedia.org/wiki?curid=10777" title="Friedrich Wöhler">
Friedrich Wöhler

Friedrich Wöhler (; 31 July 1800 – 23 September 1882) was a German chemist, best known for his synthesis of urea, but also the first to isolate several chemical elements.

He was born in Eschersheim, which belonged to
Hanau at the time but is nowadays a district of Frankfurt am Main. In 1823 Wöhler finished his study of medicine in Heidelberg at the laboratory of Leopold Gmelin, who arranged for him to work under Jöns Jakob Berzelius in Stockholm, Sweden. He taught chemistry from 1826 to 1831 at the Polytechnic School in Berlin until 1839 when he was stationed at the Polytechnic School at Kassel. Afterwards, he became Ordinary Professor of Chemistry in the University of Göttingen, where he remained until his death in 1882. In 1834, he was elected a foreign member of the Royal Swedish Academy of Sciences.

Wöhler is regarded as a pioneer in organic chemistry as a result of his (accidentally) synthesizing urea from ammonium cyanate in the Wöhler synthesis in 1828. In a letter to Swedish chemist Jöns Jacob Berzelius the same year, he wrote, 'In a manner of speaking, I can no longer hold my chemical water. I must tell you that I can make urea without the use of kidneys of any animal, be it man or dog.'

This discovery has become celebrated as a refutation of vitalism, the hypothesis that living things are alive because of some special "vital force". However, contemporary accounts do not support that notion. This "Wöhler Myth", as historian of science Peter J. Ramberg called it, originated from a popular history of chemistry published in 1931, which, "ignoring all pretense of historical accuracy, turned Wöhler into a crusader who made attempt after attempt to synthesize a natural product that would refute vitalism and lift the veil of ignorance, until 'one afternoon the miracle happened'". Nevertheless, it was the beginning of the end of one popular vitalist hypothesis, that of Jöns Jakob Berzelius, that "organic" compounds could be made only by living things.

Wöhler was also known for being a co-discoverer of beryllium, silicon and silicon nitride, as well as the synthesis of calcium carbide, among others. In 1834, Wöhler and Justus Liebig published an investigation of the oil of bitter almonds. They proved by their experiments that a group of carbon, hydrogen, and oxygen atoms can behave like an element, take the place of an element, and be exchanged for elements in chemical compounds. Thus the foundation was laid of the doctrine of compound radicals, a doctrine which had a profound influence on the development of chemistry.

Since the discovery of potassium by Humphry Davy, it had been assumed that alumina, the basis of clay, contained a metal in combination with oxygen. Davy, Ørsted, and Berzelius attempted the extraction of this metal, but failed. Wöhler then worked on the same subject, and discovered the metal aluminium in 1827. To him also is due the isolation of the elements yttrium, beryllium, and titanium, the observation that "silicium" (silicon) can be obtained in crystals, and that some meteoric stones contain organic matter. He analyzed meteorites, and for many years wrote the digest on the literature of meteorites in the "Jahresberichte über die Fortschritte der Chemie"; he possessed the best private collection of meteoric stones and irons existing. Wöhler and Sainte Claire Deville discovered the crystalline form of boron, and Wöhler and Heinrich Buff discovered silane in 1856. Wöhler also prepared urea, a constituent of urine, from ammonium cyanate in the laboratory without the help of a living cell.

Wöhler's discoveries had great influence on the theory of chemistry. The journals of every year from 1820 to 1881 contain contributions from him. In the "Scientific American" supplement for 1882, it was remarked that "for two or three of his researches he deserves the highest honor a scientific man can obtain, but the sum of his work is absolutely overwhelming. Had he never lived, the aspect of chemistry would be very different from that it is now".

Wöhler had several students who became notable chemists. Among them were Georg Ludwig Carius, Heinrich Limpricht, Rudolph Fittig, Adolph Wilhelm Hermann Kolbe, Albert Niemann, and Vojtěch Šafařík.

Further works from Wöhler:






</doc>
<doc id="10778" url="https://en.wikipedia.org/wiki?curid=10778" title="Funk">
Funk

Funk is a music genre that originated in African-American communities in the mid-1960s when African-American musicians created a rhythmic, danceable new form of music through a mixture of soul music, jazz, and rhythm and blues (R&B). Funk de-emphasizes melody and chord progressions and focuses on a strong rhythmic groove of a bass line played by an electric bassist and a drum part played by a drummer, often at slower tempos than other popular music. Like much of African-inspired music, funk typically consists of a complex groove with rhythm instruments playing interlocking grooves that created a "hypnotic" and "danceable feel". Funk uses the same richly colored extended chords found in bebop jazz, such as minor chords with added sevenths and elevenths, or dominant seventh chords with altered ninths and thirteenths.

Funk originated in the mid-1960s, with James Brown's development of a signature groove that emphasized the downbeat—with heavy emphasis on the first beat of every measure ("The One"), and the application of swung 16th notes and syncopation on all bass lines, drum patterns, and guitar riffs. Other musical groups, including Sly and the Family Stone, the Meters, and Parliament-Funkadelic, soon began to adopt and develop Brown's innovations. While much of the written history of funk focuses on men, there have been notable funk women, including Chaka Khan, Marva Whitney, Lyn Collins, Brides of Funkenstein, Vicki Anderson, Anna King(The JB's singer), and Parlet.

Funk derivatives include the psychedelic funk of Sly Stone and George Clinton; the avant-funk of groups such as Talking Heads and the Pop Group; boogie, a form of post-disco dance music; electro music, a hybrid of electronic music and funk; funk metal (e.g., Living Colour, Faith No More); G-funk, a mix of gangsta rap and funk; Timba, a form of funky Cuban popular dance music; and funk jam. Funk samples and breakbeats have been used extensively in hip hop and various forms of electronic dance music, such as house music, and Detroit techno. It is also the main influence of go-go, a subgenre associated with funk.

The word "funk" initially referred (and still refers) to a strong odor. It is originally derived from Latin "fumigare" (which means "to smoke") via Old French "fungiere" and, in this sense, it was first documented in English in 1620. In 1784 "funky" meaning "musty" was first documented, which, in turn, led to a sense of "earthy" that was taken up around 1900 in early jazz slang for something "deeply or strongly felt". Ethnomusicologist Portia Maultsby states that the expression "funk" comes from the Central African word "lu-funki" and art historian Robert Farris Thompson says the word comes from the Kikongo term "lu-fuki"; in both proposed origins, the term refers to body odor. Thompson's proposed Kikingo origin word, "lu-fuki" is used by African musicians to praise people "for the integrity of their art" and for having "worked out" to reach their goals. Even though in white culture, the term "funk" has negative connotations of odor or being in a bad mood ("in a funk"), in African communities, the term "funk", while still linked to body odor, had the positive sense that a musician's hard-working, honest effort led to sweat, and from their "physical exertion" came an "exquisite" and "superlative" performance.

In early jam sessions, musicians would encourage one another to "get down" by telling one another, "Now, put some "stank" on it!". At least as early as 1907, jazz songs carried titles such as "Funky". The first example is an unrecorded number by Buddy Bolden, remembered as either "Funky Butt" or "Buddy Bolden's Blues" with improvised lyrics that were, according to Donald M. Marquis, either "comical and light" or "crude and downright obscene" but, in one way or another, referring to the sweaty atmosphere at dances where Bolden's band played. As late as the 1950s and early 1960s, when "funk" and "funky" were used increasingly in the context of jazz music, the terms still were considered indelicate and inappropriate for use in polite company. According to one source, New Orleans-born drummer Earl Palmer "was the first to use the word 'funky' to explain to other musicians that their music should be made more syncopated and danceable." The style later evolved into a rather hard-driving, insistent rhythm, implying a more carnal quality. This early form of the music set the pattern for later musicians. The music was identified as slow, sexy, loose, riff-oriented and danceable.

Like soul, funk is based on dance music, so it has a strong "rhythmic role". The sound of funk is as much based on the "spaces between the notes" as the notes that are played; as such, rests between notes are important. While there are rhythmic similarities between funk and disco, funk has a "central dance beat that's slower, sexier and more syncopated than disco", and funk rhythm section musicians add more "subtextures", complexity and "personality" onto the main beat than a programmed synth-based disco ensemble.

Before funk, most pop music was based on sequences of eighth notes, because the fast tempos made further subdivisions of the beat infeasible. The innovation of funk was that by using slower tempos, funk "created space for further rhythmic subdivision, so a bar of 4/4 could now accommodate 16 possible note placements." Specifically, by having the guitar and drums play in "motoring" sixteenth-note rhythms, it created the opportunity for the other instruments to play "more syncopated, broken-up style", which facilitated a move to more "liberated" basslines. Together, these "interlocking parts" created a "hypnotic" and "danceable feel".

A great deal of funk is rhythmically based on a two-celled onbeat/offbeat structure, which originated in sub-Saharan African music traditions. New Orleans appropriated the bifurcated structure from the Afro-Cuban mambo and conga in the late 1940s, and made it its own. New Orleans funk, as it was called, gained international acclaim largely because James Brown's rhythm section used it to great effect.

Funk uses the same richly colored extended chords found in bebop jazz, such as minor chords with added sevenths and elevenths, or dominant seventh chords with altered ninths. Some examples of chords used in funk are minor eleventh chords (e.g., F minor 11th); dominant seventh with added sharp ninth and a suspended fourth (e.g., C7 (#9) sus 4); dominant ninth chords (e.g., F9); and minor sixth chords (e.g., C minor 6). The six-ninth chord is used in funk (e.g., F 6/9); it is a major chord with an added sixth and ninth. In funk, minor seventh chords are more common than minor triads because minor triads were found to be too "thin"-sounding. Some of the best known and most skillful soloists in funk have jazz backgrounds. Trombonist Fred Wesley and saxophonist Pee Wee Ellis and Maceo Parker are among the most notable musicians in the funk music genre, with both of them working with James Brown, George Clinton and Prince.

However, unlike bebop jazz, with its complex, rapid-fire chord changes, funk virtually abandoned chord changes, creating static single chord vamps (often alternating a minor seventh chord and a related dominant seventh chord, such as a minor to D7) with melodo-harmonic movement and a complex, driving rhythmic feel. Even though some funk songs are mainly one-chord vamps, the rhythm section musicians may embellish this chord by moving it up or down a semitone or a tone to create chromatic passing chords. For example, "Play that funky music" (by Wild Cherry) mainly uses an E ninth chord, but it also uses F#9 and F9.

The chords used in funk songs typically imply a Dorian or Mixolydian mode, as opposed to the major or natural minor tonalities of most popular music. Melodic content was derived by mixing these modes with the blues scale. In the 1970s, jazz music drew upon funk to create a new subgenre of jazz-funk, which can be heard in recordings by Miles Davis ("Live-Evil", "On the Corner"), and Herbie Hancock ("Head Hunters").

Funk continues the African musical tradition of improvisation, in that in a funk band, the group would typically "feel" when to change, by "jamming" and "grooving", even in the studio recording stage, which might only be based on the skeleton framework for each song. Funk uses "collective improvisation", in which musicians at rehearsals would have what was metaphorically a musical "conversation", an approach which extended to the onstage performances.

Funk creates an intense groove by using strong guitar riffs and bass lines played on electric bass. Like Motown recordings, funk songs use bass lines as the centerpiece of songs. Indeed, funk has been called the style in which the bass line is most prominent in the songs, with the bass playing the "hook" of the song. Early funk basslines used syncopation (typically syncopated eighth notes), but with the addition of more of a "driving feel" than in New Orleans funk, and they used blues scale notes along with the major third above the root. Later funk basslines use sixteenth note syncopation, blues scales, and repetitive patterns, often with leaps of an octave or a larger interval.

Funk bass lines emphasize repetitive patterns, locked-in grooves, continuous playing, and slap and popping bass. Slapping and popping uses a mixture of thumb-slapped low notes (also called "thumped") and finger "popped" (or plucked) high notes, allowing the bass to have a drum-like rhythmic role, which became a distinctive element of funk. Notable slap and funky players include Bernard Edwards(Chic), Robert 'Kool' Bell, Mark Adams(Slave), Victor Wooten, Johnny Flippin(Fatback) and Bootsy Collins. While slap and funky is important, some influential bassists who play funk, such as Rocco Prestia (from Tower of Power), did not use the approach, and instead used a typical fingerstyle method based on James Jamerson's Motown playing style. Larry Graham from Sly and the Family Stone is an influential bassist.

Funk bass has an "earthy, percussive kind of feel", in part due to the use of muted, rhythmic ghost notes (also called "dead notes"). Some funk bass players use electronic effects units to alter the tone of their instrument, such as "envelope filters" (an auto-wah effect that creates a "gooey, slurpy, quacky, and syrupy" sound) and imitate keyboard synthesizer bass tones (e.g., the Mutron envelope filter) and overdriven fuzz bass effects, which are used to create the "classic fuzz tone that sounds like old school Funk records". Other effects that are used include the flanger and bass chorus. Collins also used a Mutron Octave Divider, an octave pedal that, like the Octavia pedal popularized by Hendrix, can double a note an octave above and below to create a "futuristic and fat low-end sound".

Funk drumming creates a groove by emphasizing the drummer's "feel and emotion", which including "occasional tempo fluctuations", the use of swing feel in some songs (e.g., "Cissy Strut" by The Meters and "I'll Take You There" by The Staple Singers, which have a half-swung feel), and less use of fills (as they can lessen the groove). Drum fills are "few and economical", to ensure that the drumming stays "in the pocket", with a steady tempo and groove. These playing techniques are supplemented by a set-up for the drum kit that often includes muffled bass drums and toms and tightly tuned snare drums. Double bass drumming sounds are often done by funk drummers with a single pedal, an approach which "accents the second note... [and] deadens the drumhead's resonance", which gives a short, muffled bass drum sound.

In Tower Of Power drummer David Garibaldi's playing, there are many "ghost notes" and rim shots. A key part of the funk drumming style is using the hi-hat, with opening and closing the hi-hats during playing (to create "splash" accent effects) being an important approach. Two-handed sixteenth notes on the hi-hats, sometimes with a degree of swing feel, is used in funk.

Jim Payne states that funk drumming uses a "wide-open" approach to improvisation around rhythmic ideas from Latin music, ostinatos, that are repeated "with only slight variations", an approach which he says causes the "mesmerizing" nature of funk. Payne states that funk can be thought of as "rock played in a more syncopated manner", particularly with the bass drum, which plays syncopated eighth note and sixteenth note patterns that were innovated by Clive Williams (with Joe Tex); George Brown (with Kool & the Gang) and James "Diamond" Williams (with The Ohio Players). As with rock, the snare backbeats on beats two and four are still used in most funk (albeit with additional soft ghost notes).

Some funk bands used two drummers in shows, such as James Brown's band, the JBs. By using two drummers, the JB band was able to maintain a "solid syncopated" rhythmic sound, which contributed to the band's distinctive "Funky Drummer" rhythm.

In funk, guitarists often mix playing chords of a short duration (nicknamed "stabs") with faster rhythms and riffs. Guitarists playing rhythmic parts often play sixteenth notes, including with percussive "ghost notes". Chord extensions are favored, such as ninth chords. Typically, funk uses "two interlocking [electric] guitar parts", with a rhythm guitarist and a "tenor guitarist" who plays single notes. The two guitarists trade off their lines to create a "call-and-response, intertwined pocket." If a band only has one guitarist, this effect may be recreated by overdubbing in the studio, or, in a live show, by having a single guitarist play both parts, to the degree that this is possible.

In funk bands, guitarists typically play in a percussive style, using a style of picking called the "chank" or "chicken scratch", in which the guitar strings are pressed lightly against the fingerboard and then quickly released just enough to get a muted “scratching” sound that is produced by rapid rhythmic strumming of the opposite hand near the bridge. The technique can be broken down into three approaches: the "chika", the "chank" and the "choke". With the "chika" comes a muted sound of strings being hit against the fingerboard; "chank" is a staccato attack done by releasing the chord with the fretting hand after strumming it; and "choking" generally uses all the strings being strummed and heavily muted. 
The result of these factors was a rhythm guitar sound that seemed to float somewhere between the low-end thump of the electric bass and the cutting tone of the snare and hi-hats, with a rhythmically melodic feel that fell deep in the pocket. Guitarist Jimmy Nolen, longtime guitarist for James Brown, developed this technique. On Brown's "Give It Up or Turnit a Loose" (1969), however, Jimmy Nolen's guitar part has a bare bones tonal structure. The pattern of attack-points is the emphasis, not the pattern of pitches. The guitar is used the way that an African drum, or idiophone would be used. Nolen created a "clean, trebly tone" by using "hollow-body jazz guitars with single-coil P-90 pickups" plugged into a Fender Twin Reverb amp with the mid turned down low and the treble turned up high.

Funk guitarists playing rhythm guitar generally avoid distortion effects and amp overdrive to get a clean sound, and given the importance of a crisp, high sound, Fender Stratocasters and Telecasters were widely used for their cutting treble tone. The mids are often cut by guitarists to help the guitar sound different from the horn section, keyboards and other instruments. Given the focus on providing a rhythmic groove, and the lack of emphasis on instrumental guitar melodies and guitar solos, sustain is not sought out by funk rhythm guitarists. Funk rhythm guitarists use compressor volume-control effects to enhance the sound of muted notes, which boosts the “clucking” sound and adds "percussive excitement to funk rhythms" (an approach used by Nile Rodgers).

Guitarist Eddie Hazel from Funkadelic is notable for his solo improvisation (partiularly for the solo on "Maggot Brain") and guitar riffs, the tone of which was shaped by a Maestro Fuzz-Tone FZ-1A pedal. Hazel, along with guitarist Ernie Isley of the Isley Brothers, was influenced by Jimi Hendrix's improvised, wah-wah infused solos. Ernie Isley was tutored at an early age by Hendrix, when Hendrix was a part of the Isley Brothers backing band and temporarily lived in the Isleys' household.Funk guitarists use the wah-wah sound effect along with muting the notes to create a percussive sound for their guitar riffs. The phaser effect is often used in funk and R&B guitar playing for its filter sweeping sound effect, an example being the Isley Brothers' song "Who's That Lady". Michael Hampton, another P-Funk guitarist, was able to play Hazel's virtuostic solo on "Maggot Brain", using a solo approach that added in string bends and Hendrix-style feedback.

A range of keyboard instruments are used in funk. Acoustic piano is used in funk, including in “September” by Earth Wind & Fire and “Will It Go ‘Round in Circles” by Billy Preston. The electric piano is used on songs such as Herbie Hancock’s “Chameleon” (a Fender Rhodes) and “Mercy, Mercy, Mercy” by Joe Zawinul (a Wurlitzer). The clavinet is used for its percussive tone, and it can be heard in songs such as Stevie Wonder's “Superstition” and “Higher Ground” and Bill Withers' song “Use Me”. The Hammond B-3 organ is used in funk, in songs such as “Cissy Strut” by The Meters and “Love the One You’re With” (with Aretha Franklin singing and Billy Preston on keyboards).

Bernie Worrell's range of keyboards from his recordings with Parliament Funkadelic demonstrate the wide range of keyboards used in funk, as they include the Hammond organ (“Funky Woman,” “Hit It and Quit It,” “Wars of Armageddon”); RMI electric piano (“I Wanna Know If It’s Good to You?,” “Free Your Mind,” “Loose Booty”); acoustic piano (“Funky Dollar Bill,” “Jimmy’s Got a Little Bit of Bitch in Him”); clavinet (“Joyful Process,” “Up for the Down Stroke,” “Red Hot Mama”); Minimoog synthesizer (“Flash Light,” “Aqua Boogie,” “Knee Deep,” “Let’s Take It to the Stage”); and ARP string ensemble synth (“Chocolate City,” “Undisco Kidd”).

Synthesizers were used in funk both to add to the deep sound of the electric bass, or even to replace the electric bass altogether in some songs. Funk synthesizer bass was used because it could create layered sounds and new electronic tones that were not feasible on electric bass.

In the 1970s, funk used many of the same vocal styles that were used in African-American music in the 1960s, including singing influences from blues, gospel, jazz and doo-wop. Like these other African-American styles, funk used "[y]ells, shouts, hollers, moans, humming, and melodic riffs", along with styles such as call and response and narration of stories (like the African oral tradition approach). The call and response in funk can be between the lead singer and the band members who act as backup vocalists.

As funk emerged from soul, the vocals in funk share soul's approach; however, funk vocals tend to be "more punctuated, energetic, rhythmically percussive[,] and less embellished" with ornaments, and the vocal lines tend to resemble horn parts and have "pushed" rhythms. Funk bands such as Earth, Wind & Fire have harmony vocal parts. Songs like "Super Bad" by James Brown included "double-voice" along with "yells, shouts and screams". Funk singers used a "black aesthetic" to perform that made use of "colorful and lively exchange of gestures, facial expressions, body posture, and vocal phrases" to create an engaging performance.

The lyrics in funk music addressed issues faced by the African American community in the United States during the 1970s, which arose due to the move away from an industrial, working-class economy to an information economy, which harmed the Black working class. Funk songs by The Ohio Players, Earth, Wind & Fire, and James Brown raised issues faced by lower-income Blacks in their song lyrics, such as poor "economic conditions and themes of poor inner-city life in the black communities".

The Funkadelic song "One Nation Under A Groove" (1978) is about the challenges that Blacks overcame during the 1960s civil rights movement, and it includes an exhortation for Blacks in the 1970s to capitalize on the new "social and political opportunities" that had become available in the 1970s. The Isley Brothers song "Fight the Power" (1975) has a political message. Parliament's song "Chocolate City" (1975) metaphorically refers to Washington D.C. and other US cities that have a mainly Black population, and it draws attention to the potential power that Black voters wield and suggests that a Black President be considered in the future.

The political themes of funk songs and the aiming of the messages to a Black audience echoed the new image of Blacks that was created in Blaxploitation films, which depicted "African-America men and women standing their ground and fighting for what was right". Both funk and Blaxploitation films addressed issues faced by Blacks and told stories from a Black perspective. Another link between 1970s funk and Blaxploitation films is that many of these films used funk soundtracks (e.g., Curtis Mayfield for "Superfly"; James Brown and Fred Wesley for "Black Caesar" and War for "Youngblood").

Funk songs included metaphorical language that was understood best by listeners who were "familiar with the black aesthetic and [black] vernacular". For example, funk songs included expressions such as "shake your money maker", "funk yourself right out" and "move your boogie body". Another example is the use of "bad" in the song "Super Bad" (1970), which black listeners knew meant "good" or "great".

In the 1970s, to get around radio obscenity restrictions, funk artists would use words that sounded like non-allowed words and double entendres to get around these restrictions. For example, The Ohio Players had a song entitled "Fopp" which referred to "Fopp me right, don't you fopp me wrong/We'll be foppin' all night long...". Some funk songs used made-up words which suggested that they were "writing lyrics in a constant haze of marijuana smoke", such as Parliament's "Aqua Boogie (A Psychoalphadiscobetabioaquadooloop)", which includes words such as "bioaquadooloop". The mainstream white listener base was often not able to understand funk's lyrical messages, which contributed to funk's lack of popular music chart success with white audiences during the 1970s.

Horn section arrangements with groups of brass instruments are used in funk songs. Funk horn sections could include saxophone (often tenor sax), trumpet, trombone, and for larger horn sections, such as quintets and sextets, a baritone sax. Horn sections played "rhythmic and syncopated" parts, often with "offbeat phrases" that emphasize "rhythmic displacement". Funk song introductions are being an important place for using horn arrangements.

Funk horn sections performed in a "rhythmic percussive style" that mimicked the approach used by funk rhythm guitarists. Horn sections would "punctuate" the lyrics by playing in the spaces between vocals, using "short staccato rhythmic blast[s]". Notable funk horn players included Alfred "PeeWee" Ellis, trombonist Fred Wesley, and alto sax player Maceo Parker. Notable funk horn sections including the "Phoenix Horns" (with Earth, Wind & Fire), the "Horny Horns" (with Parliament), the "Memphis Horns" (with Isaac Hayes), and "MFSB" (with Curtis Mayfield).

The instruments in funk horn sections varied. If there were two brass instruments, it could be trumpet and tenor sax, trumpet and trombone, or two saxes. If there were three brass players, it could be trumpet, sax and trombone or a trumpet and two saxes. A quartet of brass instruments would often be a pair of an instrument type and two other instruments. Quintets would typically take a pair of brass instruments (saxes or trumpets), and add different high and low brass instruments. With six instruments, a brass section would typically be two pairs of brass instruments plus a trombone and baritone sax holding down the bottom end.

Notable songs with funk horn sections include:

In bands or shows where hiring a horn section is not feasible, a keyboardist can play the horn section parts on a synthesizer with "keyboard brass patches", however, choosing an authentic-sounding synthesizer and brass patch is important. In the 2010s, with micro-MIDI synths, it may even be possible to have another instrumentalist play the keyboard brass parts, thus enabling the keyboardist to continue to comp throughout the song.

Funk bands in the 1970s adopted Afro-American fashion and style, including "Bell-bottom pants, platform shoes, hoop earring[s], Afros [hairstyles], leather vests... beaded necklaces", dashiki shirts, jumpsuits and boots. In contrast to earlier bands such as The Temptations, which wore "matching suits" and "neat haircuts" to appeal to white mainstream audiences, funk bands adopted an "African spirit" in their outfits and style. George Clinton and Parliament are known for their imaginative costumes and "freedom of dress", which included bedsheets acting as robes and capes.

The distinctive characteristics of African-American musical expression are rooted in sub-Saharan African music traditions, and find their earliest expression in spirituals, work chants/songs, praise shouts, gospel, blues, and "body rhythms" (hambone, patting juba, and ring shout clapping and stomping patterns). Funk music is an amalgam of soul music, soul jazz, R&B, and Afro-Cuban rhythms absorbed and reconstituted in New Orleans. Like other styles of African-American musical expression including jazz, soul music and R&B, funk music accompanied many protest movements during and after the Civil Rights Movement. Funk allowed everyday experiences to be expressed to challenge daily struggles and hardships fought by lower and working class communities.

Gerhard Kubik notes that with the exception of New Orleans, early blues lacked complex polyrhythms, and there was a "very specific absence of asymmetric time-line patterns (key patterns) in virtually all early twentieth century African-American music ... only in some New Orleans genres does a hint of simple time line patterns occasionally appear in the form of transient so-called 'stomp' patterns or stop-time chorus. These do not function in the same way as African time lines."

In the late 1940s this changed somewhat when the two-celled time line structure was brought into New Orleans blues. New Orleans musicians were especially receptive to Afro-Cuban influences precisely at the time when R&B was first forming. Dave Bartholomew and Professor Longhair (Henry Roeland Byrd) incorporated Afro-Cuban instruments, as well as the clave pattern and related two-celled figures in songs such as "Carnival Day" (Bartholomew 1949) and "Mardi Gras In New Orleans" (Longhair 1949). Robert Palmer reports that, in the 1940s, Professor Longhair listened to and played with musicians from the islands and "fell under the spell of Perez Prado's mambo records." Professor Longhair's particular style was known locally as "rumba-boogie".

One of Longhair's great contributions was his particular approach of adopting two-celled, clave-based patterns into New Orleans rhythm and blues (R&B). Longhair's rhythmic approach became a basic template of funk. According to Dr. John (Malcolm John "Mac" Rebennack, Jr.), the Professor "put funk into music ... Longhair's thing had a direct bearing I'd say on a large portion of the funk music that evolved in New Orleans." In his "Mardi Gras in New Orleans", the pianist employs the 2-3 clave onbeat/offbeat motif in a rumba-boogie "guajeo".

The syncopated, but straight subdivision feel of Cuban music (as opposed to swung subdivisions) took root in New Orleans R&B during this time. Alexander Stewart states: "Eventually, musicians from outside of New Orleans began to learn some of the rhythmic practices [of the Crescent City]. Most important of these were James Brown and the drummers and arrangers he employed. Brown's early repertoire had used mostly shuffle rhythms, and some of his most successful songs were 12/8 ballads (e.g. 'Please, Please, Please' (1956), 'Bewildered' (1961), 'I Don't Mind' (1961)). Brown's change to a funkier brand of soul required 4/4 metre and a different style of drumming." Stewart makes the point: "The singular style of rhythm & blues that emerged from New Orleans in the years after World played an important role in the development of funk. In a related development, the underlying rhythms of American popular music underwent a basic, yet generally unacknowledged transition from triplet or shuffle feel to even or straight eighth notes."

Chuck Connors, Little Richard's drums-studded (and native from New Orleans), mid-1950s R&B road band was credited by James Brown and others as being the first to put the funk in the rock'n'roll beat. Following his temporary exit from secular music to become an evangelist in 1957, some of Little Richard's band members joined Brown and the Famous Flames, beginning a long string of hits for them in 1958. By the mid-1960s, James Brown had developed his signature groove that emphasized the downbeat—with heavy emphasis on the first beat of every measure to etch his distinctive sound, rather than the backbeat that typified African-American music. Brown often cued his band with the command "On the one!," changing the percussion emphasis/accent from the one-two-three-four backbeat of traditional soul music to the one-two-three-four downbeat – but with an even-note syncopated guitar rhythm (on quarter notes two and four) featuring a hard-driving, repetitive brassy swing. This one-three beat launched the shift in Brown's signature music style, starting with his 1964 hit single, "Out of Sight" and his 1965 hits, "Papa's Got a Brand New Bag" and "I Got You (I Feel Good)".

Brown's style of funk was based on interlocking, contrapuntal parts: syncopated bass lines, 16th beat drum patterns, and syncopated guitar riffs. The main guitar ostinatos for "Ain't it Funky" (c. late 1960s) are an example of Brown's refinement of New Orleans funk— an irresistibly danceable riff, stripped down to its rhythmic essence. On "Ain't it Funky" the tonal structure is barebones. Brown's innovations led to him and his band becoming the seminal funk act; they also pushed the funk music style further to the forefront with releases such as "Cold Sweat" (1967), "Mother Popcorn" (1969) and "Get Up (I Feel Like Being A) Sex Machine" (1970), discarding even the twelve-bar blues featured in his earlier music. Instead, Brown's music was overlaid with "catchy, anthemic vocals" based on "extensive vamps" in which he also used his voice as "a percussive instrument with frequent rhythmic grunts and with rhythm-section patterns ... [resembling] West African polyrhythms" – a tradition evident in African-American work songs and chants. Throughout his career, Brown's frenzied vocals, frequently punctuated with screams and grunts, channeled the "ecstatic ambiance of the black church" in a secular context.

After 1965, Brown's bandleader and arranger was Alfred "Pee Wee" Ellis. Ellis credits Clyde Stubblefield's adoption of New Orleans drumming techniques, as the basis of modern funk: "If, in a studio, you said 'play it funky' that could imply almost anything. But 'give me a New Orleans beat' – you got exactly what you wanted. And Clyde Stubblefield was just the epitome of this funky drumming." Stewart states that the popular feel was passed along from "New Orleans—through James Brown's music, to the popular music of the 1970s." Concerning the various funk motifs, Stewart states that this model "...is different from a time line (such as clave and tresillo) in that it is not an exact pattern, but more of a loose organizing principle."

In a 1990 interview, Brown offered his reason for switching the rhythm of his music: "I changed from the upbeat to the downbeat ... Simple as that, really." According to Maceo Parker, Brown's former saxophonist, playing on the downbeat was at first hard for him and took some getting used to. Reflecting back to his early days with Brown's band, Parker reported that he had difficulty playing "on the one" during solo performances, since he was used to hearing and playing with the accent on the second beat.

Other musical groups picked up on the rhythms and vocal style developed by James Brown and his band, and the funk style began to grow. Dyke and the Blazers, based in Phoenix, Arizona, released "Funky Broadway" in 1967, perhaps the first record of the soul music era to have "funky" in the title. In 1969 Jimmy McGriff released "Electric Funk", featuring his distinctive organ over a blazing horn section. Meanwhile, on the West Coast, Charles Wright & the Watts 103rd Street Rhythm Band was releasing funk tracks beginning with its first album in 1967, culminating in the classic single "Express Yourself" in 1971. Also from the West Coast area, more specifically Oakland, San Francisco, came the band Tower of Power (TOP), which formed in 1968. Their debut album "East Bay Grease", released 1970, is considered an important milestone in funk. Throughout the 1970s, TOP had many hits, and the band helped to make funk music a successful genre, with a broader audience.

In 1970, Sly & the Family Stone's "Thank You (Falettinme Be Mice Elf Agin)" reached #1 on the charts, as did "Family Affair" in 1971. Notably, these afforded the group and the genre crossover success and greater recognition, yet such success escaped comparatively talented and moderately popular funk band peers. The Meters defined funk in New Orleans, starting with their top ten R&B hits "Sophisticated Cissy" and "Cissy Strut" in 1969. Another group who defined funk around this time were the Isley Brothers, whose funky 1969 #1 R&B hit, "It's Your Thing", signaled a breakthrough in African-American music, bridging the gaps of the jazzy sounds of Brown, the psychedelic rock of Jimi Hendrix, and the upbeat soul of Sly & the Family Stone and Mother's Finest. The Temptations, who had previously helped to define the "Motown Sound" – a distinct blend of pop-soul – adopted this new psychedelic sound towards the end of the 1960s as well. Their producer, Norman Whitfield, became an innovator in the field of psychedelic soul, creating hits with a newer, funkier sound for many Motown acts, including "War" by Edwin Starr, "Smiling Faces Sometimes" by the Undisputed Truth and "Papa Was A Rollin' Stone" by the Temptations. Motown producers Frank Wilson ("Keep On Truckin'") and Hal Davis ("Dancing Machine") followed suit. Stevie Wonder and Marvin Gaye also adopted funk beats for some of their biggest hits in the 1970s, such as "Superstition" and "You Haven't Done Nothin'", and "I Want You" and "Got To Give It Up", respectively.

A new group of musicians began to further develop the "funk rock" approach. Innovations were prominently made by George Clinton, with his bands Parliament and Funkadelic. Together, they produced a new kind of funk sound heavily influenced by jazz and psychedelic rock. The two groups shared members and are often referred to collectively as "Parliament-Funkadelic." The breakout popularity of Parliament-Funkadelic gave rise to the term "P-Funk", which referred to the music by George Clinton's bands, and defined a new subgenre. Clinton played a principal role in several other bands, including Parlet, the Horny Horns, and the Brides of Funkenstein, all part of the P-Funk conglomerate. "P-funk" also came to mean something in its quintessence, of superior quality, or "sui generis".

The 1970s were the era of highest mainstream visibility for funk music. In addition to Parliament Funkadelic, artists like Sly and the Family Stone, Rufus & Chaka Khan, Bootsy's Rubber Band, the Isley Brothers, Ohio Players, Con Funk Shun, Kool and the Gang, the Bar-Kays, Commodores, Roy Ayers, and Stevie Wonder, among others, were successful in getting radio play. Disco music owed a great deal to funk. Many early disco songs and performers came directly from funk-oriented backgrounds. Some disco music hits, such as all of Barry White's hits, "Kung Fu Fighting" by Biddu and Carl Douglas, Donna Summer's "Love To Love You Baby", Diana Ross' "Love Hangover", KC and the Sunshine Band's "I'm Your Boogie Man", "I'm Every Woman" by Chaka Khan (also known as the Queen of Funk), and Chic's "Le Freak" conspicuously include riffs and rhythms derived from funk. In 1976, Rose Royce scored a number-one hit with a purely dance-funk record, "Car Wash". Even with the arrival of disco, funk became increasingly popular well into the early 1980s.

Funk music was also exported to Africa, and it melded with African singing and rhythms to form Afrobeat. Nigerian musician Fela Kuti, who was heavily influenced by James Brown's music, is credited with creating the style and terming it "Afrobeat".

Jazz-funk is a subgenre of jazz music characterized by a strong back beat (groove), electrified sounds and an early prevalence of analog synthesizers. The integration of funk, soul, and R&B music and styles into jazz resulted in the creation of a genre whose spectrum is quite wide and ranges from strong jazz improvisation to soul, funk or disco with jazz arrangements, jazz riffs, and jazz solos, and sometimes soul vocals. Jazz-funk is primarily an American genre, where it was popular throughout the 1970s and the early 1980s, but it also achieved noted appeal on the club-circuit in England during the mid-1970s. Similar genres include soul jazz and jazz fusion, but neither entirely overlap with jazz-funk. Notably jazz-funk is less vocal, more arranged and featured more improvisation than soul jazz, and retains a strong feel of groove and R&B versus some of the jazz fusion production.

In the 1970s, at the same time that jazz musicians began to explore blending jazz with rock to create jazz fusion, major jazz performers began to experiment with funk. Jazz-funk recordings typically used electric bass and electric piano in the rhythm section, in place of the double bass and acoustic piano that were typically used in jazz up till that point. Pianist and bandleader Herbie Hancock was the first of many big jazz artists who embraced funk during the decade. Hancock's Headhunters band (1973) played the jazz-funk style. The Headhunters' lineup and instrumentation, retaining only wind player Bennie Maupin from Hancock's previous sextet, reflected his new musical direction. He used percussionist Bill Summers in addition to a drummer. Summers blended African, Afro-Cuban, and Afro-Brazilian instruments and rhythms into Hancock's jazzy funk sound.

"On the Corner" (1972) was jazz trumpeter-composer Miles Davis's seminal foray into jazz-funk. Like his previous works though, "On the Corner" was experimental. Davis stated that "On the Corner" was an attempt at reconnecting with the young black audience which had largely forsaken jazz for rock and funk. While there is a discernible funk influence in the timbres of the instruments employed, other tonal and rhythmic textures, such as the Indian tambora and tablas, and Cuban congas and bongos, create a multi-layered soundscape. From a musical standpoint, the album was a culmination of sorts of the recording studio-based "musique concrète" approach that Davis and producer Teo Macero (who had studied with Otto Luening at Columbia University's Computer Music Center) had begun to explore in the late 1960s. Both sides of the record featured heavy funk drum and bass grooves, with the melodic parts snipped from hours of jams and mixed in the studio.

Also cited as musical influences on the album by Davis were the contemporary composer Karlheinz Stockhausen.

In the 1980s, largely as a reaction against what was seen as the over-indulgence of disco, many of the core elements that formed the foundation of the P-Funk formula began to be usurped by electronic instruments, drum machines and synthesizers. Horn sections of saxophones and trumpets were replaced by synth keyboards, and the horns that remained were given simplified lines, and few horn solos were given to soloists. The classic electric keyboards of funk, like the Hammond B3 organ, the Hohner Clavinet and/or the Fender Rhodes piano began to be replaced by the new digital synthesizers such as the Yamaha DX7. Electronic drum machines such as the Roland TR-808 began to replace the "funky drummers" of the past, and the slap and pop style of bass playing were often replaced by synth keyboard bass lines. Lyrics of funk songs began to change from suggestive double entendres to more graphic and sexually explicit content.

In the late 1970s, the electronic music band Yellow Magic Orchestra (YMO) began experimenting with electronic funk music, introducing "videogame-funk" sounds with hits such as "Computer Game" (1978), which had a strong influence on the later electro-funk genre. In 1980, YMO was the first band to use the TR-808 programmable drum machine, while YMO member Ryuichi Sakamoto's "" developed the beats and sounds of electro-funk that same year, influencing later electro-funk artists such as Afrika Bambaataa and Mantronix.

Rick James was the first funk musician of the 1980s to assume the funk mantle dominated by P-Funk in the 1970s. His 1981 album "Street Songs", with the singles "Give It to Me Baby" and "Super Freak", resulted in James becoming a star, and paved the way for the future direction of explicitness in funk.

Beginning in the late 1970s, Prince used a stripped-down, dynamic instrumentation similar to James. However, Prince went on to have as much of an impact on the sound of funk as any one artist since Brown; he combined eroticism, technology, an increasing musical complexity, and an outrageous image and stage show to ultimately create music as ambitious and imaginative as P-Funk. Prince formed the Time, originally conceived as an opening act for him and based on his "Minneapolis sound", a hybrid mixture of funk, R&B, rock, pop & new wave. Eventually, the band went on to define their own style of stripped-down funk based on tight musicianship and sexual themes.

Similar to Prince, other bands emerged during the P-Funk era and began to incorporate uninhibited sexuality, dance-oriented themes, synthesizers and other electronic technologies to continue to craft funk hits. These included Cameo, Zapp, the Gap Band, the Bar-Kays, and the Dazz Band, who all found their biggest hits in the early 1980s. By the latter half of the 80s, pure funk had lost its commercial impact; however, pop artists from Michael Jackson to Duran Duran often used funk beats.

Influenced by Yellow Magic Orchestra and Kraftwerk, the American musician Afrika Bambaataa developed electro-funk, a minimalist machine-driven style of funk with his single "Planet Rock" in 1982. Also known simply as electro, this style of funk was driven by synthesizers and the electronic rhythm of the TR-808 drum machine. The single "Renegades of Funk" followed in 1983.

While funk was all but driven from the radio by slick commercial hip hop, contemporary R&B and new jack swing, its influence continued to spread. Artists like Steve Arrington and Cameo still received major airplay and had huge global followings. Rock bands began copying elements of funk to their sound, creating new combinations of "funk rock" and "funk metal". Extreme, Red Hot Chili Peppers, Living Colour, Jane's Addiction, Prince, Primus, Fishbone, Faith No More, Rage Against the Machine, Infectious Grooves, and Incubus spread the approach and styles garnered from funk pioneers to new audiences in the mid-to-late 1980s and the 1990s. These bands later inspired the underground mid-1990s funkcore movement and current funk-inspired artists like Outkast, Malina Moye, Van Hunt, and Gnarls Barkley.

In the 1990s, artists like Me'shell Ndegeocello and the (predominantly UK-based) acid jazz movement including artists and bands such as Jamiroquai, Incognito, Galliano, Omar, Los Tetas and the Brand New Heavies carried on with strong elements of funk. However, they never came close to reaching the commercial success of funk in its heyday, with the exception of Jamiroquai whose album "Travelling Without Moving" sold about 11.5 million units worldwide. Meanwhile, in Australia and New Zealand, bands playing the pub circuit, such as Supergroove, Skunkhour and the Truth, preserved a more instrumental form of funk.

Since the late 1980s hip hop artists have regularly sampled old funk tunes. James Brown is said to be the most sampled artist in the history of hip hop, while P-Funk is the second most sampled artist; samples of old Parliament and Funkadelic songs formed the basis of West Coast G-funk.

Original beats that feature funk-styled bass or rhythm guitar riffs are also not uncommon. Dr. Dre (considered the progenitor of the G-funk genre) has freely acknowledged to being heavily influenced by George Clinton's psychedelic funk: "Back in the 70s that's all people were doing: getting high, wearing Afros, bell-bottoms and listening to Parliament-Funkadelic. That's why I called my album "The Chronic" and based my music and the concepts like I did: because his shit was a big influence on my music. Very big". Digital Underground was a large contributor to the rebirth of funk in the 1990s by educating their listeners with knowledge about the history of funk and its artists. George Clinton branded Digital Underground as "Sons of the P", as their second full-length release is also titled. DU's first release, Sex Packets, was full of funk samples, with the most widely known "The Humpty Dance" sampling Parliament's "Let's Play House". A very strong funk album of DU's was their 1996 release "Future Rhythm". Much of contemporary club dance music, drum and bass in particular has heavily sampled funk drum breaks.

Funk is a major element of certain artists identified with the jam band scene of the late 1990s and 2000s. Phish began playing funkier jams in their sets around 1996, and 1998's "The Story of the Ghost" was heavily influenced by funk. Medeski Martin & Wood, Robert Randolph & the Family Band, Galactic, Widespread Panic, Jam Underground, Diazpora, Soulive, and Karl Denson's Tiny Universe all drew heavily from the funk tradition. Lettuce, a band of Berklee College Of Music graduates, was formed in the late 1990s as a pure-funk emergence was being felt through the jam band scene. Many members of the band including keyboardist Neal Evans went on to other projects such as Soulive or the Sam Kininger Band. Dumpstaphunk builds upon the New Orleans tradition of funk, with their gritty, low-ended grooves and soulful four-part vocals. Formed in 2003 to perform at the New Orleans Jazz & Heritage Festival, the band features keyboardist Ivan Neville and guitarist Ian Neville of the famous Neville family, with two bass players and female funk drummer Nikki Glaspie (formerly of Beyoncé Knowles's world touring band, as well as the Sam Kininger Band), who joined the group in 2011.

Since the mid-1990s the nu-funk scene, centered on the Deep Funk collectors scene, is producing new material influenced by the sounds of rare funk 45s. Labels include Desco, Soul Fire, Daptone, Timmion, Neapolitan, Bananarama, Kay-Dee, and Tramp. These labels often release on 45 rpm records. Although specializing in music for rare funk DJs, there has been some crossover into the mainstream music industry, such as Sharon Jones' 2005 appearance on "Late Night with Conan O'Brien".

In the early 2000s, some punk funk bands such as Out Hud and Mongolian MonkFish performed in the indie rock scene. Indie band Rilo Kiley, in keeping with their tendency to explore a variety of rockish styles, incorporated funk into their song "The Moneymaker" on the album "Under the Blacklight". Prince, with his later albums, gave a rebirth to the funk sound with songs like "The Everlasting Now", "Musicology", "Ol' Skool Company", and "Black Sweat".

Funk has also been incorporated into modern R&B music by many female singers such as Beyoncé with her 2003 hit "Crazy in Love" (which samples the Chi-Lites' "Are You My Woman"), Mariah Carey in 2005 with "Get Your Number" (which samples "Just an Illusion" by British band Imagination), Jennifer Lopez in 2005 with "Get Right" (which samples Maceo Parker's "Soul Power '74" horn sound), Amerie with her song "1 Thing" (which samples the Meters' "Oh, Calcutta!"), and also Tamar Braxton in 2013 with "The One" (which samples "Juicy Fruit" by Mtume).

Despite funk's popularity in modern music, few people have examined the work of funk women. Notable funk women include Chaka Khan, Labelle, Brides of Funkenstein, Klymaxx, Mother's Finest, Lyn Collins, Betty Davis and Teena Marie. As cultural critic Cheryl Keyes explains in her essay "She Was Too Black for Rock and Too Hard for Soul: (Re)discovering the Musical Career of Betty Mabry Davis," most of the scholarship around funk has focused on the cultural work of men. She states that "Betty Davis is an artist whose name has gone unheralded as a pioneer in the annals of funk and rock. Most writing on these musical genres has traditionally placed male artists like Jimi Hendrix, George Clinton (of Parliament-Funkadelic), and bassist Larry Graham as trendsetters in the shaping of a rock music sensibility."

In "The Feminist Funk Power of Betty Davis and Renée Stout", Nikki A. Greene notes that Davis' provocative and controversial style helped her rise to popularity in the 1970s as she focused on sexually motivated, self-empowered subject matter. Furthermore, this affected the young artist's ability to draw large audiences and commercial success. Greene also notes that Davis was never made an official spokesperson or champion for the civil rights and feminist movements of the time, although more recently her work has become a symbol of sexual liberation for women of color. Davis' song "If I'm In Luck I Just Might Get Picked Up", on her self-titled debut album, sparked controversy and was banned by the Detroit NAACP. Maureen Mahan, a musicologist and anthropologist, examines Davis' impact on the music industry and the American public in her article "They Say She's Different: Race, Gender, Genre, and the Liberated Black Femininity of Betty Davis." Mahan notes that Davis "took pleasure in her frank and public exploration of a black woman's sexual agency, but she did so in a context that offered limited opportunities for black female-centered expressions" (24).

Laina Dawes, the author of "What Are You Doing Here: A Black Woman's Life and Liberation in Heavy Metal", believes respectability politics is the reason artists like Davis do not get the same recognition as their male counterparts: "I blame what I call respectability politics as part of the reason the funk-rock some of the women from the '70s aren't better known. Despite the importance of their music and presence, many of the funk-rock females represented the aggressive behavior and sexuality that many people were not comfortable with."

According to Francesca T. Royster, in Rickey Vincent's book "Funk: The Music, The People, and The Rhythm of The One", he analyzes the impact of Labelle but only in limited sections. Royster criticizes Vincent's analysis of the group, stating: "It is a shame, then, that Vincent gives such minimal attention to Labelle's performances in his study. This reflects, unfortunately, a still consistent sexism that shapes the evaluation of funk music. In "Funk", Vincent's analysis of Labelle is brief—sharing a single paragraph with the Pointer Sisters in his three-page sub chapter, 'Funky Women.' He writes that while 'Lady Marmalade' 'blew the lid off of the standards of sexual innuendo and skyrocketed the group's star status,' the band's 'glittery image slipped into the disco undertow and was ultimately wasted as the trio broke up in search of solo status" (Vincent, 1996, 192). Many female artists who are considered to be in the genre of funk, also share songs in the disco, soul, and R&B genres; Labelle falls into this category of women who are split among genres due to a critical view of music theory and the history of sexism in the United States.

In recent years, artists like Janelle Monáe have opened the doors for more scholarship and analysis on the female impact on the funk music genre. Monáe's style bends concepts of gender, sexuality, and self-expression in a manner similar to the way some male pioneers in funk broke boundaries. Her albums center around Afro-futuristic concepts, centering on elements of female and black empowerment and visions of a dystopian future. In his article, "Janelle Monáe and Afro-sonic Feminist Funk", Matthew Valnes writes that Monae's involvement in the funk genre is juxtaposed with the traditional view of funk as a male-centered genre. Valnes acknowledges that funk is male-dominated, but provides insight to the societal circumstances that led to this situation.

Monáe's influences include her mentor Prince, Funkadelic, Lauryn Hill, and other funk and R&B artists, but according to Emily Lordi, "[Betty] Davis is seldom listed among Janelle Monáe's many influences, and certainly the younger singer's high-tech concepts, virtuosic performances, and meticulously produced songs are far removed from Davis's proto-punk aesthetic. But... like Davis, she also is closely linked with a visionary male mentor (Prince). The title of Monáe's 2013 album, "The Electric Lady", alludes to Hendrix's "Electric Ladyland", but it also implicitly cites the coterie of women that inspired Hendrix himself: that group, called the Cosmic Ladies or Electric Ladies, was together led by Hendrix's lover Devon Wilson and Betty Davis."

From the early 1970s onwards, funk has developed various subgenres. While George Clinton and the Parliament were making a harder variation of funk, bands such as Kool and the Gang, Ohio Players and Earth, Wind and Fire were making disco-influenced funk music.

Following the work of Jimi Hendrix in the late 1960s, black funk artists such as Sly and the Family Stone pioneered a style known as psychedelic funk by borrowing techniques from psychedelic rock music, including wah pedals, fuzz boxes, echo chambers, and vocal distorters, as well as blues rock and jazz. In the following years, groups such as George Clinton's Parliament-Funkadelic continued this sensibility, employing synthesizers and rock-oriented guitar work.

Funk rock (also written as "funk-rock" or "funk/rock") fuses funk and rock elements. Its earliest incarnation was heard in the late '60s through the mid-'70s by musicians such as Jimi Hendrix, Frank Zappa, Gary Wright, David Bowie, Mother's Finest, and Funkadelic on their earlier albums.

Many instruments may be incorporated into funk rock, but the overall sound is defined by a definitive bass or drum beat and electric guitars. The bass and drum rhythms are influenced by funk music but with more intensity, while the guitar can be funk-or-rock-influenced, usually with distortion. Prince, Jesse Johnson, Red Hot Chili Peppers and Fishbone are major artists in funk rock.

The term "avant-funk" has been used to describe acts who combined funk with art rock's concerns. Simon Frith described the style as an application of progressive rock mentality to rhythm rather than melody and harmony. Simon Reynolds characterized avant-funk as a kind of psychedelia in which "oblivion was to be attained not through rising above the body, rather through immersion in the physical, self loss through animalism."

Acts in the genre include German krautrock band Can, American funk artists Sly Stone and George Clinton, and a wave of early 1980s UK and US post-punk artists (including Public Image Ltd, Talking Heads, the Pop Group, Cabaret Voltaire, D.A.F., A Certain Ratio, and 23 Skidoo) who embraced black dance music styles such as disco and funk. The artists of the late 1970s New York no wave scene also explored avant-funk, influenced by figures such as Ornette Coleman. Reynolds noted these artists' preoccupations with issues such as alienation, repression and technocracy of Western modernity.

Go-go originated in the Washington, D.C. area with which it remains associated, along with other spots in the Mid-Atlantic. Inspired by singers such as Chuck Brown, the "Godfather of Go-go", it is a blend of funk, rhythm and blues, and early hip hop, with a focus on lo-fi percussion instruments and in-person jamming in place of dance tracks. As such, it is primarily a dance music with an emphasis on live audience call and response. Go-go rhythms are also incorporated into street percussion.

Boogie (or electro-funk) is an electronic music mainly influenced by funk and post-disco. The minimalist approach of boogie, consisting of synthesizers and keyboards, helped to establish electro and house music. Boogie, unlike electro, emphasizes the slapping techniques of bass guitar but also bass synthesizers. Artists include Vicky "D", Komiko, Peech Boys, Kashif, and later Evelyn King.

Electro funk is a hybrid of electronic music and funk. It essentially follows the same form as funk, and retains funk's characteristics, but is made entirely (or partially) with a use of electronic instruments such as the TR-808. Vocoders or talkboxes were commonly implemented to transform the vocals. The pioneering electro band Zapp commonly used such instruments in their music. Other artists include Herbie Hancock, Afrika Bambaataa, Egyptian Lover, Vaughan Mason & Crew, Midnight Star and Cybotron.

Funk metal (sometimes typeset differently such as "funk-metal") is a fusion genre of music which emerged in the 1980s, as part of the alternative metal movement. It typically incorporates elements of funk and heavy metal (often thrash metal), and in some cases other styles, such as punk and experimental music. It features hard-driving heavy metal guitar riffs, the pounding bass rhythms characteristic of funk, and sometimes hip hop-style rhymes into an alternative rock approach to songwriting. A primary example is the all-African-American rock band Living Colour, who have been said to be "funk-metal pioneers" by "Rolling Stone". During the late 1980s and early 1990s, the style was most prevalent in California – particularly Los Angeles and San Francisco.

G-funk is a fusion genre of music which combines gangsta rap and funk. It is generally considered to have been invented by West Coast rappers and made famous by Dr. Dre. It incorporates multi-layered and melodic synthesizers, slow hypnotic grooves, a deep bass, background female vocals, the extensive sampling of P-Funk tunes, and a high-pitched portamento saw wave synthesizer lead. Unlike other earlier rap acts that also utilized funk samples (such as EPMD and the Bomb Squad), G-funk often used fewer, unaltered samples per song.

Timba is a form of funky Cuban popular dance music. By 1990, several Cuban bands had incorporated elements of funk and hip-hop into their arrangements, and expanded upon the instrumentation of the traditional conjunto with an American drum set, saxophones and a two-keyboard format. Timba bands like La Charanga Habanera or Bamboleo often have horns or other instruments playing short parts of tunes by Earth, Wind and Fire, Kool and the Gang or other U.S. funk bands. While many funk motifs exhibit a clave-based structure, they are created intuitively, without a conscious intent of aligning the various parts to a guide-pattern. Timba incorporates funk motifs into an overt and intentional clave structure.

Funk jam is a fusion genre of music which emerged in the 1990s. It typically incorporates elements of funk and often exploratory guitar, along with extended cross genre improvisations; often including elements of jazz, ambient, electronic, Americana, and hip hop including improvised lyrics. Phish, Soul Rebels Brass Band, Galactic, and Soulive are all examples of funk bands that play funk jam.




</doc>
<doc id="10779" url="https://en.wikipedia.org/wiki?curid=10779" title="Frequency">
Frequency

Frequency is the number of occurrences of a repeating event per unit of time. It is also referred to as temporal "frequency", which emphasizes the contrast to spatial frequency and angular frequency. The period is the duration of time of one cycle in a repeating event, so the period is the reciprocal of the frequency. For example: if a newborn baby's heart beats at a frequency of 120 times a minute, its period—the time interval between beats—is half a second (60 seconds divided by 120 beats). Frequency is an important parameter used in science and engineering to specify the rate of oscillatory and vibratory phenomena, such as mechanical vibrations, audio signals (sound), radio waves, and light.

For cyclical processes, such as rotation, oscillations, or waves, frequency is defined as a number of cycles per unit time. In physics and engineering disciplines, such as optics, acoustics, and radio, frequency is usually denoted by a Latin letter "f" or by the Greek letter "formula_1" or "ν" (nu) (see e.g. Planck's formula).

The relation between the frequency and the period formula_2of a repeating event or oscillation is given by

The SI derived unit of frequency is the hertz (Hz), named after the German physicist Heinrich Hertz. One hertz means that an event repeats once per second. If a TV has a refresh rate of 1 hertz the TV's screen will change (or refresh) its picture once a second. A previous name for this unit was cycles per second (cps). The SI unit for period is the second.

A traditional unit of measure used with rotating mechanical devices is revolutions per minute, abbreviated r/min or rpm. 60 rpm equals one hertz.

As a matter of convenience, longer and slower waves, such as ocean surface waves, tend to be described by wave period rather than frequency. Short and fast waves, like audio and radio, are usually described by their frequency instead of period. These commonly used conversions are listed below:



For periodic waves in nondispersive media (that is, media in which the wave speed is independent of frequency), frequency has an inverse relationship to the wavelength, "λ" (lambda). Even in dispersive media, the frequency "f" of a sinusoidal wave is equal to the phase velocity "v" of the wave divided by the wavelength "λ" of the wave:

In the special case of electromagnetic waves moving through a vacuum, then "v = c", where "c" is the speed of light in a vacuum, and this expression becomes:

When waves from a monochrome source travel from one medium to another, their frequency remains the same—only their wavelength and speed change.

Measurement of frequency can be done in the following ways,

Calculating the frequency of a repeating event is accomplished by counting the number of times that event occurs within a specific time period, then dividing the count by the length of the time period. For example, if 71 events occur within 15 seconds the frequency is:
If the number of counts is not very large, it is more accurate to measure the time interval for a predetermined number of occurrences, rather than the number of occurrences within a specified time. The latter method introduces a random error into the count of between zero and one count, so on average half a count. This is called "gating error" and causes an average error in the calculated frequency of formula_11, or a fractional error of formula_12 where formula_13 is the timing interval and formula_14 is the measured frequency. This error decreases with frequency, so it is generally a problem at low frequencies where the number of counts N is small.

An older method of measuring the frequency of rotating or vibrating objects is to use a stroboscope. This is an intense repetitively flashing light (strobe light) whose frequency can be adjusted with a calibrated timing circuit. The strobe light is pointed at the rotating object and the frequency adjusted up and down. When the frequency of the strobe equals the frequency of the rotating or vibrating object, the object completes one cycle of oscillation and returns to its original position between the flashes of light, so when illuminated by the strobe the object appears stationary. Then the frequency can be read from the calibrated readout on the stroboscope. A downside of this method is that an object rotating at an integral multiple of the strobing frequency will also appear stationary.

Higher frequencies are usually measured with a frequency counter. This is an electronic instrument which measures the frequency of an applied repetitive electronic signal and displays the result in hertz on a digital display. It uses digital logic to count the number of cycles during a time interval established by a precision quartz time base. Cyclic processes that are not electrical in nature, such as the rotation rate of a shaft, mechanical vibrations, or sound waves, can be converted to a repetitive electronic signal by transducers and the signal applied to a frequency counter. As of 2018, frequency counters can cover the range up to about 100 GHz. This represents the limit of direct counting methods; frequencies above this must be measured by indirect methods.

Above the range of frequency counters, frequencies of electromagnetic signals are often measured indirectly by means of heterodyning (frequency conversion). A reference signal of a known frequency near the unknown frequency is mixed with the unknown frequency in a nonlinear mixing device such as a diode. This creates a heterodyne or "beat" signal at the difference between the two frequencies. If the two signals are close together in frequency the heterodyne is low enough to be measured by a frequency counter. This process only measures the difference between the unknown frequency and the reference frequency. To reach higher frequencies, several stages of heterodyning can be used. Current research is extending this method to infrared and light frequencies (optical heterodyne detection).

Visible light is an electromagnetic wave, consisting of oscillating electric and magnetic fields traveling through space. The frequency of the wave determines its color: is red light, is violet light, and between these (in the range 4-) are all the other colors of the visible spectrum. An electromagnetic wave can have a frequency less than , but it will be invisible to the human eye; such waves are called infrared (IR) radiation. At even lower frequency, the wave is called a microwave, and at still lower frequencies it is called a radio wave. Likewise, an electromagnetic wave can have a frequency higher than , but it will be invisible to the human eye; such waves are called ultraviolet (UV) radiation. Even higher-frequency waves are called X-rays, and higher still are gamma rays.

All of these waves, from the lowest-frequency radio waves to the highest-frequency gamma rays, are fundamentally the same, and they are all called electromagnetic radiation. They all travel through a vacuum at the same speed (the speed of light), giving them wavelengths inversely proportional to their frequencies.

where "c" is the speed of light ("c" in a vacuum, or less in other media), "f" is the frequency and λ is the wavelength.

In dispersive media, such as glass, the speed depends somewhat on frequency, so the wavelength is not quite inversely proportional to frequency.

Sound propagates as mechanical vibration waves of pressure and displacement, in air or other substances.. In general, frequency components of a sound determine its "color", its timbre. When speaking about the frequency (in singular) of a sound, it means the property that most determines pitch.

The frequencies an ear can hear are limited to a specific range of frequencies. The audible frequency range for humans is typically given as being between about 20 Hz and 20,000 Hz (20 kHz), though the high frequency limit usually reduces with age. Other species have different hearing ranges. For example, some dog breeds can perceive vibrations up to 60,000 Hz.

In many media, such as air, the speed of sound is approximately independent of frequency, so the wavelength of the sound waves (distance between repetitions) is approximately inversely proportional to frequency.

In Europe, Africa, Australia, Southern South America, most of Asia, and Russia, the frequency of the alternating current in household electrical outlets is 50 Hz (close to the tone G), whereas in North America and Northern South America, the frequency of the alternating current in household electrical outlets is 60 Hz (between the tones B♭ and B; that is, a minor third above the European frequency). The frequency of the 'hum' in an audio recording can show where the recording was made, in countries using a European, or an American, grid frequency.



</doc>
<doc id="10781" url="https://en.wikipedia.org/wiki?curid=10781" title="Film festival">
Film festival

A film festival is an organized, extended presentation of films in one or more cinemas or screening venues, usually in a single city or region. Increasingly, film festivals show some films outdoors. Films may be of recent date and, depending upon the festival's focus, can include international and domestic releases. Some festivals focus on a specific film-maker or genre (e.g., film noir) or subject matter (e.g., horror film festivals). A number of film festivals specialise in short films of a defined maximum length. Film festivals are typically annual events. Some film historians, including Jerry Beck, do not consider film festivals official releases of film.

The most prestigious film festivals in the world are generally considered to be Cannes, Berlin, and Venice. These festivals are sometimes called the "Big Three." The Toronto International Film Festival is North America's most popular festival in terms of attendance; "Time" wrote it had "grown from its place as the most influential fall film festival to the most influential film festival, period." The Venice Film Festival is the oldest film festival in the world.

The Venice Film Festival in Italy began in 1932, and is the oldest film festival still running. Raindance Film Festival is the UK's largest celebration of independent film-making, and takes place in London in October.

Mainland Europe's biggest independent film festival is ÉCU The European Independent Film Festival, that started in 2006 and takes place every spring in Paris, France. Edinburgh International Film Festival is the longest running festival in Great Britain as well as the longest continually running film festival in the world. 

Australia's first and longest running film festival is the Melbourne International Film Festival (1952), followed by the Sydney Film Festival (1954).

North America's first and longest running short film festival is the Yorkton Film Festival, established in 1947. The first film festival in the United States was the Columbus International Film & Video Festival, also known as The Chris Awards, held in 1953. According to the Film Arts Foundation in San Francisco, ""The Chris Awards" (is) one of the most prestigious documentary, educational, business and informational competitions in the U.S; (it is) the oldest of its kind in North America and celebrating its 54th year." It was followed four years later by the San Francisco International Film Festival, held in March 1957, which emphasized feature-length dramatic films. The festival played a major role in introducing foreign films to American audiences. Films in the first year included Akira Kurosawa's "Throne of Blood" and Satyajit Ray's "Pather Panchali".

Today, thousands of film festivals take place around the world—from high-profile festivals such as Sundance Film Festival and Slamdance Film Festival (Park City, Utah), to horror festivals such as Terror Film Festival (Philadelphia), and the Park City Film Music Festival, the first U.S. film festival dedicated to honoring music in film.

Film Funding competitions such as Writers and Filmmakers were introduced when the cost of production could be lowered significantly and internet technology allowed for the collaboration of film production.

Although there are notable for-profit festivals such as SXSW, most festivals operate on a nonprofit membership-based model, with a combination of ticket sales, membership fees, and corporate sponsorship constituting the majority of revenue. Unlike other arts nonprofits (performing arts, museums, etc.), film festivals typically receive few donations from the general public and are occasionally organized as nonprofit business associations instead of public charities. Film industry members often have significant curatorial input, and corporate sponsors are given opportunities to promote their brand to festival audiences in exchange for cash contributions. Private parties, often to raise investments for film projects, constitute significant "fringe" events. Larger festivals maintain year-round staffs often engaging in community and charitable projects outside festival season.

While entries from established filmmakers are usually considered pluses by the organizers, most festivals require new or relatively unknown filmmakers to pay an entry fee to have their works considered for screening. This is especially so in larger film festivals, such as the Cannes Film Festival, Toronto International Film Festival, Sundance Film Festival, South by Southwest, Montreal World Film Festival, and even smaller "boutique" festivals such as the Miami International Film Festival, British Urban Film Festival in London and Mumbai Women's International Film Festival in India.

On the other hand, some festivals—usually those accepting fewer films, and perhaps not attracting as many "big names" in their audiences as do Sundance and Telluride—require no entry fee. Rotterdam Film Festival, Mumbai Film Festival, and many smaller film festivals in the United States (the Stony Brook Film Festival on Long Island, the Northwest Filmmakers' Festival, and the Sicilian Film Festival in Miami), are examples.
The Portland International Film Festival charges an entry fee, but waives it for filmmakers from the Northwestern United States, and some others with regional focuses have similar approaches.

Several film festival submission portal websites exist to streamline filmmakers' entries into multiple festivals. They provide databases of festival calls for entry and offer filmmakers a convenient "describe once, submit many" service.

The core tradition of film festivals is competition, that is, the consideration of films with the intention of judging which are most deserving of various forms of recognition. In contrast to those films, some festivals may screen (i.e., project onto a movie screen before an audience) some films without treating them as part of the competition; the films are said to be "screened out..." (or "outside...") "of competition".

The three most prestigious film festivals are generally considered to be Cannes, Berlin, and Venice. These festivals are sometimes called the "Big Three." Polish director Krzysztof Kieślowski's "The Three Colors Trilogy" were each made for these festivals, with "" for Venice, "" for Berlin, and "" for Cannes.

The Toronto International Film Festival is North America's most popular festival. "Time" wrote it had "grown from its place as the most influential fall film festival to the most influential film festival, period." Seattle International Film Festival is credited as being the largest film festival in the USA, regularly showing over 400 films in a month across the city. The Sundance Film Festival, Tribeca Film Festival, South by Southwest, New York Film Festival, Woodstock Film Festival, Montreal World Film Festival, and Vancouver International Film Festival are also major North American festivals.

The festivals in Berlin, Cairo, Cannes, Goa, Karlovy Vary, Locarno, Mar del Plata, Montreal, Moscow, San Sebastián, Shanghai, Tallinn, Tokyo, Venice, and Warsaw are accredited by the International Federation of Film Producers Associations (FIAPF) in the category of competitive feature films.

Ann Arbor Film Festival started in 1963. It is the oldest continually operated experimental film festival in North America, and has become one of the premiere film festivals for independent and, primarily, experimental filmmakers to showcase work.

In the US, Telluride Film Festival, Sundance Film Festival, Austin Film Festival, Austin's South by Southwest, New York City's Tribeca Film Festival, London's London Eco-Film Festival and Slamdance Film Festival are all considered significant festivals for independent film. The Zero Film Festival is significant as the first and only festival exclusive to self-financed filmmakers. The biggest independent film festival in the UK is Raindance Film Festival.

A few film festivals have focused on highlighting specific issues/ subjects. These festivals have included both mainstream and independent films. Some examples include health-related film festivals and human rights films festivals.

The San Francisco International Film Festival, founded by Irving "Bud" Levin started in 1957, is the oldest continuously annual film festival in the United States. It highlights current trends in international filmmaking and video production with an emphasis on work that has not yet secured U.S. distribution.

The Toronto International Film Festival, founded by Bill Marshall, Henk Van der Kolk and Dusty Cohl, is regarded as North America's most major and most prestigious film festival, and is the most widely attended.

The Sundance film festival founded by Sterling Van Wagenen (then head of Wildwood, Robert Redford's company), John Earle, and Cirina Hampton Catania (both serving on the Utah Film Commission at the time) is a major festival for independent film.

The Woodstock Film Festival was launched in 2000 by filmmakers Meira Blaustein and Laurent Rejto with the goal to bring high quality independent film to the Hudson Valley region of New York. Indiewire has named the Woodstock Film Festival among the top 50 independent film festivals worldwide.

The Regina International Film Festival and Awards (RIFFA) founded by John Thimothy, one of the top leading international film festivals in western Canada (Regina, Saskatchewan) represented 92 countries in 2015 festival . RIFFA annual Award show and red carpet arrival event is getting noticed in the contemporary film and fashion industries in Western Canada.

Toronto's Hot Docs founded by filmmaker Paul Jay, is the leading North American documentary film festival. Toronto has the largest number of film festivals in the world, ranging from cultural, independent, and historic films.

The Seattle International Film Festival, which screens 270 features and approximately 150 short films, is one of the largest festivals in terms of the number of feature productions.

The Whistler Film Festival has more than 80 screenings and an industry summit.

The Cartagena Film Festival, founded by Victor Nieto in 1960, is the oldest film festival in Latin America. The Festival de Gramado (or Gramado Film Festival) Gramado, Brazil, along with the Guadalajara International Film Festival in Guadalajara, the Morelia International Film Festival in Morelia, Michoacan Mexico, and the Los Cabos International Film Festival founded by Scott Cross (film director), Sean Cross, and Eduardo Sanchez Navarro, in Los Cabos Baja Sur Mexico are considered the most important film festivals of Latin America. In 2015, Variety called the Los Cabos International Film Festival the "Cannes of Latin America". The Huelva Ibero-American Film Festival has been held since 1975 in that Spanish city.

The Expresión en Corto International Film Festival is the largest competitive film festival in Mexico. It specializes in emerging talent, and is held in the last week of each July in the two colonial cities of San Miguel de Allende and Guanajuato. Oaxaca Film Fest. For Spanish-speaking countries, the Dominican International Film Festival takes place annually in Puerto Plata, Dominican Republic. The Valdivia International Film Festival is held annually in the city of Valdivia. It is arguable the most important film festival in Chile. There is also Filmambiente, held in Rio de Janeiro, Brazil, an international festival on environmental films and videos.

The Havana Film Festival was founded in 1979 and is the oldest continuous annual film festival in the Caribbean. Its focus is on Latin American cinema.

The Trinidad and Tobago Film Festival, founded in 2006, is dedicated to screening the newest films from the English-, Spanish, French- and Dutch-speaking Caribbean, as well as the region's diaspora. It also seeks to facilitate the growth of Caribbean cinema by offering a wide-ranging industry programme and networking opportunities.

The Lusca Fantastic Film Fest (formely Puerto Rico Horror Film Fest) was also founded in 2006 and is the first and only international fantastic film festival in the Caribbean devoted to Sci-Fi, Thriller, Fantasy, Dark Humor, Bizarre, Horror, Anime, Adventure, Virtual Reality and Animation in Short and Feature Films.

Many film festivals are dedicated exclusively to animation.


A variety of regional festivals happen in various countries. Austin Film Festival is accredited by the Academy of Motion Picture Arts & Sciences, which makes all their jury award-winning narrative short and animated short films eligible for an Academy Award.

The International Film Festival of India, organized by the government of India, was founded in 1952. The Kolkata International Film Festival, founded in 1995, is the second oldest international film festival in India. The International Film Festival of Kerala organised by Government of Kerala held annually at Thiruvananthapuram is the biggest film festival in India in terms of public participation.

The International Documentary and Short Film Festival of Kerala (IDSFFK), hosted by the Kerala State Chalachitra Academy, is a major documentary and short film festival.

Other notable festivals in India include the Osian's-Cinefan: Festival of Asian Cinema at New Delhi, which recently expanded to include Arab Cinema, Chennai Women's International Film Festival (CWIFF), the Annual Mumbai Film Festival in India, with its 200,000 USD cash prize (www.mumbaifilmfest.com), and Mumbai Women's International Film Festival (MWIFF), an annual film festival in Mumbai featuring films made by women directors and women technicians.

Notable festivals include the Hong Kong International Film Festival (HKIFF), Busan International Film Festival (BIFF), and Kathmandu International Mountain Film Festival.

There are several significant film festivals held regularly in Africa. The biannual Panafrican Film and Television Festival of Ouagadougou (FESPACO) in Burkina Faso was established in 1969 and accepts for competition only films by African filmmakers and chiefly produced in Africa. The annual Durban International Film Festival in South Africa and Zanzibar International Film Festival in Tanzania have grown in importance for the film and entertainment industry, as they often screen the African premieres of many international films.

The Sahara International Film Festival, held annually in the Sahrawi refugee camps in western Algeria near the border of Western Sahara, is notable as the only film festival in the world to take place in a refugee camp. The festival has the two-fold aim of providing cultural entertainment and educational opportunities to refugees, and of raising awareness of the plight of the Sahrawi people, who have been exiled from their native Western Sahara for more than three decades.

The most important European film festivals are Venice Film Festival, Cannes Film Festival and Berlin International Film Festival, founded in 1932, 1946 and 1951 respectively. They are also considered the three most prestigious film festivals in the world.




</doc>
<doc id="10783" url="https://en.wikipedia.org/wiki?curid=10783" title="History of film">
History of film

Although the start of the history of film is not clearly defined, the commercial, public screening of ten of Lumière brothers' short films in Paris on 28 December 1895 can be regarded as the breakthrough of projected cinematographic motion pictures. There had been earlier cinematographic results and screenings but these lacked either the quality or the momentum that propelled the cinématographe Lumière into a worldwide success.

Soon film production companies were established all over the world. The first decade of motion picture saw film moving from a novelty to an established mass entertainment industry.

The earliest films were in black and white, under a minute long and without recorded sound.

During the 1890s films became several minutes long and started to consist of several shots. The first film studios were built in 1897. The first rotating camera for taking panning shots was built in 1898. Special effects were introduced and film continuity, involving action moving from one sequence into another, began to be used.

In the 1900s, continuity of action across successive shots was achieved and the first close-up shot was introduced (some claim D. W. Griffith was the inventor). Most films of this period were what came to be called "chase films". The first successful permanent theatre showing only films was "Nickelodeon" in Pittsburgh in 1905. The first feature length film multi-reel was a 1906 Australian production. By 1910, actors began to receive screen credit for their roles, opening the way for the creation of film stars. Regular newsreels were exhibited from 1910 and was a popular way for finding out the news, as well as creating a regular audience. From about 1910, American films had the largest share of the market in Australia and in all European countries except France.

New film techniques were introduced in this period including the use of artificial lighting, fire effects and low-key lighting (i.e. lighting in which most of the frame is dark) for enhanced atmosphere during sinister scenes. As films grew longer, specialist writers were employed to simplify more complex stories derived from novels or plays into a form that could be contained on one reel and be easier to be understood by the audience an audience that was new to this form of storytelling. Genres began to be used as categories; the main division was into comedy and drama but these categories were further subdivided. During the First World War there was a complex transition for the film industry. The exhibition of films changed from short one-reel programs to feature films. Exhibition venues became larger and began charging higher prices. By 1914, continuity cinema was the established mode of commercial cinema. One of the advanced continuity techniques involved an accurate and smooth transition from one shot to another.

D. W. Griffith had the highest standing among American directors in the industry, because of the dramatic excitement he conveyed to the audience through his films. The American film industry, or "Hollywood", as it was becoming known after its new geographical center in Hollywood, a neighborhood in Los Angeles, California, gained the position it has held, more or less, ever since: film factory for the world and exporting its product to most countries. By the 1920s, the United States reached what is still its era of greatest-ever output, producing an average of 800 "feature" films annually, or 82% of the global total (Eyman, 1997). During late 1927, Warner Bros. released "The Jazz Singer", with the first synchronized dialogue (and singing) in a feature film. By the end of 1929, Hollywood was almost all-talkie, with several competing sound systems (soon to be standardized). Sound saved the Hollywood studio system in the face of the Great Depression (Parkinson, 1995). However, the advent of the talkies meant a very high conversion cost for cinemas as well as producers.

The desire for wartime propaganda created a renaissance in the film industry in Britain, with realistic war dramas. The onset of American involvement in World War II also brought a proliferation of films as both patriotism and propaganda. The House Un-American Activities Committee investigated Hollywood in the early 1950s. During the immediate post-war years the cinematic industry was also threatened by television and the increasing popularity of the medium meant that some film theatres would bankrupt and close. The 1950s was considered a "Golden Age" for non-English cinema.

"Roundhay Garden Scene" is an 1888 short silent film recorded by French inventor Louis Le Prince. It is believed to be the oldest surviving film in existence, as noted by the "Guinness Book of Records".
The film "Sortie de l'usine Lumière de Lyon" (1895) by French Louis Lumière is considered the "first true motion picture".

Film as an art form has drawn on several earlier traditions in the fields such as (oral) storytelling, literature, theatre and visual arts. Forms of art and entertainment that had already featured moving and/or projected images include:

Some ancient sightings of gods and spirits may have been conjured up by means of (concave) mirrors, camera obscura or unknown projectors. By the 16th century necromantic ceremonies and the conjuring of ghostly apparitions by charlatan "magicians" and "witches" seemed commonplace. The very first magic lantern shows seem to have continued this tradition with images of death, monsters and other scary figures. Around 1790 this was developed into multi-media ghost shows known as phantasmagoria that could feature mechanical slides, rear projection, mobile projectors, superimposition, dissolves, live actors, smoke (sometimes to project images upon), odors, sounds and even electric shocks. While the first magic lantern images seem to have been intended to scare audiences, soon all sorts of subjects appeared and the lantern was not only used for storytelling but also for education. In the 19th century several new and popular magic lantern techniques were developed, including dissolving views and several types of mechanical slides that created dazzling abstract effects (chromatrope, etc.) or that showed for instance falling snow, or the planets and their moons revolving.

In the 1890s, films were seen mostly via temporary storefront spaces and traveling exhibitors or as acts in vaudeville programs. A film could be under a minute long and would usually present a single scene, authentic or staged, of everyday life, a public event, a sporting event or slapstick. There was little to no cinematic technique, the film was usually black and white and it was without sound.
The novelty of realistic moving photographs was enough for a motion picture industry to blossom before the end of the century, in countries around the world. "The Cinema" was to offer a cheaper, simpler way of providing entertainment to the masses. Filmmakers could record actors' performances, which then could be shown to audiences around the world. Travelogues would bring the sights of far-flung places, with movement, directly to spectators' hometowns. Movies would become the most popular visual art form of the late Victorian age.

The Berlin Wintergarten theater hosted an early movie presentation in front of an audience, shown by the Skladanowsky brothers in 1895. The Melbourne Athenaeum started to screen movies in 1896. Movie theaters became popular entertainment venues and social hubs in the early 20th century, much like cabarets and other theaters.
Until 1927, motion pictures were produced without sound. This era is referred to as the silent era of film. To enhance the viewers' experience, silent films were commonly accompanied by live musicians in an orchestra, a theatre organ, and sometimes sound effects and even commentary spoken by the showman or projectionist. In most countries, intertitles came to be used to provide dialogue and narration for the film, thus dispensing with narrators, but in Japanese cinema human narration remained popular throughout the silent era. The technical problems were resolved by 1923.

Illustrated songs were a notable exception to this trend that began in 1894 in vaudeville houses and persisted as late as the late 1930s in film theaters. Live performance or sound recordings were paired with hand-colored glass slides projected through stereopticons and similar devices. In this way, song narrative was illustrated through a series of slides whose changes were simultaneous with the narrative development. The main purpose of illustrated songs was to encourage sheet music sales, and they were highly successful with sales reaching into the millions for a single song. Later, with the birth of film, illustrated songs were used as filler material preceding films and during reel changes.

The 1914 "The Photo-Drama of Creation" was a non-commercial attempt to combine the motion picture with a combination of slides and synchronize the resulting moving picture with audio. The film included hand-painted slides as well as other previously used techniques. Simultaneously playing the audio while the film was being played with a projector was required. Produced by the Watch Tower Bible and Tract Society of Pennsylvania (Jehovah's Witnesses), this eight–hour bible drama was being shown in 80 cities every day and almost eight million people in the United States and Canada saw the presentation.

Within eleven years of motion pictures, the films moved from a novelty show to an established large-scale entertainment industry. Films moved from a single shot, completely made by one person with a few assistants, towards films several minutes long consisting of several shots, which were made by large companies in something like industrial conditions.

By 1900, the first motion pictures that can be considered as "films" emerged, and film-makers began to introduce basic editing techniques and film narrative.

Early movie cameras were fastened to the head of a tripod with only simple levelling devices provided. These cameras were effectively fixed during the course of a shot, and the first camera movements were the result of mounting a camera on a moving vehicle. The Lumière brothers shot a scene from the back of a train in 1896.

The first rotating camera for taking panning shots was built by Robert W. Paul in 1897, on the occasion of Queen Victoria's Diamond Jubilee. He used his camera to shoot the procession in one shot. His device had the camera mounted on a vertical axis that could be rotated by a worm gear driven by turning a crank handle, and Paul put it on general sale the next year. Shots taken using such a "panning" head were also referred to as 'panoramas' in the film catalogues.

Georges Méliès built one of the first film studios in May 1897. It had a glass roof and three glass walls constructed after the model of large studios for still photography, and it was fitted with thin cotton cloths that could be stretched below the roof to diffuse the direct rays of the sun on sunny days. Beginning in 1896, Méliès would go on to produce, direct, and distribute over 500 short films. The majority of these films were short, one-shot films completed in one take. Méliès drew many comparisons between film and the stage, which was apparent in his work. He realized that film afforded him the ability (via his use of time lapse photography) to "produce visual spectacles not achievable in the theater.

"The Execution of Mary Stuart", produced by the Edison Company for viewing with the Kinetoscope, showed Mary Queen of Scots being executed in full view of the camera. The effect was achieved by replacing the actor with a dummy for the final shot. Georges Méliès also utilized this technique in the making of "Escamotage d'un dame chez Robert-Houdin (The Vanishing Lady)". The woman is seen to vanish through the use of stop motion techniques.
The other basic technique for trick cinematography was the double exposure of the film in the camera. This was pioneered by George Albert Smith in July 1898 in England. The set was draped in black, and after the main shot, the negative was re-exposed to the overlaid scene. His "The Corsican Brothers" was described in the catalogue of the Warwick Trading Company in 1900: "By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow.”

G.A. Smith also initiated the special effects technique of reverse motion. He did this by repeating the action a second time, while filming it with an inverted camera, and then joining the tail of the second negative to that of the first. The first films made using this device were "Tipsy, Topsy, Turvy" and "The Awkward Sign Painter". The earliest surviving example of this technique is Smith's "The House That Jack Built", made before September 1900.

Cecil Hepworth took this technique further, by printing the negative of the forwards motion backwards frame by frame, so producing a print in which the original action was exactly reversed. To do this he built a special printer in which the negative running through a projector was projected into the gate of a camera through a special lens giving a same-size image. This arrangement came to be called a "projection printer", and eventually an "optical printer".

The use of different camera speeds also appeared around 1900 in the films of Robert W. Paul and Hepworth. Paul shot scenes from "On a Runaway Motor Car through Piccadilly Circus" (1899) with the camera turning very slowly. When the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Hepworth used the opposite effect in "The Indian Chief and the Seidlitz Powder" (1901). The Chief's movements are sped up by cranking the camera much faster than 16 frames per second. This gives what we would call a "slow motion" effect.

The first films to consist of more than one shot appeared toward the end of the 19th century. A notable example was the French film of the life of Jesus Christ, "La vie du Christ (The Birth, the Life and the Death of Christ)", by Alice Guy. These weren't represented as a continuous film, the separate scenes were interspersed with lantern slides, a lecture, and live choral numbers, to increase the running time of the spectacle to about 90 minutes. Another example of this is the reproductions of scenes from the Greco-Turkish war, made by Georges Méliès in 1897. Although each scene was sold separately, they were shown one after the other by the exhibitors. Even Méliès' "Cendrillon (Cinderella)" of 1898 contained no action moving from one shot to the next one. To understand what was going on in the film the audience had to know their stories beforehand, or be told them by a presenter.
Real film continuity, involving action moving from one sequence into another, is attributed to British film pioneer Robert W. Paul's "Come Along, Do!", made in 1898 and one of the first films to feature more than one shot. In the first shot, an elderly couple is outside an art exhibition having lunch and then follow other people inside through the door. The second shot shows what they do inside. Paul's 'Cinematograph Camera No. 1' of 1895 was the first camera to feature reverse-cranking, which allowed the same film footage to be exposed several times and thereby to create super-positions and multiple exposures. This technique was first used in his 1901 film "Scrooge, or, Marley's Ghost".

The further development of action continuity in multi-shot films continued in 1899 at the Brighton School in England. In the latter part of that year, George Albert Smith made "The Kiss in the Tunnel". This started with a shot from a "phantom ride" at the point at which the train goes into a tunnel, and continued with the action on a set representing the interior of a railway carriage, where a man steals a kiss from a woman, and then cuts back to the phantom ride shot when the train comes out of the tunnel. A month later, the Bamforth company in Yorkshire made a restaged version of this film under the same title, and in this case they filmed shots of a train entering and leaving a tunnel from beside the tracks, which they joined before and after their version of the kiss inside the train compartment.
In 1900, continuity of action across successive shots was definitively established by George Albert Smith and James Williamson, who also worked in Brighton. In that year Smith made "As Seen Through a Telescope", in which the main shot shows street scene with a young man tying the shoelace and then caressing the foot of his girlfriend, while an old man observes this through a telescope. There is then a cut to close shot of the hands on the girl's foot shown inside a black circular mask, and then a cut back to the continuation of the original scene. Even more remarkable is James Williamson's "Attack on a China Mission Station" (1900). The first shot shows Chinese Boxer rebels at the gate; it then cuts to the missionary family in the garden, where a fight ensues. The wife signals to British sailors from the balcony, who come and rescue them. The film also used the first "reverse angle" cut in film history.

G.A Smith pioneered the use of the close-up shot in his 1900 films "As Seen Through a Telescope" and "Grandma's Reading Glass". He further developed the ideas of breaking a scene shot in one place into a series of shots taken from different camera positions over the next couple of years, starting with "The Little Doctors" of 1901. In a series of films he produced at this time, he also introduced the use of subjective and objective point-of-view shots, the creation of dream-time and the use of reversing. He summed up his work in "Mary Jane's Mishap" of 1903, with repeated cuts in to a close shot of a housemaid fooling around, along with superimpositions and other devices, before abandoning film-making to invent the Kinemacolor system of colour cinematography. His films were the first to establish the basics of coherent narrative and what became known as film language, or "film grammar".

James Williamson concentrated on making films taking action from one place shown in one shot to the next shown in another shot in films like "Stop Thief!", made in 1901, and many others. He also experimented with the close-up, and made perhaps the most extreme one of all in "The Big Swallow", when his character approaches the camera and appears to swallow it. These two film makers of the Brighton School also pioneered the editing of the film; they tinted their work with color and used trick photography to enhance the narrative. By 1900, their films were extended scenes of up to 5 minutes long.

Most films of this period were what came to be called "chase films". These were inspired by James Williamson's "Stop Thief!" of 1901, which showed a tramp stealing a leg of mutton from a butcher's boy in the first shot, then being chased through the second shot by the butcher's boy and assorted dogs, and finally being caught by the dogs in the third shot. Several British films made in the first half of 1903 extended the chase method of film construction. These included "An Elopement à la Mode" and "The Pickpocket: A Chase Through London", made by Alf Collins for the British branch of the French Gaumont company, "Daring Daylight Burglary", made by Frank Mottershaw at the Sheffield Photographic Company, and "Desperate Poaching Affray", made by William Haggar. Haggar in particular innovated the first extant panning shots; the poachers are chased by gamekeepers and police officers and the camera pans along, creating a sense of urgency and speed. His films were also recognised for their intelligent use of depth of staging and screen edges, while film academic Noël Burch praised Haggar's effective use of off-screen space. He was also one of the first film makers to purposefully introduce violence for entertainment; in "Desperate Poaching Affray" the villains are seen firing guns at their pursuers.
Other filmmakers took up all these ideas including the American Edwin S. Porter, who started making films for the Edison Company in 1901. Porter, a projectionist, was hired by Thomas Edison to develop his new projection model known as the Vitascope. Porter wanted to develop a style of filmmaking that would move away from the one-shot short films into a "story-telling [narrative]" style. When he began making longer films in 1902, he put a dissolve between every shot, just as Georges Méliès was already doing, and he frequently had the same action repeated across the dissolves. His film, "The Great Train Robbery" (1903), had a running time of twelve minutes, with twenty separate shots and ten different indoor and outdoor locations. He used cross-cutting editing method to show simultaneous action in different places. The time continuity in "The Great Train Robbery" was actually more confusing than that in the films it was modeled on, but nevertheless it was a greater success than them due to its Wild West violence. "The Great Train Robbery" served as one of the vehicles that would launch the film medium into mass popularity.

The Pathé company in France also made imitations and variations of Smith and Williamson's films from 1902 onwards using cuts between the shots, which helped to standardize the basics of film construction. An influential French film of the period was Méliès's 14-minute-long "A Trip to the Moon". It was extremely popular at the time of its release, and is the best-known of the hundreds of films made by Méliès. It was one of the first known science fiction films, and used innovative animation and special effects, including the well-known image of the spaceship landing in the Moon's eye. The sheer volume of Pathé's production led to their filmmakers giving a further precision and polish to the details of film continuity.

The first use of animation in movies was in 1899, with the production of the short film Matches: An Appeal by British film pioneer Arthur Melbourne-Cooper- a thirty-second long stop-motion animated piece intended to encourage the audience to send matches to British troops fighting the Boer War. The film contains an appeal to send money to Bryant and May who would then send matches to the troops fighting in South Africa. It was shown in December 1899 at The Empire Theatre in London. This film is the earliest known example of stop-motion animation. Little puppets, constructed of matchsticks, are writing the appeal on a black wall. Their movements are filmed frame by frame, movement by movement.
The relative sophistication of this piece was not followed up for some time, with subsequent works in animation being limited to short, two or three frame effects, such as appeared in Edwin Stanton Porter's 1902 short "Fun in a Bakery Shop", where a lump of dough was made to smile over the course of a three-frame sequence. Works rivaling the British short in length did not appear until 1905, when Edwin Porter made "How Jones Lost His Roll", and "The Whole Dam Family and the Dam Dog". Both of these films had intertitles which were formed by the letters moving into place from a random scattering to form the words of the titles. This was done by exposing the film one frame at a time, and moving the letters a little bit towards their final position between each exposure. This is what has come to be called "single frame animation" or "object animation", and it needs a slightly adapted camera that exposes only one frame for each turn of the crank handle, rather than the usual eight frames per turn.
In 1906, Albert Edward Smith and James Stuart Blackton at Vitagraph Studios took the next step, and in their "Humorous Phases of Funny Faces", what appear to be cartoon drawings of people move from one pose to another. This is done for most of the length of this film by moving jointed cut-outs of the figures frame by frame between the exposures, just as Porter moved his letters. However, there is a very short section of the film where things are made to appear to move by altering the drawings themselves from frame to frame, which is how standard animated cartoons have since been made up to today.

The technique of single frame animation was further developed in 1907 by Edwin S. Porter in "The Teddy Bears" and by J. Stuart Blackton with "Work Made Easy". In the first of these the toy bears were made to move, apparently on their own, and in the latter film building tools were made to perform construction tasks without human intervention, by using frame-by-frame animation. The technique got to Europe almost immediately, and Segundo de Chomon and others at Pathé took it further, adding clay animation, in which sculptures were deformed from one thing into another thing frame by frame in "Sculpture moderne" (1908), and then Pathé made the next step to the animation of silhouette shapes. Also in France, Émile Cohl fully developed drawn animation in a series of films starting with "Fantasmagorie" (1908), in which humans and objects drawn as outline figures went through a series of remarkable interactions and transformations. In the United States the response was from the famous strip cartoon artist Winsor McCay, who drew much more realistic animated figures going through smoother, more naturalistic motion in a series of films starting with the film "Little Nemo", made for Vitagraph in 1911. In the next few years various others took part in this development of animated cartoons in the United States and elsewhere.

The world's first animated feature film was "El Apóstol" (1917), made by Italian-Argentine cartoonist Quirino Cristiani utilizing cutout animation. Cristiani also directed the first animated feature film with sound, "Peludópolis", released with a vitaphone sound-on-disc synchronization system soundtrack. Unfortunately, a fire that destroyed producer Federico Valle's film studio incinerated the only known copies of the movies, and they are now considered as lost films.

In 1932, the first short animated film created entirely with technicolor (the trichromatic procedure (green, red, blue), whose use required a triple photographic impression, incorporation of chromatic filters and cameras of enormous dimensions) was Walt Disney's "Flowers and Trees", directed by Burt Gillett.

Films at the time were no longer than one reel, although some multi-reel films had been made on the life of Christ in the first few years of cinema. The first feature length multi-reel film in the world was the 1906 Australian production called "The Story of the Kelly Gang".

It traced the life of the legendary infamous outlaw and bushranger Ned Kelly (1855–1880) and ran for more than an hour with a reel length of approximately 4,000 feet (1,200 m). It was first shown at the Athenaeum Hall in Collins Street, Melbourne, Australia on 26 December 1906 and in the UK in January 1908.

The first successful permanent theatre showing only films was "The Nickelodeon", which was opened in Pittsburgh in 1905. By then there were enough films several minutes long available to fill a programme running for at least half an hour, and which could be changed weekly when the local audience became bored with it. Other exhibitors in the United States quickly followed suit, and within a couple of years there were thousands of these nickelodeons in operation. The American experience led to a worldwide boom in the production and exhibition of films from 1906 onwards.

By 1907 purpose-built cinemas for motion pictures were being opened across the United States, Britain and France. The films were often shown with the accompaniment of music provided by a pianist, though there could be more musicians. There were also a very few larger cinemas in some of the biggest cities. Initially, the majority of films in the programmes were Pathé films, but this changed fairly quickly as the American companies cranked up production. The programme was made up of just a few films, and the show lasted around 30 minutes. The reel of film, of maximum length , which usually contained one individual film, became the standard unit of film production and exhibition in this period. The programme was changed twice or more a week, but went up to five changes of programme a week after a couple of years. In general, cinemas were set up in the established entertainment districts of the cities. In 1907, Pathé began renting their films to cinemas through film exchanges rather than selling the films outright.
By about 1910, actors began to receive screen credit for their roles, and the way to the creation of film stars was opened. Films were increasingly longer, and began to feature proper plots and development.

The litigation over patents between all the major American film-making companies led to the formation of a trust to control the American film business, with each company in the trust being allocated production quotas (two reels a week for the biggest ones, one reel a week for the smaller). However, although 6,000 exhibitors signed up to the trust, about 2,000 others did not and began to fund new film producing companies. By 1912 the independents had nearly half of the market and the government defeated the trust by initiating anti-trust action at the same time.

In the early 20th century, before Hollywood, the motion picture industry was based in Fort Lee, New Jersey across the Hudson River from New York City. In need of a winter headquarters, moviemakers were attracted to Jacksonville, Florida due to its warm climate, exotic locations, excellent rail access, and cheaper labor, earning the city the title of "The Winter Film Capital of the World." New York-based Kalem Studios was the first to open a permanent studio in Jacksonville in 1908. Over the course of the next decade, more than 30 silent film companies established studios in town, including Metro Pictures (later MGM), Edison Studios, Majestic Films, King Bee Film Company, Vim Comedy Company, Norman Studios, Gaumont Studios and the Lubin Manufacturing Company. Comedic actor and Georgia native Oliver "Babe" Hardy began his motion picture career here in 1914. He starred in over 36 short silent films his first year acting. With the closing of Lubin in early 1915, Oliver moved to New York then New Jersey to find film jobs. Acquiring a job with the Vim Company in early 1915, he returned to Jacksonville in the spring of 1917 before relocating to Los Angeles in October 1917. The first motion picture made in Technicolor and the first feature-length color movie produced in the United States, The Gulf Between, was also filmed on location in Jacksonville in 1917.

Jacksonville was especially important to the African American film industry. One notable individual in this regard is the European American producer Richard Norman, who created a string of films starring black actors in the vein of Oscar Micheaux and the Lincoln Motion Picture Company. In contrast to the degrading parts offered in certain white films such as The Birth of a Nation, Norman and his contemporaries sought to create positive stories featuring African Americans in what he termed "splendidly assuming different roles."

Jacksonville's mostly conservative residents, however, objected to the hallmarks of the early movie industry, such as car chases in the streets, simulated bank robberies and fire alarms in public places, and even the occasional riot. In 1917, conservative Democrat John W. Martin was elected mayor on the platform of taming the city's movie industry. By that time, southern California was emerging as the major movie production center, thanks in large part to the move of film pioneers like William Selig and D.W. Griffith to the area. These factors quickly sealed the demise of Jacksonville as a major film destination.

Another factor for the industry's move west was that up until 1913, most American film production was still carried out around New York, but due to the monopoly of Thomas A. Edison, Inc.'s film patents and its litigious attempts to preserve it, many filmmakers moved to Southern California, starting with Selig in 1909. The sunshine and scenery was important for the production of Westerns, which came to form a major American film genre with the first cowboy stars, G.M. Anderson ("Broncho Billy") and Tom Mix. Selig pioneered the use of (fairly) wild animals from a zoo for a series of exotic adventures, with the actors being menaced or saved by the animals. Kalem Company sent film crews to places in America and abroad to film stories in the actual places they were supposed to have happened. Kalem also pioneered the female action heroine from 1912, with Ruth Roland playing starring roles in their Westerns.

In France, Pathé retained its dominant position, followed still by Gaumont, and then other new companies that appeared to cater to the film boom. A film company with a different approach was Film d'Art. This was set up at the beginning of 1908 to make films of a serious artistic nature. Their declared programme was to make films using only the best dramatists, artists and actors. The first of these was "L'Assassinat du Duc de Guise" ("The Assassination of the Duc de Guise"), a historical subject set in the court of Henri III. This film used leading actors from the Comédie-Française, and had a special accompanying score written by Camille Saint-Saëns. The other French majors followed suit, and this wave gave rise to the English-language description of films with artistic pretensions aimed at a sophisticated audience as "art films". By 1910, the French film companies were starting to make films as long as two, or even three reels, though most were still one reel long. This trend was followed in Italy, Denmark, and Sweden.

In Britain, the Cinematograph Act 1909 was the first primary legislation to specifically regulate the film industry. Film exhibitions often took place in temporary venues and the use of highly flammable cellulose nitrate for film, combined with limelight illumination, created a significant fire hazard. The Act specified a strict building code which required, amongst other things, that the projector be enclosed within a fire resisting enclosure.

Regular newsreels were exhibited from 1910 and soon became a popular way for finding out the news the British Antarctic Expedition to the South Pole was filmed for the newsreels as were the suffragette demonstrations that were happening at the same time. F. Percy Smith was an early nature documentary pioneer working for Charles Urban and he pioneered the use of time lapse and micro cinematography in his 1910 documentary on the growth of flowers.

With the worldwide film boom, yet more countries now joined Britain, France, Germany and the United States in serious film production. In Italy, production was spread over several centres, with Turin being the first and biggest. There, Ambrosio was the first company in the field in 1905, and remained the largest in the country through this period. Its most substantial rival was Cines in Rome, which started producing in 1906. The great strength of the Italian industry was historical epics, with large casts and massive scenery. As early as 1911, Giovanni Pastrone's two-reel "La Caduta di Troia (The Fall of Troy)" made a big impression worldwide, and it was followed by even bigger glasses like "Quo Vadis?" (1912), which ran for 90 minutes, and Pastrone's "Cabiria" of 1914, which ran for two and a half hours.

Italian companies also had a strong line in slapstick comedy, with actors like André Deed, known locally as "Cretinetti", and elsewhere as "Foolshead" and "Gribouille", achieving worldwide fame with his almost surrealistic gags.

The most important film-producing country in Northern Europe up until the First World War was Denmark. The Nordisk company was set up there in 1906 by Ole Olsen, a fairground showman, and after a brief period imitating the successes of French and British filmmakers, in 1907 he produced 67 films, most directed by Viggo Larsen, with sensational subjects like "Den hvide Slavinde (The White Slave)", "Isbjørnenjagt (Polar Bear Hunt)" and "Løvejagten (The Lion Hunt)". By 1910, new smaller Danish companies began joining the business, and besides making more films about the white slave trade, they contributed other new subjects. The most important of these finds was Asta Nielsen in "Afgrunden (The Abyss)", directed by Urban Gad for Kosmorama, This combined the circus, sex, jealousy and murder, all put over with great conviction, and pushed the other Danish filmmakers further in this direction. By 1912, the Danish film companies were multiplying rapidly.

The Swedish film industry was smaller and slower to get started than the Danish industry. Here, the important man was Charles Magnusson, a newsreel cameraman for the Svenskabiografteatern cinema chain. He started fiction film production for them in 1909, directing a number of the films himself. Production increased in 1912, when the company engaged Victor Sjöström and Mauritz Stiller as directors. They started out by imitating the subjects favoured by the Danish film industry, but by 1913 they were producing their own strikingly original work, which sold very well.

Russia began its film industry in 1908 with Pathé shooting some fiction subjects there, and then the creation of real Russian film companies by Aleksandr Drankov and Aleksandr Khanzhonkov. The Khanzhonkov company quickly became much the largest Russian film company, and remained so until 1918.

In Germany, Oskar Messter had been involved in film-making from 1896, but did not make a significant number of films per year until 1910. When the worldwide film boom started, he, and the few other people in the German film business, continued to sell prints of their own films outright, which put them at a disadvantage. It was only when Paul Davidson, the owner of a chain of cinemas, brought Asta Nielsen and Urban Gad to Germany from Denmark in 1911, and set up a production company, Projektions-AG "Union" (PAGU), for them, that a change-over to renting prints began. Messter replied with a series of longer films starring Henny Porten, but although these did well in the German-speaking world, they were not particularly successful internationally, unlike the Asta Nielsen films. Another of the growing German film producers just before World War I was the German branch of the French Éclair company, Deutsche Éclair. This was expropriated by the German government, and turned into DECLA when the war started. But altogether, German producers only had a minor part of the German market in 1914.

Overall, from about 1910, American films had the largest share of the market in all European countries except France, and even in France, the American films had just pushed the local production out of first place on the eve of World War I. So even if the war had not happened, American films may have become dominant worldwide. Although the war made things much worse for European producers, the technical qualities of American films made them increasingly attractive to audiences everywhere.

New film techniques that were introduced in this period include the use of artificial lighting, fire effects and Low-key lighting (i.e. lighting in which most of the frame is dark) for enhanced atmosphere during sinister scenes.

Continuity of action from shot to shot was also refined, such as in Pathé's "le Cheval emballé (The Runaway Horse)" (1907) where cross-cutting between parallel actions is used. D. W. Griffith also began using cross-cutting in the film "The Fatal Hour", made in July 1908. Another development was the use of the Point of View shot, first used in 1910 in Vitagraph's "Back to Nature". Insert shots were also used for artistic purposes; the Italian film "La mala planta (The Evil Plant)", directed by Mario Caserini had an insert shot of a snake slithering over the "Evil Plant".

As films grew longer, specialist writers were employed to simplify more complex stories derived from novels or plays into a form that could be contained on one reel. Genres began to be used as categories; the main division was into comedy and drama, but these categories were further subdivided.

Intertitles containing lines of dialogue began to be used consistently from 1908 onwards, such as in Vitagraph's "An Auto Heroine; or, The Race for the Vitagraph Cup and How It Was Won". The dialogue was eventually inserted into the middle of the scene and became commonplace by 1912. The introduction of dialogue titles transformed the nature of film narrative. When dialogue titles came to be always cut into a scene just after a character starts speaking, and then left with a cut to the character just before they finish speaking, then one had something that was effectively the equivalent of a present-day sound film.

The years of the First World War were a complex transitional period for the film industry. The exhibition of films changed from short one-reel programmes to feature films. Exhibition venues became larger and began charging higher prices.

In the United States, these changes brought destruction to many film companies, the Vitagraph company being an exception. Film production began to shift to Los Angeles during World War I. The Universal Film Manufacturing Company was formed in 1912 as an umbrella company. New entrants included the Jesse Lasky Feature Play Company, and Famous Players, both formed in 1913, and later amalgamated into Famous Players-Lasky. The biggest success of these years was David Wark Griffith's "The Birth of a Nation" (1915). Griffith followed this up with the even bigger "Intolerance" (1916), but, due to the high quality of film produced in the US, the market for their films was high.

In France, film production shut down due to the general military mobilization of the country at the start of the war. Although film production began again in 1915, it was on a reduced scale, and the biggest companies gradually retired from production. Italian film production held up better, although so called "diva films", starring anguished female leads were a commercial failure. In Denmark, the Nordisk company increased its production so much in 1915 and 1916 that it could not sell all its films, which led to a very sharp decline in Danish production, and the end of Denmark's importance on the world film scene.

The German film industry was seriously weakened by the war. The most important of the new film producers at the time was Joe May, who made a series of thrillers and adventure films through the war years, but Ernst Lubitsch also came into prominence with a series of very successful comedies and dramas.

At this time, studios were blacked out to allow shooting to be unaffected by changing sunlight. This was replaced with floodlights and spotlights. The widespread adoption of irising-in and out to begin and end scenes caught on in this period. This is the revelation of a film shot in a circular mask, which gradually gets larger until it expands beyond the frame. Other shaped slits were used, including vertical and diagonal apertures.

A new idea taken over from still photography was "soft focus". This began in 1915, with some shots being intentionally thrown out of focus for expressive effect, as in Mary Pickford starrer "Fanchon the Cricket".

It was during this period that camera effects intended to convey the subjective feelings of characters in a film really began to be established. These could now be done as Point of View (POV) shots, as in Sidney Drew's "The Story of the Glove" (1915), where a wobbly hand-held shot of a door and its keyhole represents the POV of a drunken man. The use of anamorphic (in the general sense of distorted shape) images first appears in these years with Abel Gance directed "la Folie du Docteur Tube (The Madness of Dr. Tube)". In this film the effect of a drug administered to a group of people was suggested by shooting the scenes reflected in a distorting mirror of the fair-ground type.

Symbolic effects taken over from conventional literary and artistic tradition continued to make some appearances in films during these years. In D. W. Griffith's "The Avenging Conscience" (1914), the title "The birth of the evil thought" precedes a series of three shots of the protagonist looking at a spider, and ants eating an insect. Symbolist art and literature from the turn of the century also had a more general effect on a small number of films made in Italy and Russia. The supine acceptance of death resulting from passion and forbidden longings was a major feature of this art, and states of delirium dwelt on at length were important as well.
The use of insert shots, i.e. close-ups of objects other than faces, had already been established by the Brighton school, but were infrequently used before 1914. It is really only with Griffith's "The Avenging Conscience" that a new phase in the use of the Insert Shot starts. As well as the symbolic inserts already mentioned, the film also made extensive use of large numbers of Big Close Up shots of clutching hands and tapping feet as a means of emphasizing those parts of the body as indicators of psychological tension.

Atmospheric inserts were developed in Europe in the late 1910s. This kind of shot is one in a scene which neither contains any of the characters in the story, nor is a Point of View shot seen by one of them. An early example is in Maurice Tourneur directed "The Pride of the Clan" (1917), in which there is a series of shots of waves beating on a rocky shore to demonstrate the harsh lives of the fishing folk. Maurice Elvey's "Nelson; The Story of England's Immortal Naval Hero" (1919) has a symbolic sequence dissolving from a picture of Kaiser Wilhelm II to a peacock, and then to a battleship.

By 1914, continuity cinema was the established mode of commercial cinema. One of the advanced continuity techniques involved an accurate and smooth transition from one shot to another. Cutting to "different" angles within a scene also became well-established as a technique for dissecting a scene into shots in American films. If the direction of the shot changes by more than ninety degrees, it is called a reverse-angle cutting. The leading figure in the full development of reverse-angle cutting was Ralph Ince in his films, such as "The Right Girl" and "His Phantom Sweetheart"

The use of flash-back structures continued to develop in this period, with the usual way of entering and leaving a flash-back being through a dissolve. The Vitagraph company's "The Man That Might Have Been" (William J. Humphrey, 1914), is even more complex, with a series of reveries and flash-backs that contrast the protagonist's real passage through life with what might have been, if his son had not died.

After 1914, cross cutting between parallel actions came to be used more so in American films than in European ones. Cross-cutting was often used to get new effects of contrast, such as the cross-cut sequence in Cecil B. DeMille's "The Whispering Chorus" (1918), in which a supposedly dead husband is having a liaison with a Chinese prostitute in an opium den, while simultaneously his unknowing wife is being remarried in church.

The general trend in the development of cinema, led from the United States, was towards using the newly developed specifically filmic devices for expression of the narrative content of film stories, and combining this with the standard dramatic structures already in use in commercial theatre. D. W. Griffith had the highest standing amongst American directors in the industry, because of the dramatic excitement he conveyed to the audience through his films. Cecil B. DeMille's "The Cheat" (1915), brought out the moral dilemmas facing their characters in a more subtle way than Griffith. DeMille was also in closer touch with the reality of contemporary American life. Maurice Tourneur was also highly ranked for the pictorial beauties of his films, together with the subtlety of his handling of fantasy, while at the same time he was capable of getting greater naturalism from his actors at appropriate moments, as in "A Girl's Folly" (1917).

Sidney Drew was the leader in developing "polite comedy", while slapstick was refined by Fatty Arbuckle and Charles Chaplin, who both started with Mack Sennett's Keystone company. They reduced the usual frenetic pace of Sennett's films to give the audience a chance to appreciate the subtlety and finesse of their movement, and the cleverness of their gags. By 1917 Chaplin was also introducing more dramatic plot into his films, and mixing the comedy with sentiment.

In Russia, Yevgeni Bauer put a slow intensity of acting combined with Symbolist overtones onto film in a unique way.

In Sweden, Victor Sjöström made a series of films that combined the realities of people's lives with their surroundings in a striking manner, while Mauritz Stiller developed sophisticated comedy to a new level.

In Germany, Ernst Lubitsch got his inspiration from the stage work of Max Reinhardt, both in bourgeois comedy and in spectacle, and applied this to his films, culminating in his "die Puppe" ("The Doll"), "die Austernprinzessin" ("The Oyster Princess") and "Madame DuBarry".

At the start of the First World War, French and Italian cinema had been the most globally popular. The war came as a devastating interruption to European film industries. The American industry, or "Hollywood", as it was becoming known after its new geographical center in California, gained the position it has held, more or less, ever since: film factory for the world and exporting its product to most countries on earth.

By the 1920s, the United States reached what is still its era of greatest-ever output, producing an average of 800 "feature" films annually, or 82% of the global total (Eyman, 1997). The comedies of Charlie Chaplin and Buster Keaton, the swashbuckling adventures of Douglas Fairbanks and the romances of Clara Bow, to cite just a few examples, made these performers' faces well known on every continent. The Western visual norm that would become classical continuity editing was developed and exported – although its adoption was slower in some non-Western countries without strong realist traditions in art and drama, such as Japan.

This development was contemporary with the growth of the studio system and its greatest publicity method, the star system, which characterized American film for decades to come and provided models for other film industries. The studios' efficient, top-down control over all stages of their product enabled a new and ever-growing level of lavish production and technical sophistication. At the same time, the system's commercial regimentation and focus on glamorous escapism discouraged daring and ambition beyond a certain degree, a prime example being the brief but still legendary directing career of the iconoclastic Erich von Stroheim in the late teens and the 1920s.

During late 1927, Warners released "The Jazz Singer", which was mostly silent but contained what is generally regarded as the first synchronized dialogue (and singing) in a feature film; but this process was actually accomplished first by Charles Taze Russell in 1914 with the lengthy film "The Photo-Drama of Creation". This drama consisted of picture slides and moving pictures synchronized with phonograph records of talks and music. The early sound-on-disc processes such as Vitaphone were soon superseded by sound-on-film methods like Fox Movietone, DeForest Phonofilm, and RCA Photophone. The trend convinced the largely reluctant industrialists that "talking pictures", or "talkies", were the future. A lot of attempts were made before the success of "The Jazz Singer", that can be seen in the List of film sound systems.

The change was remarkably swift. By the end of 1929, Hollywood was almost all-talkie, with several competing sound systems (soon to be standardized). Total changeover was slightly slower in the rest of the world, principally for economic reasons. Cultural reasons were also a factor in countries like China and Japan, where silents co-existed successfully with sound well into the 1930s, indeed producing what would be some of the most revered classics in those countries, like Wu Yonggang's "The Goddess" (China, 1934) and Yasujirō Ozu's "I Was Born, But..." (Japan, 1932). But even in Japan, a figure such as the benshi, the live narrator who was a major part of Japanese silent cinema, found his acting career was ending.

Sound further tightened the grip of major studios in numerous countries: the vast expense of the transition overwhelmed smaller competitors, while the novelty of sound lured vastly larger audiences for those producers that remained. In the case of the U.S., some historians credit sound with saving the Hollywood studio system in the face of the Great Depression (Parkinson, 1995). Thus began what is now often called "The Golden Age of Hollywood", which refers roughly to the period beginning with the introduction of sound until the late 1940s. The American cinema reached its peak of efficiently manufactured glamour and global appeal during this period. The top actors of the era are now thought of as the classic film stars, such as Clark Gable, Katharine Hepburn, Humphrey Bogart, Greta Garbo, and the greatest box office draw of the 1930s, child performer Shirley Temple.

Creatively, however, the rapid transition was a difficult one, and in some ways, film briefly reverted to the conditions of its earliest days. The late '20s were full of static, stagey talkies as artists in front of and behind the camera struggled with the stringent limitations of the early sound equipment and their own uncertainty as to how to utilize the new medium. Many stage performers, directors and writers were introduced to cinema as producers sought personnel experienced in dialogue-based storytelling. Many major silent filmmakers and actors were unable to adjust and found their careers severely curtailed or even ended.

This awkward period was fairly short-lived. 1929 was a watershed year: William Wellman with "Chinatown Nights" and "The Man I Love", Rouben Mamoulian with "Applause", Alfred Hitchcock with "Blackmail" (Britain's first sound feature), were among the directors to bring greater fluidity to talkies and experiment with the expressive use of sound (Eyman, 1997). In this, they both benefited from, and pushed further, technical advances in microphones and cameras, and capabilities for editing and post-synchronizing sound (rather than recording all sound directly at the time of filming).
Sound films emphasized black history and benefited different genres more so than silents did. Most obviously, the musical film was born; the first classic-style Hollywood musical was "The Broadway Melody" (1929) and the form would find its first major creator in choreographer/director Busby Berkeley ("42nd Street", 1933, "Dames", 1934). In France, avant-garde director René Clair made surreal use of song and dance in comedies like "Under the Roofs of Paris" (1930) and "Le Million" (1931). Universal Pictures began releasing gothic horror films like "Dracula" and "Frankenstein" (both 1931). In 1933, RKO Pictures released Merian C. Cooper's classic "giant monster" film "King Kong". The trend thrived best in India, where the influence of the country's traditional song-and-dance drama made the musical the basic form of most sound films (Cook, 1990); virtually unnoticed by the Western world for decades, this Indian popular cinema would nevertheless become the world's most prolific. ("See also Bollywood.")

At this time, American gangster films like "Little Caesar" and Wellman's "The Public Enemy" (both 1931) became popular. Dialogue now took precedence over "slapstick" in Hollywood comedies: the fast-paced, witty banter of "The Front Page" (1931) or "It Happened One Night" (1934), the sexual double entrendres of Mae West ("She Done Him Wrong", 1933) or the often subversively anarchic nonsense talk of the Marx Brothers ("Duck Soup", 1933). Walt Disney, who had previously been in the short cartoon business, stepped into feature films with the first English-speaking animated feature "Snow White and the Seven Dwarfs"; released by RKO Pictures in 1937. 1939, a major year for American cinema, brought such films as "The Wizard of Oz" and "Gone with The Wind".

Previously, it was believed that color films were first projected in 1909 at the Palace Theatre in London (the main problem with the color being that the technique, created by George Smith, (Kinemacolor) only used two colors: green and red, which were mixed additively). But in fact, it was in 1901 when the first color film in history was created. This untitled film was directed by photographer Edward Raymond Turner and his patron Frederick Marshall Lee. The way they did it was to use black and white film rolls, but have green, red, and blue filters go over the camera individually as it shot. To complete the film, they joined the original footage and filters on a special projector. However, both the shooting of the film and its projection suffered from major unrelated issues that, eventually, sank the idea.

Subsequently, in 1916, the technicolor technique arrived (trichromatic procedure (green, red, blue). Its use required a triple photographic impression, incorporation of chromatic filters and cameras of enormous dimensions). The first audiovisual piece that was completely realized with this technique was the short of Walt Disney "Flowers and Trees", directed by Burt Gillett in 1932. Even so, the first film to be performed with this technique will be "The Vanities Fair" (1935) by Rouben Mamoulian. Later on, the technicolor was extended mainly in the musical field as "The Wizard of Oz" or "Singin' in the Rain", in films such as "The Adventures of Robin Hood" or the animation film, "Snow White and the Seven Dwarfs".

The desire for wartime propaganda against the opposition created a renaissance in the film industry in Britain, with realistic war dramas like "49th Parallel" (1941), "Went the Day Well?" (1942), "The Way Ahead" (1944) and Noël Coward and David Lean's celebrated naval film "In Which We Serve" in 1942, which won a special Academy Award. These existed alongside more flamboyant films like Michael Powell and Emeric Pressburger's "The Life and Death of Colonel Blimp" (1943), "A Canterbury Tale" (1944) and "A Matter of Life and Death" (1946), as well as Laurence Olivier's 1944 film "Henry V", based on the Shakespearean history "Henry V". The success of "Snow White and the Seven Dwarfs" allowed Disney to make more animated features like "Pinocchio" (1940), "Fantasia" (1940), "Dumbo" (1941) and "Bambi" (1942).

The onset of US involvement in World War II also brought a proliferation of films as both patriotism and propaganda. American propaganda films included "Desperate Journey" (1942), "Mrs. Miniver" (1942), "Forever and a Day" (1943) and "Objective, Burma!" (1945). Notable American films from the war years include the anti-Nazi "Watch on the Rhine" (1943), scripted by Dashiell Hammett; "Shadow of a Doubt" (1943), Hitchcock's direction of a script by Thornton Wilder; the George M. Cohan biopic, "Yankee Doodle Dandy" (1942), starring James Cagney, and the immensely popular "Casablanca", with Humphrey Bogart. Bogart would star in 36 films between 1934 and 1942 including John Huston's "The Maltese Falcon" (1941), one of the first films now considered a classic film noir. In 1941, RKO Pictures released "Citizen Kane" made by Orson Welles. It is often considered the greatest film of all time. It would set the stage for the modern motion picture, as it revolutionized film story telling.

The strictures of wartime also brought an interest in more fantastical subjects. These included Britain's Gainsborough melodramas (including "The Man in Grey" and "The Wicked Lady"), and films like "Here Comes Mr. Jordan", "Heaven Can Wait", "I Married a Witch" and "Blithe Spirit". Val Lewton also produced a series of atmospheric and influential small-budget horror films, some of the more famous examples being "Cat People", "Isle of the Dead" and "The Body Snatcher". The decade probably also saw the so-called "women's pictures", such as "Now, Voyager", "Random Harvest" and "Mildred Pierce" at the peak of their popularity.

1946 saw RKO Radio releasing "It's a Wonderful Life" directed by Italian-born filmmaker Frank Capra. Soldiers returning from the war would provide the inspiration for films like "The Best Years of Our Lives", and many of those in the film industry had served in some capacity during the war. Samuel Fuller's experiences in World War II would influence his largely autobiographical films of later decades such as "The Big Red One". The Actor's Studio was founded in October 1947 by Elia Kazan, Robert Lewis, and Cheryl Crawford, and the same year Oskar Fischinger filmed "Motion Painting No. 1".

In 1943, "Ossessione" was screened in Italy, marking the beginning of Italian neorealism. Major films of this type during the 1940s included "Bicycle Thieves", "Rome, Open City", and "La Terra Trema". In 1952 "Umberto D" was released, usually considered the last film of this type.

In the late 1940s, in Britain, Ealing Studios embarked on their series of celebrated comedies, including "Whisky Galore!", "Passport to Pimlico", "Kind Hearts and Coronets" and "The Man in the White Suit", and Carol Reed directed his influential thrillers "Odd Man Out", "The Fallen Idol" and "The Third Man". David Lean was also rapidly becoming a force in world cinema with "Brief Encounter" and his Dickens adaptations "Great Expectations" and "Oliver Twist", and Michael Powell and Emeric Pressburger would experience the best of their creative partnership with films like "Black Narcissus" and "The Red Shoes".

The House Un-American Activities Committee investigated Hollywood in the early 1950s. Protested by the Hollywood Ten before the committee, the hearings resulted in the blacklisting of many actors, writers and directors, including Chayefsky, Charlie Chaplin, and Dalton Trumbo, and many of these fled to Europe, especially the United Kingdom.

The Cold War era zeitgeist translated into a type of near-paranoia manifested in themes such as invading armies of evil aliens, ("Invasion of the Body Snatchers", "The War of the Worlds"); and communist fifth columnists, ("The Manchurian Candidate").

During the immediate post-war years the cinematic industry was also threatened by television, and the increasing popularity of the medium meant that some film theatres would bankrupt and close. The demise of the "studio system" spurred the self-commentary of films like "Sunset Boulevard" (1950) and "The Bad and the Beautiful" (1952).

In 1950, the Lettrists avante-gardists caused riots at the Cannes Film Festival, when Isidore Isou's "Treatise on Slime and Eternity" was screened. After their criticism of Charlie Chaplin and split with the movement, the Ultra-Lettrists continued to cause disruptions when they showed their new hypergraphical techniques.
The most notorious film is Guy Debord's "Howls for Sade" of 1952.

Distressed by the increasing number of closed theatres, studios and companies would find new and innovative ways to bring audiences back. These included attempts to widen their appeal with new screen formats. Cinemascope, which would remain a 20th Century Fox distinction until 1967, was announced with 1953's "The Robe". VistaVision, Cinerama, and Todd-AO boasted a "bigger is better" approach to marketing films to a dwindling US audience. This resulted in the revival of epic films to take advantage of the new big screen formats. Some of the most successful examples of these Biblical and historical spectaculars include "The Ten Commandments" (1956), "The Vikings" (1958), "Ben-Hur" (1959), "Spartacus" (1960) and "El Cid" (1961). Also during this period a number of other significant films were produced in Todd-AO, developed by Mike Todd shortly before his death, including Oklahoma! (1955), Around the World in 80 Days (1956), South Pacific (1958) and Cleopatra (1963) plus many more.

Gimmicks also proliferated to lure in audiences. The fad for 3-D film would last for only two years, 1952–1954, and helped sell "House of Wax" and "Creature from the Black Lagoon". Producer William Castle would tout films featuring "Emergo" "Percepto", the first of a series of gimmicks that would remain popular marketing tools for Castle and others throughout the 1960s.

In the U.S., a post-WW2 tendency toward questioning the establishment and societal norms and the early activism of the civil rights movement was reflected in Hollywood films such as "Blackboard Jungle" (1955), "On the Waterfront" (1954), Paddy Chayefsky's "Marty" and Reginald Rose's "12 Angry Men" (1957). Disney continued making animated films, notably; "Cinderella" (1950), "Peter Pan" (1953), "Lady and the Tramp" (1955), and "Sleeping Beauty" (1959). He began, however, getting more involved in live action films, producing classics like "20,000 Leagues Under the Sea" (1954), and "Old Yeller" (1957). Television began competing seriously with films projected in theatres, but surprisingly it promoted more filmgoing rather than curtailing it.

"Limelight" is probably a unique film in at least one interesting respect. Its two leads, Charlie Chaplin and Claire Bloom, were in the industry in no less than three different centuries. In the 19th Century, Chaplin made his theatrical debut at the age of eight, in 1897, in a clog dancing troupe, The Eight Lancaster Lads. In the 21st Century, Bloom is still enjoying a full and productive career, having appeared in dozens of films and television series produced up to and including 2013. She received particular acclaim for her role in "The King's Speech" (2010).

Following the end of World War II in the 1940s, the following decade, the 1950s, marked a 'golden age' for non-English world cinema, especially for Asian cinema. Many of the most critically acclaimed Asian films of all time were produced during this decade, including Yasujirō Ozu's "Tokyo Story" (1953), Satyajit Ray's "The Apu Trilogy" (1955–1959) and "Jalsaghar" (1958), Kenji Mizoguchi's "Ugetsu" (1954) and "Sansho the Bailiff" (1954), Raj Kapoor's "Awaara" (1951), Mikio Naruse's "Floating Clouds" (1955), Guru Dutt's "Pyaasa" (1957) and "Kaagaz Ke Phool" (1959), and the Akira Kurosawa films "Rashomon" (1950), "Ikiru" (1952), "Seven Samurai" (1954) and "Throne of Blood" (1957).

During Japanese cinema's 'Golden Age' of the 1950s, successful films included "Rashomon" (1950), "Seven Samurai" (1954) and "The Hidden Fortress" (1958) by Akira Kurosawa, as well as Yasujirō Ozu's "Tokyo Story" (1953) and Ishirō Honda's "Godzilla" (1954). These films have had a profound influence on world cinema. In particular, Kurosawa's "Seven Samurai" has been remade several times as Western films, such as "The Magnificent Seven" (1960) and "Battle Beyond the Stars" (1980), and has also inspired several Bollywood films, such as "Sholay" (1975) and "China Gate" (1998). "Rashomon" was also remade as "The Outrage" (1964), and inspired films with "Rashomon effect" storytelling methods, such as "Andha Naal" (1954), "The Usual Suspects" (1995) and "Hero" (2002). "The Hidden Fortress" was also the inspiration behind George Lucas' "Star Wars" (1977). Other famous Japanese filmmakers from this period include Kenji Mizoguchi, Mikio Naruse, Hiroshi Inagaki and Nagisa Oshima. Japanese cinema later became one of the main inspirations behind the New Hollywood movement of the 1960s to 1980s.

During Indian cinema's 'Golden Age' of the 1950s, it was producing 200 films annually, while Indian independent films gained greater recognition through international film festivals. One of the most famous was "The Apu Trilogy" (1955–1959) from critically acclaimed Bengali film director Satyajit Ray, whose films had a profound influence on world cinema, with directors such as Akira Kurosawa, Martin Scorsese, James Ivory, Abbas Kiarostami, Elia Kazan, François Truffaut, Steven Spielberg, Carlos Saura, Jean-Luc Godard, Isao Takahata, Gregory Nava, Ira Sachs, Wes Anderson and Danny Boyle being influenced by his cinematic style. According to Michael Sragow of "The Atlantic Monthly", the "youthful coming-of-age dramas that have flooded art houses since the mid-fifties owe a tremendous debt to the Apu trilogy". Subrata Mitra's cinematographic technique of bounce lighting also originates from "The Apu Trilogy". Other famous Indian filmmakers from this period include Guru Dutt, Ritwik Ghatak, Mrinal Sen, Raj Kapoor, Bimal Roy, K. Asif and Mehboob Khan.

The cinema of South Korea also experienced a 'Golden Age' in the 1950s, beginning with director Lee Kyu-hwan's tremendously successful remake of "Chunhyang-jon" (1955). That year also saw the release of "Yangsan Province" by the renowned director, Kim Ki-young, marking the beginning of his productive career. Both the quality and quantity of filmmaking had increased rapidly by the end of the 1950s. South Korean films, such as Lee Byeong-il's 1956 comedy "Sijibganeun nal (The Wedding Day)", had begun winning international awards. In contrast to the beginning of the 1950s, when only 5 films were made per year, 111 films were produced in South Korea in 1959.

The 1950s was also a 'Golden Age' for Philippine cinema, with the emergence of more artistic and mature films, and significant improvement in cinematic techniques among filmmakers. The studio system produced frenetic activity in the local film industry as many films were made annually and several local talents started to earn recognition abroad. The premiere Philippine directors of the era included Gerardo de Leon, Gregorio Fernández, Eddie Romero, Lamberto Avellana, and Cirio Santiago.

During the 1960s, the studio system in Hollywood declined, because many films were now being made on location in other countries, or using studio facilities abroad, such as Pinewood in the UK and Cinecittà in Rome. "Hollywood" films were still largely aimed at family audiences, and it was often the more old-fashioned films that produced the studios' biggest successes. Productions like "Mary Poppins" (1964), "My Fair Lady" (1964) and "The Sound of Music" (1965) were among the biggest money-makers of the decade. The growth in independent producers and production companies, and the increase in the power of individual actors also contributed to the decline of traditional Hollywood studio production.

There was also an increasing awareness of foreign language cinema in America during this period. During the late 1950s and 1960s, the French New Wave directors such as François Truffaut and Jean-Luc Godard produced films such as "Les quatre cents coups", "Breathless" and "Jules et Jim" which broke the rules of Hollywood cinema's narrative structure. As well, audiences were becoming aware of Italian films like Federico Fellini's "La Dolce Vita" and the stark dramas of Sweden's Ingmar Bergman.

In Britain, the "Free Cinema" of Lindsay Anderson, Tony Richardson and others lead to a group of realistic and innovative dramas including "Saturday Night and Sunday Morning", "A Kind of Loving" and "This Sporting Life". Other British films such as "Repulsion", "Darling", "Alfie", "Blowup" and "Georgy Girl" (all in 1965–1966) helped to reduce prohibitions of sex and nudity on screen, while the casual sex and violence of the James Bond films, beginning with "Dr. No" in 1962 would render the series popular worldwide.

During the 1960s, Ousmane Sembène produced several French- and Wolof-language films and became the "father" of African Cinema. In Latin America, the dominance of the "Hollywood" model was challenged by many film makers. Fernando Solanas and Octavio Getino called for a politically engaged Third Cinema in contrast to Hollywood and the European auteur cinema.

Further, the nuclear paranoia of the age, and the threat of an apocalyptic nuclear exchange (like the 1962 close-call with the USSR during the Cuban missile crisis) prompted a reaction within the film community as well. Films like Stanley Kubrick's "Dr. Strangelove" and "Fail Safe" with Henry Fonda were produced in a Hollywood that was once known for its overt patriotism and wartime propaganda.

In documentary film the sixties saw the blossoming of Direct Cinema, an observational style of film making as well as the advent of more overtly partisan films like "In the Year of the Pig" about the Vietnam War by Emile de Antonio. By the late 1960s however, Hollywood filmmakers were beginning to create more innovative and groundbreaking films that reflected the social revolution taken over much of the western world such as "Bonnie and Clyde" (1967), "The Graduate" (1967), "" (1968), "Rosemary's Baby" (1968), "Midnight Cowboy" (1969), "Easy Rider" (1969) and "The Wild Bunch" (1969). "Bonnie and Clyde" is often considered the beginning of the so-called New Hollywood.

In Japanese cinema, Academy Award-winning director Akira Kurosawa produced "Yojimbo" (1961), which like his previous films also had a profound influence around the world. The influence of this film is most apparent in Sergio Leone's "A Fistful of Dollars" (1964) and Walter Hill's "Last Man Standing" (1996). "Yojimbo" was also the origin of the "Man with No Name" trend.

The New Hollywood was the period following the decline of the studio system during the 1950s and 1960s and the end of the production code, (which was replaced in 1968 by the MPAA film rating system). During the 1970s, filmmakers increasingly depicted explicit sexual content and showed gunfight and battle scenes that included graphic images of bloody deaths a good example of this is Wes Craven's "The Last House on the Left" (1972).

Post-classical cinema is the changing methods of storytelling of the New Hollywood producers. The new methods of drama and characterization played upon audience expectations acquired during the classical/Golden Age period: story chronology may be scrambled, storylines may feature unsettling "twist endings", main characters may behave in a morally ambiguous fashion, and the lines between the antagonist and protagonist may be blurred. The beginnings of post-classical storytelling may be seen in 1940s and 1950s film noir films, in films such as "Rebel Without a Cause" (1955), and in Hitchcock's "Psycho". 1971 marked the release of controversial films like "Straw Dogs", "A Clockwork Orange", "The French Connection" and "Dirty Harry". This sparked heated controversy over the perceived escalation of violence in cinema.

During the 1970s, a new group of American filmmakers emerged, such as Martin Scorsese, Francis Ford Coppola, George Lucas, Woody Allen, Terrence Malick, and Robert Altman. This coincided with the increasing popularity of the auteur theory in film literature and the media, which posited that a film director's films express their personal vision and creative insights. The development of the auteur style of filmmaking helped to give these directors far greater control over their projects than would have been possible in earlier eras. This led to some great critical and commercial successes, like Scorsese's "Taxi Driver", Coppola's "The Godfather" films, William Friedkin's "The Exorcist", Altman's "Nashville", Allen's "Annie Hall" and "Manhattan", Malick's "Badlands" and "Days of Heaven", and Polish immigrant Roman Polanski's "Chinatown". It also, however, resulted in some failures, including Peter Bogdanovich's "At Long Last Love" and Michael Cimino's hugely expensive Western epic "Heaven's Gate", which helped to bring about the demise of its backer, United Artists.

The financial disaster of "Heaven's Gate" marked the end of the visionary "auteur" directors of the "New Hollywood", who had unrestrained creative and financial freedom to develop films. The phenomenal success in the 1970s of Spielberg's "Jaws" originated the concept of the modern "blockbuster". However, the enormous success of George Lucas' 1977 film "Star Wars" led to much more than just the popularization of blockbuster film-making. The film's revolutionary use of special effects, sound editing and music had led it to become widely regarded as one of the single most important films in the medium's history, as well as the most influential film of the 1970s. Hollywood studios increasingly focused on producing a smaller number of very large budget films with massive marketing and promotional campaigns. This trend had already been foreshadowed by the commercial success of disaster films such as "The Poseidon Adventure" and "The Towering Inferno".

During the mid-1970s, more pornographic theatres, euphemistically called "adult cinemas", were established, and the legal production of hardcore pornographic films began. Porn films such as "Deep Throat" and its star Linda Lovelace became something of a popular culture phenomenon and resulted in a spate of similar sex films. The porn cinemas finally died out during the 1980s, when the popularization of the home VCR and pornography videotapes allowed audiences to watch sex films at home. In the early 1970s, English-language audiences became more aware of the new West German cinema, with Werner Herzog, Rainer Werner Fassbinder and Wim Wenders among its leading exponents.

In world cinema, the 1970s saw a dramatic increase in the popularity of martial arts films, largely due to its reinvention by Bruce Lee, who departed from the artistic style of traditional Chinese martial arts films and added a much greater sense of realism to them with his Jeet Kune Do style. This began with "The Big Boss" (1971), which was a major success across Asia. However, he didn't gain fame in the Western world until shortly after his death in 1973, when "Enter the Dragon" was released. The film went on to become the most successful martial arts film in cinematic history, popularized the martial arts film genre across the world, and cemented Bruce Lee's status as a cultural icon. Hong Kong action cinema, however, was in decline due to a wave of "Bruceploitation" films. This trend eventually came to an end in 1978 with the martial arts comedy films, "Snake in the Eagle's Shadow" and "Drunken Master", directed by Yuen Woo-ping and starring Jackie Chan, laying the foundations for the rise of Hong Kong action cinema in the 1980s.

While the musical film genre had declined in Hollywood by this time, musical films were quickly gaining popularity in the cinema of India, where the term "Bollywood" was coined for the growing Hindi film industry in Bombay (now Mumbai) that ended up dominating South Asian cinema, overtaking the more critically acclaimed Bengali film industry in popularity. Hindi filmmakers combined the Hollywood musical formula with the conventions of ancient Indian theatre to create a new film genre called "Masala", which dominated Indian cinema throughout the late 20th century. These "Masala" films portrayed action, comedy, drama, romance and melodrama all at once, with "filmi" song and dance routines thrown in. This trend began with films directed by Manmohan Desai and starring Amitabh Bachchan, who remains one of the most popular film stars in South Asia. The most popular Indian film of all time was "Sholay" (1975), a "Masala" film inspired by a real-life dacoit as well as Kurosawa's "Seven Samurai" and the Spaghetti Westerns.

The end of the decade saw the first major international marketing of Australian cinema, as Peter Weir's films "Picnic at Hanging Rock" and "The Last Wave" and Fred Schepisi's "The Chant of Jimmie Blacksmith" gained critical acclaim. In 1979, Australian filmmaker George Miller also garnered international attention for his violent, low-budget action film "Mad Max".

During the 1980s, audiences began increasingly watching films on their home VCRs. In the early part of that decade, the film studios tried legal action to ban home ownership of VCRs as a violation of copyright, which proved unsuccessful. Eventually, the sale and rental of films on home video became a significant "second venue" for exhibition of films, and an additional source of revenue for the film industries.

The Lucas–Spielberg combine would dominate "Hollywood" cinema for much of the 1980s, and lead to much imitation. Two follow-ups to "Star Wars", three to "Jaws", and three "Indiana Jones" films helped to make sequels of successful films more of an expectation than ever before. Lucas also launched THX Ltd, a division of Lucasfilm in 1982, while Spielberg enjoyed one of the decade's greatest successes in "E.T. the Extra-Terrestrial" the same year. 1982 also saw the release of Disney's "Tron" which was one of the first films from a major studio to use computer graphics extensively. American independent cinema struggled more during the decade, although Martin Scorsese's "Raging Bull" (1980), "After Hours" (1985), and "The King of Comedy" (1983) helped to establish him as one of the most critically acclaimed American film makers of the era. Also during 1983 "Scarface" was released, which was very profitable and resulted in even greater fame for its leading actor Al Pacino. Probably the most successful film commercially was Tim Burton's 1989 version of Bob Kane's creation, "Batman", which broke box-office records. Jack Nicholson's portrayal of the demented Joker earned him a total of $60,000,000 after figuring in his percentage of the gross.

British cinema was given a boost during the early 1980s by the arrival of David Puttnam's company Goldcrest Films. The films "Chariots of Fire", "Gandhi", "The Killing Fields" and "A Room with a View" appealed to a "middlebrow" audience which was increasingly being ignored by the major Hollywood studios. While the films of the 1970s had helped to define modern blockbuster motion pictures, the way "Hollywood" released its films would now change. Films, for the most part, would premiere in a wider number of theatres, although, to this day, some films still premiere using the route of the limited/roadshow release system. Against some expectations, the rise of the multiplex cinema did not allow less mainstream films to be shown, but simply allowed the major blockbusters to be given an even greater number of screenings. However, films that had been overlooked in cinemas were increasingly being given a second chance on home video.
During the 1980s, Japanese cinema experienced a revival, largely due to the success of anime films. At the beginning of the 1980s, "Space Battleship Yamato" (1973) and "Mobile Suit Gundam" (1979), both of which were unsuccessful as television series, were remade as films and became hugely successful in Japan. In particular, "Mobile Suit Gundam" sparked the Gundam franchise of Real Robot mecha anime. The success of "" also sparked a Macross franchise of mecha anime. This was also the decade when Studio Ghibli was founded. The studio produced Hayao Miyazaki's first fantasy films, "Nausicaä of the Valley of the Wind" (1984) and "Castle in the Sky" (1986), as well as Isao Takahata's "Grave of the Fireflies" (1988), all of which were very successful in Japan and received worldwide critical acclaim. Original video animation (OVA) films also began during this decade; the most influential of these early OVA films was Noboru Ishiguro's cyberpunk film "Megazone 23" (1985). The most famous anime film of this decade was Katsuhiro Otomo's cyberpunk film "Akira" (1988), which although initially unsuccessful at Japanese theaters, went on to become an international success.

Hong Kong action cinema, which was in a state of decline due to endless Bruceploitation films after the death of Bruce Lee, also experienced a revival in the 1980s, largely due to the reinvention of the action film genre by Jackie Chan. He had previously combined the comedy film and martial arts film genres successfully in the 1978 films "Snake in the Eagle's Shadow" and "Drunken Master". The next step he took was in combining this comedy martial arts genre with a new emphasis on elaborate and highly dangerous stunts, reminiscent of the silent film era. The first film in this new style of action cinema was "Project A" (1983), which saw the formation of the Jackie Chan Stunt Team as well as the "Three Brothers" (Chan, Sammo Hung and Yuen Biao). The film added elaborate, dangerous stunts to the fights and slapstick humor, and became a huge success throughout the Far East. As a result, Chan continued this trend with martial arts action films containing even more elaborate and dangerous stunts, including "Wheels on Meals" (1984), "Police Story" (1985), "Armour of God" (1986), "Project A Part II" (1987), "Police Story 2" (1988), and "Dragons Forever" (1988). Other new trends which began in the 1980s were the "girls with guns" subgenre, for which Michelle Yeoh gained fame; and especially the "heroic bloodshed" genre, revolving around Triads, largely pioneered by John Woo and for which Chow Yun-fat became famous. These Hong Kong action trends were later adopted by many Hollywood action films in the 1990s and 2000s.

The early 1990s saw the development of a commercially successful independent cinema in the United States. Although cinema was increasingly dominated by special-effects films such as "" (1991), "Jurassic Park" (1993) and "Titanic" (1997), the latter of which became the highest-grossing film of all time at the time up until "Avatar" (2009), also directed by James Cameron, independent films like Steven Soderbergh's "Sex, Lies, and Videotape" (1989) and Quentin Tarantino's "Reservoir Dogs" (1992) had significant commercial success both at the cinema and on home video. Filmmakers associated with the Danish film movement Dogme 95 introduced a manifesto aimed to purify filmmaking. Its first few films gained worldwide critical acclaim, after which the movement slowly faded out.
Major American studios began to create their own "independent" production companies to finance and produce non-mainstream fare. One of the most successful independents of the 1990s, Miramax Films, was bought by Disney the year before the release of Tarantino's runaway hit "Pulp Fiction" in 1994. The same year marked the beginning of film and video distribution online. Animated films aimed at family audiences also regained their popularity, with Disney's "Beauty and the Beast" (1991), "Aladdin" (1992), and "The Lion King" (1994). During 1995, the first feature length computer-animated feature, "Toy Story", was produced by Pixar Animation Studios and released by Disney. After the success of Toy Story, computer animation would grow to become the dominant technique for feature length animation, which would allow competing film companies such as DreamWorks Animation and 20th Century Fox to effectively compete with Disney with successful films of their own. During the late 1990s, another cinematic transition began, from physical film stock to digital cinema technology. Meanwhile, DVDs became the new standard for consumer video, replacing VHS tapes.

The documentary film also rose as a commercial genre for perhaps the first time, with the success of films such as "March of the Penguins" and Michael Moore's "Bowling for Columbine" and "Fahrenheit 9/11". A new genre was created with Martin Kunert and Eric Manes' "Voices of Iraq", when 150 inexpensive DV cameras were distributed across Iraq, transforming ordinary people into collaborative filmmakers. The success of "Gladiator" led to a revival of interest in epic cinema, and "Moulin Rouge!" renewed interest in musical cinema. Home theatre systems became increasingly sophisticated, as did some of the special edition DVDs designed to be shown on them. "The Lord of the Rings trilogy" was released on DVD in both the theatrical version and in a special extended version intended only for home cinema audiences.

In 2001, the "Harry Potter" film series began, and by its end in 2011, it had become the highest-grossing film franchise of all time until the Marvel Cinematic Universe passed it in 2015.

More films were also being released simultaneously to IMAX cinema, the first was in 2002's Disney animation "Treasure Planet"; and the first live action was in 2003's "The Matrix Revolutions" and a re-release of "The Matrix Reloaded". Later in the decade, "The Dark Knight" was the first major feature film to have been at least partially shot in IMAX technology.

There has been an increasing globalization of cinema during this decade, with foreign-language films gaining popularity in English-speaking markets. Examples of such films include "Crouching Tiger, Hidden Dragon" (Mandarin), "Amélie" (French), "Lagaan" (Hindi), "Spirited Away" (Japanese), "City of God" (Brazilian Portuguese), "The Passion of the Christ" (Aramaic), "Apocalypto" (Mayan) and "Inglourious Basterds" (multiple European languages). Italy is the most awarded country at the Academy Award for Best Foreign Language Film, with 14 awards won, 3 Special Awards and 31 nominations.

In 2003 there was a revival in 3D film popularity the first being James Cameron's "Ghosts of the Abyss" which was released as the first full-length 3-D IMAX feature filmed with the Reality Camera System. This camera system used the latest HD video cameras, not film, and was built for Cameron by Emmy nominated Director of Photography Vince Pace, to his specifications. The same camera system was used to film "" (2003), "Aliens of the Deep" IMAX (2005), and "The Adventures of Sharkboy and Lavagirl in 3-D" (2005).

After James Cameron's 3D film "Avatar" became the highest-grossing film of all time, 3D films gained brief popularity with many other films being released in 3D, with the best critical and financial successes being in the field of feature film animation such as Universal Pictures/Illumination Entertainment's "Despicable Me" and DreamWorks Animation's "How To Train Your Dragon", "Shrek Forever After" and "Megamind". "Avatar" is also note-worthy for pioneering highly sophisticated use of motion capture technology and influencing several other films such as "Rise of the Planet of the Apes".

, the largest film industries by number of feature films produced were those of India, the United States, China, Nigeria and Japan. 

In Hollywood, superhero films have greatly increased in popularity and financial success, with films based on Marvel and DC comics regularly being released every year up to the present. , the superhero genre has been the most dominant genre as far as American box office receipts are concerned.


\ Jones. Based on the book (above); written by Basten & Jones. Documentary, (1998).



</doc>
<doc id="10784" url="https://en.wikipedia.org/wiki?curid=10784" title="Cinema of France">
Cinema of France

Cinema of France refers to the film industry based in France. The French cinema comprises the art of film and creative movies made within the nation of France or by French filmmakers abroad.

France is the birthplace of cinema and was responsible for many of its significant contributions to the art form and the film-making process itself. Several important cinematic movements, including the Nouvelle Vague, began in the country. It is noted for having a particularly strong film industry, due in part to protections afforded by the French government.

Apart from its strong and innovative film tradition, France has also been a gathering spot for artists from across Europe and the world. For this reason, French cinema is sometimes intertwined with the cinema of foreign nations. Directors from nations such as Poland (Roman Polanski, Krzysztof Kieślowski, and Andrzej Żuławski), Argentina (Gaspar Noé and Edgardo Cozarinsky), Russia (Alexandre Alexeieff, Anatole Litvak), Austria (Michael Haneke), and Georgia (Géla Babluani, Otar Iosseliani) are prominent in the ranks of French cinema. Conversely, French directors have had prolific and influential careers in other countries, such as Luc Besson, Jacques Tourneur, or Francis Veber in the United States.

Another element supporting this fact is that Paris has the highest density of cinemas in the world, measured by the number of movie theaters per inhabitant, and that in most "downtown Paris" movie theaters, foreign movies which would be secluded to "art houses" cinemas in other places are shown alongside "mainstream" works. Philippe Binant realized, on 2 February 2000, the first digital cinema projection in Europe, with the DLP CINEMA technology developed by Texas Instruments, in Paris. Paris also boasts the Cité du cinéma, a major studio north of the city, and Disney Studio, a theme park devoted to the cinema and the third theme park near the city behind Disneyland and Parc Asterix.

France is the most successful film industry in Europe in terms of number of films produced per annum, with a record-breaking 300 feature-length films produced in 2015. France is also one of the few countries where non-American productions have the biggest share: American films only represented 44.9% of total admissions in 2014. This is largely due to the commercial strength of domestic productions, which accounted for 44,5% of admissions in 2014 (35.5% in 2015; 35.3% in 2016). Also, the French film industry is closer to being entirely self-sufficient than any other country in Europe, recovering around 80–90% of costs from revenues generated in the domestic market alone.

In 2013, France was the 2nd largest exporter of films in the world after the United States. A study in April 2014 showed the positive image which French cinema maintains around the world, being the most appreciated cinema after American cinema.

Les frères Lumière released the first projection with the Cinematograph, in Paris on 28 December 1895. The French film industry in the late 19th century and early 20th century was the world's most important. Auguste and Louis Lumière invented the cinématographe and their "L'Arrivée d'un train en gare de La Ciotat" in Paris in 1895 is considered by many historians as the official birth of cinematography.

The early days of the industry, from 1896 to 1902, saw the dominance of four firms: Pathé Frères, the Gaumont Film Company, the Georges Méliès company, and the Lumières. Méliès invented many of the techniques of cinematic grammar, and among his fantastic, surreal short subjects is the first science fiction film "A Trip to the Moon" ("Le Voyage dans la Lune") in 1902).

In 1902 the Lumières abandoned everything but the production of film stock, leaving Méliès as the weakest player of the remaining three. (He would retire in 1914.) From 1904 to 1911 the Pathé Frères company led the world in film production and distribution.

At Gaumont, pioneer Alice Guy-Blaché (M. Gaumont's former secretary) was made head of production and oversaw about 400 films, from her first, "La Fée aux Choux", in 1896, through 1906. She then continued her career in the United States, as did Maurice Tourneur and Léonce Perret after World War I.

In 1907 Gaumont owned and operated the biggest movie studio in the world, and along with the boom in construction of "luxury cinemas" like the Gaumont-Palace and the Pathé-Palace (both 1911), cinema became an economic challenger to legitimate theater by 1914. Among the most prolific film scholars on French Cinema in the English-speaking world is Dr Catherine O'Brien, former Senior Lecturer in Film Studies and French at Kingston University, London who obtained a Bachelor of Arts (1985) as well as a Doctor of Philosophy (Ph.D.;1994) both in French and German from the University of Hull.

After World War I, the French film industry suffered because of a lack of capital, and film production decreased as it did in most other European countries. This allowed the United States film industry to enter the European cinema market, because American films could be sold more cheaply than European productions, since the studios already had recouped their costs in the home market. When film studios in Europe began to fail, many European countries began to set import barriers. France installed an import quota of 1:7, meaning for every seven foreign films imported to France, one French film was to be produced and shown in French cinemas.

During the period between World War I and World War II, Jacques Feyder and Jean Vigo became two of the founders of poetic realism in French cinema. They also dominated French impressionist cinema, along with Abel Gance, Germaine Dulac and Jean Epstein.

In 1931, Marcel Pagnol filmed the first of his great trilogy "Marius", "Fanny", and "César". He followed this with other films including "The Baker's Wife". Other notable films of the 1930s included René Clair's "Under the Roofs of Paris" (1930), Jean Vigo's "L'Atalante" (1934), Jacques Feyder's "Carnival in Flanders" (1935), and Julien Duvivier's "La belle equipe" (1936). In 1935, renowned playwright and actor Sacha Guitry directed his first film and went on to make more than 30 films that were precursors to the New Wave era. In 1937, Jean Renoir, the son of painter Pierre-Auguste Renoir, directed "La Grande Illusion" ("The Grand Illusion"). In 1939, Renoir directed "La Règle du Jeu" ("The Rules of the Game"). Several critics have cited this film as one of the greatest of all-time, particularly for its innovative camerawork, cinematography and sound editing.

Marcel Carné's "Les Enfants du Paradis" ("Children of Paradise") was filmed during World War II and released in 1945. The three-hour film was extremely difficult to make due to the Nazi occupation. Set in Paris in 1828, it was voted Best French Film of the Century in a poll of 600 French critics and professionals in the late 1990s.

In the magazine "Cahiers du cinéma", founded by André Bazin and two other writers in 1951, film critics raised the level of discussion of the cinema, providing a platform for the birth of modern film theory. Several of the "Cahiers" critics, including Jean-Luc Godard, François Truffaut, Claude Chabrol, Jacques Rivette and Éric Rohmer, went on to make films themselves, creating what was to become known as the French New Wave. Some of the first films of this new movement were Godard's "Breathless" ("À bout de souffle", 1960), starring Jean-Paul Belmondo, Rivette's "Paris Belongs to Us" ("Paris nous appartient", 1958 – distributed in 1961), starring Jean-Claude Brialy and Truffaut's "The 400 Blows" ("Les Quatre Cent Coups", 1959) starring Jean-Pierre Léaud. 

Many contemporaries of Godard and Truffaut followed suit, or achieved international critical acclaim with styles of their own, such as the minimalist films of Robert Bresson and Jean-Pierre Melville, the Hitchcockian-like thrillers of Henri-Georges Clouzot, and other New Wave films by Agnès Varda and Alain Resnais. The movement, while an inspiration to other national cinemas and unmistakably a direct influence on the future New Hollywood directors, slowly faded by the end of the 1960s.

During this period, French commercial film also made a name for itself. Immensely popular French comedies with Louis de Funès topped the French box office. The war comedy "La Grande Vadrouille" (1966), from Gérard Oury with Bourvil and Terry-Thomas, was the most successful film in French theaters for more than 30 years. Another example was "La Folie des grandeurs" with Yves Montand. French cinema also was the birthplace for many subgenres of the crime film, most notably the modern caper film, starting with 1955's "Rififi" by American-born director Jules Dassin and followed by a large number of serious, noirish heist dramas as well as playful caper comedies throughout the sixties, and the "polar," a typical French blend of film noir and detective fiction. In addition, French movie stars began to claim fame abroad as well as at home. Popular actors of the period included Brigitte Bardot, Alain Delon, Romy Schneider, Catherine Deneuve, Jeanne Moreau, Simone Signoret, Yves Montand, Jean-Paul Belmondo and Jean Gabin. Since the Sixties and Seventies they are completed and followed by Michel Piccoli, Philippe Noiret, Annie Girardot, Jean-Louis Trintignant, Jean-Pierre Léaud, Claude Jade, Isabelle Huppert, Anny Duperey, Gérard Depardieu, Patrick Dewaere, Jean-Pierre Cassel, Miou-Miou, Brigitte Fossey, Stéphane Audran and Isabelle Adjani.

The 1979 film "La Cage aux Folles" ran for well over a year at the Paris Theatre, an arthouse cinema in New York City, and was a commercial success at theaters throughout the country, in both urban and rural areas. It won the Golden Globe Award for Best Foreign Language Film, and for years it remained the most successful foreign film to be released in the United States.
Jean-Jacques Beineix's "Diva" (1981) sparked the beginning of the 1980s wave of French cinema. Movies which followed in its wake included "Betty Blue" ("37°2 le matin", 1986) by Beineix, "The Big Blue" ("Le Grand bleu", 1988) by Luc Besson, and "The Lovers on the Bridge" ("Les Amants du Pont-Neuf", 1991) by Léos Carax. These films, made with a slick commercial style and emphasizing the alienation of their main characters, was known as Cinema du look.

"Camille Claudel", directed by newcomer Bruno Nuytten and starring Isabelle Adjani and Gérard Depardieu, was a major commercial success in 1988, earning Adjani, who was also the film's co-producer, a César Award for best actress. The historical drama film "Jean de Florette" (1986) and its sequel "Manon des Sources" (1986) were among the highest grossing French films in history and brought Daniel Auteuil international recognition.

According to Raphaël Bassan, in his article «"The Angel": Un météore dans le ciel de l'animation,» "La Revue du cinéma", n° 393, avril 1984. , Patrick Bokanowski's "The Angel", shown in 1982 at the Cannes Film Festival, can be considered the beginnings of contemporary animation. The masks erase all human personality in the characters. Patrick Bokanowski would thus have total control over the "matter" of the image and its optical composition. This is especially noticeable throughout the film, with images taken through distorted objectives or a plastic work on the sets and costumes, for example in the scene of the designer. Patrick Bokanowski creates his own universe and obeys his own aesthetic logic. It takes us through a series of distorted areas, obscure visions, metamorphoses and synthetic objects. Indeed, in the film, the human may be viewed as a fetish object (for example, the doll hanging by a thread), with reference to Kafkaesque and Freudian theories on automata and the fear of man faced with something as complex as him. The ascent of the stairs would be the liberation of the ideas of death, culture, and sex that makes us reach the emblematic figure of the angel.

Jean-Paul Rappeneau's "Cyrano de Bergerac" was a major box-office success in 1990, earning several César Awards, including best actor for Gérard Depardieu, as well as an Academy Award nomination for best foreign picture.

Luc Besson made "Nikita" in 1990, a movie that inspired remakes in both United States and in Hong Kong. In 1994, he also made "" (starring Jean Reno and a young Natalie Portman), and in 1997 "The Fifth Element", which became a cult favorite and launched the career of Milla Jovovich.

Jean-Pierre Jeunet made "Delicatessen" and "The City of Lost Children" ("La Cité des enfants perdus"), both of which featured a distinctly fantastical style.

In 1992, Claude Sautet co-wrote (with Jacques Fieschi) and directed "Un Coeur en Hiver", considered by many to be a masterpiece. Mathieu Kassovitz's 1995 film "Hate" ("La Haine") received critical praise and made Vincent Cassel a star, and in 1997, Juliette Binoche won the Academy Award for Best Supporting Actress for her role in "The English Patient".

The success of Michel Ocelot's "Kirikou and the Sorceress" in 1998 rejuvenated the production of original feature-length animated films by such filmmakers as Jean-François Laguionie and Sylvain Chomet.

In 2000, Philippe Binant realized the first digital cinema projection in Europe, with the DLP CINEMA technology developed by Texas Instruments, in Paris.

In 2001, after a brief stint in Hollywood, Jean-Pierre Jeunet returned to France with "Amélie" ("Le Fabuleux Destin d'Amélie Poulain") starring Audrey Tautou. It became the highest-grossing French-language film ever released in the United States. The following year, "Brotherhood of the Wolf" became the second-highest-grossing French-language film in the United States since 1980 and went on to gross more than $70 million worldwide.

In 2008, Marion Cotillard won the Academy Award for Best Actress and the BAFTA Award for Best Actress in a Leading Role for her portrayal of legendary French singer Édith Piaf in "La Vie en Rose", the first French-language performance to be so honored. The film won two Oscars and four BAFTAs and became the third-highest-grossing French-language film in the United States since 1980. Cotillard was the first female and second person to win both an Academy Award and César Award for the same performance.

At the 2008 Cannes Film Festival, "Entre les murs" ("The Class") won the Palme d'Or, the 6th French victory at the festival. The 2000s also saw an increase in the number of individual competitive awards won by French artists at the Cannes Festival, for direction (Tony Gatlif, "Exils", 2004), screenplay (Agnès Jaoui and Jean-Pierre Bacri, "Look at Me", 2004), female acting (Isabelle Huppert, "The Piano Teacher", 2001; Charlotte Gainsbourg, "Antichrist", 2009) and male acting (Jamel Debbouze, Samy Naceri, Roschdy Zem, Sami Bouajila and Bernard Blancan, "Days of Glory", 2006).

The 2008 rural comedy "Bienvenue chez les Ch'tis" drew an audience of more than 20 million, the first French film to do so. Its $193 million gross in France puts it just behind "Titanic" as the most successful film of all time in French theaters.

In the 2000s, several French directors made international productions, often in the action genre. These include Gérard Pirès ("Riders", 2002), Pitof ("Catwoman", 2004), Jean-François Richet ("Assault on Precinct 13", 2005), Florent Emilio Siri ("Hostage", 2005), Christophe Gans ("Silent Hill", 2006), Mathieu Kassovitz ("Babylon A.D.", 2008), Louis Leterrier ("The Transporter", 2002; "Transporter 2", 2005; Olivier Megaton directed "Transporter 3", 2008), Alexandre Aja ("Mirrors", 2008), and Pierre Morel ("Taken", 2009).

Surveying the entire range of French filmmaking today, Tim Palmer calls contemporary cinema in France a kind of eco-system, in which commercial cinema co-exists with artistic radicalism, first-time directors (who make up about 40% of all France's directors each year) mingle with veterans, and there even occasionally emerges a fascinating pop-art hybridity, in which the features of intellectual and mass cinemas are interrelated (as in filmmakers like Valeria Bruni-Tedeschi, Olivier Assayas, Maïwenn, Sophie Fillières, Serge Bozon, and others).

One of the most noticed and best reviewed films of 2010 was the drama "Of Gods and Men" ("Des hommes et des dieux"), about the assassination of seven monks in Tibhirine, Algeria. 2011 saw the release of "The Artist", a silent film shot in black and white by Michel Hazanavicius that reflected on the end of Hollywood's silent era.

French cinema continued its upward trend of earning awards at the Cannes Festival, including the prestigious Grand Prix for "Of Gods and Men" (2010) and the Jury Prize for Poliss (2011); the Best Director Award for Mathieu Amalric ("On Tour", 2010); the Best Actress Award for Juliette Binoche ("Certified Copy", 2010); and the Best Actor Award for Jean Dujardin ("The Artist", 2011).

In 2011, the film "Intouchables" became the most watched film in France (including the foreign films). After ten weeks nearly 17.5 million people had seen the film in France, Intouchables was the second most-seen French movie of all-time in France, and the third including foreign movies.

In 2012, with 226 million admissions (1,900 million USD) in the world for French films (582 films released in 84 countries), including 82 million admissions in France (700 million USD), 2012 was the fourth best year since 1985. With 144 million admissions outside France (1,200 million USD), 2012 was the best year since at least 1994 (since Unifrance collects data), and the French cinema reached a market share of 2.95% of worldwide admissions and of 4.86% of worldwide sales. Three films particularly contributed to this record year: "Taken 2", "The Intouchables" and "The Artist". In 2012, films shot in French ranked 4th in admissions (145 million) behind films shot in English (more than a billion admissions in the US alone), Hindi (?: no accurate data but estimated at 3 billion for the whole India/Indian languages) and Chinese (275 million in China plus a few million abroad), but above films shot in Korean (115 million admissions in South Korea plus a few millions abroad) and Japanese (102 million admissions in Japan plus a few million abroad, a record since 1973 et its 104 million admissions). French-language movies ranked 2nd in export (outside of French-speaking countries) after films in English. 2012 was also the year French animation studio Mac Guff was acquired by an American studio, Universal Pictures, through its Illumination Entertainment subsidiary. Illumination Mac Guff became the animation studio for some of the top English-language animated movies of the 2010s, including "The Lorax" and the "Despicable Me" franchise.

In 2015 French cinema sold 106 million tickets and grossed €600 million outside of the country. The highest-grossing film was "Taken 3" (€261.7 million) and the largest territory in admissions was China (14.7 million).

As the advent of television threatened the success of cinema, countries were faced with the problem of reviving movie-going. The French cinema market, and more generally the French-speaking market, is smaller than the English-speaking market; one reason being that some major markets, including prominently the United States, are reluctant to generally accept foreign films, especially foreign-language and subtitled productions. As a consequence, French movies have to be amortized on a relatively small market and thus generally have budgets far lower than their American counterparts, ruling out expensive settings and special effects.

The French government has implemented various measures aimed at supporting local film production and movie theaters. The Canal+ TV channel has a broadcast license requiring it to support the production of movies. Some taxes are levied on movies and TV channels for use as subsidies for movie production. Some tax breaks are given for investment in movie productions, as is common elsewhere including in the United States. The sale of DVDs is prohibited for four months after the showing in theaters, so as to ensure some revenue for movie theaters. Recently, Messerlin and Parc (2014, 2017) described the effect of subsidies in the French film industry.

The French national and regional governments involve themselves in film production. For example, the award-winning documentary "In the Land of the Deaf" ("Le Pays des sourds") was created by Nicolas Philibert in 1992. The film was co-produced by multinational partners, which reduced the financial risks inherent in the project; and co-production also ensured enhanced distribution opportunities.


In Anglophone distribution, "In the Land of the Deaf" was presented in French Sign Language (FSL) and French, with English subtitles and closed captions.

Notable French film distribution and/or production companies include:





</doc>
<doc id="10786" url="https://en.wikipedia.org/wiki?curid=10786" title="Cinema of the Soviet Union">
Cinema of the Soviet Union

The cinema of the Soviet Union includes films produced by the constituent republics of the Soviet Union reflecting elements of their pre-Soviet culture, language and history, albeit they were all regulated by the central government in Moscow. Most prolific in their republican films, after the Russian Soviet Federative Socialist Republic, were Armenia, Azerbaijan, Georgia, Ukraine, and, to a lesser degree, Lithuania, Belarus and Moldavia. At the same time, the nation's film industry, which was fully nationalized throughout most of the country's history, was guided by philosophies and laws propounded by the monopoly Soviet Communist Party which introduced a new view on the cinema, socialist realism, which was different from the one before or after the existence of the Soviet Union.

Upon the establishment of the Russian Soviet Socialist Republic (RSFSR) on November 7, 1917 (although the Union of Soviet Socialist Republics did not officially come into existence until December 30, 1922), what had formerly been the Russian Empire began quickly to come under the domination of a Soviet reorganization of all its institutions. From the outset, the leaders of this new state held that film would be the most ideal propaganda tool for the Soviet Union because of its widespread popularity among the established citizenry of the new land. Vladimir Lenin viewed film as the most important medium for educating the masses in the ways, means and successes of communism. As a consequence Lenin issued the "Directives on the Film Business" on 17 January 1922, which instructed the People's Commissariat for Education to systemise the film business, registering and numbering all films shown in the Russian Soviet Federative Socialist Republic, extracting rent from all privately owned cinemas and subject them to censorship. Joseph Stalin later also regarded cinema as of the prime importance.

However, between World War I and the Russian Revolution, the Russian film industry and the infrastructure needed to support it (e.g., electrical power) had deteriorated to the point of unworkability. The majority of cinemas had been in the corridor between Moscow and Saint Petersburg, and most were out of commission. Additionally, many of the performers, producers, directors and other artists of pre-Soviet Russia had fled the country or were moving ahead of Red Army forces as they pushed further and further south into what remained of the Russian Empire. Furthermore, the new government did not have the funds to spare for an extensive reworking of the system of filmmaking. Thus, they initially opted for project approval and censorship guidelines while leaving what remained of the industry in private hands. As this amounted mostly to cinema houses, the first Soviet films consisted of recycled films of the Russian Empire and its imports, to the extent that these were not determined to be offensive to the new Soviet ideology. Ironically, the first new film released in Soviet Russia did not exactly fit this mold: this was "Father Sergius", a religious film completed during the last weeks of the Russian Empire but not yet exhibited. It appeared on Soviet screens in 1918.

Beyond this, the government was principally able to fund only short, educational films, the most famous of which were the agitki – propaganda films intended to "agitate", or energize and enthuse, the masses to participate fully in approved Soviet activities, and deal effectively with those who remained in opposition to the new order. These short (often one small reel) films were often simple visual aids and accompaniments to live lectures and speeches, and were carried from city to city, town to town, village to village (along with the lecturers) to educate the entire countryside, even reaching areas where film had not been previously seen.

Newsreels, as documentaries, were the other major form of earliest Soviet cinema. Dziga Vertov's newsreel series "Kino-Pravda", the best known of these, lasted from 1922 to 1925 and had a propagandistic bent; Vertov used the series to promote socialist realism but also to experiment with cinema.

Still, in 1921, there was not one functioning cinema in Moscow until late in the year. Its rapid success, utilizing old Russian and imported feature films, jumpstarted the industry significantly, especially insofar as the government did not heavily or directly regulate what was shown, and by 1923 an additional 89 cinemas had opened. Despite extremely high taxation of ticket sales and film rentals, there was an incentive for individuals to begin making feature film product again – there were places to show the films - albeit they now had to conform their subject matter to a Soviet world view. In this context, the directors and writers who were in support of the objectives of communism assumed quick dominance in the industry, as they were the ones who could most reliably and convincingly turn out films that would satisfy government censors.
New talent joined the experienced remainder, and an artistic community assembled with the goal of defining "Soviet film" as something distinct and better from the output of "decadent capitalism". The leaders of this community viewed it essential to this goal to be free to experiment with the entire nature of film, a position which would result in several well-known creative efforts but would also result in an unforeseen counter-reaction by the increasingly solidifying administrators of the government-controlled society.

Sergei Eisenstein's "Battleship Potemkin" was released to wide acclaim in 1925; the film was heavily fictionalized and also propagandistic, giving the party line about the virtues of the proletariat.

One of the most popular films released in the 1930s was "Circus". Immediately after the end of the Second World War, color movies such as "The Stone Flower" (1946), "Ballad of Siberia" (1947), and "Cossacks of the Kuban" (1949) were released.
Other notable films from the 1940s include "Alexander Nevsky" and "Ivan the Terrible".

In the late 1950s and early 1960s Soviet cinema produced "Ballad of a Soldier", which won the 1961 BAFTA Award for Best Film, and "The Cranes Are Flying".

"Height" is considered to be one of the best films of the 1950s (it also became the foundation of the bard movement).

In the 1980s there was a diversification of subject matter. Touchy issues could now be discussed openly. The results were films like "Repentance", which dealt with repression in Georgia, and the allegorical science fiction movie "Kin-dza-dza!".

After the death of Stalin, Soviet filmmakers were given a freer hand to film what they believed audiences wanted to see in their film's characters and stories. The industry remained a part of the government and any material that was found politically offensive or undesirable, was either removed, edited, reshot, or shelved. The definition of "socialist realism" was liberalized to allow development of more human characters, but communism still had to remain uncriticized in its fundamentals. Additionally, the degree of relative artistic liberality was changed from administration to administration.

Examples created by censorship include:

The first Soviet Russian state film organization, the Film Subdepartment of the People's Commissariat for Education, was established in 1917. The work of the nationalized motion-picture studios was administered by the All-Russian Photography and Motion Picture Department, which was recognized in 1923 into Goskino, which in 1926 became Sovkino. The world's first state-filmmaking school, the First State School of Cinematography, was established in Moscow in 1919.

During the Russian Civil War, agitation trains and ships visited soldiers, workers, and peasants. Lectures, reports, and political meetings were accompanied by newsreels about events at the various fronts.

In the 1920s, the documentary film group headed by Dziga Vertov blazed the trail from the conventional newsreel to the "image centered publicistic film", which became the basis of the Soviet film documentary. Typical of the 1920s were the topical news serial "Kino-Pravda" and the film "Forward, Soviet!" by Vertov, whose experiments and achievements in documentary films influenced the development of Russian and world cinematography. Other important films of the 1920s were Esfir Shub's historical-revolutionary films such as "The Fall of the Romanov Dynasty". The film "Hydropeat" by Yuri Zhelyabuzhsky marked the beginning of popular science films. Feature-length agitation films in 1918-21 were important in the development of the film industry. Innovation in Russian filmmaking was expressed particularly in the work of Eisenstein. "Battleship Potemkin" was noteworthy for its innovative montage and metaphorical quality of its film language. It won world acclaim. Eisenstein developed concepts of the revolutionary epic in the film "". Also noteworthy was Vsevolod Pudovkin's adaptation of Maxim Gorky's "Mother" to the screen in 1926. Pudovkin developed themes of revolutionary history in the film "The End of St. Petersburg" (1927). Other noteworthy silent films were films dealing with contemporary life such as Boris Barnet's "The House on Trubnaya". The films of Yakov Protazanov were devoted to the revolutionary struggle and the shaping of a new way of life, such as "Don Diego and Pelagia" (1928). Ukrainian director Alexander Dovzhenko was noteworthy for the historical-revolutionary epic "Zvenigora", "Arsenal" and the poetic film "Earth".

In the early 1930s, Russian filmmakers applied socialist realism to their work. Among the most outstanding films was "Chapaev", a film about Russian revolutionaries and society during the Revolution and Civil War. Revolutionary history was developed in films such as "Golden Mountains" by Sergei Yutkevich, "Outskirts" by Boris Barnet, and the Maxim trilogy by Grigori Kozintsev and Leonid Trauberg: "The Youth of Maxim", "The Return of Maxim", and "The Vyborg Side". Also notable were biographical films about Vladimir Lenin such as Mikhail Romm's "Lenin in October" and "Lenin in 1918". The life of Russian society and everyday people were depicted in films such as "Courageous Seven" and "City of Youth" by Sergei Gerasimov. The comedies of Grigori Aleksandrov such as "Circus", "Volga-Volga", and "Tanya" as well as "The Rich Bride" by Ivan Pyryev and "By the Bluest of Seas" by Boris Barnet focus on the psychology of the common person, enthusiasm for work and intolerance for remnants of the past. Many films focused on national heroes, including "Alexander Nevsky" by Sergei Eisenstein, "Minin and Pozharsky" by Vsevolod Pudovkin, and "Bogdan Khmelnitsky" by Igor Savchenko. There were adaptations of literary classics, particularly Mark Donskoy's trilogy of films about Maxim Gorky: "The Childhood of Maxim Gorky", "My Apprenticeship", and "My Universities".

During the late 1920s and early 1930s the Stalin wing of the Communist Party consolidated its authority and set about transforming the Soviet Union on both the economic and cultural fronts. The economy moved from the market-based New Economic Policy (NEP) to a system of central planning. The new leadership declared a "cultural revolution" in which the party would exercise control over cultural affairs, including artistic expression. Cinema existed at the intersection of art and economics; so it was destined to be thoroughly reorganized in this episode of economic and cultural transformation.

To implement central planning in cinema, the new entity Soyuzkino was created in 1930. All the hitherto autonomous studios and distribution networks that had grown up under NEP's market would now be coordinated in their activities by this planning agency. Soyuzkino's authority also extended to the studios of the national republics such as VUFKU, which had enjoyed more independence during the 1920s. Soyuzkino consisted of an extended bureaucracy of economic planners and policy specialists who were charged to formulate annual production plans for the studios and then to monitor the distribution and exhibition of finished films.

With central planning came more centralized authority over creative decision making. Script development became a long, torturous process under this bureaucratic system, with various committees reviewing drafts and calling for cuts or revisions. In the 1930s censorship became more exacting with each passing year. Feature film projects would drag out for months or years and might be terminated at any point.

Alexander Dovzhenko drew from Ukrainian folk culture in such films as "Earth" (1930)
along the way because of the capricious decision of one or another censoring committee.
This redundant oversight slowed down production and inhibited creativity. Although central planning was supposed to increase the film industry's productivity, production levels declined steadily through the 1930s. The industry was releasing over one-hundred features annually at the end of the NEP period, but that figure fell to seventy by 1932 and to forty-five by 1934. It never again reached triple digits during the remainder of the Stalin era. Veteran directors experienced precipitous career declines under this system of control; whereas Eisenstein was able to make four features between 1924 and 1929, he completed only one film, "Alexander Nevsky" (1938) during the entire decade of the 1930s. His planned adaptation of the Ivan Turgenev story "Bezhin Meadow" (1935–37) was halted during production in 1937 and officially banned, one of many promising film projects that fell victim to an exacting censorship system.

Meanwhile, the USSR cut off its film contacts with the West. It stopped importing films after 1931 out of concern that foreign films exposed audiences to capitalist ideology. The industry also freed itself from dependency on foreign technologies. During its industrialization effort of the early 1930s, the USSR finally built an array of factories to supply the film industry with the nation's own technical resources.

To secure independence from the West, industry leaders mandated that the USSR develop its own sound technologies, rather than taking licenses on Western sound systems. Two Soviet scientists, Alexander Shorin in Leningrad (present-day St. Petersburg) and Pavel Tager in Moscow, conducted research through the late 1920s on complementary sound systems, which were ready for use by 1930. The implementation process, including the cost of refitting movie theaters, proved daunting, and the USSR did not complete the transition to sound until 1935. Nevertheless, several directors made innovative use of sound once the technology became available. In "Enthusiasm: The Symphony of Donbass" (1930), his documentary on coal mining and heavy industry, Dziga Vertov based his soundtrack on an elegantly orchestrated array of industrial noises. In "The Deserter" (1933) Pudovkin experimented with a form of "sound counterpoint" by exploiting tensions and ironic dissonances between sound elements and the image track. And in "Alexander Nevsky", Eisenstein collaborated with the composer Sergei Prokofiev on an "operatic" film style that elegantly coordinated the musical score and the image track.

As Soviet cinema made the transition to sound and central planning in the early 1930s, it was also put under a mandate to adopt a uniform film style, commonly identified as "socialist realism". In 1932 the party leadership ordered the literary community to abandon the avant-garde practices of the 1920s and to embrace socialist realism, a literary style that, in practice, was actually close to 19th-century realism. The other arts, including cinema, were subsequently instructed to develop the aesthetic equivalent. For cinema, this meant adopting a film style that would be legible to a broad audience, thus avoiding a possible split between the avant-garde and mainstream cinema that was evident in the late 1920s. The director of Soyuzkino and chief policy officer for the film industry, Boris Shumyatsky (1886–1938), who served from 1931 to 1938, was a harsh critic of the montage aesthetic. He championed a "cinema for the millions", which would use clear, linear narration. Although American movies were no longer being imported in the 1930s, the Hollywood model of continuity editing was readily available, and it had a successful track record with Soviet movie audiences. Soviet socialist realism was built on this style, which assured tidy storytelling. Various guidelines were then added to the doctrine: positive heroes to act as role models for viewers; lessons in good citizenship for spectators to embrace; and support for reigning policy decisions of the Communist Party.

Such aesthetic policies, enforced by the rigorous censorship apparatus of Soyuzkino, resulted in a number of formulaic films. Apparently, they did succeed in sustaining a true "cinema of the masses". The 1930s witnessed some stellar examples of popular cinema. The single most successful film of the decade, in terms of both official praise and genuine affection from the mass audience, was "Chapayev" (1934), directed by the Vasilyev brothers. Based on the life of a martyred Red Army commander, the film was touted as a model of socialist realism, in that Chapayev and his followers battled heroically for the revolutionary cause. The film also humanized the title character, giving him personal foibles, an ironic sense of humour, and a rough peasant charm. These qualities endeared him to the viewing public: spectators reported seeing the film multiple times during its first run in 1934, and "Chapayev" was periodically re-released for subsequent generations of audiences.

A genre that emerged in the 1930s to consistent popular acclaim was the musical comedy, and a master of that form was Grigori Aleksandrov (1903–1984). He effected a creative partnership with his wife, the brilliant comic actress and chanteuse Lyubov Orlova (1902–1975), in a series of crowd-pleasing musicals. Their pastoral comedy "Volga-Volga" (1938) was surpassed only by "Chapayev" in terms of box-office success. The fantasy element of their films, with lively musical numbers reviving the montage aesthetic, sometimes stretched the boundaries of socialist realism, but the genre could also allude to contemporary affairs. In Aleksandrov's 1940 musical "Tanya", Orlova plays a humble servant girl who rises through the ranks of the Soviet industrial leadership after developing clever labour-saving work methods. Audiences could enjoy the film's comic turn on the "Cinderella" story while also learning about the value of efficiency in the workplace.

Immediately after the end of the Second World War, color movies such as "The Stone Flower" (1946), "Ballad of Siberia" (1947), and "Cossacks of the Kuban" (1949) were released.
Other notable films from the 1940s include the black and white films, "Alexander Nevsky", "Ivan the Terrible" and the "Encounter at the Elbe".

The Soviet film industry suffered during the period after World War II. On top of dealing with the severe physical and monetary losses of the war, Stalin's regime tightened social control and censorship in order to manage the effects recent exposure to the West had on the people. The postwar period was marked by an end of almost all autonomy in the Soviet Union. The "Catalogue of Soviet Films" recorded remarkably low numbers of films being produced from 1945 to 1953, with as few as nine films produced in 1951 and a maximum of twenty-three produced in 1952. These numbers do not, however, include many of the works which are not generally considered to be "film" in an elitist sense, such as filmed versions of theatrical works and operas, feature-length event documentaries and travelogues, short films for children, and experimental stereoscopic films. But compared to the four hundred to five hundred films produced every year by Hollywood, the Soviet film industry was practically dead.

Even as the economy of the Soviet Union strengthened, film production continued to decrease. A resolution passed by the Council of Ministers in 1948 further crippled the film industry. The resolution criticized the work of the industry, saying that an emphasis placed on quantity over quality had ideologically weakened the films. Instead, the council insisted that every film produced must be a masterpiece for promoting communist ideas and the Soviet system. Often, Stalin had the ultimate decision on whether a newly produced film was appropriate for public viewing. In private screenings after meetings of the Politburo, the Minister of the Film Industry privately screened films for Stalin and top members of Soviet government. The strict limitations on content and complex, centralized process for approval drove many screenwriters away, and studios had much difficulty producing any of the quality films mandated by the 1948 resolution.

Movie theaters in the postwar period faced the problem of satisfying the growing appetites of Soviet audiences for films while dealing with the shortage of newly produced works from studios. In response, cinemas played the same films for months at a time, many of them the works of the late 1930s. Anything new drew millions of people to the box office, and many theaters screened foreign films to attract larger audiences. Most of these foreign films were "trophy films", two thousand films brought into the country by the Red Army after the occupation of Germany and Eastern Europe in World War II. In the top secret minutes for the CPSU Committee Meeting on August 31, 1948, the committee permitted the Minister of the Film Industry to release fifty of these films in the Soviet Union. Of these fifty, Bolshakov was only allowed to release twenty-four for screening to the general public, mainly films made in Germany, Austria, Italy, and France. The other twenty-six films, consisting almost entirely of American films, were only allowed to be shown in private screenings. The minutes also include a separate list of permitted German musical films, which were mainly German and Italian film adaptations of famous operas. Most of the trophy films were released in 1948–49, but somewhat strangely, compiled lists of the released films include ones not previously mentioned in the official minutes of the Central Committee.

The public release of these trophy films seems contradictory in the context of the 1940s Soviet Union. The Soviet government allowed the exhibition of foreign films which contained far more subversive ideas than any a Soviet director would have ever attempted putting in a film at a time when Soviet artists found themselves unemployed because of censorship laws. Historians hypothesize many possible reasons why the Soviet government showed such seemingly inexplicable leniency toward the foreign films. The government may have granted cinemas the right to show the films so they could stay in business after the domestic film industry had declined. A second hypothesis speculates that the government saw the films as an easy source of money to help rebuild the nation after the war. The minutes of the CPSU Central Committee meeting seem to support the latter idea with instructions that the films are to bring in a net income of at least 750 million rubles to the State coffers over the course of a year from public and private screenings, and 250 million rubles of this were supposed to come from rentals to the trade union camera network.

In addition to releasing the films, the committee also charged Bolshakov and the Agitation and Propaganda Department of the CPSU Central Committee "with making the necessary editorial corrections to the films and with providing an introductory text and carefully edited subtitles for each film." In general, the captured Nazi films were considered apolitical enough to be shown to the general populace. Still the Propaganda and Agitation Section of the Central Committee ran into trouble with the censoring of two films slated for release. The censors found it impossible to remove the "Zionist" ideas from "Jud Suss", an anti-Semitic, Nazi propaganda film. The censors also had trouble with a film adaptation of "Of Mice and Men" because of the representation of the poor as a detriment to society.

There is very little direct evidence of how Soviet audiences received the trophy films. Soviet magazines or newspapers never reviewed the films, there were no audience surveys, and no records exist of how many people viewed the films. In order to judge the reception and popularity of these foreign films, historians have mainly relied on anecdotal evidence. The German musical comedy "The Woman of My Dreams" has received mixed reviews according to this evidence. "" published a supposed survey compiled of readers' letters to the editor in March, 1947 which criticize the film for being idealess, low brow, and even harmful. Bulat Okudzhava wrote a contradicting viewpoint in "Druzhba narodov" in 1986, saying that everyone in the city of Tbilisi was crazy about the film. According to him, everywhere he went people were talking about the film and whistling the songs. Of the two accounts, film historians generally consider Okudzhava's more reliable than the one presented by "Kultura i Zhizn". Films such as "His Butler's Sister", "The Thief of Bagdad", "Waterloo Bridge" and "Sun Valley Serenade", although not technically trophies as they had been purchased legally during the wartime alliance with America, were highly popular with Soviet audiences. In "Vecherniaia Moskva" (October 4, 1946), M. Chistiakov reprimanded theaters and the Soviet film industry for the fact that over a six-month timespan, sixty of the films shown had been tasteless Western films rather than Soviet ones. Even in criticism of the films and the crusading efforts of the anti-cosmopolitan campaign against the trophy films, it is clear to see they had quite an impact on Soviet society.

With the start of the Cold War, writers, still considered the primary auteurs, were all the more reluctant to take up script writing, and the early 1950s saw only a handful of feature films completed during any year. The death of Stalin was a relief to some people, and all the more so was the official trashing of his public image as a benign and competent leader by Nikita Khrushchev two years later. This latter event gave filmmakers the margin of comfort they needed to move away from the narrow stories of socialist realism, expand its boundaries, and begin work on a wider range of entertaining and artistic Soviet films.

Notable films include:

The 1960s and 1970s saw the creation of many films, many of which molded Soviet and post-Soviet culture. They include:

Soviet films tend to be rather culture-specific and are difficult for many foreigners to understand without having been exposed to the culture first. Various Soviet directors were more concerned with artistic success than with economical success (They were paid by the academy, and so money was not a critical issue). This contributed to the creation of a large number of more philosophical and poetical films. Most well-known examples of such films are those by directors Andrei Tarkovsky, Sergei Parajanov and Nikita Mikhalkov. In keeping with Russian culture, tragi-comedies were very popular. These decades were also prominent in the production of the Eastern or Red Western.

Animation was a respected genre, with many directors experimenting with technique. "Tale of Tales" (1979) by Yuriy Norshteyn was twice given the title of "Best Animated Film of All Eras and Nations" by animation professionals from around the world, in 1984 and 2002.

In the year of the 60th anniversary of the Soviet cinema (1979), on April 25, a decision of the Presidium of the Supreme Soviet of the USSR established a commemorative "". It was then celebrated in the USSR each year on August 27, the day on which Vladimir Lenin signed a decree to nationalise the country's cinematic and photographic industries.

The policies of perestroika and glasnost saw a loosening of the censorship of earlier eras. A genre known as "" (from the Russian word for "gore"), including films such as "Little Vera", portrayed the harsher side of Soviet life. Notable films of this period include:












Early personalities in the development of Soviet cinema:

Later personalities:





</doc>
<doc id="10787" url="https://en.wikipedia.org/wiki?curid=10787" title="Cinema of Italy">
Cinema of Italy

The Cinema of Italy comprises the films made within Italy or by Italian directors. The first Italian director is considered to be , a collaborator of the Lumière Brothers, who filmed Pope Leo XIII in 1896. Since its beginning, Italian cinema has influenced film movements worldwide. As of 2018, Italian films have won 14 Academy Awards for Best Foreign Language Film (the most of any country) as well as 12 Palmes d'Or (the second-most of any country), one Academy Award for Best Picture and many Golden Lions and Golden Bears.

Italy is the birthplace of Art Cinema and the stylistic aspect of film has been the most important factor in the history of Italian movies. In the early 1900s, artistic and epic films such as "Otello" (1906), "The Last Days of Pompeii" (1908), "L'Inferno" (1911), "Quo Vadis" (1913), and "Cabiria" (1914), were made as adaptations of books or stage plays. Italian filmmakers were utilizing complex set designs, lavish costumes, and record budgets, to produce pioneering films. One of the first cinematic avante-garde movements, Italian Futurism, took place in Italy in the late 1910s. After a period of decline in the 1920s, the Italian film industry was revitalized in the 1930s with the arrival of sound film. A popular Italian genre during this period, the "Telefoni Bianchi", consisted of comedies with glamorous backgrounds.

While Italy's Fascist government provided financial support for the nation's film industry, most notably the construction of the Cinecittà studios (the largest film studio in Europe), it also engaged in censorship, and thus many Italian films produced in the late 1930s were propaganda films. Post-World War II Italy saw the rise of the influential Italian neorealist movement, which launched the directorial careers of Luchino Visconti, Roberto Rossellini, and Vittorio De Sica. Neorealism declined in the late 1950s in favor of lighter films, such as those of the "Commedia all'italiana" genre and important directors like Federico Fellini and Michelangelo Antonioni. Actresses such as Sophia Loren, Giulietta Masina and Gina Lollobrigida achieved international stardom during this period.

The Spaghetti Western achieved popularity in the mid-1960s, peaking with Sergio Leone's "Dollars Trilogy", which featured enigmatic scores by composer Ennio Morricone. Erotic Italian thrillers, or "giallos", produced by directors such as Mario Bava and Dario Argento in the 1970s, influenced the horror genre worldwide. During the 1980s and 1990s, directors such as Ermanno Olmi, Bernardo Bertolucci, Giuseppe Tornatore, Gabriele Salvatores and Roberto Benigni brought critical acclaim back to Italian cinema.

The country is also famed for its prestigious Venice Film Festival, the oldest film festival in the world, held annually since 1932 and awarding the Golden Lion. In 2008 the Venice Days ("Giornate degli Autori"), a section held in parallel to the Venice Film Festival, has produced in collaboration with Cinecittà studios and the Ministry of Cultural Heritage a list of 100 films that have changed the collective memory of the country between 1942 and 1978: the "100 Italian films to be saved".

The French Lumière brothers commenced public screenings in Italy in 1896: in March 1896, in Rome and Milan; in April in Naples, Salerno and Bari; in June in Livorno; in August in Bergamo, Bologna and Ravenna; in October in Ancona; and in December in Turin, Pescara and Reggio Calabria. Lumière trainees produced short films documenting everyday life and comic strips in the late 1890s and early 1900s. Pioneering Italian cinematographer Filoteo Alberini patented his "Kinetograph" during this period.

The Italian film industry took shape between 1903 and 1908, led by three major organizations: Cines, based in Rome; and the Turin-based companies Ambrosio Film and Itala Film. Other companies soon followed in Milan and Naples, and these early companies quickly attained a respectable production quality and were able to market their products both within Italy and abroad.

Early Italian films typically consisted of adaptations of books or stage plays, such as Mario Caserini's "Otello" (1906) and Arturo Ambrosio's 1908 adaptation of the novel, "The Last Days of Pompeii". Also popular during this period were films about historical figures, such as Caserini's "Beatrice Cenci" (1909) and Ugo Falena's "Lucrezia Borgia" (1910). "L'Inferno", produced by Milano Films in 1911, was the first full-length Italian feature film ever made. Popular early Italian actors included Emilio Ghione, Alberto Collo, Bartolomeo Pagano, Amleto Novelli, Lyda Borelli, Ida Carloni Talli, Lidia Quaranta and Maria Jacobini.

Enrico Guazzone's 1913 film "Quo Vadis" was one of the earliest "blockbusters" in cinema history, utilizing thousands of extras and a lavish set design. Giovanni Pastrone's 1914 film "Cabiria" was an even larger production, requiring two years and a record budget to produce, and it was the first epic film ever made. Nino Martoglio's "Lost in Darkness", also produced in 1914, documented life in the slums of Naples, and is considered a precursor to the Neorealist movement of the 1940s and 1950s.

Between 1911 and 1919, Italy was home to the first avant-garde movement in cinema, inspired by the country's Futurism movement. The 1916 Manifesto of Futuristic Cinematography was signed by Filippo Marinetti, Armando Ginna, Bruno Corra, Giacomo Balla and others. To the Futurists, cinema was an ideal art form, being a fresh medium, and able to be manipulated by speed, special effects and editing. Most of the futuristic-themed films of this period have been lost, but critics cite "Thaïs" (1917) by Anton Giulio Bragaglia as one of the most influential, serving as the main inspiration for German Expressionist cinema in the following decade.

The Italian film industry struggled against rising foreign competition in the years following World War I. Several major studios, among them Cines and Ambrosio, formed the Unione Cinematografica Italiana to coordinate a national strategy for film production. This effort was largely unsuccessful, however, due to a wide disconnect between production and exhibition (some movies weren't released until several years after they were produced). Among the notable Italian films of the late silent era were Mario Camerini's "Rotaio" (1929) and Alessandro Blasetti's "Sun" (1929).

In 1930, Gennaro Righelli directed the first Italian talking picture, "The Song of Love". This was followed by Blasetti's "Mother Earth" (1930) and "Resurrection" (1931), and Camerini's "Figaro and His Great Day" (1931). The advent of talkies led to stricter censorship by the Fascist government.

During the 1930s, light comedies known as "telefoni bianchi" ("white telephones") were predominant in Italian cinema. These films, which featured lavish set designs, promoted conservative values and respect for authority, and thus typically avoided the scrutiny of government censors. Important examples of "telefoni bianchi" include Guido Brignone's "Paradiso" (1932), Carlo Bragaglia's "O la borsa o la vita" (1933), and Righelli's "Together in the Dark" (1935). Historical films such as Blasetti's "1860" (1934) and Carmine Gallone's "" (1937) were also popular during this period.

In 1934, the Italian government created the General Directorate for Cinema ("Direzione Generale per le Cinematografia"), and appointed Luigi Freddi its director. With the approval of Benito Mussolini, this directorate called for the establishment of a town southeast of Rome devoted exclusively to cinema, dubbed the "Cinecittà" ("Cinema City"). Completed in 1937, the Cinecittà provided everything necessary for filmmaking: theaters, technical services, and even a cinematography school, the Centro Sperimentale di Cinematografia, for younger apprentices. The Cinecittà studios were Europe's most advanced production facilities, and greatly boosted the technical quality of Italian films. Many films are still shot entirely in Cinecittà.

During this period, Mussolini's son, Vittorio, created a national production company and organized the work of noted authors, directors and actors (including even some political opponents), thereby creating an interesting communication network among them, which produced several noted friendships and stimulated cultural interaction.

By the end of World War II, the Italian "neorealist" movement had begun to take shape. Neorealist films typically dealt with the working class (in contrast to the "Telefoni Bianchi"), and were shot on location. Many neorealist films, but not all, utilized non-professional actors. Though the term "neorealism" was used for the first time to describe Luchino Visconti’s 1943 film, "Ossessione", there were several important precursors to the movement, most notably Camerini's "What Scoundrels Men Are!" (1932), which was the first Italian film shot entirely on location, and Blasetti's 1942 film, "Four Steps in the Clouds".

"Ossessione" angered Fascist officials. Upon viewing the film, Vittorio Mussolini is reported to have shouted, "This is not Italy!" before walking out of the theater. The film was subsequently banned in the Fascist-controlled parts of Italy. While neorealism exploded after the war, and was incredibly influential at the international level, neorealist films made up only a small percentage of Italian films produced during this period, as postwar Italian moviegoers preferred escapist comedies starring actors such as Totò and Alberto Sordi.

Neorealist works such as Roberto Rossellini's trilogy "Rome, Open City" (1945), "Paisà" (1946), and "Germany, Year Zero" (1948), with professional actors such as Anna Magnani and a number of non-professional actors, attempted to describe the difficult economic and moral conditions of postwar Italy and the changes in public mentality in everyday life. Visconti's "The Earth Trembles" (1948) was shot on location in a Sicilian fishing village, and utilized local non-professional actors. Giuseppe De Santis, on other hand, used actors such as Silvana Mangano and Vittorio Gassman in his 1949 film, "Bitter Rice", which is set in the Po Valley during rice-harvesting season.

Poetry and cruelty of life were harmonically combined in the works that Vittorio De Sica wrote and directed together with screenwriter Cesare Zavattini: among them, "Shoeshine" (1946), "The Bicycle Thief" (1948) and "Miracle in Milan" (1951). The 1952 film "Umberto D." showed a poor old man with his little dog, who must beg for alms against his dignity in the loneliness of the new society. This work is perhaps De Sica's masterpiece and one of the most important works in Italian cinema. It was not a commercial success and since then it has been shown on Italian television only a few times. Yet it is perhaps the most violent attack, in the apparent quietness of the action, against the rules of the new economy, the new mentality, the new values, and it embodies both a conservative and a progressive view.

Although "Umberto D." is considered the end of the neorealist period, later films such as Federico Fellini's "La Strada" (1954) and De Sica's 1960 film "Two Women" (for which Sophia Loren won the Oscar for Best Actress) are grouped with the genre. Director Pier Paolo Pasolini's first film, "Accattone" (1961), shows a strong neorealist influence. Italian neorealist cinema influenced filmmakers around the world, and helped inspire other film movements, such as the French New Wave and the Polish Film School. The Neorealist period is often simply referred to as "The Golden Age" of Italian Cinema by critics, filmmakers, and scholars.

It has been said that after "Umberto D." nothing more could be added to neorealism. Possibly because of this, neorealism effectively ended with that film; subsequent works turned toward lighter atmospheres, perhaps more coherent with the improving conditions of the country, and this genre has been called "pink neorealism". This trend allowed better-"equipped" actresses to become real celebrities, such as Sophia Loren, Gina Lollobrigida, Silvana Pampanini, Lucia Bosé, Barbara Bouchet, Eleonora Rossi Drago, Silvana Mangano, Virna Lisi, Claudia Cardinale and Stefania Sandrelli. Soon pink neorealism, such as "Pane, amore e gelosia" (1954, released in the US as "Frisky") with Vittorio DeSica and Gina Lollobrigida, was replaced by the "Commedia all'italiana", a unique genre that, born on an ideally humouristic line, talked instead very seriously about important social themes.

At this time, on the more commercial side of production, the phenomenon of Totò, a Neapolitan actor who is acclaimed as the major Italian comic, exploded. His films (often with Peppino De Filippo and almost always with Mario Castellani) expressed a sort of neorealistic satire, in the means of a "guitto" (a "hammy" actor) as well as with the art of the great dramatic actor he also was. A "film-machine" who produced dozens of titles per year, his repertoire was frequently repeated. His personal story (a prince born in the poorest "rione" (section of the city) of Naples), his unique twisted face, his special mimic expressions and his gestures created an inimitable personage and made him one of the most beloved Italians of the 1960s.

Italian Comedy is generally considered to have started with Mario Monicelli's "I soliti Ignoti" ("Big Deal on Madonna Street", 1958) and derives its name from the title of Pietro Germi's "Divorzio all'Italiana" ("Divorce Italian Style", 1961). For a long time this definition was used with a derogatory intention.

Vittorio Gassman, Marcello Mastroianni, Ugo Tognazzi, Alberto Sordi, Claudia Cardinale, Monica Vitti and Nino Manfredi were among the stars of these movies, that described the years of the economical reprise and investigated Italian customs, a sort of self-ethnological research.

In 1961 Dino Risi directed "Una vita difficile" ("A Difficult Life"), then "Il sorpasso" (The Easy Life), now a cult-movie, followed by: "I Mostri" ("The Monsters", also known as "15 From Rome"), "In nome del Popolo Italiano" ("In the Name of the Italian People") and "Profumo di donna" ("Scent of a Woman").

Monicelli's works include "La grande guerra" ("The Great War"), "I compagni" ("Comrades", also known as "The Organizer"), "L'Armata Brancaleone", "Vogliamo i colonnelli" ("We Want the Colonels"), "Romanzo popolare" (Popular Novel) and the "Amici miei" series.

A series of black-and-white films based on Don Camillo character created by the Italian writer and journalist Giovannino Guareschi were made between 1952 and 1965. These were French-Italian coproductions, and starred Fernandel as Don Camillo and Gino Cervi as Peppone. The titles are: "The Little World of Don Camillo", "The Return of Don Camillo", "Don Camillo's Last Round", "", and "Don Camillo in Moscow". Mario Camerini began filming the film "Don Camillo e i giovani d'oggi" but had to stop filming due to Fernandel's falling ill, which resulted in his untimely death. The film was then completed in 1972 with Gastone Moschin playing the role of Don Camillo and Lionel Stander as Peppone. A Don Camillo ("The World of Don Camillo") film was remade in 1983, an Italian production with Terence Hill directing and also starring as Don Camillo. Colin Blakely performed Peppone in one of his last film roles.

In the late 1940s, Hollywood studios began to shift production abroad to Europe. Italy was, along with Britain, one of the major destinations for American film companies. Shooting at Cinecittà, large-budget films such as "Quo Vadis" (1951), "Roman Holiday" (1953), "Ben-Hur" (1959), and "Cleopatra" (1963) were made in English with international casts and sometimes, but not always, Italian settings or themes. The heyday of what was dubbed '"Hollywood on the Tiber" was between 1950 and 1970, during which time many of the most famous names in world cinema made films in Italy.

With the release of 1958's "Hercules", starring American bodybuilder Steve Reeves, the Italian film industry gained entree to the American film market. These films, many with mythological or Bible themes, were low-budget costume/adventure dramas, and had immediate appeal with both European and American audiences. Besides the many films starring a variety of muscle men as Hercules, heroes such as Samson and Italian fictional hero Maciste were common. Sometimes dismissed as low-quality escapist fare, the Peplums allowed newer directors such as Sergio Leone and Mario Bava a means of breaking into the film industry. Some, such as Mario Bava's "Hercules in the Haunted World" (Italian: Ercole Al Centro Della Terra) are considered seminal works in their own right. As the genre matured, budgets sometimes increased, as evidenced in 1962's "I sette gladiatori" ("The Seven Gladiators" in 1964 US release), a wide-screen epic with impressive sets and matte-painting work. Most Peplum films were in color, whereas previous Italian efforts had often been black and white.

On the heels of the Peplum craze, a related genre, the Spaghetti Western arose and was popular both in Italy and elsewhere. These films differed from traditional westerns by being filmed in Europe on limited budgets, but featured vivid cinematography.

The most popular Spaghetti Westerns were those of Sergio Leone, whose Dollars Trilogy ("A Fistful of Dollars", "For a Few Dollars More", and "The Good, the Bad and the Ugly"), featuring Clint Eastwood and scores by Ennio Morricone, came to define the genre along with "Once Upon a Time in the West".

Also considered Spaghetti Westerns is a film genre which combined traditional western ambiance with a Commedia all'italiana-type comedy; films including "They Call Me Trinity" and "Trinity Is STILL My Name!", which featured Bud Spencer and Terence Hill, the stage names of Carlo Pedersoli and Mario Girotti.

During the 1960s and 70s, Italian filmmakers Mario Bava, Riccardo Freda, Antonio Margheriti and Dario Argento developed "giallo" horror films that become classics and influenced the genre in other countries. Representative films include: "Black Sunday", "Castle of Blood", "Twitch of the Death Nerve", "The Bird with the Crystal Plumage", "Deep Red" and "Suspiria."

Due to the success of the James Bond film series the Italian film industry made large amounts of imitations and spoofs in the Eurospy genre from 1964-1967.

Following the 1960s boom of shockumentary "Mondo films" such as Gualtiero Jacopetti's "Mondo Cane", during the late 1970s and early 1980s, Italian cinema became internationally synonymous with violent horror films. These films were primarily produced for the video market and were credited with fueling the "video nasty" era in the United Kingdom.

Directors in this genre included Lucio Fulci, Joe D'Amato, Umberto Lenzi and Ruggero Deodato. Some of their films faced legal challenges in the United Kingdom; after the Video Recordings Act of 1984, it became a legal offense to sell a copy of such films as "Cannibal Holocaust" and "SS Experiment Camp". Italian films of this period are usually grouped together as exploitation films.

Several countries charged Italian studios with exceeding the boundaries of acceptability with their late-1970s Nazi exploitation films, inspired by American movies such as "Ilsa, She Wolf of the SS". The Italian works included the notorious but comparatively tame "SS Experiment Camp" and the far more graphic "Last Orgy of the Third Reich" (Italian: L'ultima orgia del III Reich). These films showed, in great detail, sexual crimes against prisoners at concentration camps. These films may still be banned in the United Kingdom and other countries.

Poliziotteschi (; plural of poliziottesco) films constitute a subgenre of crime and action film that emerged in Italy in the late 1960s and reached the height of their popularity in the 1970s. They are also known as polizieschi, Italo-crime, Euro-crime or simply Italian crime films. Most notable international actors acted in this genre of films such Alain Delon, Henry Silva, Fred Williamson, Charles Bronson, Tomas Milian and others international stars.

Between the late 1970s and mid 1980s, Italian cinema was in crisis; "art films" became increasingly isolated, separating from the mainstream Italian cinema.

Among the major artistic films of this era were "La città delle donne", "E la nave va", "Ginger and Fred" by Fellini, "L'albero degli zoccoli" by Ermanno Olmi (winner of the Palme d'Or at the Cannes Film Festival), "La notte di San Lorenzo" by Paolo and Vittorio Taviani, Antonioni's "Identificazione di una donna", and "Bianca" and "La messa è finita" by Nanni Moretti. Although not entirely Italian, Bertolucci's "The Last Emperor", winner of 9 Oscars, and "Once Upon a Time in America" of Sergio Leone came out of this period also.

During this time, commedia sexy all'italiana films, described as "trash films", were popular in Italy. These comedy films were of little artistic value and reached their popularity by confronting Italian social taboos, most notably in the sexual sphere. Actors such as Lino Banfi, Diego Abatantuono, Alvaro Vitali, Gloria Guida, Barbara Bouchet and Edwige Fenech owe much of their popularity to these films.

Also considered part of the trash genre are films which feature Fantozzi, a comic personage invented by Paolo Villaggio. Although Villaggio's movies tend to bridge trash comedy with a more elevated social satire; this character had a great impact on Italian society, to such a degree that the adjective "fantozziano" entered the lexicon. Of the many films telling of Fantozzi's misadventures, the most notable were "Fantozzi" and "Il secondo tragico Fantozzi".

A new generation of directors has helped return Italian cinema to a healthy level since the end of the 1980s. Probably the most noted film of the period is "Nuovo Cinema Paradiso", for which Giuseppe Tornatore won a 1989 Oscar (awarded in 1990) for Best Foreign Language Film. This award was followed when Gabriele Salvatores's "Mediterraneo" won the same prize for 1991. "" (1994), directed by and starring Massimo Troisi, received five nominations at the Academy Awards, and won for Best Original Score. Another exploit was in 1998 when Roberto Benigni won three oscars for his movie "Life Is Beautiful" ("La vita è bella)" (Best Actor, Best Foreign Film, Best Music). In 2001 Nanni Moretti's film "The Son's Room" ("La stanza del figlio") received the Palme d'Or at the Cannes Film Festival.

Other noteworthy recent Italian films include: "Jona che visse nella balena" directed by Roberto Faenza, "Il grande cocomero" by Francesca Archibugi, "The Profession of Arms" ("Il mestiere delle armi") by Olmi, "L'ora di religione" by Marco Bellocchio, "Il ladro di bambini", "Lamerica", "The Keys to the House" ("Le chiavi di casa") by Gianni Amelio, "I'm Not Scared" ("Io non ho paura") by Gabriele Salvatores, "Le fate ignoranti", "Facing Windows" ("La finestra di fronte") by Ferzan Özpetek, "Good Morning, Night" ("Buongiorno, notte") by Marco Bellocchio, "The Best of Youth" ("La meglio gioventù") by Marco Tullio Giordana, "The Beast in the Heart" ("La bestia nel cuore") by Cristina Comencini.

In 2008 Paolo Sorrentino's "Il Divo", a biographical film based on the life of Giulio Andreotti, won the Jury prize and "Gomorra", a crime drama film, directed by Matteo Garrone won the Gran Prix at the Cannes Film Festival.

Paolo Sorrentino's "The Great Beauty" ("La Grande Bellezza") won the 2014 Academy Award for Best Foreign Language Film.

The two highest-grossing Italian films in Italy have both been directed by Gennaro Nunziante and starred Checco Zalone: "Sole a catinelle" (2013) with €51.8 million, and "Quo Vado?" (2016) with €65.3 million.

"They Call Me Jeeg", a 2016 critically acclaimed superhero film directed by Gabriele Mainetti and starring Claudio Santamaria, won many awards, such as eight David di Donatello, two Nastro d'Argento, and a Globo d'oro.

Gianfranco Rosi's documentary film "Fire at Sea" (2016) won the Golden Bear at the 66th Berlin International Film Festival. "They Call Me Jeeg" and "Fire at Sea" were also selected as the Italian entry for the Best Foreign Language Film at the 89th Academy Awards, but they were not nominated.

Other successful 2010s Italian films include: "Vincere" by Marco Bellocchio, "The First Beautiful Thing" ("La prima cosa bella"), "Human Capital" ("Il capitale umano") and "Like Crazy" ("La pazza gioia") by Paolo Virzì, "We Have a Pope" ("Habemus Papam") and "Mia Madre" by Nanni Moretti, "Caesar Must Die" ("Cesare deve morire") by Paolo and Vittorio Taviani, "Don't Be Bad" ("Non essere cattivo") by Claudio Caligari, "Romanzo Criminale" by Michele Placido (that spawned a TV series, "Romanzo criminale - La serie"), "Youth" ("La giovinezza") by Paolo Sorrentino, "Suburra" by Stefano Sollima, "Perfect Strangers" ("Perfetti sconosciuti") by Paolo Genovese, "Italian Race" ("Veloce come il vento") by Matteo Rovere, "Mediterranea" and "A Ciambra" by Jonas Carpignano, and "Tale of Tales" ("Il racconto dei racconti") and "Dogman" by Matteo Garrone.

"Call Me by Your Name" (2017), the final installment in Luca Guadagnino's thematic "Desire" trilogy, following "I Am Love" (2009) and "A Bigger Splash" (2015), received widespread acclaim and numerous accolades, including the Academy Award for Best Adapted Screenplay in 2018.

After the United States and the United Kingdom, Italy has the most Academy Awards wins.

Italy is the most awarded country at the Academy Award for Best Foreign Language Film, with 14 awards won, 3 Special Awards and 31 nominations. Winners with the year of the ceremony:


In 1961, Sophia Loren won the Academy Award for Best Actress for her role as a woman who is raped in World War II, along with her adolescent daughter, in Vittorio De Sica's "Two Women". She was the first actress to win an Academy Award for a performance in any foreign language, and the second Italian leading lady Oscar-winner, after Anna Magnani for "The Rose Tattoo". In 1998, Roberto Benigni was the first Italian actor to win for the Best Actor for "Life Is Beautiful".

Italian-born filmmaker Frank Capra won three times at the Academy Award for Best Director, for "It Happened One Night", "Mr. Deeds Goes to Town" and "You Can't Take It with You". Bernardo Bertolucci won the award for "The Last Emperor", and also Best Adapted Screenplay for the same movie.

Ennio De Concini, Alfredo Giannetti and Pietro Germi won the award for Best Original Screenplay for "Divorce Italian Style". The Academy Award for Best Film Editing was won by Gabriella Cristiani for "The Last Emperor" and by Pietro Scalia for "JFK" and "Black Hawk Down". 
The award for Best Original Score was won by Nino Rota for "The Godfather Part II"; Giorgio Moroder for "Midnight Express"; Nicola Piovani for "Life is Beautifull"; Dario Marianelli for "Atonement"; and Ennio Morricone for "The Hateful Eight". Giorgio Moroder also won the award for Best Original Song for "Flashdance" and "Top Gun".

The Italian winners at the Academy Award for Best Production Design are Dario Simoni for "Lawrence of Arabia" and "Doctor Zhivago"; Elio Altramura and Gianni Quaranta for "A Room with a View"; Bruno Cesari, Osvaldo Desideri and Ferdinando Scarfiotti for "The Last Emperor"; Luciana Arrighi for "Howards End"; and Dante Ferretti and Francesca Lo Schiavo for "The Aviator", "" and "Hugo".

The winners at the Academy Award for Best Cinematography are: Tony Gaudio for "Anthony Adverse"; Pasqualino De Santis for "Romeo and Juliet"; Vittorio Storaro for "Apocalypse Now", "Reds" and "The Last Emperor"; and Mauro Fiore for "Avatar".

The winners at the Academy Award for Best Costume Design are Piero Gherardi for "La dolce vita" and "8½"; Vittorio Nino Novarese for "Cleopatra" and "Cromwell"; Danilo Donati for "The Taming of the Shrew", "Romeo and Juliet", and "Fellini's Casanova"; Franca Squarciapino for "Cyrano de Bergerac"; Gabriella Pescucci for "The Age of Innocence"; and Milena Canonero for "Barry Lyndon", "Chariots of Fire", "Marie Antoinette" and "The Grand Budapest Hotel". 

Special effects artist Carlo Rambaldi won three Oscars: one Special Achievement Academy Award for Best Visual Effects for "King Kong" and two Academy Awards for Best Visual Effects for "Alien" (1979) and "E.T. the Extra-Terrestrial". The Academy Award for Best Makeup and Hairstyling was won by Manlio Rocchetti for "Driving Miss Daisy", and Alessandro Bertolazzi and Giorgio Gregorini for "Suicide Squad".

Sophia Loren, Federico Fellini, Michelangelo Antonioni, Dino De Laurentiis, Ennio Morricone, and Piero Tosi also received the Academy Honorary Award.



Italy has produced many important cinematography "auteurs", including Federico Fellini, Michelangelo Antonioni, Roberto Rossellini, Vittorio De Sica, Luchino Visconti, Ettore Scola, Sergio Leone, Luigi Comencini, Pier Paolo Pasolini, Bernardo Bertolucci, Franco Zeffirelli, Ermanno Olmi, Valerio Zurlini, Florestano Vancini, Mario Monicelli, Marco Ferreri, Elio Petri, Dino Risi and Mauro Bolognini. These directors' works often span many decades and genres. Present "auteurs" include Giuseppe Tornatore, Marco Bellocchio, Nanni Moretti, Gabriele Salvatores, Gianni Amelio, Dario Argento and Paolo Sorrentino.





</doc>
<doc id="10789" url="https://en.wikipedia.org/wiki?curid=10789" title="Cinema of Poland">
Cinema of Poland

The history of cinema in Poland is almost as long as history of cinematography, and it has universal achievements, even though Polish films tend to be less commercially available than films from several other European nations.

After World War II, the communist government built an auteur-based national cinema, trained hundreds of new directors and empowered them to make films. Filmmakers like Roman Polanski, Krzysztof Kieślowski, Agnieszka Holland, Andrzej Wajda, Andrzej Żuławski, Andrzej Munk, and Jerzy Skolimowski impacted the development of Polish filmmaking. In more recent years, the industry has been producer-led with finance being the key to a film being made, and with a large number of independent filmmakers of all genres, Polish productions tend to be more inspired by American film.

The first cinema in Poland (then occupied by the Russian Empire) was founded in Łódź in 1899, several years after the invention of the Cinematograph. Initially dubbed "Living Pictures Theatre", it gained much popularity and by the end of the next decade there were cinemas in almost every major town of Poland. Arguably the first Polish filmmaker was Kazimierz Prószyński, who filmed various short documentaries in Warsaw. His pleograph film camera had been patented before the Lumière brothers' invention and he is credited as the author of the earliest surviving Polish documentary titled "Ślizgawka w Łazienkach" ("Skating-rink in the Royal Baths"), as well as the first short narrative films "Powrót birbanta" ("Rake's return home") and "Przygoda dorożkarza" ("Cabman's Adventure"), both created in 1902. Another pioneer of cinema was Bolesław Matuszewski, who became one of the first filmmakers working for the Lumière company - and the official "cinematographer" of the Russian tsars in 1897.

The earliest surviving feature film, the "Antoś pierwszy raz w Warszawie" ("Antoś for the First Time in Warsaw") was made in 1908 by Antoni Fertner. The date of its première, October 22, 1908, is considered the founding date of Polish film industry. Soon Polish artists started experimenting with other genres of cinema: in 1910 Władysław Starewicz made one of the first animated cartoons in the world - and the first to use the stop motion technique, the "Piękna Lukanida" ("Beautiful Lukanida"). By the start of World War I the cinema in Poland was already in full swing, with numerous adaptations of major works of Polish literature screened (notably the "Dzieje grzechu", "Meir Ezofowicz" and "Nad Niemnem".

During the World War I the Polish cinema crossed borders. Films made in Warsaw or Vilna were often rebranded with German language intertitles and shown in Berlin. That was how a young actress Pola Negri (born Barbara Apolonia Chałupiec) gained fame in Germany and eventually became one of the European super-stars of silent film.

During the World War II Polish filmmakers in Great Britain created anti-nazi color film "Calling Mr. Smith" (1943) about current nazi crimes in occupied Europe and about lies of nazi propaganda. It was one of the first anti-nazi films in history being both avant-garde and documentary film.

In November 1945 the communist government founded the film production and distribution organization Film Polski, and put the well-known Polish People's Army filmmaker Aleksander Ford in charge. Starting with a few railway carriages full of film equipment taken from the Germans they proceed to train and build a Polish film industry. The FP output was limited; only thirteen features were released between 1947 and its dissolution in 1952, concentrating on Polish suffering at the hands of the Nazis. In 1947 Ford moved to help establish the new National Film School in Łódź, where he taught for 20 years.

The industry used imported cameras and film stocks. At first ORWO black and white film stock from East Germany and then Eastman colour negative stock and ORWO print stocks for rushes and release prints. Poland made its own lighting equipment. Because of the high costs of film stock Polish films were shot with very low shooting ratios, the amount of film stock used in shooting the film to length of the finished film. The equipment and film stock were not the best and budgets were modest but the film makers received probably the best training in the world from the Polish Film School. Another advantage was that as Film Polski was a state organisation film makers had access to all Polish institutions and their cooperation in making their films. Film cameras were able to enter almost every aspect of Polish life.

The first film produced in Poland following the World War II was "Zakazane piosenki" (1946), directed by Leonard Buczkowski, which was seen by 10.8 million people (out of 23,8 total population) in its initial theatrical run. Buczkowski continued to make films regularly until his death in 1967. Other important films of early post-World War II period were "The Last Stage" (1948), directed by Wanda Jakubowska, who continued to make films until the transition from communism to capitalism in 1989, and "Border Street" (1949), directed by Aleksander Ford.

By the mid 1950s, following the end of Stalinism in Poland, Film production was organised into film groups. A film group was a collection of film makers, led by an experienced film director and consisting of writers, film directors and production managers. They would write scripts, create budgets, apply for funding off the Ministry of Culture and produce the picture. They would hire actors and crew, and use studios and laboratories controlled by Film Polski.

The change in political climate gave rise to the Polish Film School movement, a training ground for some of the icons of the world cinematography, e.g., Roman Polanski ("Knife in the Water", "Rosemary's Baby", "Frantic", "The Pianist") and Krzysztof Zanussi (a leading director of the so-called "cinema of moral anxiety" of the 1970s). Andrzej Wajda's films offer insightful analyses of the universal element of the Polish experience - the struggle to maintain dignity under the most trying circumstances. His films defined several Polish generations. In 2000, Wajda was awarded an honorary Oscar for his overall contribution to cinema. Four of his films were nominated for Best Foreign Language Film award at Academy Awards with five other Polish directors receiving one nomination each: Roman Polański, Jerzy Kawalerowicz, Jerzy Hoffman, Jerzy Antczak and Agnieszka Holland. In 2015, Polish filmmaker Paweł Pawlikowski received this award for his film "Ida". In 2019, he was also nominated to the award for his next film "Cold War" in two categories - Best Foreign Language Film and Best Director.

It is also important to note that during the 80's, the People's Republic of Poland instituted the martial law to vanquish and censor all forms of opposition against the communist rule of the nation, including outlets such as cinema and radio. A notable film to have emerged during this period was Ryszard Bugajski's 1982 film "Interrogation" ("Przesluchanie"), which depicts the story of an unfortunate woman (played by Krystyna Janda) who is arrested and tortured by the secret police into confessing a crime she knows nothing about. The anti-communist nature of the film brought about the film's over seven-year ban. In 1989, the ban was repealed after the overthrow of the Communist government in Poland, and the film was shown in theaters for the first time later that year. The film is still lauded today for its audacity in depicting the cruelty of the Stalinist regime, as many artists feared persecution during that time.
In the 1990s, Krzysztof Kieślowski won a universal acclaim with productions such as "Dekalog" (made for television), "The Double Life of Véronique" and the "Three Colors" trilogy. Another of the most famous movies in Poland is Krzysztof Krauze’s The Debt, which became a blockbuster. It showed the brutal reality of Polish capitalism and the growth of poverty. A considerable number of Polish film directors (e.g., Agnieszka Holland and Janusz Kamiński) have worked in American studios. Polish animated films - like those by Jan Lenica and Zbigniew Rybczyński (Oscar, 1983) - drew on a long tradition and continued to derive their inspiration from Poland's graphic arts. Other notable Polish film directors include: Tomasz Bagiński, Małgorzata Szumowska, Jan Jakub Kolski, Jerzy Kawalerowicz, Stanisław Bareja and Janusz Zaorski.

Among prominent annual film festivals taking place in Poland are: Warsaw International Film Festival, Camerimage, International Festival of Independent Cinema Off Camera, New Horizons Film Festival as well as Gdynia Film Festival and Polish Film Awards.

The Communist government invested resources into building a sophisticated cinema audience. All the cinema were state owned and consisted of first run premiere cinema, local cinema and art house cinemas. Tickets were cheap and students and old people received discounts. In the city of Lodz there were 36 cinemas in the 1970s showing films from all over the world. There were the Italian films of Fellini, French comedies, American crime movies such as Don Siegel's "Charley Varrick" . Films were shown in their original versions with Polish subtitles. Anti communist and cold war films were not shown, but a bigger restriction was the cost of some films. There were popular film magazines like "Film" and "Screen", critical magazines such as "Kino". This all helped to build a well informed film audience.





</doc>
<doc id="10790" url="https://en.wikipedia.org/wiki?curid=10790" title="Cinema of Japan">
Cinema of Japan

The has a history that spans more than 100 years. Japan has one of the oldest and largest film industries in the world; as of 2010, it was the fourth largest by number of feature films produced. In 2011 Japan produced 411 feature films that earned 54.9% of a box office total of US$2.338 billion. Films have been produced in Japan since 1897, when the first foreign cameramen arrived.

In a "Sight & Sound" list of the best films produced in Asia, Japanese works made up eight of the top 12, with "Tokyo Story" (1953) ranked number one. "Tokyo Story" also topped the 2012 "Sight & Sound" directors' poll of The Top 50 Greatest Films of All Time, dethroning "Citizen Kane", while Akira Kurosawa's "Seven Samurai" (1954) was voted the greatest foreign-language film of all time in BBC's 2018 poll of 209 critics in 43 countries. Japan has won the Academy Award for the Best Foreign Language Film four times, more than any other Asian country.

The kinetoscope, first shown commercially by Thomas Edison in the United States in 1894, was first shown in Japan in November 1896. The Vitascope and the Lumière Brothers' Cinematograph were first presented in Japan in early 1897, by businessmen such as Inabata Katsutaro. Lumière cameramen were the first to shoot films in Japan. Moving pictures, however, were not an entirely new experience for the Japanese because of their rich tradition of pre-cinematic devices such as "gentō" ("utsushi-e") or the magic lantern. The first successful Japanese film in late 1897 showed sights in Tokyo.

In 1898 some ghost films were made, the Shirō Asano shorts "Bake Jizo" (Jizo the Spook / 化け地蔵) and "Shinin no sosei" (Resurrection of a Corpse). The first documentary, the short "Geisha no teodori" (芸者の手踊り), was made in June 1899. Tsunekichi Shibata made a number of early films, including "Momijigari", an 1899 record of two famous actors performing a scene from a well-known kabuki play. Early films were influenced by traditional theater – for example, kabuki and bunraku.

At the dawn of the twentieth century theaters in Japan hired benshi, storytellers who sat next to the screen and narrated silent movies. They were descendants of kabuki jōruri, kōdan storytellers, theater barkers and other forms of oral storytelling. Benshi could be accompanied by music like silent films from cinema of the West. With the advent of sound in the early 1930s, the benshi gradually declined.

In 1908, Shōzō Makino, considered the pioneering director of Japanese film, began his influential career with "Honnōji gassen" (本能寺合戦), produced for Yokota Shōkai. Shōzō recruited Matsunosuke Onoe, a former kabuki actor, to star in his productions. Onoe became Japan's first film star, appearing in over 1,000 films, mostly shorts, between 1909 and 1926. The pair pioneered the "jidaigeki" genre. Tokihiko Okada was a popular romantic lead of the same era.

The first Japanese film production studio was built in 1909 by the Yoshizawa Shōten company in Tokyo.

The first female Japanese performer to appear in a film professionally was the dancer/actress Tokuko Nagai Takagi, who appeared in four shorts for the American-based Thanhouser Company between 1911 and 1914.

Among intellectuals, critiques of Japanese cinema grew in the 1910s and eventually developed into a movement that transformed Japanese film. Film criticism began with early film magazines such as "Katsudō shashinkai" (begun in 1909) and a full-length book written by Yasunosuke Gonda in 1914, but many early film critics often focused on chastising the work of studios like Nikkatsu and Tenkatsu for being too theatrical (using, for instance, elements from kabuki and shinpa such as onnagata) and for not utilizing what were considered more cinematic techniques to tell stories, instead relying on benshi. In what was later named the Pure Film Movement, writers in magazines such as "Kinema Record" called for a broader use of such cinematic techniques. Some of these critics, such as Norimasa Kaeriyama, went on to put their ideas into practice by directing such films as "The Glow of Life" (1918), which was one of the first films to use actresses (in this case, Harumi Hanayagi). There were parallel efforts elsewhere in the film industry. In his 1917 film "The Captain's Daughter", Masao Inoue started using techniques new to the silent film era, such as the close-up and cut back. The Pure Film Movement was central in the development of the gendaigeki and scriptwriting.

New studios established around 1920, such as Shochiku and Taikatsu, aided the cause for reform. At Taikatsu, Thomas Kurihara directed films scripted by the novelist Junichiro Tanizaki, who was a strong advocate of film reform. Even Nikkatsu produced reformist films under the direction of Eizō Tanaka. By the mid-1920s, actresses had replaced onnagata and films used more of the devices pioneered by Inoue. Some of the most discussed silent films from Japan are those of Kenji Mizoguchi, whose later works (e.g., "The Life of Oharu") are still highly regarded.

Japanese films gained popularity in the mid-1920s against foreign films, in part fueled by the popularity of movie stars and a new style of jidaigeki. Directors such as Daisuke Itō and Masahiro Makino made samurai films like "A Diary of Chuji's Travels" and "Roningai" featuring rebellious antiheroes in fast-cut fight scenes that were both critically acclaimed and commercial successes. Some stars, such as Tsumasaburo Bando, Kanjūrō Arashi, Chiezō Kataoka, Takako Irie and Utaemon Ichikawa, were inspired by Makino Film Productions and formed their own independent production companies where directors such as Hiroshi Inagaki, Mansaku Itami and Sadao Yamanaka honed their skills. Director Teinosuke Kinugasa created a production company to produce the experimental masterpiece "A Page of Madness", starring Masao Inoue, in 1926. Many of these companies, while surviving during the silent era against major studios like Nikkatsu, Shochiku, Teikine, and Toa Studios, could not survive the cost involved in converting to sound.

With the rise of left-wing political movements and labor unions at the end of the 1920s arose so-called tendency films with left-wing "tendencies", with prominent examples being directed by Kenji Mizoguchi, Daisuke Itō, Shigeyoshi Suzuki, and Tomu Uchida. In contrast with these commercially produced 35 mm films, the Marxist Proletarian Film League of Japan (Prokino) made works independently in smaller gauges (such as 9.5mm and 16mm), with more radical intentions. Tendency films suffered from severe censorship heading into the 1930s, and Prokino members were arrested and the movement effectively crushed. Such moves by the government had profound effects on the expression of political dissent in 1930s cinema. Films from this period include: "Sakanaya Honda, Jitsuroku Chushingura, Horaijima, Orochi, Maboroshi, Kurutta Ippeji, Jujiro, ", and "Kurama Tengu".

A later version of "The Captain's Daughter" was one of the first talkie films. It used the Mina Talkie System. The Japanese film industry later split into two groups; one retained the Mina Talkie System, while the other used the Iisutofyon Talkie System used to make Tojo Masaki's films.

The 1923 earthquake, the bombing of Tokyo during World War II, and the natural effects of time and Japan's humidity on inflammable and unstable nitrate film have resulted in a great dearth of surviving films from this period.

Unlike in the West, silent films were still being produced in Japan well into the 1930s; as late as 1938, a third of Japanese films were silent. For instance, Yasujirō Ozu's "An Inn in Tokyo" (1935), considered a precursor to the neorealism genre, was a silent film. A few Japanese sound shorts were made in the 1920s and 1930s, but Japan's first feature-length talkie was "Fujiwara Yoshie no furusato" (1930), which used the "Mina Talkie System". Notable talkies of this period include Mikio Naruse's "Wife, Be Like A Rose!" ("Tsuma Yo Bara No Yoni", 1935), which was one of the first Japanese films to gain a theatrical release in the U.S.; Kenji Mizoguchi's "Sisters of the Gion" ("Gion no shimai", 1936); "Osaka Elegy" (1936); and "The Story of the Last Chrysanthemums" (1939); and Sadao Yamanaka's "Humanity and Paper Balloons" (1937).

Film criticism shared this vitality, with many film journals such as "Kinema Junpo" and newspapers printing detailed discussions of the cinema of the day, both at home and abroad. A cultured "impressionist" criticism pursued by critics such as Tadashi Iijima, Fuyuhiko Kitagawa, and Matsuo Kishi was dominant, but opposed by leftist critics such as Akira Iwasaki and Genjū Sasa who sought an ideological critique of films.

The 1930s also saw increased government involvement in cinema, which was symbolized by the passing of the Film Law, which gave the state more authority over the film industry, in 1939. The government encouraged some forms of cinema, producing propaganda films and promoting documentary films (also called "bunka eiga" or "culture films"), with important documentaries being made by directors such as Fumio Kamei. Realism was in favor; film theorists such as Taihei Imamura and Heiichi Sugiyama advocated for documentary or realist drama, while directors such as Hiroshi Shimizu and Tomotaka Tasaka produced fiction films that were strongly realistic in style.

Because of World War II and the weak economy, unemployment became widespread in Japan, and the cinema industry suffered.

During this period, when Japan was expanding its Empire, the Japanese government saw cinema as a propaganda tool to show the glory and invincibility of the Empire of Japan. Thus, many films from this period depict patriotic and militaristic themes. In 1942 Kajiro Yamamoto's film "Hawai Mare oki kaisen" or "The War at Sea from Hawaii to Malaya" portrayed the attack on Pearl Harbor; the film made use of special effects directed by Eiji Tsuburaya, including a miniature scale model of Pearl Harbor itself.

Akira Kurosawa made his feature film debut with "Sugata Sanshiro" in 1943. The first collaborations between Kurosawa and actor Toshiro Mifune were "Drunken Angel" in 1948 and "Stray Dog" in 1949. Yasujirō Ozu directed the critically and commercially successful "Late Spring" in 1949.

The Mainichi Film Award was created in 1946.

With the SCAP occupation following the end of World War II, Japan was exposed to over a decade's worth of American animation that had been banned under the war-time government.

The 1950s are widely considered the Golden Age of Japanese cinema. Three Japanese films from this decade ("Rashomon", "Seven Samurai" and "Tokyo Story") appeared in the top ten of "Sight & Sound"s critics' and directors' polls for the best films of all time in 2002. They also appeared in the 2012 polls, with "Tokyo Story" (1953) dethroning "Citizen Kane" at the top of the 2012 directors' poll.

The period after the American Occupation led to a rise in diversity in movie distribution thanks to the increased output and popularity of the film studios of Toho, Daiei, Shochiku, Nikkatsu, and Toei. This period gave rise to the four great artists of Japanese cinema: Masaki Kobayashi, Akira Kurosawa, Kenji Mizoguchi, and Yasujirō Ozu. Each director dealt with the effects the war and subsequent occupation by America in unique and innovative ways.

The decade started with Akira Kurosawa's "Rashomon" (1950), which won the Golden Lion at the Venice Film Festival in 1951 and the Academy Honorary Award for Best Foreign Language Film in 1952, and marked the entrance of Japanese cinema onto the world stage. It was also the breakout role for legendary star Toshiro Mifune. In 1953 "Entotsu no mieru basho" by Heinosuke Gosho was in competition at the 3rd Berlin International Film Festival.

The first Japanese film in color was "Carmen Comes Home" directed by Keisuke Kinoshita and released in 1951. There was also a black-and-white version of this film available. "Tokyo File 212" (1951) was the first American feature film to be shot entirely in Japan. The lead roles were played by Florence Marly and Robert Peyton. It featured the geisha Ichimaru in a short cameo. Suzuki Ikuzo's Tonichi Enterprises Company co-produced the film. "Gate of Hell", a 1953 film by Teinosuke Kinugasa, was the first movie that filmed using Eastmancolor film, "Gate of Hell" was both Daiei's first color film and the first Japanese color movie to be released outside Japan, receiving an Academy Honorary Award in 1954 for Best Costume Design by Sanzo Wada and an Honorary Award for Best Foreign Language Film. It also won the Palme d'Or at the Cannes Film Festival, the first Japanese film to achieve that honour.

The year 1954 saw two of Japan's most influential films released. The first was the Kurosawa epic "Seven Samurai", about a band of hired samurai who protect a helpless village from a rapacious gang of thieves. The same year, Ishirō Honda directed the anti-nuclear monster-drama "Godzilla", which was released in America as "Godzilla, King of the Monsters". Though edited for its Western release, Godzilla became an international icon of Japan and spawned an entire subgenre of "kaiju" films, as well as the longest-running film franchise in history. Also in 1954, another Kurosawa film, "Ikiru" was in competition at the 4th Berlin International Film Festival.

In 1955, Hiroshi Inagaki won an Academy Honorary Award for Best Foreign Language Film for of his "Samurai" trilogy and in 1958 won the Golden Lion at the Venice Film Festival for "Rickshaw Man". Kon Ichikawa directed two anti-war dramas: "The Burmese Harp" (1956), which was nominated for Best Foreign Language Film at the Academy Awards, and "Fires On The Plain" (1959), along with "Enjo" (1958), which was adapted from Yukio Mishima's novel "Temple Of The Golden Pavilion". Masaki Kobayashi made three films which would collectively become known as "The Human Condition Trilogy": "No Greater Love" (1959), and "The Road To Eternity" (1959). The trilogy was completed in 1961, with "A Soldier's Prayer".

Kenji Mizoguchi, who died in 1956, ended his career with a series of masterpieces including "The Life of Oharu" (1952), "Ugetsu" (1953) and "Sansho the Bailiff" (1954). He won the Silver Bear at the Venice Film Festival for "Ugetsu". Mizoguchi's films often deal with the tragedies inflicted on women by Japanese society. Mikio Naruse made "Repast" (1950), "Late Chrysanthemums" (1954), "The Sound of the Mountain" (1954) and "Floating Clouds" (1955). Yasujirō Ozu began directing color films beginning with "Equinox Flower" (1958), and later "Good Morning" (1959) and "Floating Weeds" (1958), which was adapted from his earlier silent "A Story of Floating Weeds" (1934), and was shot by "Rashomon" and "Sansho the Bailiff" cinematographer Kazuo Miyagawa.

The Blue Ribbon Awards were established in 1950. The first winner for Best Film was "Until We Meet Again" by Tadashi Imai.

The number of films produced, and the cinema audience reached a peak in the 1960s. Most films were shown in double bills, with one half of the bill being a "program picture" or B-movie. A typical program picture was shot in four weeks. The demand for these program pictures in quantity meant the growth of film series such as "The Hoodlum Soldier" or "Akumyo".

The huge level of activity of 1960s Japanese cinema also resulted in many classics. Akira Kurosawa directed the 1961 classic "Yojimbo". Yasujirō Ozu made his final film, "An Autumn Afternoon", in 1962. Mikio Naruse directed the wide screen melodrama "When a Woman Ascends the Stairs" in 1960; his final film was 1967's "Scattered Clouds".

Kon Ichikawa captured the watershed 1964 Olympics in his three-hour documentary "Tokyo Olympiad" (1965). Seijun Suzuki was fired by Nikkatsu for "making films that don't make any sense and don't make any money" after his surrealist yakuza flick "Branded to Kill" (1967).

The 1960s were the peak years of the "Japanese New Wave" movement, which began in the 1950s and continued through the early 1970s. Nagisa Oshima, Kaneto Shindo, Masahiro Shinoda, Susumu Hani and Shohei Imamura emerged as major filmmakers during the decade. Oshima's "Cruel Story of Youth", "Night and Fog in Japan" and "Death By Hanging", along with Shindo's "Onibaba", Hani's "Kanojo to kare" and Imamura's "The Insect Woman", became some of the better-known examples of Japanese New Wave filmmaking. Documentary played a crucial role in the New Wave, as directors such as Hani, Kazuo Kuroki, Toshio Matsumoto, and Hiroshi Teshigahara moved from documentary into fiction film, while feature filmmakers like Oshima and Imamura also made documentaries. Shinsuke Ogawa and Noriaki Tsuchimoto became the most important documentarists: "two figures [that] tower over the landscape of Japanese documentary."

Teshigahara's "Woman in the Dunes" (1964) won the Special Jury Prize at the Cannes Film Festival, and was nominated for Best Director and Best Foreign Language Film Oscars. Masaki Kobayashi's "Kwaidan" (1965) also picked up the Special Jury Prize at Cannes and received a nomination for Best Foreign Language Film at the Academy Awards. "Bushido, Samurai Saga" by Tadashi Imai won the Golden Bear at the 13th Berlin International Film Festival. "Immortal Love" by Keisuke Kinoshita and "Twin Sisters of Kyoto" and "Portrait of Chieko", both by Noboru Nakamura, also received nominations for Best Foreign Language Film at the Academy Awards. "Lost Spring", also by Nakamura, was in competition for the Golden Bear at the 17th Berlin International Film Festival.

The 1970s saw the cinema audience drop due to the spread of television. Total audience declined from 1.2 billion in 1960 to 0.2 billion in 1980.
Film companies fought back in various ways, such as the bigger budget films of Kadokawa Pictures, or including increasingly sexual or violent content and language which could not be shown on television. The resulting pink film industry became the stepping stone for many young independent filmmakers. The seventies also saw the start of the "idol eiga", films starring young "idols", who would bring in audiences due to their fame and popularity.

Toshiya Fujita made the revenge film "Lady Snowblood" in 1973. In the same year, Yoshishige Yoshida made the film "Coup d'État", a portrait of Ikki Kita, the leader of the Japanese coup of February 1936. Its experimental cinematography and mise-en-scène, as well as its avant-garde score by Ichiyanagi Sei, garnered it wide critical acclaim within Japan.

In 1976 the Hochi Film Award was created. The first winner for Best Film was "The Inugamis" by Kon Ichikawa. Nagisa Oshima directed "In the Realm of the Senses" (1976), a film detailing a crime of passion involving Sada Abe set in the 1930s. Controversial for its explicit sexual content, it has never been seen uncensored in Japan.

Kinji Fukasaku completed the epic "Battles Without Honor and Humanity" series of yakuza films. Yoji Yamada introduced the commercially successful "Tora-San" series, while also directing other films, notably the popular "The Yellow Handkerchief", which won the first Japan Academy Prize for Best Film in 1978. New wave filmmakers Susumu Hani and Shōhei Imamura retreated to documentary work, though Imamura made a dramatic return to feature filmmaking with "Vengeance Is Mine" (1979).

"Dodes'ka-den" by Akira Kurosawa and "Sandakan No. 8" by Kei Kumai were nominated to the Academy Award for Best Foreign Language Film.

The 1980s saw the decline of the major Japanese film studios and their associated chains of cinemas, with major studios Toho and Toei barely staying in business, Shochiku supported almost solely by the "Otoko wa tsurai" films, and Nikkatsu declining even further.

Of the older generation of directors, Akira Kurosawa directed "Kagemusha" (1980), which won the Palme d'Or at the 1980 Cannes Film Festival, and "Ran" (1985). Seijun Suzuki made a comeback beginning with "Zigeunerweisen" in 1980. Shohei Imamura won the Palme d'Or at the Cannes Film Festival for "The Ballad of Narayama" (1983). Yoshishige Yoshida made "A Promise" (1986), his first film since 1973's "Coup d'État".

New directors who appeared in the 80s include actor Juzo Itami, who directed his first film, "The Funeral", in 1984, and achieved critical and box office success with "Tampopo" in 1985. Kiyoshi Kurosawa, who would generate international attention beginning in the mid-1990s, made his initial debut with pink films and genre horror.

During the 1980s, anime gained in popularity, with new animated movies released every summer and winter, often based upon popular anime television series. Mamoru Oshii released his landmark "Angel's Egg" in 1985. Hayao Miyazaki adapted his manga series "Nausicaä of the Valley of Wind" into a feature film of the same name in 1984. Katsuhiro Otomo followed suit by adapting his own manga "Akira" into a feature film of the same name in 1988.

Home video made possible the creation of a direct-to-video film industry.

Because of economic recessions, the number of movie theaters in Japan had been steadily decreasing since the 1960s. The 1990s saw the reversal of this trend and the introduction of the Multiplex in Japan.

Takeshi Kitano emerged as a significant filmmaker with works such as "Sonatine" (1993), "Kids Return" (1996) and "Hana-bi" (1997), which was given the Golden Lion at the Venice Film Festival. Shōhei Imamura again won the Golden Palm (shared with Iranian director Abbas Kiarostami), this time for "The Eel" (1997). He became the fifth two-time recipient, joining Alf Sjöberg, Francis Ford Coppola, Emir Kusturica and Bille August.

Kiyoshi Kurosawa gained international recognition following the release of "Cure" (1997). Takashi Miike launched a prolific career with titles such as "Audition" (1999), "Dead or Alive" (1999) and "The Bird People in China" (1998). Former documentary filmmaker Hirokazu Koreeda launched an acclaimed feature career with "Maborosi" (1996) and "After Life" (1999).

Hayao Miyazaki directed two mammoth box office and critical successes, "Porco Rosso" (1992) – which beat "E.T. the Extra-Terrestrial" (1982) as the highest-grossing film in Japan – and "Princess Mononoke" (1997), which also claimed the top box office spot until "Titanic" (1997).

Several new anime directors rose to widespread recognition, bringing with them notions of anime as not only entertainment, but modern art. Mamoru Oshii released the internationally acclaimed philosophical science fiction action film "Ghost in the Shell" in 1996. Satoshi Kon directed the award-winning psychological thriller "Perfect Blue". Hideaki Anno also gained considerable recognition with "The End of Evangelion" in 1997.

The number of movies being shown in Japan steadily increased, with about 821 films released in 2006. Movies based on Japanese television series were especially popular during this period. Anime films now accounted for 60 percent of Japanese film production. The 1990s and 2000s are considered to be "Japanese Cinema's Second Golden Age", due to the immense popularity of anime, both within Japan and overseas.

Although not a commercial success, "All About Lily Chou-Chou" directed by Shunji Iwai was honored at the Berlin, the Yokohama and the Shanghai Film Festivals in 2001. Takeshi Kitano appeared in "Battle Royale" and directed and starred in "Dolls" and "Zatoichi". Several horror films, "Kairo", "Dark Water", "Yogen", the "Grudge" series and "One Missed Call" met with commercial success. In 2004, "", directed by Ryuhei Kitamura, was released to celebrate the 50th anniversary of Godzilla. In 2005, director Seijun Suzuki made his 56th film, "Princess Raccoon". Hirokazu Koreeda claimed film festival awards around the world with two of his films "Distance" and "Nobody Knows". Female film director Naomi Kawase's film "The Mourning Forest" won the Grand Prix at the Cannes Film Festival in 2007. Yoji Yamada, director of the Otoko wa Tsurai yo series, made a trilogy of acclaimed revisionist samurai films, 2002's "Twilight Samurai", followed by "The Hidden Blade" in 2004 and "Love and Honor" in 2006.

In anime, Hayao Miyazaki directed "Spirited Away" in 2001, breaking Japanese box office records and winning several awards—including the Academy Award for Best Animated Feature in 2003—followed by "Howl's Moving Castle" and "Ponyo" in 2004 and 2008 respectively. In 2004, Mamoru Oshii released the anime movie "" which received critical praise around the world. His 2008 film "The Sky Crawlers" was met with similarly positive international reception. Satoshi Kon also released three quieter, but nonetheless highly successful films: "Millennium Actress", "Tokyo Godfathers", and "Paprika". Katsuhiro Otomo released "Steamboy", his first animated project since the 1995 short film compilation "Memories", in 2004. In collaboration with Studio 4C, American director Michael Arias released "Tekkon Kinkreet" in 2008, to international acclaim. After several years of directing primarily lower-key live-action films, Hideaki Anno formed his own production studio and revisited his still-popular "Evangelion" franchise with the "Rebuild of Evangelion" tetralogy, a new series of films providing an alternate retelling of the original story.

In February 2000, the Japan Film Commission Promotion Council was established. On November 16, 2001, the Japanese Foundation for the Promotion of the Arts laws were presented to the House of Representatives. These laws were intended to promote the production of media arts, including film scenery, and stipulate that the government – on both the national and local levels – must lend aid in order to preserve film media. The laws were passed on November 30 and came into effect on December 7. In 2003, at a gathering for the Agency of Cultural Affairs, twelve policies were proposed in a written report to allow public-made films to be promoted and shown at the Film Center of the National Museum of Modern Art.

Four films have so far received international recognition by being selected to compete in major film festivals: "Caterpillar" by Kōji Wakamatsu was in competition for the Golden Bear at the 60th Berlin International Film Festival and won the Silver Bear for Best Actress, "Outrage" by Takeshi Kitano was In Competition for the Palme d'Or at the 2010 Cannes Film Festival, "Himizu" by Sion Sono was in competition for the Golden Lion at the 68th Venice International Film Festival.

In 2011, Takashi Miike's "" was In Competition for the Palme d'Or at the 2012 Cannes Film Festival, the first 3D film ever to screen In Competition at Cannes. The film was co-produced by British independent producer Jeremy Thomas, who had successfully broken Japanese titles such as Nagisa Oshima's "Merry Christmas, Mr Lawrence" and " Taboo", Takeshi Kitano's "Brother", and Miike's "13 Assassins" onto the international stage as producer.

In 2018, Hirokazu Kore-Eda won the Palme d'Or for his movie "Shoplifters" at the 71st Cannes Film Festival, a festival that also featured Ryūsuke Hamaguchi's Asako I & II in competition.

Genres of Japanese film include:





</doc>
<doc id="10791" url="https://en.wikipedia.org/wiki?curid=10791" title="Cinema of China">
Cinema of China

The cinema of Mainland China is one of three distinct historical threads of Chinese-language cinema together with the cinema of Hong Kong and the cinema of Taiwan.

Cinema was introduced in China in 1896 and the first Chinese film, "Dingjun Mountain", was made in 1905, with the film industry being centered on Shanghai in the first decades. The first sound film, "Sing-Song Girl Red Peony", using the sound-on-disc technology, was made in 1931. The 1930s, considered the first "golden period" of Chinese cinema, saw the advent of the Leftist cinematic movement and the dispute between Nationalists and Communists was reflected in the films produced. After the Japanese invasion of China and the occupation of Shanghai, the industry in the city was severely curtailed, with filmmakers moving to Hong Kong, Chongqing and other places, starting a "Solitary Island" period in Shanghai, referring to the city's foreign concessions, with the remaining filmmakers working there. "Princess Iron Fan" (1941), the first Chinese animated feature film, was released at the end of this period. It influenced wartime Japanese animation and later Osamu Tezuka. After being completely engulfed by the occupation in 1941, and until the end of the war in 1945, the film industry in the city was under Japanese control.

After the end of the war, a second golden age took place, with production in Shanghai resuming, with films such as "Spring in a Small Town" (1948), named the best Chinese-language film at the 24th Hong Kong Film Awards. After the communist revolution in 1949, previous and some foreign films were banned in 1951, and movie attendance increased sharply. During the Cultural Revolution, the film industry was severely restricted, coming almost to a standstill from 1967 to 1972. The industry flourished following the end of the Cultural Revolution, including the "scar dramas" of the 1980s, such as "Evening Rain" (1980), "Legend of Tianyun Mountain" (1980) and "Hibiscus Town" (1986), depicting the emotional traumas left by the period. Starting in the mid to late 1980s, with films such as "One and Eight" (1983) and "Yellow Earth" (1984), the rise of the Fifth Generation brought increased popularity to Chinese cinema abroad, especially among Western arthouse audiences, with films like "Red Sorghum" (1987), "The Story of Qiu Ju" (1992) and "Farewell My Concubine" (1993) winning major international awards. The movement partially ended after the Tiananmen Square protests of 1989. The post-1990 period saw the rise of the Sixth Generation and post-Sixth Generation, both mostly making films outside the main Chinese film system and played mostly on the international film festival circuit.

Following the international commercial success of films such as "Crouching Tiger, Hidden Dragon" (2000) and "Hero" (2002), the number of co-productions in Chinese-language cinema has increased and there has been a movement of Chinese-language cinema into a domain of large scale international influence. After "The Dream Factory" (1997) demonstrated the viability of the commercial model, and with the growth of the Chinese box office in the new millennium, Chinese films have broken box office records and, as of January 2017, 5 of the top 10 highest-grossing films in China are domestic productions. "Lost in Thailand" (2012) was the first Chinese film to reach at the Chinese box office, "Monster Hunt" (2015) was the first to reach , "The Mermaid" (2016) was the first to and "Wolf Warrior 2" (2017) is currently the highest-grossing film in China.

China is the home of the largest movie & drama production complex and film studios in the world, the Oriental Movie Metropolis and Hengdian World Studios, and in 2010 it had the third largest film industry by number of feature films produced annually. In 2012 the country became the second-largest market in the world by box office receipts. In 2016, the gross box office in China was (). The country has the largest number of screens in the world since 2016, and is expected to become the largest theatrical market by 2019. China has also become a major hub of business for Hollywood studios.

In November 2016, China passed a film law banning content deemed harmful to the “dignity, honor and interests” of the People’s Republic and encouraging the promotion of “socialist core values", approved by the National People’s Congress Standing Committee. Due to industry regulations, films are typically allowed to stay in theaters for one month. However, studios may apply to regulators to have the limit extended.

Motion pictures were introduced to China in 1896. The first recorded screening of a motion picture in China occurred in Shanghai on August 11, 1896, as an "act" on a variety bill. The first Chinese film, a recording of the Peking opera, "Dingjun Mountain", was made in November 1905 in Beijing. For the next decade the production companies were mainly foreign-owned, and the domestic film industry was centered on Shanghai, a thriving entrepot and the largest city in the Far East. In 1913, the first independent Chinese screenplay, "The Difficult Couple", was filmed in Shanghai by Zheng Zhengqiu and Zhang Shichuan. Zhang Shichuan then set up the first Chinese-owned film production company in 1916. The first truly successful home-grown feature film was "Yan Ruisheng" () released in 1921. During the 1920s film technicians from the United States trained Chinese technicians in Shanghai, and American influence continued to be felt there for the next two decades.

It was during this period that some of the more important production companies first came into being, notably Mingxing ("Star") and the Shaw brothers' Tianyi ("Unique"). Mingxing, founded by Zheng Zhengqiu and Zhang Shichuan in 1922, initially focused on comic shorts, including the oldest surviving complete Chinese film, "Laborer's Love" (1922). This soon shifted, however, to feature-length films and family dramas including "Orphan Rescues Grandfather" (1923). Meanwhile, Tianyi shifted their model towards folklore dramas, and also pushed into foreign markets; their film "White Snake" (1926) proved a typical example of their success in the Chinese communities of Southeast Asia. In 1931, the first Chinese sound film "Sing-Song Girl Red Peony" was made, the product of a cooperation between the Mingxing Film Company's image production and Pathé Frères's sound technology. However, the sound was disc-recorded, and the first sound-on-film talkie made in China was either "Spring on Stage" (歌場春色) by Tianyi, or "Clear Sky After Storm" by Great China Studio and Jinan Studio.

However, the first truly important Chinese films were produced beginning in the 1930s, with the advent of the "progressive" or "left-wing" movement, like Cheng Bugao's "Spring Silkworms" (1933), Wu Yonggang's "The Goddess" (1934), and Sun Yu's "The Big Road" (1935). These films were noted for their emphasis on class struggle and external threats (i.e. Japanese aggression), as well as on their focus on common people, such as a family of silk farmers in "Spring Silkworms" and a prostitute in "The Goddess". In part due to the success of these kinds of films, this post-1930 era is now often referred to as the first "golden period" of Chinese cinema. The Leftist cinematic movement often revolved around the Western-influenced Shanghai, where filmmakers portrayed the struggling lower class of an overpopulated city.

Three production companies dominated the market in the early to mid- 1930s: the newly formed Lianhua ("United China"), the older and larger Mingxing and Tianyi. Both Mingxing and Lianhua leaned left (Lianhua's management perhaps more so), while Tianyi continued to make less socially conscious fare.

The period also produced the first big Chinese movie stars, such as Hu Die, Ruan Lingyu, Li Lili, Chen Yanyan, Zhou Xuan, Zhao Dan and Jin Yan. Other major films of the period include "Love and Duty" (1931), "Little Toys" (1933), "New Women" (1934), "Song of the Fishermen" (1934), "Plunder of Peach and Plum" (1934), "Crossroads" (1937), and "Street Angel" (1937). Throughout the 1930s, the Nationalists and the Communists struggled for power and control over the major studios; their influence can be seen in the films the studios produced during this period.

The Japanese invasion of China in 1937, in particular the Battle of Shanghai, ended this golden run in Chinese cinema. All production companies except Xinhua Film Company ("New China") closed shop, and many of the filmmakers fled Shanghai, relocating to Hong Kong, the wartime Nationalist capital Chongqing, and elsewhere. The Shanghai film industry, though severely curtailed, did not stop however, thus leading to the "Solitary Island" period (also known as the "Sole Island" or "Orphan Island"), with Shanghai's foreign concessions serving as an "island" of production in the "sea" of Japanese-occupied territory. It was during this period that artists and directors who remained in the city had to walk a fine line between staying true to their leftist and nationalist beliefs and Japanese pressures. Director Bu Wancang's "Mulan Joins the Army" (1939), with its story of a young Chinese peasant fighting against a foreign invasion, was a particularly good example of Shanghai's continued film-production in the midst of war. This period ended when Japan declared war on the Western allies on December 7, 1941; the solitary island was finally engulfed by the sea of the Japanese occupation. With the Shanghai industry firmly in Japanese control, films like the Greater East Asia Co-Prosperity Sphere-promoting "Eternity" (1943) were produced. At the end of World War II, one of the most controversial Japanese-authorized company, Manchukuo Film Association, would be separated and integrated into Chinese cinema.

The film industry continued to develop after 1945. Production in Shanghai once again resumed as a new crop of studios took the place that Lianhua and Mingxing had occupied in the previous decade. In 1946, Cai Chusheng returned to Shanghai to revive the Lianhua name as the "Lianhua Film Society." This in turn became Kunlun Studios which would go on to become one of the most important studios of the era, putting out the classics "The Spring River Flows East" (1947), "Myriad of Lights" (1948), "Crows and Sparrows" (1949) and "San Mao, The Little Vagabond" (1949).
Many of these films showed the disillusionment with the oppressive rule of Chiang Kai-shek's Nationalist Party. "The Spring River Flows East", a three-hour-long two-parter directed by Cai Chusheng and Zheng Junli, was a particularly strong success. Its depiction of the struggles of ordinary Chinese during the Second Sino-Japanese war, replete with biting social and political commentary, struck a chord with audiences of the time.

Meanwhile, companies like the Wenhua Film Company ("Culture Films"), moved away from the leftist tradition and explored the evolution and development of other dramatic genres. Wenhua treated postwar problems in universalistic and humanistic ways, avoiding the family narrative and melodramatic formulae. Excellent examples of Wenhua's fare are its first two postwar features, "Unending Emotions" (1947) and "Fake Bride, Phony Bridegroom" (1947). Another memorable Wenhua film is "Long Live the Missus" (1947), like "Unending Emotions" with an original screenplay by writer Eileen Chang. Wenhua's romantic drama "Spring in a Small Town" (1948), a film by director Fei Mu shortly prior to the revolution, is often regarded by Chinese film critics as one of the most important films in the history of Chinese cinema, with it being named by the Hong Kong Film Awards in 2004 as the greatest Chinese-language film ever made. Ironically, it was precisely its artistic quality and apparent lack of "political grounding" that led to its labeling by the Communists as rightist or reactionary, and the film was quickly forgotten by those on the mainland following the Communist victory in China in 1949. However, with the China Film Archive's re-opening after the Cultural Revolution, a new print was struck from the original negative, allowing "Spring of the Small Town" to find a new and admiring audience and to influence an entire new generation of filmmakers. Indeed, an acclaimed remake was made in 2002 by Tian Zhuangzhuang. "A Wedding in the Dream" (1948), by the same director, was the first Chinese color film.

With the communist revolution in China in 1949, the government saw motion pictures as an important mass production art form and tool for propaganda. Starting from 1951, pre-1949 Chinese films, Hollywood and Hong Kong productions were banned as the Communist Party of China sought to tighten control over mass media, producing instead movies centering on peasants, soldiers and workers, such as "Bridge" (1949) and "The White Haired Girl" (1950). One of the production bases in the middle of all the transition was the Changchun Film Studio.

The private studios in Shanghai, including Kunming, Wenhua, Guotai and Datong, were encouraged to make new films from 1949 to 1951. They made approximately 47 films during this period, but soon ran into trouble, owing to the furore over the Kunlun-produced drama "The Life of Wu Xun" (1950), directed by Sun Yu and starring veteran Zhao Dan. The feature was accused in an anonymous article in "People's Daily" in May 1951 of spreading feudal ideas. After the article was revealed to be penned by Mao Zedong, the film was banned, a Film Steering Committee was formed to "re-educate" the film industry and within two years, these private studios were all incorporated into the state-run Shanghai Film Studio.

The Communist regime solved the problem of a lack of film theaters by building mobile projection units which could tour the remote regions of China, ensuring that even the poorest could have access to films. By 1965 there were around 20,393 such units. The number of movie-viewers hence increased sharply, partly bolstered by the fact that film tickets were given out to work units and attendance was compulsory, with admissions rising from 47 million in 1949 to 4.15 billion in 1959. In the 17 years between the founding of the People's Republic of China and the Cultural Revolution, 603 feature films and 8,342 reels of documentaries and newsreels were produced, sponsored mostly as Communist propaganda by the government. For example, in "Guerrilla on the Railroad" (铁道游击队), dated 1956, the Chinese Communist Party was depicted as the primary resistance force against the Japanese in the war against invasion. Chinese filmmakers were sent to Moscow to study the Soviet socialist realism style of filmmaking. In 1956, the Beijing Film Academy was opened. One important film of this era is "This Life of Mine" (1950), directed by Shi Hu, which follows an old beggar reflecting on his past life as a policeman working for the various regimes since 1911. The first widescreen Chinese film was produced in 1960. Animated films using a variety of folk arts, such as papercuts, shadow plays, puppetry, and traditional paintings, also were very popular for entertaining and educating children. The most famous of these, the classic "Havoc in Heaven" (two parts, 1961, 4), was made by Wan Laiming of the Wan Brothers and won Outstanding Film award at the London International Film Festival.

The thawing of censorship in 1956–57 (known as the Hundred Flowers Campaign) and the early 1960s led to more indigenous Chinese films being made which were less reliant on their Soviet counterparts. During this campaign the sharpest criticisms came from the satirical comedies of Lü Ban. "Before the New Director Arrives" exposes the hierarchical relationships occurring between the cadres, while his next film, "The Unfinished Comedy" (1957), was labelled as a "poisonous weed" during the Anti-Rightist Movement and Lü was banned from directing for life."The Unfinished Comedy" was only screened after Mao's death. Other noteworthy films produced during this period were adaptations of literary classics, such as Sang Hu's "The New Year's Sacrifice" (1956; adapted from a Lu Xun story) and Shui Hua's "The Lin Family Shop" (1959; adapted from a Mao Dun story). The most prominent filmmaker of this era was Xie Jin, whose three films in particular, "Woman Basketball Player No. 5" (1957), "The Red Detachment of Women" (1961) and "Two Stage Sisters" (1964), exemplify China's increased expertise at filmmaking during this time. Films made during this period are polished and exhibit high production value and elaborate sets. While Beijing and Shanghai remained the main centers of production, between 1957–60 the government built regional studios in Guangzhou, Xi'an and Chengdu to encourage representations of ethnic minorities in films. Chinese cinema began to directly address the issue of such ethnic minorities during the late 1950s and early 1960s, in films like "Five Golden Flowers" (1959), "Third Sister Liu" (1960), "Serfs" (1963), "Ashima" (1964).

During the Cultural Revolution, the film industry was severely restricted. Almost all previous films were banned, and only a few new ones were produced, the so-called "revolutionary model operas". The most notable of these was a ballet version of the revolutionary opera "The Red Detachment of Women", directed by Pan Wenzhan and Fu Jie in 1970. Feature film production came almost to a standstill in the early years from 1967 to 1972. Movie production revived after 1972 under the strict jurisdiction of the Gang of Four until 1976, when they were overthrown. The few films that were produced during this period, such as 1975's "Breaking with Old Ideas", were highly regulated in terms of plot and characterization.

In the years immediately following the Cultural Revolution, the film industry again flourished as a medium of popular entertainment. Production rose steadily, from 19 features in 1977 to 125 in 1986. Domestically produced films played to large audiences, and tickets for foreign film festivals sold quickly. The industry tried to revive crowds by making more innovative and "exploratory" films like their counterparts in the West.

In the 1980s the film industry fell on hard times, faced with the dual problems of competition from other forms of entertainment and concern on the part of the authorities that many of the popular thriller and martial arts films were socially unacceptable. In January 1986 the film industry was transferred from the Ministry of Culture to the newly formed Ministry of Radio, Cinema, and Television to bring it under "stricter control and management" and to "strengthen supervision over production."

The end of the Cultural Revolution brought the release of "scar dramas", which depicted the emotional traumas left by this period. The best-known of these is probably Xie Jin's "Hibiscus Town" (1986), although they could be seen as late as the 1990s with Tian Zhuangzhuang's "The Blue Kite" (1993). In the 1980s, open criticism of certain past Communist Party policies was encouraged by Deng Xiaoping as a way to reveal the excesses of the Cultural Revolution and the earlier Anti-Rightist Campaign, also helping to legitimize Deng's new policies of "reform and opening up." For instance, the Best Picture prize in the inaugural 1981 Golden Rooster Awards was given to two "scar dramas", "Evening Rain" (Wu Yonggang, Wu Yigong, 1980) and "Legend of Tianyun Mountain" (Xie Jin, 1980).

Many scar dramas were made by members of the Fourth Generation whose own careers or lives had suffered during the events in question, while younger, Fifth Generation directors such as Tian tended to focus on less controversial subjects of the immediate present or the distant past. Official enthusiasm for scar dramas waned by the 1990s when younger filmmakers began to confront negative aspects of the Mao era. "The Blue Kite", though sharing a similar subject as the earlier scar dramas, was more realistic in style, and was made only through obfuscating its real script. Shown abroad, it was banned from release in mainland China, while Tian himself was banned from making any films for nearly a decade afterward. After the events of June 4, 1989 in Tiananmen Square, few if any scar dramas were released domestically in mainland China.

Beginning in the mid-late 1980s, the rise of the so-called Fifth Generation of Chinese filmmakers brought increased popularity of Chinese cinema abroad. Most of the filmmakers who made up the Fifth Generation had graduated from the Beijing Film Academy in 1982 and included Zhang Yimou, Tian Zhuangzhuang, Chen Kaige, Zhang Junzhao and others. These graduates constituted the first group of filmmakers to graduate since the Cultural Revolution and they soon jettisoned traditional methods of storytelling and opted for a more free and unorthodox approach. After the so-called scar literature in fiction had paved the way for frank discussion, Zhang Junzhao's "One and Eight" (1983) and Chen Kaige's "Yellow Earth" (1984) in particular were taken to mark the beginnings of the Fifth Generation. The most famous of the Fifth Generation directors, Chen Kaige and Zhang Yimou, went on to produce celebrated works such as "King of the Children" (1987), "Ju Dou" (1989), "Farewell My Concubine" (1993) and "Raise the Red Lantern" (1991), which were not only acclaimed by Chinese cinema-goers but by the Western arthouse audience. Tian Zhuangzhuang's films, though less well known by Western viewers, were well noted by directors such as Martin Scorsese. It was during this period that Chinese cinema began reaping the rewards of international attention, including the 1988 Golden Bear for "Red Sorghum", the 1992 Golden Lion for "The Story of Qiu Ju", the 1993 Palme d'Or for "Farewell My Concubine", and three Best Foreign Language Film nominations from the Academy Awards. All these award-winning films starred actress Gong Li, who became the Fifth Generation's most recognizable star, especially to international audiences.

Diverse in style and subject, the Fifth Generation directors' films ranged from black comedy (Huang Jianxin's "The Black Cannon Incident", 1985) to the esoteric (Chen Kaige's "Life on a String", 1991), but they share a common rejection of the socialist-realist tradition worked by earlier Chinese filmmakers in the Communist era. Other notable Fifth Generation directors include Wu Ziniu, Hu Mei, Li Shaohong and Zhou Xiaowen. Fifth Generation filmmakers reacted against the ideological purity of Cultural Revolution cinema. By relocating to regional studios, they began to explore the actuality of local culture in a somewhat documentarian fashion. Instead of stories depicting heroic military struggles, the films were built out of the drama of ordinary people's daily lives. They also retained political edge, but aimed at exploring issues rather than recycling approved policy. While Cultural Revolution films used character, the younger directors favored psychological depth along the lines of European cinema. They adopted complex plots, ambiguous symbolism, and evocative imagery. Some of their bolder works with political overtones were banned by Chinese authorities.

These films came with a new style of shooting as well, directors utilized extensive color and long shots. As a result of the new films being so intricate, the films were for more educated audiences than anything. The new style was profitable for some and helped filmmakers to make strides in the business. It allowed directors to get away from reality and show their artistic sense.

The Fourth Generation also returned to prominence. Given their label after the rise of the Fifth Generation, these were directors whose careers were stalled by the Cultural Revolution and who were professionally trained prior to 1966. Wu Tianming, in particular, made outstanding contributions by helping to finance major Fifth Generation directors under the auspices of the Xi'an Film Studio (which he took over in 1983), while continuing to make films like "Old Well" (1986) and "The King of Masks" (1996).

The Fifth Generation movement ended in part after the 1989 Tiananmen Incident, although its major directors continued to produce notable works. Several of its filmmakers went into self-imposed exile: Wu Tianming moved to the United States (but later returned), Huang Jianxin left for Australia, while many others went into television-related works.

In 1987, the Ministry of Radio, Film and Television issued a statement encouraging the making of movies which emphasizes "the main melody" (主旋律) to "invigorate national spirit and national pride". These "main melody" films (主旋律电影), still produced regularly nowadays, try to emulate the commercial mainstream by the use of Hollywood-style music and special effects.
They are often subsidized by the state and have free access to government and military personnel. August 1st Film Studio, the film and TV production arm of the People's Liberation Army, is a studio which produces "main melody" cinema. "Main melody" films, which often depict past military engagements or are biopics of first-generation CCP leaders, have won several Best Picture prizes at the Golden Rooster Awards. Some of the more famous "main melody" dramas include the ten-hour epic "Decisive Engagement" (大决战, 1991), directed by Cai Jiawei, Yang Guangyuan and Wei Lian; "The Opium War" (1997), directed by Xie Jin; and "The Founding of a Republic" (2009), directed by Han Sanping and Fifth Generation director Huang Jianxin.

The post-1990 era has seen what some observers term the "return of the amateur filmmaker" as state censorship policies after the Tiananmen Square demonstrations produced an edgy underground film movement loosely referred to as the Sixth Generation. Owing to the lack of state funding and backing, these films were shot quickly and cheaply, using materials like 16 mm film and digital video and mostly non-professional actors and actresses, producing a documentary feel, often with long takes, hand-held cameras, and ambient sound; more akin to Italian neorealism and cinéma vérité than the often lush, far more considered productions of the Fifth Generation. Unlike the Fifth Generation, the Sixth Generation brings a more individualistic, anti-romantic life-view and pays far closer attention to contemporary urban life, especially as affected by disorientation, rebellion and dissatisfaction with China's contemporary social tensions. Many were made with an extremely low budget (an example is Jia Zhangke, who shoots on digital video, and formerly on 16 mm; Wang Xiaoshuai's "The Days" (1993) were made for US$10,000). The title and subjects of many of these films reflect the Sixth Generation's concerns. The Sixth Generation takes an interest in marginalized individuals and the less represented fringes of society. For example, Zhang Yuan's hand-held "Beijing Bastards" (1993) focuses on youth punk subculture, featuring artists like Cui Jian, Dou Wei and He Yong frowned upon by many state authorities, while Jia Zhangke's debut film "Xiao Wu" (1997) concerns a provincial pickpocket.

As the Sixth Generation gained international exposure, many subsequent movies were joint ventures and projects with international backers, but remained quite resolutely low-key and low budget. Jia's "Platform" (2000) was funded in part by Takeshi Kitano's production house, while his "Still Life" was shot on HD video. "Still Life" was a surprise addition and Golden Lion winner of the 2006 Venice International Film Festival. "Still Life", which concerns provincial workers around the Three Gorges region, sharply contrasts with the works of Fifth Generation Chinese directors like Zhang Yimou and Chen Kaige who were at the time producing "House of Flying Daggers" (2004) and "The Promise" (2005). It featured no star of international renown and was acted mostly by non-professionals.

Many Sixth Generation films have highlighted the negative attributes of China's entry into the modern capitalist market. Li Yang's "Blind Shaft" (2003) for example, is an account of two murderous con-men in the unregulated and notoriously dangerous mining industry of northern China. (Li refused the tag of Sixth Generation, although admitted he was not Fifth Generation). While Jia Zhangke's "The World" (2004) emphasizes the emptiness of globalization in the backdrop of an internationally themed amusement park.

Some of the more prolific Sixth Generation directors to have emerged are Wang Xiaoshuai ("The Days", "Beijing Bicycle"), Zhang Yuan ("Beijing Bastards", "East Palace West Palace"), Jia Zhangke ("Xiao Wu", "Unknown Pleasures", "Platform", "The World"), He Jianjun ("Postman") and Lou Ye ("Suzhou River", "Summer Palace"). One young director who does not share most of the concerns of the Sixth Generation is Lu Chuan ("", 2004; "City of Life and Death", 2010).

There is a growing number of independent post-Sixth Generation filmmakers making films with extremely low budgets and using digital equipment. They are the so-called dGeneration (for digital). These films, like those from Sixth Generation filmmakers, are mostly made outside the Chinese film system and are shown mostly on the international film festival circuit. Ying Liang and Jian Yi are two of these dGeneration filmmakers. Ying's "Taking Father Home" (2005) and "The Other Half" (2006) are both representative of the dGeneration trends of feature film. Liu Jiayin made two dGeneration feature films, "Oxhide" (2004) and "Oxhide II" (2010), blurring the line between documentary and narrative film. "Oxhide", made by Liu when she was a film student, frames herself and her parents in their claustrophobic Beijing apartment in a narrative praised by critics.

Two decades of reform and commercialization have brought dramatic social changes in mainland China, reflected not only in fiction film but in a growing documentary movement. Wu Wenguang's 70-minute "" (1990) is now seen as one of the first works of this "New Documentary Movement" (NDM) in China. "Bumming", made between 1988 and 1990, contains interviews with five young artists eking out a living in Beijing, subject to state authorized tasks. Shot using a camcorder, the documentary ends with four of the artists moving abroad after the 1989 Tiananmen Protests. "Dance with the Farm Workers" (2001) is another documentary by Wu.

Another internationally acclaimed documentary is Wang Bing's nine-hour tale of deindustrialization "" (2003). Wang's subsequent documentaries, "" (2007), "Crude Oil" (2008), "Man with no name" (2009), "Three Sisters" (2012) and "Feng ai" (2013), cemented his reputation as a leading documentarist of the movement.

Li Hong, the first woman in the NDM, in "Out of Phoenix Bridge" (1997) relates the story of four young women, who moving from rural areas to the big cities like millions of other men and women, have come to Beijing to make a living.

The New Documentary Movement in recent times has overlapped with the dGeneration filmmaking, with most documentaries being shot cheaply and independently in the digital format. Xu Xin's "Karamay" (2010), Zhao Liang's Behemoth, Huang Weikai's "Disorder" (2009), Zhao Dayong's "Ghost Town" (2009), Du Haibing's "1428" (2009), Xu Tong's "Fortune Teller" (2010) and Li Ning’s "Tape" (2010) were all shot in digital format. All had made their impact in the international documentary scene and the use of digital format allows for works of vaster lengths.

With China's liberalization in the late 1970s and its opening up to foreign markets, commercial considerations have made its impact in post-1980s filmmaking. Traditionally arthouse movies screened seldom make enough to break even. An example is Fifth Generation director Tian Zhuangzhuang's "The Horse Thief" (1986), a narrative film with minimal dialog on a Tibetan horse thief. The film, showcasing exotic landscapes, was well received by Chinese and some Western arthouse audiences, but did poorly at the box office.
Tian's later "The Warrior and the Wolf" (2010) was a similar commercial failure. Prior to these, there were examples of successful commercial films in the post-liberalization period. One was the romance film "Romance on the Lu Mountain" (1980), which was a success with older Chinese. The film broke the Guinness Book of Records as the longest-running film on a first run. Jet Li's cinematic debut "Shaolin Temple" (1982) was an instant hit at home and abroad (in Japan and the Southeast Asia, for example). Another successful commercial film was "Murder in 405" (405谋杀案, 1980), a murder thriller.

Feng Xiaogang's "The Dream Factory" (1997) was heralded as a turning point in Chinese movie industry, a "hesui pian" (Chinese New Year-screened film) which demonstrated the viability of the commercial model in China's socialist market society. Feng has become the most successful commercial director in the post-1997 era. Almost all his films made high returns domestically while he used ethnic Chinese co-stars like Rosamund Kwan, Jacqueline Wu, Rene Liu and Shu Qi to boost his films' appeal.

In the decade following 2010, owing to the influx of Hollywood films (though the number screened each year is curtailed), Chinese domestic cinema faces mounting challenges. The industry is growing and domestic films are starting to achieve the box office impact of major Hollywood blockbusters. However, not all domestic films are successful financially. In January 2010 James Cameron's "Avatar" was pulled out from non-3D theaters for Hu Mei's biopic "Confucius", but this move led to a backlash on Hu's film. Zhang Yang's 2005 "Sunflower" also made little money, but his earlier, low-budget "Spicy Love Soup" (1997) grossed ten times its budget of ¥3 million. Likewise, the 2006 "Crazy Stone", a sleeper hit, was made for just 3 million HKD/US$400,000. In 2009–11, Feng's "Aftershock" (2009) and Jiang Wen's "Let the Bullets Fly" (2010) became China's highest grossing domestic films, with "Aftershock" earning ¥670 million (US$105 million) and "Let the Bullets Fly" ¥674 million (US$110 million). "Lost in Thailand" (2012) became the first Chinese film to reach ¥1 billion at the Chinese box office and "Monster Hunt" (2015) became the first to reach . As of November 2015, 5 of the top 10 highest-grossing films in China are domestic productions. On February 8, 2016, the Chinese box office set a new single-day gross record, with , beating the previous record of on July 18, 2015. Also on February 2016, "The Mermaid", directed by Stephen Chow, became the highest-grossing film in China, overtaking "Monster Hunt". It is also the first film to reach .

He Ping is a director of mostly Western-like films set in Chinese locale. His "Swordsmen in Double Flag Town" (1991) and "Sun Valley" (1995) explore narratives set in the sparse terrain of West China near the Gobi Desert. His historical drama "Red Firecracker, Green Firecracker" (1994) won a myriad of prizes home and abroad.

Recent cinema has seen Chinese cinematographers direct some acclaimed films. Other than Zhang Yimou, Lü Yue made "Mr. Zhao" (1998), a black comedy film well received abroad. Gu Changwei's minimalist epic "Peacock" (2005), about a quiet, ordinary Chinese family with three very different siblings in the post-Cultural Revolution era, took home the Silver Bear prize for 2005 Berlin International Film Festival. Hou Yong is another cinematographer who made films ("Jasmine Women", 2004) and TV series. There are actors who straddle the dual roles of acting and directing. Xu Jinglei, a popular Chinese actress, has made six movies to date. Her second film "Letter from an Unknown Woman" (2004) landed her the San Sebastián International Film Festival Best Director award. Another popular actress and director is Zhao Wei, whose directorial debut "So Young" (2013) was a huge box office and critical success.

The most highly regarded Chinese actor-director is undoubtedly Jiang Wen, who has directed several critically acclaimed movies while following on his acting career. His directorial debut, "In the Heat of the Sun" (1994) was the first PRC film to win Best Picture at the Golden Horse Film Awards held in Taiwan. His other films, like "Devils on the Doorstep" (2000, Cannes Grand Prix) and "Let the Bullets Fly" (2010), were similarly well received. By the early 2011, "Let the Bullets Fly" had become the highest grossing domestic film in China's history.

Since the late 1980s and progressively in the 2000s, Chinese films have enjoyed considerable box office success abroad. Formerly viewed only by cineastes, its global appeal mounted after the international box office and critical success of Ang Lee's period martial arts film "Crouching Tiger, Hidden Dragon" which won Academy Award for Best Foreign Language Film in 2000. This multi-national production increased its appeal by featuring stars from all parts of the Chinese-speaking world. It provided an introduction to Chinese cinema (and especially the wuxia genre) for many and increased the popularity of many earlier Chinese films. To date "Crouching Tiger" remains the most commercially successful foreign-language film in U.S. history.

Similarly, in 2002, Zhang Yimou's "Hero" was another international box office success. Its cast featured famous actors from the Mainland China and Hong Kong who were also known to some extent in the West, including Jet Li, Zhang Ziyi, Maggie Cheung and Tony Leung Chiu-Wai. Despite criticisms by some that these two films pander somewhat to Western tastes, "Hero" was a phenomenal success in most of Asia and topped the U.S. box office for two weeks, making enough in the U.S. alone to cover the production costs.

Other films such as "Farewell My Concubine", "2046", "Suzhou River", "The Road Home" and "House of Flying Daggers" were critically acclaimed around the world. The Hengdian World Studios can be seen as the "Chinese Hollywood", with a total area of up to 330 ha. and 13 shooting bases, including a 1:1 copy of the Forbidden City.

The successes of "Crouching Tiger, Hidden Dragon" and "Hero" make it difficult to demarcate the boundary between "Mainland Chinese" cinema and a more international-based "Chinese-language cinema". "Crouching Tiger", for example, was directed by a Taiwan-born American director (Ang Lee) who works often in Hollywood. Its pan-Chinese leads include Mainland Chinese (Zhang Ziyi), Hong Kong (Chow Yun-Fat), Taiwan (Chang Chen) and Malaysian (Michelle Yeoh) actors and actresses; the film was co-produced by an array of Chinese, American, Hong Kong, and Taiwan film companies. Likewise, Lee's Chinese-language "Lust, Caution" (2007) drew a crew and cast from Mainland China, Hong Kong and Taiwan, and includes an orchestral score by French composer Alexandre Desplat. This merging of people, resources and expertise from the three regions and the broader East Asia and the world, marks the movement of Chinese-language cinema into a domain of large scale international influence. Other examples of films in this mold include "The Promise" (2005), "The Banquet" (2006), "Fearless" (2006), "The Warlords" (2007), "Bodyguards and Assassins" (2009) and "Red Cliff" (2008-9). The ease with which ethnic Chinese actresses and actors straddle the mainland and Hong Kong has significantly increased the number of co-productions in Chinese-language cinema. Many of these films also feature South Korean or Japanese actors to appeal to their East Asian neighbours. Some artistes originating from the mainland, like Hu Jun, Zhang Ziyi, Tang Wei and Zhou Xun, obtained Hong Kong residency under the Quality Migrant Admission Scheme and have acted in many Hong Kong productions.

In 2010, Chinese cinema was the third largest film industry by number of feature films produced annually. In 2013, China's gross box office was ¥21.8 billion (US$3.6 billion), the second-largest film market in the world by box office receipts. In January 2013, "Lost in Thailand" (2012) became the first Chinese film to reach ¥1 billion at the box office. As of May 2013, 7 of the top 10 highest-grossing films in China were domestic productions. As of 2014, around half of all tickets are sold online, with the largest ticket selling sites being Maoyan.com (82 million), Gewara.com (45 million) and Wepiao.com (28 million). In 2014, Chinese films earned ¥1.87 billion outside China. By December 2013 there were 17,000 screens in the country. By January 6, 2014, there were 18,195 screens in the country. Greater China has around 251 IMAX theaters. There were 299 cinema chains (252 rural, 47 urban), 5,813 movie theaters and 24,317 screens in the country in 2014.

The country added about 8,035 screens in 2015 (at an average of 22 new screens per day, increasing its total by about 40% to around 31,627 screens, which is about 7,373 shy of the number of screens in the United States. Chinese films accounted for 61.48% of ticket sales in 2015 (up from 54% last year) with more than 60% of ticket sales being made online. Average ticket price was down about 2.5% to $5.36 in 2015. It also witnessed 51.08% increase in admissions, with 1.26 billion people buying tickets to the cinema in 2015. Chinese films grossed overseas in 2015. During the week of the 2016 Chinese New Year, the country set a new record for the highest box office gross during one week in one territory with , overtaking the previous record of of December 26, 2015 to January 1, 2016 in the United States and Canada. Chinese films grossed () in foreign markets in 2016.

As of April 2015, the largest Chinese film company by worth was Alibaba Pictures (US$8.77 billion). Other large companies include Huayi Brothers Media (US$7.9 billion), Enlight Media (US$5.98 billion) and Bona Film Group (US$542 million). The biggest distributors by market share in 2014 were: China Film Group (32.8%), Huaxia Film (22.89%), Enlight Pictures (7.75%), Bona Film Group (5.99%), Wanda Media (5.2%), Le Vision Pictures (4.1%), Huayi Brothers (2.26%), United Exhibitor Partners (2%), Heng Ye Film Distribution (1.77%) and Beijing Anshi Yingna Entertainment (1.52%). The biggest cinema chains in 2014 by box office gross were: Wanda Cinema Line (), China Film Stellar (393.35 million), Dadi Theater Circuit (378.17 million), Shanghai United Circuit (355.07 million), Guangzhou Jinyi Zhujiang (335.39 million), China Film South Cinema Circuit (318.71 million), Zhejiang Time Cinema (190.53 million), China Film Group Digital Cinema Line (177.42 million), Hengdian Cinema Line (170.15 million) and Beijing New Film Association (163.09 million).






</doc>
<doc id="10793" url="https://en.wikipedia.org/wiki?curid=10793" title="Cinema of the United Kingdom">
Cinema of the United Kingdom

The United Kingdom has had a significant film industry for over a century. While film production reached an all-time high in 1936, the "golden age" of British cinema is usually thought to have occurred in the 1940s, during which the directors David Lean, Michael Powell, (with Emeric Pressburger) and Carol Reed produced their most critically acclaimed works. Many British actors have achieved worldwide fame and critical success, such as Maggie Smith, Roger Moore, Michael Caine, Sean Connery, Daniel Day-Lewis, Gary Oldman, Emma Thompson and Kate Winslet. Some of the films with the largest ever box office returns have been made in the United Kingdom, including the third and fourth highest-grossing film franchises ("Harry Potter" and "James Bond").

The identity of the British industry, particularly as it relates to Hollywood, has often been the subject of debate. Its history has often been affected by attempts to compete with the American industry. The career of the producer Alexander Korda was marked by this objective, the Rank Organisation attempted to do so in the 1940s, and Goldcrest in the 1980s. Numerous British-born directors, including Alfred Hitchcock and Ridley Scott, and performers, such as Charlie Chaplin and Cary Grant, have achieved success primarily through their work in the United States.

In 2009, British films grossed around $2 billion worldwide and achieved a market share of around 7% globally and 17% in the United Kingdom. UK box-office takings totalled £1.1 billion in 2012, with 172.5 million admissions.

The British Film Institute has produced a poll ranking what they consider to be the 100 greatest British films of all time, the BFI Top 100 British films. The annual BAFTA awards hosted by the British Academy of Film and Television Arts are considered to be the British equivalent of the Academy Awards.

The first moving picture was shot in Leeds by Louis Le Prince in 1888 and the first moving pictures developed on celluloid film were made in Hyde Park, London in 1889 by British inventor William Friese Greene, who patented the process in 1890.
The first people to build and run a working 35 mm camera in Britain were Robert W. Paul and Birt Acres. They made the first British film "Incident at Clovelly Cottage" in February 1895, shortly before falling out over the camera's patent. Soon several British film companies had opened to meet the demand for new films, such as Mitchell and Kenyon in Blackburn.

Although the earliest British films were of everyday events, the early 20th century saw the appearance of narrative shorts, mainly comedies and melodramas. The early films were often melodramatic in tone, and there was a distinct preference for story lines already known to the audience, in particular, adaptations of Shakespeare plays and Dickens novels.

The Lumière brothers first brought their show to London in 1896. In 1898 American producer Charles Urban expanded the London-based Warwick Trading Company to produce British films, mostly documentary and news.

In 1898 Gaumont-British Picture Corp. was founded as a subsidiary of the French Gaumont Film Company, constructing Lime Grove Studios in West London in 1915 in the first building built in Britain solely for film production. Also in 1898 Hepworth Studios was founded in Lambeth, South London by Cecil Hepworth, the Bamforths began producing films in Yorkshire, and William Haggar began producing films in Wales.

In 1902 Ealing Studios was founded by Will Barker, becoming the oldest continuously-operating film studio in the world.

In 1902 the earliest colour film in the world was made; like other films made at the time, it is of everyday events. In 2012 it was found by the National Science and Media Museum in Bradford after lying forgotten in an old tin for 110 years. The previous title for earliest colour film, using Urban's inferior Kinemacolor process, was thought to date from 1909. The re-discovered films were made by pioneer Edward Raymond Turner from London who patented his process on 22 March 1899.

In 1903 Urban formed the Charles Urban Trading Company, which produced early colour films using his patented Kinemacolor process. This was later challenged in court by Greene, causing the company to go out of business in 1915.

In 1903 Frank Mottershaw of Sheffield produced the film "A Daring Daylight Robbery", which launched the chase genre.

In 1911 the Ideal Film Company was founded in Soho, London, distributing almost 400 films by 1934, and producing 80.

In 1913 stage director Maurice Elvey began directing British films, becoming Britain's most prolific film director, with almost 200 by 1957.

In 1914 Elstree Studios was founded, and acquired in 1928 by German-born Ludwig Blattner, who invented a magnetic steel tape recording system that was adopted by the BBC in 1930.

In 1920 Gaumont opened Islington Studios, where Alfred Hitchcock got his start, selling out to Gainsborough Pictures in 1927. Also in 1920 Cricklewood Studios was founded by Sir Oswald Stoll, becoming Britain's largest film studio, known for Fu Manchu and Sherlock Holmes film series.

In 1920 the short-lived company Minerva Films was founded in London by the actor Leslie Howard (also producer and director) and his friend and story editor Adrian Brunel. Some of their early films include four written by A. A. Milne including "The Bump", starring C. Aubrey Smith; "Twice Two"; "Five Pound Reward"; and "Bookworms".

By the mid-1920s the British film industry was losing out to heavy competition from the United States, which was helped by its much larger home market – in 1914 25% of films shown in the UK were British, but by 1926 this had fallen to 5%. The Slump of 1924 caused many British film studios to close, resulting in the passage of the Cinematograph Films Act 1927 to boost local production, requiring that cinemas show a certain percentage of British films. The act was technically a success, with audiences for British films becoming larger than the quota required, but it had the effect of creating a market for poor quality, low cost films, made to satisfy the quota. The "quota quickies", as they became known, are often blamed by historians for holding back the development of the industry. However, some British film makers, such as Michael Powell, learnt their craft making such films. The act was modified with the Cinematograph Films Act 1938 assisted the British film industry by specifying only films made by and shot in Great Britain would be included in the quota, an act that severely reduced Canadian and Australian film production.

Ironically, the biggest star of the silent era, English comedian Charlie Chaplin, was Hollywood-based.

Scottish solicitor John Maxwell founded British International Pictures (BIP) in 1927. Based at the former British National Studios in Elstree, the facilities original owners, including producer-director Herbert Wilcox, had run into financial difficulties. One of the company's early films, Alfred Hitchcock's "Blackmail" (1929), is often regarded as the first British sound feature. It was a part-talkie with a synchronized score and sound effects. Earlier in 1929, the first all-talking British feature, "The Clue of the New Pin" was released. It was based on a novel by Edgar Wallace, starring Donald Calthrop, Benita Home and Fred Raines, which was made by British Lion at their Beaconsfield Studios. John Maxwell's BIP became the Associated British Picture Corporation (ABPC) in 1933. ABPC's studios in Elstree came to be known as the "porridge factory", according to Lou Alexander, "for reasons more likely to do with the quantity of films that the company turned out, than their quality". Elstree (strictly speaking almost all the studios were in neighboring Borehamwood) became the center of the British film industry, with six film complexes over the years all in close proximity to each other.

With the advent of sound films, many foreign actors were in less demand, with English received pronunciation commonly used; for example, the voice of Czech actress Anny Ondra in "Blackmail" was substituted by an off-camera Joan Barry during Ondra's scenes.

Starting with John Grierson's "Drifters" (also 1929), the period saw the emergence of the school of realist Documentary Film Movement, from 1933 associated with the GPO Film Unit. It was Grierson who coined the term "documentary" to describe a non-fiction film, and he produced the movement's most celebrated early films, "Night Mail" (1936), written and directed by Basil Wright and Harry Watt, and incorporating the poem by W. H. Auden towards the end of the short.

Music halls also proved influential in comedy films of this period, and a number of popular personalities emerged, including George Formby, Gracie Fields, Jessie Matthews and Will Hay. These stars often made several films a year, and their productions remained important for morale purposes during World War II.
Many of the British films with larger budgets during the 1930s were produced by London Films, founded by Hungarian "emigre" Alexander Korda. The success of "The Private Life of Henry VIII" (1933), made at British and Dominion in Elstree, persuaded United Artists and The Prudential to invest in Korda's Denham Film Studios, which opened in May 1936, but both investors suffered losses as a result. Korda's films before the war included "Things to Come", "Rembrandt" (both 1936) and "Knight Without Armour" (1937), as well as the early Technicolour films "The Drum" (1938) and "The Four Feathers" (1939). These had followed closely on from "Wings of the Morning" (1937), the UK's first three-strip Technicolour feature film, made by the local offshoot of 20th Century Fox. Although some of Korda's films indulged in "unrelenting pro-Empire flag waving", those featuring Sabu turned him into "a huge international star"; "for many years" he had the highest profile of any actor of Indian origin. Paul Robeson was also cast in leading roles when "there were hardly any opportunities" for African Americans "to play challenging roles" in their own country's productions.

Rising expenditure and over-optimistic expectations of expansion into the American market caused a financial crisis in 1937, after an all-time high of 192 films were released in 1936. Of the 640 British production companies registered between 1925 and 1936, only 20 were still active in 1937. Moreover, the 1927 Films Act was up for renewal. The replacement Cinematograph Films Act 1938 provided incentives, via a "quality test", for UK companies to make fewer films, but of higher quality, and to eliminate the "quota quickies". Influenced by world politics, it encouraged American investment and imports. One result was the creation of MGM-British, an English subsidiary of the largest American studio, which produced four films before the war, including "Goodbye, Mr. Chips" (1939).

The new venture was initially based at Denham Studios. Korda himself lost control of the facility in 1939 to the Rank Organisation, whose own Pinewood Studios had opened at the end of September 1936. Circumstances forced Korda's "The Thief of Bagdad" (1940), a spectacular fantasy film, to be completed in California, where Korda continued his film career during the war.

By now contracted to Gaumont British, Alfred Hitchcock had settled on the thriller genre by the mid-1930s with "The Man Who Knew Too Much" (1934), "The 39 Steps" (1935) and "The Lady Vanishes" (1938). Lauded in Britain where he was dubbed "Alfred the Great" by "Picturegoer" magazine, Hitchcock's reputation was beginning to develop overseas, with a "The New York Times" feature writer asserting; "Three unique and valuable institutions the British have that we in America have not. Magna Carta, the Tower Bridge and Alfred Hitchcock, the greatest director of screen melodramas in the world." Hitchcock was then signed up to a seven-year contract by Selznick and moved to Hollywood.

Humphrey Jennings began his career as a documentary film maker just before the war, in some cases working in collaboration with co-directors. "London Can Take It" (with Harry Wat, 1940) detailed the blitz while "Listen to Britain" (with Stewart McAllister, 1942) looked at the home front. The Crown Film Unit, part of the Ministry of Information took over the responsibilities of the GPO Film Unit in 1940. Paul Rotha and Alberto Cavalcanti were colleagues of Jennings. British films began to make use of documentary techniques; Cavalcanti joined Ealing for "Went the Day Well?" (1942),

Many other films helped to shape the popular image of the nation at war. Among the best known of these films are "In Which We Serve" (1942), "We Dive at Dawn" (1943), "Millions Like Us" (1943) and "The Way Ahead" (1944). The war years also saw the emergence of The Archers partnership between director Michael Powell and the Hungarian-born writer-producer Emeric Pressburger with films such as "The Life and Death of Colonel Blimp" (1943) and "A Canterbury Tale" (1944).

Two Cities Films, an independent production company releasing their films through a Rank subsidiary, also made some important films, including the Noël Coward and David Lean collaborations "This Happy Breed" (1944) and "Blithe Spirit" (1945) as well as Laurence Olivier's "Henry V" (1944). By this time, Gainsborough Studios were releasing their series of critically derided but immensely popular period melodramas, including "The Man in Grey" (1943) and "The Wicked Lady" (1945). New stars, such as Margaret Lockwood and James Mason, emerged in the Gainsborough films.

Towards the end of the 1940s, the Rank Organisation, founded in 1937 by J. Arthur Rank, became the dominant force behind British film-making, having acquired a number of British studios and the Gaumont chain (in 1941) to add to its Odeon Cinemas. Rank's serious financial crisis in 1949, a substantial loss and debt, resulted in the contraction of its film production. In practice, Rank maintained an industry duopoly with ABPC (later absorbed by EMI) for many years.

For the moment, the industry hit new heights of creativity in the immediate post-war years. Among the most significant films produced during this period were David Lean's "Brief Encounter" (1945) and his Dickens adaptations "Great Expectations" (1946) and "Oliver Twist" (1948), Carol Reed's thrillers "Odd Man Out" (1947) and "The Third Man" (1949), and Powell and Pressburger's "A Matter of Life and Death" (1946), "Black Narcissus" (1947) and "The Red Shoes" (1948), the most commercially successful film of its year in the United States. Laurence Olivier's "Hamlet" (also 1948), was the first non-American film to win the Academy Award for Best Picture. Ealing Studios (financially backed by Rank) began to produce their most celebrated comedies, with three of the best remembered films, "Whisky Galore" (1948), "Kind Hearts and Coronets" and "Passport to Pimlico" (both 1949), being on release almost simultaneously. Their portmanteau horror film "Dead of Night" (1945) is also particularly highly regarded.

Under the Import Duties Act 1932, HM Treasury levied a 75 per cent tariff on all film imports on 6 August 1947 which became known as Dalton Duty (after Hugh Dalton then the Chancellor of the Exchequer). The tax came into effect on 8 August, applying to all imported films, of which the overwhelming majority came from the United States; American film studio revenues from the UK had been in excess of US$68 million in 1946. The following day, 9 August, the Motion Picture Association of America announced that no further films would be supplied to British cinemas until further notice. The Dalton Duty was ended on 3 May 1948 with the American studios again exported films to the UK though the Marshall Plan prohibited US film companies from taking foreign exchange out of the nations their films played in .

The Eady Levy, named after Sir Wilfred Eady was a tax on box office receipts in the United Kingdom in order to support the British Film industry. It was established in 1950 coming into effect in 1957. A direct governmental payment to British-based producers would have qualified as a subsidy under the terms of the General Agreement on Tariffs and Trade, and would have led to objections from American film producers. An indirect levy did not qualify as a subsidy, and so was a suitable way of providing additional funding for the UK film industry whilst avoiding criticism from abroad.

During the 1950s, the British industry began to concentrate on popular comedies and World War II dramas aimed more squarely at the domestic audience. The war films were often based on true stories and made in a similar low-key style to their wartime predecessors. They helped to make stars of actors like John Mills, Jack Hawkins and Kenneth More. Some of the most successful included "The Cruel Sea" (1953), "The Dam Busters" (1954), "The Colditz Story" (1955) and "Reach for the Sky" (1956).

The Rank Organisation produced some comedy successes, such as "Genevieve" (1953). The writer/director/producer team of twin brothers John and Roy Boulting also produced a series of successful satires on British life and institutions, beginning with "Private's Progress" (1956), and continuing with (among others) "Brothers in Law" (1957), "Carlton-Browne of the F.O." (1958), and "I'm All Right Jack" (1959).

Popular comedy series included the "Doctor" series, beginning with "Doctor in the House" (1954). The series originally starred Dirk Bogarde, probably the British industry's most popular star of the 1950s, though later films had Michael Craig and Leslie Phillips in leading roles. The Carry On series began in 1958 with regular instalments appearing for the next twenty years. The Italian director-producer Mario Zampi also made a number of successful black comedies, including "Laughter in Paradise" (1951), "The Naked Truth" (1957) and "Too Many Crooks" (1958). Ealing Studios had continued its run of successful comedies, including "The Lavender Hill Mob" (1951) and "The Ladykillers" (1955), but the company ceased production in 1958, after the studios had already been bought by the BBC.
Less restrictive censorship towards the end of the 1950s encouraged B-film producer Hammer Films to embark on their series of commercially successful horror films. Beginning with adaptations of Nigel Kneale's BBC science fiction serials "The Quatermass Experiment" (1955) and "Quatermass II" (1957), Hammer quickly graduated to "The Curse of Frankenstein" (1957) and "Dracula" (1958), both deceptively lavish and the first gothic horror films in colour. The studio turned out numerous sequels and variants, with English actors Peter Cushing and Christopher Lee being the most regular leads. "Peeping Tom" (1960), a now highly regarded thriller, with horror elements, set in the contemporary period, was badly received by the critics at the time, and effectively finished the career of Michael Powell, its director.

The British New Wave film makers attempted to produce social realist films (see also 'kitchen sink realism') attempted in commercial feature films released between around 1959 and 1963 to convey narratives about a wider spectrum of people in Britain than the country's earlier films had done. These individuals, principally Karel Reisz, Lindsay Anderson and Tony Richardson, were also involved in the short lived Oxford film journal "Sequence" and the "Free Cinema" documentary film movement. The 1956 statement of Free Cinema, the name was coined by Anderson, asserted: "No film can be too personal. The image speaks. Sounds amplifies and comments. Size is irrelevant. Perfection is not an aim. An attitude means a style. A style means an attitude." Anderson, in particular, was dismissive of the commercial film industry. Their documentary films included Anderson's "Every Day Except Christmas", among several sponsored by Ford of Britain, and Richardson's "Momma Don't Allow". Another member of this group, John Schlesinger, made documentaries for the BBC's "Monitor" arts series.

Together with future James Bond co-producer Harry Saltzman, dramatist John Osborne and Tony Richardson established the company Woodfall Films to produce their early feature films. These included adaptations of Richardson's stage productions of Osborne's "Look Back in Anger" (1959), with Richard Burton, and "The Entertainer" (1960) with Laurence Olivier, both from Osborne's own screenplays. Such films as Reisz's "Saturday Night and Sunday Morning" (also 1960), Richardson's "A Taste of Honey" (1961), Schlesinger's "A Kind of Loving" (1962) and "Billy Liar" (1963), and Anderson's "This Sporting Life" (1963) are often associated with a new openness about working-class life or previously taboo issues.

The team of Basil Dearden and Michael Relph, from an earlier generation, "probe[d] into the social issues that now confronted social stability and the establishment of the promised peacetime consensus". "Pool of London" (1950). and "Sapphire" (1959) were early attempts to create narratives about racial tensions and an emerging multi-cultural Britain. Dearden and Relph's "Victim" (1961), was about the blackmail of homosexuals. Influenced by the Wolfenden report of four years earlier, which advocated the decriminalising of homosexual sexual activity, this was "the first British film to deal explicitly with homosexuality". Unlike the New Wave film makers though, critical responses to Dearden's and Relph's work have not generally been positive.

As the 1960s progressed, American studios returned to financially supporting British films, especially those that capitalised on the "swinging London" image propagated by "Time" magazine in 1966. Films like "Darling", "The Knack ...and How to Get It" (both 1965), "Alfie" and "Georgy Girl" (both 1966), all explored this phenomenon. "Blowup" (also 1966), and later "Women in Love" (1969), showed female and then male full-frontal nudity on screen in mainstream British films for the first time.

At the same time, film producers Harry Saltzman and Albert R. Broccoli combined sex with exotic locations, casual violence and self-referential humour in the phenomenally successful James Bond series with Sean Connery in the leading role. The first film "Dr. No" (1962) was a sleeper hit in the UK and the second, "From Russia with Love" (1963), a hit worldwide. By the time of the third film, "Goldfinger" (1964), the series had become a global phenomenon, reaching its commercial peak with "Thunderball" the following year. The series' success led to a spy film boom with many Bond imitations. Bond co-producer Saltzman also instigated a rival series of more realistic spy films based on the novels of Len Deighton. Michael Caine starred as bespectacled spy Harry Palmer in "The Ipcress File" (1965), and two sequels in the next few years. Other more downbeat espionage films were adapted from John le Carré novels, such as "The Spy Who Came in from the Cold" (1965) and "The Deadly Affair" (1966).

American directors were regularly working in London throughout the decade, but several became permanent residents in the UK. Blacklisted in America, Joseph Losey had a significant influence on British cinema in the 1960s, particularly with his collaborations with playwright Harold Pinter and leading man Dirk Bogarde, including "The Servant" (1963) and "Accident" (1967). Voluntary exiles Richard Lester and Stanley Kubrick were also active in the UK. Lester had major hits with The Beatles film "A Hard Day's Night" (1964) and "The Knack ...and How to Get It" (1965) while Kubrick's "Dr. Strangelove" (1963) and "" (1968). While Kubrick settled in Hertfordshire in the early 1960s and would remain in England for the rest of his career, these two films retained a strong American influence. Other films of this era involved prominent filmmakers from elsewhere in Europe, "Repulsion" (1965) and "Blowup" (1966) were the first English language films of the Polish director Roman Polanski and the Italian Michelangelo Antonioni respectively.

Historical films as diverse as "Lawrence of Arabia" (1962), "Tom Jones" (1963), and "A Man for All Seasons" (1966) benefited from the investment of American studios. Major films like "Becket" (1964), "Khartoum" (1966) and "The Charge of the Light Brigade" (1968) were regularly mounted, while smaller-scale films, including "Accident" (1967), were big critical successes. Four of the decade's Academy Award winners for best picture were British productions, including six Oscars for the film musical "Oliver!" (1968), based on the Charles Dickens novel "Oliver Twist".

After directing several contributions to the BBC's "Wednesday Play" anthology series, Ken Loach began his feature film career with the social realist "Poor Cow" (1967) and "Kes" (1969). Meanwhile, the controversy around Peter Watkins "The War Game" (1965), which won the Best Documentary Film Oscar in 1967, but had been suppressed by the BBC who had commissioned it, would ultimately lead Watkins to work exclusively outside Britain.

American studios cut back on British productions, and in many cases withdrew from financing them altogether. Films financed by American interests were still being made, including Billy Wilder's "The Private Life of Sherlock Holmes" (1970), but for a time funds became hard to come by.

More relaxed censorship also brought several controversial films, including Nicolas Roeg and Donald Cammell's "Performance", Ken Russell's "The Devils" (1971), Sam Peckinpah's "Straw Dogs" (1971), and Stanley Kubrick's "A Clockwork Orange" (1971) starring Malcolm McDowell as the leader of a gang of thugs in a dystopian future Britain.

Other films during the early 1970s included the Edwardian drama "The Go-Between" (1971), which won the Palme d'Or at the Cannes Film Festival, Nicolas Roeg's Venice-set supernatural thriller "Don't Look Now" (1973) and Mike Hodges' gangster drama "Get Carter" (1971) starring Michael Caine. Alfred Hitchcock returned to Britain to shoot "Frenzy" (1972), Other productions such as Richard Attenborough's "Young Winston" (1972) and "A Bridge Too Far" (1977) met with mixed commercial success. The British horror film cycle associated with Hammer Film Productions, Amicus and Tigon drew to a close, despite attempts by Hammer to spice up the formula with added nudity and gore. Although some attempts were made to broaden the range of British horror films, such as with "The Wicker Man" (1973), these films made little impact at the box office, In 1976, British Lion, who produced "The Wicker Man", were finally absorbed into the film division of EMI, who had taken over ABPC in 1969. The duopoly in British cinema exhibition, via Rank and now EMI, continued.
Some British producers, including Hammer, turned to television for inspiration, and big screen versions of popular sitcoms like "On the Buses" (1971) and "Steptoe and Son" (1972) proved successful with domestic audiences, the former had greater domestic box office returns in its year than the Bond film, "Diamonds Are Forever" and in 1973, an established British actor Roger Moore was cast as Bond in ,"Live and Let Die", it was a commercial success and Moore would continue the role for the next 12 years.Low-budget British sex comedies included the "Confessions of ..." series starring Robin Askwith, beginning with "Confessions of a Window Cleaner" (1974). More elevated comedy films came from the Monty Python team, also from television. Their two most successful films were "Monty Python and the Holy Grail" (1975) and "Monty Python's Life of Brian" (1979), the latter a major commercial success, probably at least in part due to the controversy at the time surrounding its subject.

Some American productions did return to the major British studios in 1977–79, including the original "Star Wars" (1977) at Elstree Studios, "Superman" (1978) at Pinewood, and "Alien" (1979) at Shepperton. Successful adaptations were made in the decade of the Agatha Christie novels "Murder on the Orient Express" (1974) and "Death on the Nile" (1978). The entry of Lew Grade's company ITC into film production in the latter half of the decade brought only a few box office successes and an unsustainable number of failures

In 1980, only 31 British films were made, a 50% decline from the previous year and the lowest number since 1914, and production fell again in 1981 to 24 films. The industry suffered further blows from falling cinema attendances, which reached a record low in 1984, and the elimination of the 1957 Eady Levy, a tax concession, in the same year. The concession had made it possible for an overseas based film company to write off a large amount of its production costs by filming in the UK – this was what attracted a succession of big-budget American productions to British studios in the 1970s. These factors led to significant changes in the industry, with the profitability of British films now "increasingly reliant on secondary markets such as video and television, and Channel 4 ... [became] a crucial part of the funding equation." The 1980s soon saw a renewed optimism, led by smaller independent production companies such as Goldcrest, HandMade Films and Merchant Ivory Productions.

Handmade Films, which was partly owned by George Harrison, was originally formed to take over the production of "Monty Python's Life of Brian", after EMI's Bernard Delfont (Lew Grade's brother) had pulled out. Handmade also bought and released the gangster drama "The Long Good Friday" (1980), produced by a Lew Grade subsidiary, after its original backers became cautious. Members of the Python team were involved in other comedies during the decade, including Terry Gilliam's fantasy films "Time Bandits" (1981) and "Brazil" (1985), and John Cleese's hit "A Fish Called Wanda" (1988), while Michael Palin starred in "A Private Function" (1984), from Alan Bennett's first screenplay for the cinema screen.

Goldcrest producer David Puttnam has been described as "the nearest thing to a mogul that British cinema has had in the last quarter of the 20th century." Under Puttnam, a generation of British directors emerged making popular films with international distribution. Some of the talent backed by Puttnam — Hugh Hudson, Ridley Scott, Alan Parker, and Adrian Lyne — had shot commercials; Puttnam himself had begun his career in the advertising industry. When Hudson's "Chariots of Fire" (1981) won 4 Academy Awards in 1982, including Best Picture, its writer Colin Welland declared "the British are coming!". When "Gandhi" (1982), another Goldcrest film, picked up a Best Picture Oscar, it looked as if he was right.

It prompted a cycle of period films – some with a large budget for a British film, such as David Lean's final film "A Passage to India" (1984), alongside the lower-budget Merchant Ivory adaptations of the works of E. M. Forster, such as "A Room with a View" (1986). But further attempts to make 'big' productions for the US market ended in failure, with Goldcrest losing its independence after "Revolution" (1985) and "Absolute Beginners" (1986) were commercial and critical flops. Another Goldcrest film, Roland Joffé's "The Mission" (also 1986), won the 1986 Palme d'Or, but did not go into profit either. Joffé's earlier "The Killing Fields" (1984) had been both a critical and financial success. These were Joffé's first two feature films and were amongst those produced by Puttnam.

Mainly outside the commercial sector, film makers from the new commonwealth countries had begun to emerge during the 1970s. Horace Ové's "Pressure" (1975) had been funded by the British Film Institute as was "A Private Enterprise" (1974), these being the first Black British and Asian British films, respectively. The 1980s however saw a wave of new talent, with films such as Franco Rosso's "Babylon" (1980), Menelik Shabazz's "Burning an Illusion" (1981) and Po-Chih Leong's "Ping Pong" (1986; one of the first films about Britain's Chinese community). Many of these films were assisted by the newly formed Channel 4, which had an official remit to provide for "minority audiences." Commercial success was first achieved with "My Beautiful Laundrette" (1985). Dealing with racial and gay issues, it was developed from Hanif Kureishi's first film script. "My Beautiful Laundrette" features Daniel Day-Lewis in a leading role. Day-Lewis and other young British actors who were becoming stars, such as Gary Oldman, Colin Firth, Tim Roth and Rupert Everett, were dubbed the Brit Pack.

With the involvement of Channel 4 in film production, talents from television moved into feature films with Stephen Frears ("My Beautiful Laundrette") and Mike Newell with "Dance with a Stranger" (1985). John Boorman, who had been working in the US, was encouraged back to the UK to make "Hope and Glory" (1987). Channel Four also became a major sponsor of the British Film Institute's Production Board, which backed three of Britain's most critically acclaimed filmmakers: Derek Jarman ("The Last of England", 1987), Terence Davies ("Distant Voices, Still Lives", 1988), and Peter Greenaway; the latter of whom gained surprising commercial success with "The Draughtsman's Contract" (1982) and "The Cook, the Thief, His Wife & Her Lover" (1989). Stephen Woolley's company Palace Pictures also produced some successful films, including Neil Jordan's "The Company of Wolves" (1984) and "Mona Lisa" (1986), before collapsing amid a series of unsuccessful films. Amongst the other British films of the decade were Bill Forsyth's "Gregory's Girl" (1981) and "Local Hero" (1983), Lewis Gilbert's "Educating Rita" (1983), Peter Yates' "The Dresser" (1983) and Kenneth Branagh's directorial debut, "Henry V" (1989).

Compared to the 1980s, investment in film production rose dramatically. In 1989, annual investment was a meagre £104 million. By 1996, this figure had soared to £741 million. Nevertheless, the dependence on finance from television broadcasters such as the BBC and Channel 4 meant that budgets were often low and indigenous production was very fragmented: the film industry mostly relied on Hollywood inward investment. According to critic Neil Watson, it was hoped that the £90 million apportioned by the new National Lottery into three franchises (The Film Consortium, Pathe Pictures, and DNA) would fill the gap, but "corporate and equity finance for the UK film production industry continues to be thin on the ground and most production companies operating in the sector remain hopelessly under-capitalised."

These problems were mostly compensated by PolyGram Filmed Entertainment, a film studio whose British subsidiary Working Title Films released a Richard Curtis-scripted comedy "Four Weddings and a Funeral" (1994). It grossed $244 million worldwide and introduced Hugh Grant to global fame, led to renewed interest and investment in British films, and set a pattern for British-set romantic comedies, including "Sliding Doors" (1998) and "Notting Hill" (1999). Other Working Titles films included "Bean" (1997), "Elizabeth" (1998) and "Captain Corelli's Mandolin" (2001). PFE was eventually sold and merged with Universal Pictures in 1999, the hopes and expectations of "building a British-based company which could compete with Hollywood in its home market [had] eventually collapsed."

Tax incentives allowed American producers to increasingly invest in UK-based film production throughout the 1990s, including films such as "Interview with the Vampire" (1994), "" (1996), "Saving Private Ryan" (1998), "" (1999) and "The Mummy" (1999). Miramax also distributed Neil Jordan's acclaimed thriller "The Crying Game" (1992), which was generally ignored on its initial release in the UK, but was a considerable success in the United States. The same company also enjoyed some success releasing the BBC period drama "Enchanted April" (1992) and "The Wings of the Dove" (1997).

Among the more successful British films were the Merchant Ivory productions "Howards End" (1992) and "The Remains of the Day" (1993), Richard Attenborough's "Shadowlands" (1993), and Kenneth Branagh's Shakespeare adaptations. "The Madness of King George" (1994) proved there was still a market for British costume dramas, and other period films followed, including "Sense and Sensibility" (1995), "Restoration" (1995), "Emma" (1996), "Mrs. Brown" (1997), "Basil" (1998), "Shakespeare in Love" (1998) and "Topsy-Turvy" (1999).

After a six-year hiatus for legal reasons the James Bond films returned to production with the 17th Bond film, "GoldenEye". With their traditional home Pinewood Studios fully booked, a new studio was created for the film in a former Rolls-Royce aero-engine factory at Leavesden in Hertfordshire.

Mike Leigh emerged as a significant figure in British cinema in the 1990s, with a series of films financed by Channel 4 about working and middle class life in modern England, including "Life Is Sweet" (1991), "Naked" (1993) and his biggest hit "Secrets & Lies" (1996), which won the Palme d'Or at Cannes.

Other new talents to emerge during the decade included the writer-director-producer team of John Hodge, Danny Boyle and Andrew Macdonald responsible for "Shallow Grave" (1994) and "Trainspotting" (1996). The latter film generated interested in other "regional" productions, including the Scottish films "Small Faces" (1996), "Ratcatcher" (1999) and "My Name Is Joe" (1998).

The first decade of the 21st century was a relatively successful one for the British film industry. Many British films found a wide international audience due to funding from BBC Films, Film 4 and the UK Film Council, and some independent production companies, such as Working Title, secured financing and distribution deals with major American studios. Working Title scored three major international successes, all starring Hugh Grant and Colin Firth, with the romantic comedies "Bridget Jones's Diary" (2001), which grossed $254 million worldwide; the sequel "", which earned $228 million; and Richard Curtis's directorial debut "Love Actually" (2003), which grossed $239 million. Most successful of all, Phyllida Lloyd's "Mamma Mia!" (2008), which grossed $601 million.

The new decade saw a major new film series in the Harry Potter films, beginning with "Harry Potter and the Philosopher's Stone" in 2001. David Heyman's company Heyday Films has produced seven sequels, with the final title released in two parts – "Harry Potter and the Deathly Hallows – Part 1" in 2010 and "Harry Potter and the Deathly Hallows – Part 2" in 2011. All were filmed at Leavesden Studios in England.

Aardman Animations' Nick Park, the creator of Wallace and Gromit and the Creature Comforts series, produced his first feature-length film, "Chicken Run" in 2000. Co-directed with Peter Lord, the film was a major success worldwide and one of the most successful British films of its year. Park's follow up, "" was another worldwide hit: it grossed $56 million at the US box office and £32 million in the UK. It also won the 2005 Academy Award for Best Animated Feature.

However it was usually through domestically funded features throughout the decade that British directors and films won awards at the top international film festivals. In 2003, Michael Winterbottom won the Golden Bear at the Berlin Film Festival for "In This World". In 2004, Mike Leigh directed "Vera Drake", an account of a housewife who leads a double life as an abortionist in 1950s London. The film won the Golden Lion at the Venice Film Festival. In 2006 Stephen Frears directed "The Queen" based on the events surrounding the death of Princess Diana, which won the Best Actress prize at the Venice Film Festival and Academy Awards and the BAFTA for Best Film. In 2006, Ken Loach won the Palme d'Or at the Cannes Film Festival with his account of the struggle for Irish Independence in "The Wind That Shakes the Barley". Joe Wright's adaptation of the Ian McEwan novel "Atonement" was nominated for 7 Academy Awards, including Best Film and won the Golden Globe and BAFTA for Best Film. "Slumdog Millionaire" was filmed entirely in Mumbai with a mostly Indian cast, though with a British director (Danny Boyle), producer (Christian Colson), screenwriter (Simon Beaufoy) and star (Dev Patel)—the film was all-British financed via Film4 and Celador. It has received worldwide critical acclaim. It has won four Golden Globes, seven BAFTA Awards and eight Academy Awards, including Best Director and Best Film. "The King's Speech", which tells the story of King George VI's attempts to overcome his speech impediment, was directed by Tom Hooper and filmed almost entirely in London. It received four Academy Awards (including Best Film, Best Director, Best Actor and Best Screenplay) in 2011.

The start of the 21st century saw Asian British cinema assert itself at the box office, starting with "East Is East" (1999) and continuing with "Bend It Like Beckham" (2002). Other notable British Asian films from this period include "My Son the Fanatic" (1997), "Ae Fond Kiss... (2004)", "Mischief Night (2006)", "Yasmin" (2004) and "Four Lions" (2010). Some argue it has brought more flexible attitudes towards casting Black and Asian British actors, with Robbie Gee and Naomie Harris take leading roles in "Underworld" and "28 Days Later" respectively. The year 2005 saw the emergence of The British Urban Film Festival, a timely addition to the film festival calendar, which recognised the influence of "Kidulthood" on UK audiences and consequently began to showcase a growing profile of films in a genre previously not otherwise regularly seen in the capital's cinemas. Then in 2005 "Kidulthood", a film centring on inner-city London youth had a limited release. This was successfully followed up with a sequel "Adulthood" (2008) that was written and directed by actor Noel Clarke. Several other films dealing with inner city issues and Black Britons were released in the 2000s such as "Bullet Boy" (2004), "Life and Lyrics" (2006) and "Rollin' with the Nines" (2009).

Like the 1960s, this decade saw plenty of British films directed by imported talent. The American Woody Allen shot "Match Point" (2005) and three later films in London. The Mexican director Alfonso Cuarón helmed "Harry Potter and the Prisoner of Azkaban" (2004) and "Children of Men" (2006); New Zealand filmmaker Jane Campion made "Bright Star" (2009), a film set in 19th century London; Danish director Nicolas Winding Refn made "Bronson" (2008), a biopic about the English criminal Michael Gordon Peterson; the Spanish filmmaker Juan Carlos Fresnadillo directed "28 Weeks Later" (2007), a sequel to a British horror film; and two John le Carré adaptations were also directed by foreigners—"The Constant Gardener" by the Brazilian Fernando Meirelles and "Tinker Tailor Soldier Spy" by the Swedish Tomas Alfredson. The decade also saw English actor Daniel Craig became the new James Bond with "Casino Royale", the 21st entry in the official Eon Productions series.

Despite increasing competition from film studios in Australia and Eastern Europe, British studios such as Pinewood, Shepperton and Leavesden remained successful in hosting major productions, including "Finding Neverland", "Closer", "Batman Begins", "Charlie and the Chocolate Factory", "United 93", "The Phantom of the Opera", "", "Fantastic Mr. Fox", "Robin Hood", "", "Hugo" and "War Horse".

In November 2010, Warner Bros. completed the acquisition of Leavesden Film Studios, becoming the first Hollywood studio since the 1940s to have a permanent base in the UK, and announced plans to invest £100 million in the site.

A study by the British Film Institute published in December 2013 found that of the 613 tracked British films released between 2003 and 2010 only 7% made a profit. Films with low budgets, those that cost below £500,000 to produce, were even less likely to gain a return on outlay. Of these films, only 3.1% went into the black. At the top end of budgets for the British industry, under a fifth of films that cost £10million went into profit.

On 26 July 2010 it was announced that the UK Film Council, which was the main body responsible for the development of promotion of British cinema during the 2000s, would be abolished, with many of the abolished body's functions being taken over by the British Film Institute. Actors and professionals, including James McAvoy, Emily Blunt, Pete Postlethwaite, Damian Lewis, Timothy Spall, Daniel Barber and Ian Holm, campaigned against the Council's abolition. The move also led American actor and director Clint Eastwood (who had filmed "Hereafter" in London) to write to the British Chancellor of the Exchequer George Osborne in August 2010 to protest the decision to close the Council. Eastwood warned Osborne that the closure could result in fewer foreign production companies choosing to work in the UK. A grass-roots online campaign was launched and a petition established by supporters of the Council.

Countering this, a few professionals, including Michael Winner and Julian Fellowes, supported the Government's decision. A number of other organisations responded positively.

At the closure of the UK Film Council on 31 March 2011, "The Guardian" reported that "The UKFC's entire annual budget was a reported £3m, while the cost of closing it down and restructuring is estimated to have been almost four times that amount." One of the UKFC's last films, "The King's Speech", is estimated to have cost $15m to make and grossed $235m, besides winning several Academy Awards. UKFC invested $1.6m for a 34% share of net profits, a valuable stake that will pass to the British Film Institute.

In April 2011, The Peel Group acquired a controlling 71% interest in The Pinewood Studios Group (the owner of Pinewood Studios and Shepperton Studios) for £96 million. In June 2012, Warner opened the re-developed Leavesden studio for business. The most commercially successful British directors in recent years are Paul Greengrass, Mike Newell, Christopher Nolan, Ridley Scott and David Yates.

In January 2012, at Pinewood Studios to visit film-related businesses, UK Prime Minister David Cameron said that his government had bold ambitions for the film industry: "Our role, and that of the BFI, should be to support the sector in becoming even more dynamic and entrepreneurial, helping UK producers to make commercially successful pictures that rival the quality and impact of the best international productions. Just as the British Film Commission has played a crucial role in attracting the biggest and best international studios to produce their films here, so we must incentivise UK producers to chase new markets both here and overseas."

The film industry remains an important earner for the British economy. According to a UK Film Council press release of 20 January 2011, £1.115 billion was spent on UK film production during 2010. A 2014 survey suggested that British-made films were generally more highly rated than Hollywood productions, especially when considering low-budget UK productions.

Although it had been funding British experimental films as early as 1952, the British Film Institute's foundation of a production board in 1964—and a substantial increase in public funding from 1971 onwards—enabled it to become a dominant force in developing British art cinema in the 1970s and 80s: from the first of Bill Douglas's Trilogy "My Childhood" (1972), and of Terence Davies' Trilogy "Childhood" (1978), via Peter Greenaway's earliest films (including the surprising commercial success of "The Draughtsman's Contract" (1982)) and Derek Jarman's championing of the New Queer Cinema. The first full-length feature produced under the BFI's new scheme was Kevin Brownlow and Andrew Mollo's "Winstanley" (1975), while others included "Moon Over the Alley" (1975), "Requiem for a Village" (1975), the openly avant-garde "Central Bazaar" (1973), "Pressure" (1975) and "A Private Enterprise" (1974) -- the last two being, respectively, the first British Black and Asian features.

The release of Derek Jarman's "Jubilee" (1978) marked the beginning of a successful period of UK art cinema, continuing into the 1980s with filmmakers like Sally Potter. Unlike the previous generation of British film makers who had broken into directing and production after careers in the theatre or on television, the Art Cinema Directors were mostly the products of Art Schools. Many of these filmmakers were championed in their early career by the London Film Makers Cooperative and their work was the subject of detailed theoretical analysis in the journal "Screen Education". Peter Greenaway was an early pioneer of the use of computer generated imagery blended with filmed footage and was also one of the first directors to film entirely on high definition video for a cinema release.

With the launch of Channel 4 and its Film on Four commissioning strand, Art Cinema was promoted to a wider audience. However the Channel had a sharp change in its commissioning policy in the early 1990s and Greenaway and others were forced to seek European co-production financing.

In the 1970s and 1980s, British studios established a reputation for great special effects in films such as "Superman" (1978), "Alien" (1979), and "Batman" (1989). Some of this reputation was founded on the core of talent brought together for the filming of "" (1968) who subsequently worked together on series and feature films for Gerry Anderson. Thanks to the Bristol-based Aardman Animations, the UK is still recognised as a world leader in the use of stop-motion animation.

British special effects technicians and production designers are known for creating visual effects at a far lower cost than their counterparts in the US, as seen in "Time Bandits" (1981) and "Brazil" (1985). This reputation has continued through the 1990s and into the 21st century with films such as the James Bond series, "Gladiator" (2000) and the Harry Potter franchise.

From the 1990s to the present day, there has been a progressive movement from traditional film opticals to an integrated digital film environment, with special effects, cutting, colour grading, and other post-production tasks all sharing the same all-digital infrastructure. The London-based visual effects company Framestore, with Tim Webber the visual effects supervisor, have worked on some of the most technically and artistically challenging projects, including, "The Dark Knight" (2008) and "Gravity" (2013), with new techniques involved in "Gravity" realized by Webber and the Framestore team taking three years to complete.

The availability of high-speed Internet Protocol networks has made the British film industry capable of working closely with U.S. studios as part of globally distributed productions. As of 2005, this trend is expected to continue with moves towards (currently experimental) digital distribution and projection as mainstream technologies. The British film "This is Not a Love Song" (2003) was the first to be streamed live on the Internet at the same time as its cinema premiere.










</doc>
<doc id="10796" url="https://en.wikipedia.org/wiki?curid=10796" title="Feminist film theory">
Feminist film theory

Feminist film theory is a theoretical film criticism derived from feminist politics and feminist theory. Feminists have many approaches to cinema analysis, regarding the film elements analyzed and their theoretical underpinnings.

The development of feminist film theory was influenced by second wave feminism and women's studies in the 1960s and 1970s. Initially in the United States in the early 1970s feminist film theory was generally based on sociological theory and focused on the function of female characters in film narratives or genres. Feminist film theory, such as Marjorie Rosen’s "Popcorn Venus: Women, Movies, and the American Dream" (1973) and Molly Haskell’s "From Reverence to Rape: The Treatment of Women in Movies" (1974) analyze the ways in which women are portrayed in film, and how this relates to a broader historical context. Additionally, feminist critiques also examine common stereotypes depicted in film, the extent to which the women were shown as active or passive, and the amount of screen time given to women.

In contrast, film theoreticians in England concerned themselves with critical theory, psychoanalysis, semiotics, and Marxism. Eventually, these ideas gained hold within the American scholarly community in the 1980's. Analysis generally focused on the meaning within a film's text and the way in which the text constructs a viewing subject. It also examined how the process of cinematic production affects how women are represented and reinforces sexism.

British feminist film theorist, Laura Mulvey, best known for her essay, "Visual Pleasure and Narrative Cinema", written in 1973 and published in 1975 in the influential British film theory journal, "Screen" was influenced by the theories of Sigmund Freud and Jacques Lacan. "Visual Pleasure" is one of the first major essays that helped shift the orientation of film theory towards a psychoanalytic framework. Prior to Mulvey, film theorists such as Jean-Louis Baudry and Christian Metz used psychoanalytic ideas in their theoretical accounts of cinema. Mulvey's contribution, however, initiated the intersection of film theory, psychoanalysis and feminism.

Other key influences come from Metz's essay "The Imaginary Signifier", "Identification, Mirror," where he argues that viewing film is only possible through scopophilia (pleasure from looking, related to voyeurism), which is best exemplified in silent film. Also, according to Cynthia A. Freeland in "Feminist Frameworks for Horror Films," feminist studies of horror films have focused on psychodynamics where the chief interest is "on viewers' motives and interests in watching horror films".

Beginning in the early 1980s feminist film theory began to look at film through a more intersectional lens. The film journal "Jump Cut" published a special issue about titled "Lesbians and Film" in 1981 which examined the lack of lesbian identities in film. Jane Gaines's essay "White Privilege and Looking Relations: Race and Gender in Feminist Film Theory" examined the erasure of black women in cinema by white male filmmakers. While Lola Young argues that filmmakers of all races fail to break away from the use to tired stereotypes when depicting black women. Other theorists who wrote about feminist film theory and race include bell hooks and Michele Wallace.

From the 1990 onward the Matrixial theory of artist and psychoanalyst Bracha L. Ettinger revolutionized feminist film theory., 

Recently, scholars have expanded their work to include analysis of television and digital media. Additionally, they have begun to explore notions of difference, engaging in dialogue about the differences among women (part of movement away from essentialism in feminist work more generally), the various methodologies and perspectives contained under the umbrella of feminist film theory, and the multiplicity of methods and intended effects that influence the development of films. Scholars are also taking increasingly global perspectives, responding to postcolonialist criticisms of perceived Anglo- and Eurocentrism in the academy more generally. Increased focus has been given to, "disparate feminisms, nationalisms, and media in various locations and across class, racial, and ethnic groups throughout the world".

Considering the way that films are put together, many feminist film critics have pointed to what they argue is the "male gaze" that predominates classical Hollywood filmmaking. Budd Boetticher summarizes the view:
Laura Mulvey expands on this conception to argue that in cinema, women are typically depicted in a passive role that provides visual pleasure through scopophilia, and identification with the on-screen male actor. She asserts: "In their traditional exhibitionist role women are simultaneously looked at and displayed, with their appearance coded for strong visual and erotic impact so that they can be said to connote "to-be-looked-at-ness"," and as a result contends that in film a woman is the "bearer of meaning, not maker of meaning." Mulvey argues that the psychoanalytic theory of Jacques Lacan is the key to understanding how film creates such a space for female sexual objectification and exploitation through the combination of the patriarchal order of society, and 'looking' in itself as a pleasurable act of scopophilia, as "the cinema satisfies a primordial wish for pleasurable looking."

While Laura Mulvey's paper has a particular place in the feminist film theory, it is important to note that her ideas regarding ways of watching the cinema (from the voyeuristic element to the feelings of identification) are important to some feminist film theorists in terms of defining spectatorship from the psychoanalytical viewpoint.

Mulvey identifies three "looks" or perspectives that occur in film which, she argues, serve to sexually objectify women. The first is the perspective of the male character and how he perceives the female character. The second is the perspective of the spectator as they see the female character on screen. The third "look" joins the first two looks together: it is the male audience member's perspective of the male character in the film. This third perspective allows the male audience to take the female character as his own personal sex object because he can relate himself, through looking, to the male character in the film.

In the paper, Mulvey calls for a destruction of modern film structure as the only way to free women from their sexual objectification in film. She argues for a removal of the voyeurism encoded into film by creating distance between the male spectator and the female character. The only way to do so, Mulvey argues, is by destroying the element of voyeurism and "the invisible guest". Mulvey also asserts that the dominance men embody is only so because women exist, as without a woman for comparison, a man and his supremacy as the controller of visual pleasure are insignificant. For Mulvey, it is the presence of the female that defines the patriarchal order of society as well as the male psychology of thought.

Mulvey's argument is likely influenced by the time period in which she was writing. "Visual Pleasure and Narrative Cinema" was composed during the period of second-wave feminism, which was concerned with achieving equality for women in the workplace, and with exploring the psychological implications of sexual stereotypes. Mulvey calls for an eradication of female sexual objectivity, aligning herself with second-wave feminism. She argues that in order for women to be equally represented in the workplace, women must be portrayed as men are: as lacking sexual objectification.

Mulvey proposes in her notes to the Criterion Collection DVD of Michael Powell's controversial film, "Peeping Tom" (a film about a homicidal voyeur who films the deaths of his victims), that the cinema spectator’s own voyeurism is made shockingly obvious and even more shockingly, the spectator identifies with the perverted protagonist. The inference is that she includes female spectators in that, identifying with the male observer rather than the female object of the gaze.

The early work of Marjorie Rosen and Molly Haskell on the representation of women in film was part of a movement to depict women more realistically, both in documentaries and narrative cinema. The growing female presence in the film industry was seen as a positive step toward realizing this goal, by drawing attention to feminist issues and putting forth an alternative, true-to-life view of women. However, Rosen and Haskell argue that these images are still mediated by the same factors as traditional film, such as the "moving camera, composition, editing, lighting, and all varieties of sound." While acknowledging the value in inserting positive representations of women in film, some critics asserted that real change would only come about from reconsidering the role of film in society, often from a semiotic point of view.

Claire Johnston put forth the idea that women's cinema can function as "counter cinema." Through consciousness of the means of production and opposition of sexist ideologies, films made by women have the potential to posit an alternative to traditional Hollywood films. Initially, the attempt to show "real" women was praised, eventually critics such as Eileen McGarry claimed that the "real" women being shown on screen were still just contrived depictions. In reaction to this article, many women filmmakers integrated "alternative forms and experimental techniques" to "encourage audiences to critique the seemingly transparent images on the screen and to question the manipulative techniques of filming and editing".

B. Ruby Rich argues that feminist film theory should shift to look at films in a broader sense. Rich's essay "In the Name of Feminist Film Criticism" claims that films by women often receive praise for certain elements, while feminist undertones are ignored. Rich goes on to say that because of this feminist theory needs to focus on how film by women are being received.

Coming from a black feminist perspective, American scholar, Bell Hooks, put forth the notion of the “oppositional gaze,” encouraging black women not to accept stereotypical representations in film, but rather actively critique them. The “oppositional gaze” is a response to Mulvey's "visual pleasure" and states that just as women do not identify with female characters that are not "real," women of color should respond similarly to the one denominational caricatures of black women.
Janet Bergstrom’s article “Enunciation and Sexual Difference” (1979) uses Sigmund Freud’s ideas of bisexual responses, arguing that women are capable of identifying with male characters and men with women characters, either successively or simultaneously. Miriam Hansen, in "Pleasure, Ambivalence, Identification: Valentino and Female Spectatorship" (1984) put forth the idea that women are also able to view male characters as erotic objects of desire. In "The Master's Dollhouse: Rear Window," Tania Modleski argues that Hitchcock's film, "Rear Window", is an example of the power of male gazer and the position of the female as a prisoner of the "master's dollhouse".

Carol Clover, in her popular and influential book, "Men, Women, and Chainsaws: Gender in the Modern Horror Film" (Princeton University Press, 1992), argues that young male viewers of the Horror Genre (young males being the primary demographic) are quite prepared to identify with the female-in-jeopardy, a key component of the horror narrative, and to identify on an unexpectedly profound level. Clover further argues that the "Final Girl" in the psychosexual subgenre of exploitation horror invariably triumphs through her own resourcefulness, and is not by any means a passive, or inevitable, victim. Laura Mulvey, in response to these and other criticisms, revisited the topic in "Afterthoughts on 'Visual Pleasure and Narrative Cinema' inspired by "Duel in the Sun"" (1981). In addressing the heterosexual female spectator, she revised her stance to argue that women can take two possible roles in relation to film: a masochistic identification with the female object of desire that is ultimately self-defeating or a transsexual identification with men as the active viewers of the text. A new version of the gaze was offered in the early 1990s by Bracha Ettinger, who proposed the notion of the "matrixial gaze".





</doc>
<doc id="10798" url="https://en.wikipedia.org/wiki?curid=10798" title="Formalist film theory">
Formalist film theory

Formalist film theory is a theory of film study that is focused on the formal, or technical, elements of a film: i.e., the lighting, scoring, sound and set design, use of color, shot composition, and editing. This approach was proposed by Hugo Münsterberg, Rudolf Arnheim, Sergei Eisenstein, and Béla Balázs. Today, it is a major theory of film study.

Formalism, at its most general, considers the synthesis (or lack of synthesis) of the multiple elements of film production, and the effects, emotional and intellectual, of that synthesis and of the individual elements. For example, take the single element of editing. A formalist might study how standard Hollywood "continuity editing" creates a more comforting effect and non-continuity or jump cut editing might become more disconcerting or volatile.

Or one might consider the synthesis of several elements, such as editing, shot composition, and music. The shoot-out that ends Sergio Leone's Spaghetti Western "Dollars" trilogy is a notable example of how these elements work together to produce an effect: The shot selection goes from very wide to very close and tense; the length of shots decreases as the sequence progresses towards its end; the music builds. All of these elements, in combination rather than individually, create tension. 

Formalism is unique in that it embraces both ideological and auteurist branches of criticism. In both these cases, the common denominator for Formalist criticism is style. Ideologues focus on how socio-economic pressures create a particular style, and auteurists on how auteurs put their own stamp on the material. Formalism is primarily concerned with style and how it communicates ideas, emotions, and themes (rather than, as critics of formalism point out, concentrating on the themes of a work itself).

Two examples of ideological interpretations that are related to formalism:

The classical Hollywood cinema has a very distinct style, sometimes called the institutional mode of representation: continuity editing, massive coverage, three-point lighting, "mood" music, dissolves, all designed to make the experience as pleasant as possible. The socio-economic ideological explanation for this is, quite crassly, that Hollywood wants to make as much money and appeal to as many ticket-buyers as possible.

Film noir, which was given its name by Nino Frank, is marked by lower production values, darker images, under lighting, location shooting, and general nihilism: this is because, we are told, during the war and post-war years filmmakers (as well as filmgoers) were generally more pessimistic. Also, the German Expressionists (including Fritz Lang, who was not technically an expressionist as popularly believed) emigrated to America and brought their stylized lighting effects (and disillusionment due to the war) to American soil.

It can be argued that, by this approach, the style or 'language' of these films is directly affected not by the individuals responsible, but by social, economic, and political pressures, of which the filmmakers themselves may be aware or not. It is this branch of criticism that gives us such categories as the classical Hollywood cinema, the American independent movement, the New American independent movement, the new queer cinema, and the French, German, and Czech new waves.

If the ideological approach is concerned with broad movements and the effects of the world around the filmmaker, then the auteur theory is diametrical to it, celebrating the individual, usually in the person of the filmmaker, and how his/her personal decisions, thoughts, and style manifest themselves in the material.

This branch of criticism, begun by François Truffaut and the other young film critics writing for "Cahiers du cinéma", was created for two reasons.

First, it was created to redeem the art of film itself. By arguing that films had auteurs, or authors, Truffaut sought to make films (and their directors) at least as important as the more widely accepted art forms, such as literature, music, and painting. Each of these art forms, and the criticism thereof, is primarily concerned with a sole creative force: the author of a novel (not, for example, his editor or type-setter), the composer of a piece of music (though sometimes the performers are given credence, akin to actors in film today), or the painter of a fresco (not his assistants who mix the colours or often do some of the painting themselves). By elevating the director, and not the screenwriter, to the same importance as novelists, composers, or painters, it sought to free the cinema from its popular conception as a bastard art, somewhere between theater and literature.

Secondly, it sought to redeem many filmmakers who were looked down upon by mainstream film critics. It argued that genre filmmakers and low-budget B-movies were just as important, if not more, than the prestige pictures commonly given more press and legitimacy in France and the United States. According to Truffaut's theory, auteurs took material that was beneath their talents—a thriller, a pulpy action film, a romance—and, through their style, put their own personal stamp on it.

It is this auteur style that concerns formalism.

A perfect example of formalist criticism of auteur style would be the work of Alfred Hitchcock. Hitchcock primarily made thrillers, which, according to the "Cahiers du cinema" crowd, were popular with the public but were dismissed by the critics and the award ceremonies, although Hitchcock's "Rebecca" won the Oscar for Best Picture at the 1940 Academy Awards. Though he never won the Oscar for directing, he was nominated five times in the category. Truffaut and his colleagues argued that Hitchcock had a style as distinct as that of Flaubert or Van Gogh: the virtuoso editing, the lyrical camera movements, the droll humour. He also had "Hitchcockian" themes: the wrong man falsely accused, violence erupting at the times it was least expected, the cool blonde. Now, Hitchcock is more or less universally lauded, his films dissected shot-by-shot, his work celebrated as being that of a master. And the study of this style, his variations, and obsessions all fall quite neatly under the umbrella of formalist film theory.




</doc>
<doc id="10799" url="https://en.wikipedia.org/wiki?curid=10799" title="Film theory">
Film theory

Film theory is a set of scholarly approaches within the academic discipline of film or cinema studies that questions the essentialism of cinema and provides conceptual frameworks for understanding film's relationship to reality, the other arts, individual viewers, and society at large. Film theory is not to be confused with general film criticism, or film history, though these three disciplines interrelate.

Although film theory originated from linguistics and literary theory, it also overlaps with the philosophy of film.

French philosopher Henri Bergson's "Matter and Memory" (1896) has been cited as anticipating the development of film theory during the birth of cinema. Bergson commented on the need for new ways of thinking about movement, and coined the terms "the movement-image" and "the time-image". However, in his 1906 essay "L'illusion cinématographique" (in "L'évolution créatrice"; English: "The cinematic illusion") in "Creative", he rejects film as an exemplification of what he had in mind. Nonetheless, decades later, in "Cinéma I and Cinema II" (1983–1985), the philosopher Gilles Deleuze took "Matter and Memory" as the basis of his philosophy of film and revisited Bergson's concepts, combining them with the semiotics of Charles Sanders Peirce.

Early film theory arose in the silent era and was mostly concerned with defining the crucial elements of the medium. It largely evolved from the works of directors like Germaine Dulac, Louis Delluc, Jean Epstein, Sergei Eisenstein, Lev Kuleshov, and Dziga Vertov and film theorists like Rudolf Arnheim, Béla Balázs and Siegfried Kracauer. These thinkers emphasized how film differed from reality and how it might be considered a valid art form. In the years after World War II, the French film critic and theorist André Bazin reacted against this approach to the cinema, arguing that film's essence lay in its ability to mechanically reproduce reality, not in its difference from reality.

In the 1960s and 1970s, film theory took up residence in academia importing concepts from established disciplines like psychoanalysis, gender studies, anthropology, literary theory, semiotics and linguistics. However, not until the late 1980s or early 1990s did film theory "per se" achieve much prominence in American universities by displacing the prevailing humanistic, auteur theory that had dominated cinema studies and which had been focused on the practical elements of film writing, production, editing and criticism. American scholar David Bordwell has spoken against many prominent developments in film theory since the 1970s, i.e., he uses the derogatory term "SLAB theory" to refer to film studies based on the ideas of Saussure, Lacan, Althusser, and Barthes. Instead, Bordwell promotes what he describes as "neoformalism" (a revival of formalist film theory).

During the 1990s the digital revolution in image technologies has influenced film theory in various ways. There has been a refocus onto celluloid film's ability to capture an "indexical" image of a moment in time by theorists like Mary Ann Doane, Philip Rosen and Laura Mulvey who was informed by psychoanalysis. From a psychoanalytical perspective, after the Lacanian notion of "the Real", Slavoj Žižek offered new aspects of "the gaze" extensively used in contemporary film analysis. From the 1990 onward the Matrixial theory of artist and psychoanalyst Bracha L. Ettinger revolutionized feminist film theory. Her concept The Matrixial Gaze, that has established a feminine gaze and has articulated its differences from the phallic gaze and its relation to feminine as well as maternal specificities and potentialities of "coemergence", offering a critique of Sigmund Freud's and Jacques Lacan's psychoanalysis, is extensively used in analysis of films by female authors, like Chantal Akerman, as well as by male authors, like Pedro Almodovar. The matrixial gaze offers the female the position of a subject, not of an object, of the gaze, while deconstructing the structure of the subject itself, and offers border-time, border-space and a possibility for compassion and witnessing. Ettinger's notions articulate the links between aesthetics, ethics and trauma. There has also been a historical revisiting of early cinema screenings, practices and spectatorship modes by writers Tom Gunning, Miriam Hansen and Yuri Tsivian.

In "Critical Cinema: Beyond the Theory of Practice" (2011), Clive Meyer suggests that 'cinema is a different experience to watching a film at home or in an art gallery', and argues for film theorists to re-engage the specificity of philosophical concepts for cinema as a medium distinct from others.




</doc>
<doc id="10802" url="https://en.wikipedia.org/wiki?curid=10802" title="Film noir">
Film noir

Film noir (; ) is a cinematic term used primarily to describe stylish Hollywood crime dramas, particularly those that emphasize cynical attitudes and sexual motivations. Hollywood's classical film noir period is generally regarded as extending from the early 1920s to the late 1950s. Film noir of this era is associated with a low-key, black-and-white visual style that has roots in German Expressionist cinematography. Many of the prototypical stories and much of the attitude of classic noir derive from the hardboiled school of crime fiction that emerged in the United States during the Great Depression.

The term "film noir", French for "black film" (literal) or "dark film" (closer meaning), was first applied to Hollywood films by French critic Nino Frank in 1946, but was unrecognized by most American film industry professionals of that era. Cinema historians and critics defined the category retrospectively. Before the notion was widely adopted in the 1970s, many of the classic film noir were referred to as "melodramas". Whether film noir qualifies as a distinct genre is a matter of ongoing debate among scholars.

Film noir encompasses a range of plots: the central figure may be a private investigator ("The Big Sleep"), a plainclothes policeman ("The Big Heat"), an aging boxer ("The Set-Up"), a hapless grifter ("Night and the City"), a law-abiding citizen lured into a life of crime ("Gun Crazy"), or simply a victim of circumstance ("D.O.A."). Although film noir was originally associated with American productions, the term has been used to describe films from around the world. Many films released from the 1960s onward share attributes with film noirs of the classical period, and often treat its conventions self-referentially. Some refer to such latter-day works as neo-noir. The clichés of film noir have inspired parody since the mid-1940s.

The questions of what defines film noir, and what sort of category it is, provoke continuing debate. "We'd be oversimplifying things in calling film noir oneiric, strange, erotic, ambivalent, and cruel ..."—this set of attributes constitutes the first of many attempts to define film noir made by French critics and Étienne Chaumeton in their 1955 book "Panorama du film noir américain 1941–1953" ("A Panorama of American Film Noir"), the original and seminal extended treatment of the subject. They emphasize that not every film noir embodies all five attributes in equal measure—one might be more dreamlike; another, particularly brutal. The authors' caveats and repeated efforts at alternative definition have been echoed in subsequent scholarship: in the more than five decades since, there have been innumerable further attempts at definition, yet in the words of cinema historian Mark Bould, film noir remains an "elusive phenomenon ... always just out of reach".

Though film noir is often identified with a visual style, unconventional within a Hollywood context, that emphasizes low-key lighting and unbalanced compositions, films commonly identified as noir evidence a variety of visual approaches, including ones that fit comfortably within the Hollywood mainstream. Film noir similarly embraces a variety of genres, from the gangster film to the police procedural to the gothic romance to the social problem picture—any example of which from the 1940s and 1950s, now seen as noir's classical era, was likely to be described as a melodrama at the time.

While many critics refer to film noir as a genre itself, others argue that it can be no such thing. Foster Hirsch defines a genre as determined by "conventions of narrative structure, characterization, theme, and visual design". Hirsch, as one who has taken the position that film noir is a genre, argues that these elements are present "in abundance". Hirsch notes that there are unifying features of tone, visual style and narrative sufficient to classify noir as a distinct genre.

Others argue that film noir is not a genre. Film noir is often associated with an urban setting, but many classic noirs take place in small towns, suburbia, rural areas, or on the open road; setting, therefore, cannot be its genre determinant, as with the Western. Similarly, while the private eye and the femme fatale are character types conventionally identified with noir, the majority of film noirs feature neither; so there is no character basis for genre designation as with the gangster film. Nor does film noir rely on anything as evident as the monstrous or supernatural elements of the horror film, the speculative leaps of the science fiction film, or the song-and-dance routines of the musical.

An analogous case is that of the screwball comedy, widely accepted by film historians as constituting a "genre": the screwball is defined not by a fundamental attribute, but by a general disposition and a group of elements, some—but rarely and perhaps never all—of which are found in each of the genre's films. Because of the diversity of noir (much greater than that of the screwball comedy), certain scholars in the field, such as film historian Thomas Schatz, treat it as not a genre but a "style". Alain Silver, the most widely published American critic specializing in film noir studies, refers to film noir as a "cycle" and a "phenomenon", even as he argues that it has—like certain genres—a consistent set of visual and thematic codes. Other critics treat film noir as a "mood", characterize it as a "series", or simply address a chosen set of films they regard as belonging to the noir "canon". There is no consensus on the matter.

The aesthetics of film noir are influenced by German Expressionism, an artistic movement of the 1910s and 1920s that involved theater, photography, painting, sculpture and architecture, as well as cinema. The opportunities offered by the booming Hollywood film industry and then the threat of Nazism, led to the emigration of many film artists working in Germany who had been involved in the Expressionist movement or studied with its practitioners. "M" (1931), shot only a few years before director Fritz Lang's departure from Germany, is among the first crime films of the sound era to join a characteristically noirish visual style with a noir-type plot, in which the protagonist is a criminal (as are his most successful pursuers). Directors such as Lang, Robert Siodmak and Michael Curtiz brought a dramatically shadowed lighting style and a psychologically expressive approach to visual composition ("mise-en-scène"), with them to Hollywood, where they made some of the most famous classic noirs.

By 1931, Curtiz had already been in Hollywood for half a decade, making as many as six films a year. Movies of his such as "20,000 Years in Sing Sing" (1932) and "Private Detective 62" (1933) are among the early Hollywood sound films arguably classifiable as noir—scholar Marc Vernet offers the latter as evidence that dating the initiation of film noir to 1940 or any other year is "arbitrary". Expressionism-orientated filmmakers had free stylistic rein in Universal horror pictures such as "Dracula" (1931), "The Mummy" (1932)—the former photographed and the latter directed by the Berlin-trained Karl Freund—and "The Black Cat" (1934), directed by Austrian émigré Edgar G. Ulmer. The Universal horror film that comes closest to noir, in story and sensibility is "The Invisible Man" (1933), directed by Englishman James Whale and photographed by American Arthur Edeson. Edeson later photographed "The Maltese Falcon" (1941), widely regarded as the first major film noir of the classic era.

Josef von Sternberg was directing in Hollywood during the same period. Films of his such as "Shanghai Express" (1932) and "The Devil Is a Woman" (1935), with their hothouse eroticism and baroque visual style, anticipated central elements of classic noir. The commercial and critical success of Sternberg's silent "Underworld" (1927) was largely responsible for spurring a trend of Hollywood gangster films. Successful films in that genre such as "Little Caesar" (1931), "The Public Enemy" (1931) and "Scarface" (1932) demonstrated that there was an audience for crime dramas with morally reprehensible protagonists. An important, possibly influential, cinematic antecedent to classic noir was 1930s French poetic realism, with its romantic, fatalistic attitude and celebration of doomed heroes. The movement's sensibility is mirrored in the Warner Bros. drama "I Am a Fugitive from a Chain Gang" (1932), a forerunner of noir. Among films not considered film noirs, perhaps none had a greater effect on the development of the genre than "Citizen Kane" (1941), directed by Orson Welles. Its visual intricacy and complex, voiceover narrative structure are echoed in dozens of classic film noirs.

Italian neorealism of the 1940s, with its emphasis on quasi-documentary authenticity, was an acknowledged influence on trends that emerged in American noir. "The Lost Weekend" (1945), directed by Billy Wilder, another Vienna-born, Berlin-trained American auteur, tells the story of an alcoholic in a manner evocative of neorealism. It also exemplifies the problem of classification: one of the first American films to be described as a film noir, it has largely disappeared from considerations of the field. Director Jules Dassin of "The Naked City" (1948) pointed to the neorealists as inspiring his use of location photography with non-professional extras. This semidocumentary approach characterized a substantial number of noirs in the late 1940s and early 1950s. Along with neorealism, the style had an American precedent cited by Dassin, in director Henry Hathaway's "The House on 92nd Street" (1945), which demonstrated the parallel influence of the cinematic newsreel.

The primary literary influence on film noir was the hardboiled school of American detective and crime fiction, led in its early years by such writers as Dashiell Hammett (whose first novel, "Red Harvest", was published in 1929) and James M. Cain (whose "The Postman Always Rings Twice" appeared five years later), and popularized in pulp magazines such as "Black Mask". The classic film noirs "The Maltese Falcon" (1941) and "The Glass Key" (1942) were based on novels by Hammett; Cain's novels provided the basis for "Double Indemnity" (1944), "Mildred Pierce" (1945), "The Postman Always Rings Twice" (1946), and "Slightly Scarlet" (1956; adapted from "Love's Lovely Counterfeit"). A decade before the classic era, a story by Hammett was the source for the gangster melodrama "City Streets" (1931), directed by Rouben Mamoulian and photographed by Lee Garmes, who worked regularly with Sternberg. Released the month before Lang's "M", "City Streets" has a claim to being the first major film noir; both its style and story had many noir characteristics.

Raymond Chandler, who debuted as a novelist with "The Big Sleep" in 1939, soon became the most famous author of the hardboiled school. Not only were Chandler's novels turned into major noirs—"Murder, My Sweet" (1944; adapted from "Farewell, My Lovely"), "The Big Sleep" (1946), and "Lady in the Lake" (1947)—he was an important screenwriter in the genre as well, producing the scripts for "Double Indemnity", "The Blue Dahlia" (1946), and "Strangers on a Train" (1951). Where Chandler, like Hammett, centered most of his novels and stories on the character of the private eye, Cain featured less heroic protagonists and focused more on psychological exposition than on crime solving; the Cain approach has come to be identified with a subset of the hardboiled genre dubbed "noir fiction". For much of the 1940s, one of the most prolific and successful authors of this often downbeat brand of suspense tale was Cornell Woolrich (sometimes under the pseudonym George Hopley or William Irish). No writer's published work provided the basis for more film noirs of the classic period than Woolrich's: thirteen in all, including "Black Angel" (1946), "Deadline at Dawn" (1946), and "Fear in the Night" (1947).

Another crucial literary source for film noir was W. R. Burnett, whose first novel to be published was "Little Caesar", in 1929. It was turned into a hit for Warner Bros. in 1931; the following year, Burnett was hired to write dialogue for "Scarface", while "The Beast of the City" (1932) was adapted from one of his stories. At least one important reference work identifies the latter as a film noir despite its early date. Burnett's characteristic narrative approach fell somewhere between that of the quintessential hardboiled writers and their noir fiction compatriots—his protagonists were often heroic in their own way, which happened to be that of the gangster. During the classic era, his work, either as author or screenwriter, was the basis for seven films now widely regarded as film noirs, including three of the most famous: "High Sierra" (1941), "This Gun for Hire" (1942), and "The Asphalt Jungle" (1950).

The 1940s and 1950s are generally regarded as the "classic period" of American "film noir". While "City Streets" and other pre-WWII crime melodramas such as "Fury" (1936) and "You Only Live Once" (1937), both directed by Fritz Lang, are categorized as full-fledged "noir" in Alain Silver and Elizabeth Ward's "film noir" encyclopedia, other critics tend to describe them as "proto-noir" or in similar terms.

The film now most commonly cited as the first "true" "film noir" is "Stranger on the Third Floor" (1940), directed by Latvian-born, Soviet-trained Boris Ingster. Hungarian émigré Peter Lorre—who had starred in Lang's "M"—was top-billed, although he did not play the primary lead. He later played secondary roles in several other formative American noirs. Although modestly budgeted, at the high end of the B movie scale, "Stranger on the Third Floor" still lost its studio, RKO, US$56,000 (), almost a third of its total cost. "Variety" magazine found Ingster's work: "...too studied and when original, lacks the flare to hold attention. It's a film too arty for average audiences, and too humdrum for others." "Stranger on the Third Floor" was not recognized as the beginning of a trend, let alone a new genre, for many decades.
Most film noirs of the classic period were similarly low- and modestly-budgeted features without major stars—B movies either literally or in spirit. In this production context, writers, directors, cinematographers, and other craftsmen were relatively free from typical big-picture constraints. There was more visual experimentation than in Hollywood filmmaking as a whole: the Expressionism now closely associated with noir and the semi-documentary style that later emerged represent two very different tendencies. Narrative structures sometimes involved convoluted flashbacks uncommon in non-noir commercial productions. In terms of content, enforcement of the Production Code ensured that no film character could literally get away with murder or be seen sharing a bed with anyone but a spouse; within those bounds, however, many films now identified as noir feature plot elements and dialogue that were very risqué for the time.
Thematically, film noirs were most exceptional for the relative frequency with which they centered on women of questionable virtue—a focus that had become rare in Hollywood films after the mid-1930s and the end of the pre-Code era. The signal film in this vein was "Double Indemnity", directed by Billy Wilder; setting the mold was Barbara Stanwyck's unforgettable femme fatale, Phyllis Dietrichson—an apparent nod to Marlene Dietrich, who had built her extraordinary career playing such characters for Sternberg. An A-level feature all the way, the film's commercial success and seven Oscar nominations made it probably the most influential of the early noirs. A slew of now-renowned noir "bad girls" followed, such as those played by Rita Hayworth in "Gilda" (1946), Lana Turner in "The Postman Always Rings Twice" (1946), Ava Gardner in "The Killers" (1946), and Jane Greer in "Out of the Past" (1947). The iconic noir counterpart to the femme fatale, the private eye, came to the fore in films such as "The Maltese Falcon" (1941), with Humphrey Bogart as Sam Spade, and "Murder, My Sweet" (1944), with Dick Powell as Philip Marlowe.

The prevalence of the private eye as a lead character declined in film noir of the 1950s, a period during which several critics describe the form as becoming more focused on extreme psychologies and more exaggerated in general. A prime example is "Kiss Me Deadly" (1955); based on a novel by Mickey Spillane, the best-selling of all the hardboiled authors, here the protagonist is a private eye, Mike Hammer. As described by Paul Schrader, "Robert Aldrich's teasing direction carries "noir" to its sleaziest and most perversely erotic. Hammer overturns the underworld in search of the 'great whatsit' [which] turns out to be—joke of jokes—an exploding atomic bomb." Orson Welles's baroquely styled "Touch of Evil" (1958) is frequently cited as the last noir of the classic period. Some scholars believe film noir never really ended, but continued to transform even as the characteristic noir visual style began to seem dated and changing production conditions led Hollywood in different directions—in this view, post-1950s films in the noir tradition are seen as part of a continuity with classic noir. A majority of critics, however, regard comparable films made outside the classic era to be something other than genuine film noirs. They regard true film noir as belonging to a temporally and geographically limited cycle or period, treating subsequent films that evoke the classics as fundamentally different due to general shifts in filmmaking style and latter-day awareness of noir as a historical source for allusion.

While the inceptive noir, "Stranger on the Third Floor", was a B picture directed by a virtual unknown, many of the film noirs still remembered were A-list productions by well-known film makers. Debuting as a director with "The Maltese Falcon" (1941), John Huston followed with "Key Largo" (1948) and "The Asphalt Jungle" (1950). Opinion is divided on the noir status of several Alfred Hitchcock thrillers from the era; at least four qualify by consensus: "Shadow of a Doubt" (1943), "Notorious" (1946), "Strangers on a Train" (1951) and "The Wrong Man" (1956). Otto Preminger's success with "Laura" (1944) made his name and helped demonstrate noir's adaptability to a high-gloss 20th Century-Fox presentation. Among Hollywood's most celebrated directors of the era, arguably none worked more often in a noir mode than Preminger; his other noirs include "Fallen Angel" (1945), "Whirlpool" (1949), "Where the Sidewalk Ends" (1950) (all for Fox) and "Angel Face" (1952). A half-decade after "Double Indemnity" and "The Lost Weekend", Billy Wilder made "Sunset Boulevard" (1950) and "Ace in the Hole" (1951), noirs that were not so much crime dramas as satires on Hollywood and the news media. "In a Lonely Place" (1950) was Nicholas Ray's breakthrough; his other noirs include his debut, "They Live by Night" (1948) and "On Dangerous Ground" (1952), noted for their unusually sympathetic treatment of characters alienated from the social mainstream.
Orson Welles had notorious problems with financing but his three film noirs were well budgeted: "The Lady from Shanghai" (1947) received top-level, "prestige" backing, while "The Stranger", his most conventional film and "Touch of Evil", an unmistakably personal work, were funded at levels lower but still commensurate with headlining releases. Like "The Stranger", Fritz Lang's "The Woman in the Window" (1945) was a production of the independent International Pictures. Lang's follow-up, "Scarlet Street" (1945), was one of the few classic noirs to be officially censored: filled with erotic innuendo, it was temporarily banned in Milwaukee, Atlanta and New York State. "Scarlet Street" was a semi-independent, cosponsored by Universal and Lang's Diana Productions, of which the film's co-star, Joan Bennett, was the second biggest shareholder. Lang, Bennett and her husband, the Universal veteran and Diana production head Walter Wanger, made "Secret Beyond the Door" (1948) in similar fashion.

Before leaving the United States while subject to the Hollywood blacklist, Jules Dassin made two classic noirs that also straddled the major–independent line: "Brute Force" (1947) and the influential documentary-style "The Naked City" were developed by producer Mark Hellinger, who had an "inside/outside" contract with Universal similar to Wanger's. Years earlier, working at Warner Bros., Hellinger had produced three films for Raoul Walsh, the proto-noirs "They Drive by Night" (1940), "Manpower" (1941) and "High Sierra" (1941), now regarded as a seminal work in noir's development. Walsh had no great name during his half-century as a director but his noirs "White Heat" (1949) and "The Enforcer" (1951) had A-list stars and are seen as important examples of the cycle. Other directors associated with top-of-the-bill Hollywood film noirs include Edward Dmytryk ("Murder, My Sweet" [1944], "Crossfire" [1947])—the first important noir director to fall prey to the industry blacklist—as well as Henry Hathaway ("The Dark Corner" [1946], "Kiss of Death" [1947]) and John Farrow ("The Big Clock" [1948], "Night Has a Thousand Eyes" [1948]).

Most of the Hollywood films considered to be classic noirs fall into the category of the "B movie". Some were Bs in the most precise sense, produced to run on the bottom of double bills by a low-budget unit of one of the major studios or by one of the smaller Poverty Row outfits, from the relatively well-off Monogram to shakier ventures such as Producers Releasing Corporation (PRC). Jacques Tourneur had made over thirty Hollywood Bs (a few now highly regarded, most forgotten) before directing the A-level "Out of the Past", described by scholar Robert Ottoson as "the "ne plus ultra" of forties film noir". Movies with budgets a step up the ladder, known as "intermediates" by the industry, might be treated as A or B pictures depending on the circumstances. Monogram created Allied Artists in the late 1940s to focus on this sort of production. Robert Wise ("Born to Kill" [1947], "The Set-Up" [1949]) and Anthony Mann ("T-Men" [1947] and "Raw Deal" [1948]) each made a series of impressive intermediates, many of them noirs, before graduating to steady work on big-budget productions. Mann did some of his most celebrated work with cinematographer John Alton, a specialist in what James Naremore called "hypnotic moments of light-in-darkness". "He Walked by Night" (1948), shot by Alton and though credited solely to Alfred Werker, directed in large part by Mann, demonstrates their technical mastery and exemplifies the late 1940s trend of "police procedural" crime dramas. It was released, like other Mann-Alton noirs, by the small Eagle-Lion company; it was the inspiration for the "Dragnet" series, which debuted on radio in 1949 and television in 1951.

Several directors associated with noir built well-respected oeuvres largely at the B-movie/intermediate level. Samuel Fuller's brutal, visually energetic films such as "Pickup on South Street" (1953) and "Underworld U.S.A." (1961) earned him a unique reputation; his advocates praise him as "primitive" and "barbarous". Joseph H. Lewis directed noirs as diverse as "Gun Crazy" (1950) and "The Big Combo" (1955). The former—whose screenplay was written by the blacklisted Dalton Trumbo, disguised by a front—features a bank hold-up sequence shown in an unbroken take of over three minutes that was influential. "The Big Combo" was shot by John Alton and took the shadowy noir style to its outer limits. The most distinctive films of Phil Karlson ("The Phenix City Story" [1955] and "The Brothers Rico" [1957]) tell stories of vice organized on a monstrous scale. The work of other directors in this tier of the industry, such as Felix E. Feist ("The Devil Thumbs a Ride" [1947], "Tomorrow Is Another Day" [1951]), has become obscure. Edgar G. Ulmer spent most of his Hollywood career working at B studios and once in a while on projects that achieved intermediate status; for the most part, on unmistakable Bs. In 1945, while at PRC, he directed a noir cult classic, "Detour". Ulmer's other noirs include "Strange Illusion" (1945), also for PRC; "Ruthless" (1948), for Eagle-Lion, which had acquired PRC the previous year and "Murder Is My Beat" (1955), for Allied Artists.

A number of low- and modestly-budgeted noirs were made by independent, often actor-owned, companies contracting with larger studios for distribution. Serving as producer, writer, director and top-billed performer, Hugo Haas made films like "Pickup" (1951) and "The Other Woman" (1954). It was in this way that accomplished noir actress Ida Lupino established herself as the sole female director in Hollywood during the late 1940s and much of the 1950s. She does not appear in the best-known film she directed, "The Hitch-Hiker" (1953), developed by her company, The Filmakers, with support and distribution by RKO. It is one of the seven classic film noirs produced largely outside of the major studios that have been chosen for the United States National Film Registry. Of the others, one was a small-studio release: "Detour". Four were independent productions distributed by United Artists, the "studio without a studio": "Gun Crazy"; "Kiss Me Deadly"; "D.O.A." (1950), directed by Rudolph Maté and "Sweet Smell of Success" (1957), directed by Alexander Mackendrick. One was an independent distributed by MGM, the industry leader: "Force of Evil" (1948), directed by Abraham Polonsky and starring John Garfield, both of whom were blacklisted in the 1950s. Independent production usually meant restricted circumstances but "Sweet Smell of Success", despite the plans of the production team, was clearly not made on the cheap, though like many other cherished A-budget noirs, it might be said to have a B-movie soul.

Perhaps no director better displayed that spirit than the German-born Robert Siodmak, who had already made a score of films before his 1940 arrival in Hollywood. Working mostly on A features, he made eight films now regarded as classic-era film noirs (a figure matched only by Lang and Mann). In addition to "The Killers", Burt Lancaster's debut and a Hellinger/Universal co-production, Siodmak's other important contributions to the genre include 1944's "Phantom Lady" (a top-of-the-line B and Woolrich adaptation), the ironically titled "Christmas Holiday" (1944), and "Cry of the City" (1948). "Criss Cross" (1949), with Lancaster again the lead, exemplifies how Siodmak brought the virtues of the B-movie to the A noir. In addition to the relatively looser constraints on character and message at lower budgets, the nature of B production lent itself to the noir style for economic reasons: dim lighting saved on electricity and helped cloak cheap sets (mist and smoke also served the cause); night shooting was often compelled by hurried production schedules; plots with obscure motivations and intriguingly elliptical transitions were sometimes the consequence of hastily written scripts, of which there was not always enough time or money to shoot every scene. In "Criss Cross", Siodmak achieved these effects with purpose, wrapping them around Yvonne De Carlo, playing the most understandable of femme fatales; Dan Duryea, in one of his many charismatic villain roles; and Lancaster as an ordinary laborer turned armed robber, doomed by a romantic obsession.

Some critics regard classic film noir as a cycle exclusive to the United States; Alain Silver and Elizabeth Ward, for example, argue, "With the Western, film noir shares the distinction of being an indigenous American form ... a wholly American film style." However, although the term "film noir" was originally coined to describe Hollywood movies, it was an international phenomenon. Even before the beginning of the generally accepted classic period, there were films made far from Hollywood that can be seen in retrospect as film noirs, for example, the French productions "Pépé le Moko" (1937), directed by Julien Duvivier, and "Le Jour se lève" (1939), directed by Marcel Carné. In addition, Mexico experienced a vibrant film noir period from roughly 1946 to 1952, which was around the same time film noir was blossoming in the United States.

During the classic period, there were many films produced in Europe, particularly in France, that share elements of style, theme, and sensibility with American film noirs and may themselves be included in the genre's canon. In certain cases, the interrelationship with Hollywood noir is obvious: American-born director Jules Dassin moved to France in the early 1950s as a result of the Hollywood blacklist, and made one of the most famous French film noirs, "Rififi" (1955). Other well-known French films often classified as noir include "Quai des Orfèvres" (1947) and "Les Diaboliques" (1955), both directed by Henri-Georges Clouzot. "Casque d'Or" (1952), "Touchez pas au grisbi" (1954), and "Le Trou" (1960) directed by Jacques Becker; and "Ascenseur pour l'échafaud" (1958), directed by Louis Malle. French director Jean-Pierre Melville is widely recognized for his tragic, minimalist film noirs—"Bob le flambeur" (1955), from the classic period, was followed by "Le Doulos" (1962), "Le deuxième souffle" (1966), "Le Samouraï" (1967), and "Le Cercle rouge" (1970).

Scholar Andrew Spicer argues that British film noir evidences a greater debt to French poetic realism than to the expressionistic American mode of noir. Examples of British noir from the classic period include "Brighton Rock" (1947), directed by John Boulting; "They Made Me a Fugitive" (1947), directed by Alberto Cavalcanti; "The Small Back Room" (1948), directed by Michael Powell and Emeric Pressburger; "The October Man" (1950), directed by Roy Ward Baker; and "Cast a Dark Shadow" (1955), directed by Lewis Gilbert. Terence Fisher directed several low-budget thrillers in a noir mode for Hammer Film Productions, including "The Last Page" (a.k.a. "Man Bait"; 1952), "Stolen Face" (1952), and "Murder by Proxy" (a.k.a. "Blackout"; 1954). Before leaving for France, Jules Dassin had been obliged by political pressure to shoot his last English-language film of the classic noir period in Great Britain: "Night and the City" (1950). Though it was conceived in the United States and was not only directed by an American but also stars two American actors—Richard Widmark and Gene Tierney—it is technically a UK production, financed by 20th Century-Fox's British subsidiary. The most famous of classic British noirs is director Carol Reed's "The Third Man" (1949), from a screenplay by Graham Greene. Set in Vienna immediately after World War II, it also stars two American actors, Joseph Cotten and Orson Welles, who had appeared together in "Citizen Kane".

Elsewhere, Italian director Luchino Visconti adapted Cain's "The Postman Always Rings Twice" as "Ossessione" (1943), regarded both as one of the great noirs and a seminal film in the development of neorealism. (This was not even the first screen version of Cain's novel, having been preceded by the French "Le Dernier Tournant" in 1939.) In Japan, the celebrated Akira Kurosawa directed several films recognizable as film noirs, including "Drunken Angel" (1948), "Stray Dog" (1949), "The Bad Sleep Well" (1960), and "High and Low" (1963).

Among the first major neo-noir films—the term often applied to films that consciously refer back to the classic noir tradition—was the French "Tirez sur le pianiste" (1960), directed by François Truffaut from a novel by one of the gloomiest of American noir fiction writers, David Goodis. Noir crime films and melodramas have been produced in many countries in the post-classic area. Some of these are quintessentially self-aware neo-noirs—for example, "Il Conformista" (1969; Italy), "Der Amerikanische Freund" (1977; Germany), "The Element of Crime" (1984; Denmark), and "El Aura" (2005; Argentina). Others simply share narrative elements and a version of the hardboiled sensibility associated with classic noir, such as "Castle of Sand" (1974; Japan), "Insomnia" (1997; Norway), "Croupier" (1998; UK), and "Blind Shaft" (2003; China).

The neo-noir film genre developed mid-way into the Cold War. This cinematological trend reflected much of the cynicism and the possibility of nuclear annihilation of the era. This new genre introduced innovations that were not available with the earlier noir films. The violence was also more potent.

While it is hard to draw a line between some of the noir films of the early 1960s such as "Blast of Silence" (1961) and "Cape Fear" (1962) and the noirs of the late 1950s, new trends emerged in the post-classic era. "The Manchurian Candidate" (1962), directed by John Frankenheimer, "Shock Corridor" (1963), directed by Samuel Fuller, and "Brainstorm" (1965), directed by experienced noir character actor William Conrad, all treat the theme of mental dispossession within stylistic and tonal frameworks derived from classic film noir. "The Manchurian Candidate" examined the situation of American prisoners of war (POWs) during the Korean War. Incidents that occurred during the war as well as those post-war, functioned as an inspiration for a "Cold War Noir" subgenre. The television series "The Fugitive" (1963–67) brought classic noir themes and mood to the small screen for an extended run.
In a different vein, films began to appear that self-consciously acknowledged the conventions of classic film noir as historical archetypes to be revived, rejected, or reimagined. These efforts typify what came to be known as neo-noir. Though several late classic noirs, "Kiss Me Deadly" in particular, were deeply self-knowing and post-traditional in conception, none tipped its hand so evidently as to be remarked on by American critics at the time. The first major film to overtly work this angle was French director Jean-Luc Godard's "À bout de souffle" ("Breathless"; 1960), which pays its literal respects to Bogart and his crime films while brandishing a bold new style for a new day. In the United States, Arthur Penn (1965's "Mickey One", drawing inspiration from Truffaut's "Tirez sur le pianiste" and other French New Wave films), John Boorman (1967's "Point Blank", similarly caught up, though in the "Nouvelle vague'"s deeper waters), and Alan J. Pakula (1971's "Klute") directed films that knowingly related themselves to the original film noirs, inviting audiences in on the game.

A manifest affiliation with noir traditions—which, by its nature, allows different sorts of commentary on them to be inferred—can also provide the basis for explicit critiques of those traditions. In 1973, director Robert Altman flipped off noir piety with "The Long Goodbye". Based on the novel by Raymond Chandler, it features one of Bogart's most famous characters, but in iconoclastic fashion: Philip Marlowe, the prototypical hardboiled detective, is replayed as a hapless misfit, almost laughably out of touch with contemporary mores and morality. Where Altman's subversion of the film noir mythos was so irreverent as to outrage some contemporary critics, around the same time Woody Allen was paying affectionate, at points idolatrous homage to the classic mode with "Play It Again, Sam" (1972). The "blaxploitation" film "Shaft" (1971), wherein Richard Roundtree plays the titular African-American private eye, John Shaft, takes conventions from classic noir.

The most acclaimed of the neo-noirs of the era was director Roman Polanski's 1974 "Chinatown". Written by Robert Towne, it is set in 1930s Los Angeles, an accustomed noir locale nudged back some few years in a way that makes the pivotal loss of innocence in the story even crueler. Where Polanski and Towne raised noir to a black apogee by turning rearward, director Martin Scorsese and screenwriter Paul Schrader brought the noir attitude crashing into the present day with "Taxi Driver" (1976), a crackling, bloody-minded gloss on bicentennial America. In 1978, Walter Hill wrote and directed "The Driver", a chase film as might have been imagined by Jean-Pierre Melville in an especially abstract mood.

Hill was already a central figure in 1970s noir of a more straightforward manner, having written the script for director Sam Peckinpah's "The Getaway" (1972), adapting a novel by pulp master Jim Thompson, as well as for two tough private eye films: an original screenplay for "Hickey & Boggs" (1972) and an adaptation of a novel by Ross Macdonald, the leading literary descendant of Hammett and Chandler, for "The Drowning Pool" (1975). Some of the strongest 1970s noirs, in fact, were unwinking remakes of the classics, "neo" mostly by default: the heartbreaking "Thieves Like Us" (1973), directed by Altman from the same source as Ray's "They Live by Night", and "Farewell, My Lovely" (1975), the Chandler tale made classically as "Murder, My Sweet", remade here with Robert Mitchum in his last notable noir role. Detective series, prevalent on American television during the period, updated the hardboiled tradition in different ways, but the show conjuring the most noir tone was a horror crossover touched with shaggy, "Long Goodbye"-style humor: "" (1974–75), featuring a Chicago newspaper reporter investigating strange, usually supernatural occurrences.

The turn of the decade brought Scorsese's black-and-white "Raging Bull" (cowritten by Schrader); an acknowledged masterpiece—the American Film Institute ranks it as the greatest American film of the 1980s and the fourth greatest of all time—it is also a retreat, telling a story of a boxer's moral self-destruction that recalls in both theme and visual ambience noir dramas such as "Body and Soul" (1947) and "Champion" (1949). From 1981, the popular "Body Heat", written and directed by Lawrence Kasdan, invokes a different set of classic noir elements, this time in a humid, erotically charged Florida setting; its success confirmed the commercial viability of neo-noir, at a time when the major Hollywood studios were becoming increasingly risk averse. The mainstreaming of neo-noir is evident in such films as "Black Widow" (1987), "Shattered" (1991), and "Final Analysis" (1992). Few neo-noirs have made more money or more wittily updated the tradition of the noir double-entendre than "Basic Instinct" (1992), directed by Paul Verhoeven and written by Joe Eszterhas. The film also demonstrates how neo-noir's polychrome palette can reproduce many of the expressionistic effects of classic black-and-white noir.

Among big-budget auteurs, Michael Mann has worked frequently in a neo-noir mode, with such films as "Thief" (1981) and "Heat" (1995) and the TV series "Miami Vice" (1984–89) and "Crime Story" (1986–88). Mann's output exemplifies a primary strain of neo-noir (or as affectionately called "neon noir") , in which classic themes and tropes are revisited in a contemporary setting with an up-to-date visual style and rock- or hip hop-based musical soundtrack. Like "Chinatown", its more complex predecessor, Curtis Hanson's Oscar-winning "L.A. Confidential" (1997), based on the James Ellroy novel, demonstrates an opposite tendency—the deliberately retro film noir; its tale of corrupt cops and femmes fatales is seemingly lifted straight from a film of 1953, the year in which it is set. Director David Fincher followed the immensely successful neo-noir "Seven" (1995) with a film that developed into a cult favorite after its original, disappointing release: "Fight Club" (1999) is a "sui generis" mix of noir aesthetic, perverse comedy, speculative content, and satiric intent.

Working generally with much smaller budgets, brothers Joel and Ethan Coen have created one of the most extensive film oeuvres influenced by classic noir, with films such as "Blood Simple" (1984) and "Fargo" (1996), considered by some a supreme work in the neo-noir mode. The Coens cross noir with other generic lines in the gangster drama "Miller's Crossing" (1990)—loosely based on the Dashiell Hammett novels "Red Harvest" and "The Glass Key"—and the comedy "The Big Lebowski" (1998), a tribute to Chandler and an homage to Altman's version of "The Long Goodbye". The characteristic work of David Lynch combines film noir tropes with scenarios driven by disturbed characters such as the sociopathic criminal played by Dennis Hopper in "Blue Velvet" (1986) and the delusionary protagonist of "Lost Highway" (1997). The "Twin Peaks" cycle, both TV series (1990–91) and film, "" (1992), puts a detective plot through a succession of bizarre spasms. David Cronenberg also mixes surrealism and noir in "Naked Lunch" (1991), inspired by William S. Burroughs' novel.

Perhaps no American neo-noirs better reflect the classic noir A-movie-with-a-B-movie-soul than those of director-writer Quentin Tarantino; neo-noirs of his such as "Reservoir Dogs" (1992) and "Pulp Fiction" (1994) display a relentlessly self-reflexive, sometimes tongue-in-cheek sensibility, similar to the work of the New Wave directors and the Coens. Other films from the era readily identifiable as neo-noir (some retro, some more au courant) include director John Dahl's "Kill Me Again" (1989), "Red Rock West" (1992), and "The Last Seduction" (1993); four adaptations of novels by Jim Thompson—"The Kill-Off" (1989), "After Dark, My Sweet" (1990), "The Grifters" (1990), and the remake of "The Getaway" (1994); and many more, including adaptations of the work of other major noir fiction writers: "The Hot Spot" (1990), from "Hell Hath No Fury", by Charles Williams; "Miami Blues" (1990), from the novel by Charles Willeford; and "Out of Sight" (1998), from the novel by Elmore Leonard. Several films by director-writer David Mamet involve noir elements: "House of Games" (1987), "Homicide" (1991), "The Spanish Prisoner" (1997), and "Heist" (2001). On television, "Moonlighting" (1985–89) paid homage to classic noir while demonstrating an unusual appreciation of the sense of humor often found in the original cycle. Between 1983 and 1989, Mickey Spillane's hardboiled private eye Mike Hammer was played with wry gusto by Stacy Keach in a series and several stand-alone television films (an unsuccessful revival followed in 1997–98). The British miniseries "The Singing Detective" (1986), written by Dennis Potter, tells the story of a mystery writer named Philip Marlow; widely considered one of the finest neo-noirs in any medium, some critics rank it among the greatest television productions of all time.

The Coen brothers make reference to the noir tradition again with "The Man Who Wasn't There" (2001); a black-and-white crime melodrama set in 1949, it features a scene apparently staged to mirror one from "Out of the Past". Lynch's "Mulholland Drive" (2001) continued in his characteristic vein, making the classic noir setting of Los Angeles the venue for a noir-inflected psychological jigsaw puzzle. British-born director Christopher Nolan's black-and-white debut, "Following" (1998), was an overt homage to classic noir. During the new century's first decade, he was one of the leading Hollywood directors of neo-noir with the acclaimed "Memento" (2000) and the remake of "Insomnia" (2002).

Director Sean Penn's "The Pledge" (2001), though adapted from a very self-reflexive novel by Friedrich Dürrenmatt, plays noir comparatively straight, to devastating effect. Screenwriter David Ayer updated the classic noir bad-cop tale, typified by "Shield for Murder" (1954) and "Rogue Cop" (1954), with his scripts for "Training Day" (2001) and, adapting a story by James Ellroy, "Dark Blue" (2002); he later wrote and directed the even darker "Harsh Times" (2006). Michael Mann's "Collateral" (2004) features a performance by Tom Cruise as an assassin in the lineage of "Le Samouraï". The torments of "The Machinist" (2004), directed by Brad Anderson, evoke both "Fight Club" and "Memento". In 2005, Shane Black directed "Kiss Kiss Bang Bang", basing his screenplay in part on a crime novel by Brett Halliday, who published his first stories back in the 1920s. The film plays with an awareness not only of classic noir but also of neo-noir reflexivity itself.

With ultra-violent films such as "Sympathy for Mr. Vengeance" (2002) and "Thirst" (2009), Park Chan-wook of South Korea has been the most prominent director outside of the United States to work regularly in a noir mode in the new millennium. The most commercially successful neo-noir of this period has been "Sin City" (2005), directed by Robert Rodriguez in extravagantly stylized black and white with splashes of color. The film is based on a series of comic books created by Frank Miller (credited as the film's codirector), which are in turn openly indebted to the works of Spillane and other pulp mystery authors. Similarly, graphic novels provide the basis for "Road to Perdition" (2002), directed by Sam Mendes, and "A History of Violence" (2005), directed by David Cronenberg; the latter was voted best film of the year in the annual "Village Voice" poll. Writer-director Rian Johnson's "Brick" (2005), featuring present-day high schoolers speaking a version of 1930s hardboiled argot, won the Special Jury Prize for Originality of Vision at the Sundance Film Festival. The television series "Veronica Mars" (2004–07) also brought a youth-oriented twist to film noir. Examples of this sort of generic crossover have been dubbed "teen noir".

Neo-noir films released in the 2010s include Kim Jee-woon’s "I Saw the Devil" (2010), Fred Cavaye’s "Point Blank" (2010), Na Hong-jin’s "The Yellow Sea" (2010), Nicolas Winding Refn’s "Drive" (2011), and Claire Denis' "Bastards" (2013).

In the post-classic era, a significant trend in noir crossovers has involved science fiction. In Jean-Luc Godard's "Alphaville" (1965), Lemmy Caution is the name of the old-school private eye in the city of tomorrow. "The Groundstar Conspiracy" (1972) centers on another implacable investigator and an amnesiac named Welles. "Soylent Green" (1973), the first major American example, portrays a dystopian, near-future world via a self-evidently noir detection plot; starring Charlton Heston (the lead in "Touch of Evil"), it also features classic noir standbys Joseph Cotten, Edward G. Robinson, and Whit Bissell. The film was directed by Richard Fleischer, who two decades before had directed several strong B noirs, including "Armored Car Robbery" (1950) and "The Narrow Margin" (1952).

The cynical and stylish perspective of classic film noir had a formative effect on the cyberpunk genre of science fiction that emerged in the early 1980s; the film most directly influential on cyberpunk was "Blade Runner" (1982), directed by Ridley Scott, which pays evocative homage to the classic noir mode (Scott subsequently directed the poignant noir crime melodrama "Someone to Watch Over Me" (1987)). Scholar Jamaluddin Bin Aziz has observed how "the shadow of Philip Marlowe lingers on" in such other "future noir" films as "12 Monkeys" (1995), "Dark City" (1998) and "Minority Report" (2002). Fincher's feature debut was "Alien 3" (1992), which evoked the classic noir jail film "Brute Force".

David Cronenberg's "Crash" (1996), an adaptation of the speculative novel by J. G. Ballard, has been described as a "film noir in bruise tones". The hero is the target of investigation in "Gattaca" (1997), which fuses film noir motifs with a scenario indebted to "Brave New World". "The Thirteenth Floor" (1999), like "Blade Runner", is an explicit homage to classic noir, in this case involving speculations about virtual reality. Science fiction, noir, and anime are brought together in the Japanese films of 90s "Ghost in the Shell" (1995) and "" (2004), both directed by Mamoru Oshii. "The Animatrix" (2003), based on and set within the world of "The Matrix" film trilogy, contains an anime short film in classic noir style titled "A Detective Story". Anime television series with science fiction noir themes include "Noir" (2001) and "Cowboy Bebop" (1998).

The 2015 film "Ex Machina" puts an understated film noir spin on the Frankenstein mythos, with the sentient android Ava as a potential "femme fatale", her creator Nathan embodying the abusive husband or father trope, and her would-be rescuer Caleb as a "clueless drifter" enthralled by Ava.

Film noir has been parodied many times in many manners. In 1945, Danny Kaye starred in what appears to be the first intentional film noir parody, "Wonder Man". That same year, Deanna Durbin was the singing lead in the comedic noir "Lady on a Train", which makes fun of Woolrich-brand wistful miserablism. Bob Hope inaugurated the private-eye noir parody with "My Favorite Brunette" (1947), playing a baby-photographer who is mistaken for an ironfisted detective. In 1947 as well, The Bowery Boys appeared in "Hard Boiled Mahoney", which had a similar mistaken-identity plot; they spoofed the genre once more in "Private Eyes" (1953). Two RKO productions starring Robert Mitchum take film noir over the border into self-parody: "The Big Steal" (1949), directed by Don Siegel, and "His Kind of Woman" (1951). The "Girl Hunt" ballet in Vincente Minnelli's "The Band Wagon" (1953) is a ten-minute distillation of—and play on—noir in dance. "The Cheap Detective" (1978), starring Peter Falk, is a broad spoof of several films, including the Bogart classics "The Maltese Falcon" and "Casablanca". Carl Reiner's black-and-white "Dead Men Don't Wear Plaid" (1982) appropriates clips of classic noirs for a farcical pastiche, while his "Fatal Instinct" (1993) sends up noir classic ("Double Indemnity") and neo-noir ("Basic Instinct"). Robert Zemeckis's "Who Framed Roger Rabbit" (1988) develops a noir plot set in 1940s L.A. around a host of cartoon characters.

Noir parodies come in darker tones as well. "Murder by Contract" (1958), directed by Irving Lerner, is a deadpan joke on noir, with a denouement as bleak as any of the films it kids. An ultra-low-budget Columbia Pictures production, it may qualify as the first intentional example of what is now called a neo-noir film; it was likely a source of inspiration for both Melville's "Le Samouraï" and Scorsese's "Taxi Driver". Belying its parodic strain, "The Long Goodbye"s final act is seriously grave. "Taxi Driver" caustically deconstructs the "dark" crime film, taking it to an absurd extreme and then offering a conclusion that manages to mock every possible anticipated ending—triumphant, tragic, artfully ambivalent—while being each, all at once. Flirting with splatter status even more brazenly, the Coens' "Blood Simple" is both an exacting pastiche and a gross exaggeration of classic noir. Adapted by director Robinson Devor from a novel by Charles Willeford, "The Woman Chaser" (1999) sends up not just the noir mode but the entire Hollywood filmmaking process, with seemingly each shot staged as the visual equivalent of an acerbic Marlowe wisecrack.

In other media, the television series "Sledge Hammer!" (1986–88) lampoons noir, along with such topics as capital punishment, gun fetishism, and Dirty Harry. "Sesame Street" (1969–curr.) occasionally casts Kermit the Frog as a private eye; the sketches refer to some of the typical motifs of noir films, in particular the voiceover. Garrison Keillor's radio program "A Prairie Home Companion" features the recurring character Guy Noir, a hardboiled detective whose adventures always wander into farce (Guy also appears in the Altman-directed film based on Keillor's show). Firesign Theatre's Nick Danger has trod the same not-so-mean streets, both on radio and in comedy albums. Cartoons such as "Garfield's Babes and Bullets" (1989) and comic strip characters such as Tracer Bullet of "Calvin and Hobbes" have parodied both film noir and the kindred hardboiled tradition—one of the sources from which film noir sprang and which it now overshadows.

In their original 1955 canon of film noir, Raymond Borde and Etienne Chaumeton identified twenty-two Hollywood films released between 1941 and 1952 as core examples; they listed another fifty-nine American films from the period as significantly related to the field of noir. A half-century later, film historians and critics had come to agree on a canon of approximately three hundred films from 1940–58. There remain, however, many differences of opinion over whether other films of the era, among them a number of well-known ones, qualify as film noirs or not. For instance, "The Night of the Hunter" (1955), starring Robert Mitchum in an acclaimed performance, is treated as a film noir by some critics, but not by others. Some critics include "Suspicion" (1941), directed by Alfred Hitchcock, in their catalogues of noir; others ignore it. Concerning films made either before or after the classic period, or outside of the United States at any time, consensus is even rarer.

To support their categorization of certain films as noirs and their rejection of others, many critics refer to a set of elements they see as marking examples of the mode. The question of what constitutes the set of noir's identifying characteristics is a fundamental source of controversy. For instance, critics tend to define the model film noir as having a tragic or bleak conclusion, but many acknowledged classics of the genre have clearly happy endings (e.g., "Stranger on the Third Floor," "The Big Sleep", "Dark Passage", and "The Dark Corner"), while the tone of many other noir denouements is ambivalent. Some critics perceive classic noir's hallmark as a distinctive visual style. Others, observing that there is actually considerable stylistic variety among noirs, instead emphasize plot and character type. Still others focus on mood and attitude. No survey of classic noir's identifying characteristics can therefore be considered definitive. In the 1990s and 2000s, critics have increasingly turned their attention to that diverse field of films called neo-noir; once again, there is even less consensus about the defining attributes of such films made outside the classic period.

The low-key lighting schemes of many classic film noirs are associated with stark light/dark contrasts and dramatic shadow patterning—a style known as chiaroscuro (a term adopted from Renaissance painting). The shadows of Venetian blinds or banister rods, cast upon an actor, a wall, or an entire set, are an iconic visual in noir and had already become a cliché well before the neo-noir era. Characters' faces may be partially or wholly obscured by darkness—a relative rarity in conventional Hollywood filmmaking. While black-and-white cinematography is considered by many to be one of the essential attributes of classic noir, the color films "Leave Her to Heaven" (1945) and "Niagara" (1953) are routinely included in noir filmographies, while "Slightly Scarlet" (1956), "Party Girl" (1958), and "Vertigo" (1958) are classified as noir by varying numbers of critics.

Film noir is also known for its use of low-angle, wide-angle, and skewed, or Dutch angle shots. Other devices of disorientation relatively common in film noir include shots of people reflected in one or more mirrors, shots through curved or frosted glass or other distorting objects (such as during the strangulation scene in "Strangers on a Train"), and special effects sequences of a sometimes bizarre nature. Night-for-night shooting, as opposed to the Hollywood norm of day-for-night, was often employed. From the mid-1940s forward, location shooting became increasingly frequent in noir.

In an analysis of the visual approach of "Kiss Me Deadly", a late and self-consciously stylized example of classic noir, critic Alain Silver describes how cinematographic choices emphasize the story's themes and mood. In one scene, the characters, seen through a "confusion of angular shapes", thus appear "caught in a tangible vortex or enclosed in a trap." Silver makes a case for how "side light is used ... to reflect character ambivalence", while shots of characters in which they are lit from below "conform to a convention of visual expression which associates shadows cast upward of the face with the unnatural and ominous".

Film noirs tend to have unusually convoluted story lines, frequently involving flashbacks and other editing techniques that disrupt and sometimes obscure the narrative sequence. Framing the entire primary narrative as a flashback is also a standard device. Voiceover narration, sometimes used as a structuring device, came to be seen as a noir hallmark; while classic noir is generally associated with first-person narration (i.e., by the protagonist), Stephen Neale notes that third-person narration is common among noirs of the semidocumentary style. Neo-noirs as varied as "The Element of Crime" (surrealist), "After Dark, My Sweet" (retro), and "Kiss Kiss Bang Bang" (meta) have employed the flashback/voiceover combination.

Bold experiments in cinematic storytelling were sometimes attempted during the classic era: "Lady in the Lake", for example, is shot entirely from the point of view of protagonist Philip Marlowe; the face of star (and director) Robert Montgomery is seen only in mirrors. "The Chase" (1946) takes oneirism and fatalism as the basis for its fantastical narrative system, redolent of certain horror stories, but with little precedent in the context of a putatively realistic genre. In their different ways, both "Sunset Boulevard" and "D.O.A." are tales told by dead men. Latter-day noir has been in the forefront of structural experimentation in popular cinema, as exemplified by such films as "Pulp Fiction", "Fight Club", and "Memento".

Crime, usually murder, is an element of almost all film noirs; in addition to standard-issue greed, jealousy is frequently the criminal motivation. A crime investigation—by a private eye, a police detective (sometimes acting alone), or a concerned amateur—is the most prevalent, but far from dominant, basic plot. In other common plots the protagonists are implicated in heists or con games, or in murderous conspiracies often involving adulterous affairs. False suspicions and accusations of crime are frequent plot elements, as are betrayals and double-crosses. According to J. David Slocum, "protagonists assume the literal identities of dead men in nearly fifteen percent of all noir." Amnesia is fairly epidemic—"noir's version of the common cold", in the words of film historian Lee Server.

Film noirs tend to revolve around heroes who are more flawed and morally questionable than the norm, often fall guys of one sort or another. The characteristic protagonists of noir are described by many critics as "alienated"; in the words of Silver and Ward, "filled with existential bitterness". Certain archetypal characters appear in many film noirs—hardboiled detectives, femme fatales, corrupt policemen, jealous husbands, intrepid claims adjusters, and down-and-out writers. Among characters of every stripe, cigarette smoking is rampant. From historical commentators to neo-noir pictures to pop culture ephemera, the private eye and the femme fatale have been adopted as the quintessential film noir figures, though they do not appear in most films now regarded as classic noir. Of the twenty-six National Film Registry noirs, in only four does the star play a private eye: "The Maltese Falcon", "The Big Sleep", "Out of the Past", and "Kiss Me Deadly". Just four others readily qualify as detective stories: "Laura", "The Killers", "The Naked City", and "Touch of Evil". There is usually an element of drug or alcohol use, particularly as part of the detective's method to solving the crime, as an example the character of Mike Hammer in the 1955 film "Kiss Me Deadly" who walks into a bar saying "Give me a double bourbon, and leave the bottle". Chaumeton and Borde have argued that film noir grew out of the "literature of drugs and alcohol".

Film noir is often associated with an urban setting, and a few cities—Los Angeles, San Francisco, New York, and Chicago, in particular—are the location of many of the classic films. In the eyes of many critics, the city is presented in noir as a "labyrinth" or "maze". Bars, lounges, nightclubs, and gambling dens are frequently the scene of action. The climaxes of a substantial number of film noirs take place in visually complex, often industrial settings, such as refineries, factories, trainyards, power plants—most famously the explosive conclusion of "White Heat", set at a chemical plant. In the popular (and, frequently enough, critical) imagination, in noir it is always night and it always rains.

A substantial trend within latter-day noir—dubbed "film soleil" by critic D. K. Holm—heads in precisely the opposite direction, with tales of deception, seduction, and corruption exploiting bright, sun-baked settings, stereotypically the desert or open water, to searing effect. Significant predecessors from the classic and early post-classic eras include "The Lady from Shanghai"; the Robert Ryan vehicle "Inferno" (1953); the French adaptation of Patricia Highsmith's "The Talented Mr. Ripley", "Plein soleil" ("Purple Noon" in the United States, more accurately rendered elsewhere as "Blazing Sun" or "Full Sun"; 1960); and director Don Siegel's version of "The Killers" (1964). The tendency was at its peak during the late 1980s and 1990s, with films such as "Dead Calm" (1989), "After Dark, My Sweet" (1990), "The Hot Spot" (1990), "Delusion" (1991), "Red Rock West" (1993) and the television series "Miami Vice".

Film noir is often described as essentially pessimistic. The noir stories that are regarded as most characteristic tell of people trapped in unwanted situations (which, in general, they did not cause but are responsible for exacerbating), striving against random, uncaring fate, and frequently doomed. The films are seen as depicting a world that is inherently corrupt. Classic film noir has been associated by many critics with the American social landscape of the era—in particular, with a sense of heightened anxiety and alienation that is said to have followed World War II. In author Nicholas Christopher's opinion, "it is as if the war, and the social eruptions in its aftermath, unleashed demons that had been bottled up in the national psyche." Film noirs, especially those of the 1950s and the height of the Red Scare, are often said to reflect cultural paranoia; "Kiss Me Deadly" is the noir most frequently marshaled as evidence for this claim.

Film noir is often said to be defined by "moral ambiguity", yet the Production Code obliged almost all classic noirs to see that steadfast virtue was ultimately rewarded and vice, in the absence of shame and redemption, severely punished (however dramatically incredible the final rendering of mandatory justice might be). A substantial number of latter-day noirs flout such conventions: vice emerges triumphant in films as varied as the grim "Chinatown" and the ribald "Hot Spot".

The tone of film noir is generally regarded as downbeat; some critics experience it as darker still—"overwhelmingly black", according to Robert Ottoson. Influential critic (and filmmaker) Paul Schrader wrote in a seminal 1972 essay that ""film noir" is defined by tone", a tone he seems to perceive as "hopeless". In describing the adaptation of "Double Indemnity," noir analyst Foster Hirsch describes the "requisite hopeless tone" achieved by the filmmakers, which appears to characterize his view of noir as a whole. On the other hand, definitive film noirs such as "The Big Sleep", "The Lady from Shanghai", "Scarlet Street" and "Double Indemnity" itself are famed for their hardboiled repartee, often imbued with sexual innuendo and self-reflexive humor.




</doc>
<doc id="10803" url="https://en.wikipedia.org/wiki?curid=10803" title="Finno-Ugric languages">
Finno-Ugric languages

Finno-Ugric ( or ), Finno-Ugrian or Fenno-Ugric is a traditional grouping of all languages in the Uralic language family except the Samoyedic languages. Its commonly accepted status as a subfamily of Uralic is based on criteria formulated in the 19th century and is criticized by some contemporary linguists as inaccurate and misleading. The three most-spoken Uralic languages, Hungarian, Finnish, and Estonian, are all included in Finno-Ugric, although linguistic roots common to both branches of the traditional Finno-Ugric language tree (Finno-Permic and Ugric) are distant.

The term "Finno-Ugric", which originally referred to the entire family, is sometimes used as a synonym for the term "Uralic", which includes the Samoyedic languages, as commonly happens when a language family is expanded with further discoveries.

The validity of Finno-Ugric as a genetic grouping is under challenge, with some feeling that the Finno-Permic languages are as distinct from the Ugric languages as they are from the Samoyedic languages spoken in Siberia, or even that none of the Finno-Ugric, Finno-Permic, or Ugric branches has been established. Received opinion has been that the easternmost (and last-discovered) Samoyed had separated first and the branching into Ugric and Finno-Permic took place later, but this reconstruction does not have strong support in the linguistic data.

Attempts at reconstructing a Proto-Finno-Ugric protolanguage, a common ancestor of all Uralic languages except for the Samoyedic languages, are largely indistinguishable from Proto-Uralic, suggesting that Finno-Ugric might not be a historical grouping but a geographical one, with Samoyedic being distinct by lexical borrowing rather than actually being historically divergent. It has been proposed that the area in which Proto-Finno-Ugric was spoken reached between the Baltic Sea and the Ural Mountains.

Traditionally, the main set of evidence for the genetic proposal of Proto-Finno-Ugric has come from vocabulary. A large amount of vocabulary (e.g. the numerals "one", "three", "four" and "six"; the body-part terms "hand", "head") is only reconstructed up to the Proto-Finno-Ugric level, and only words with a Samoyedic equivalent have been reconstructed for Proto-Uralic. That methodology has been criticised, as no coherent explanation other than inheritance has been presented for the origin of most of the Finno-Ugric vocabulary (though a small number has been explained as old loanwords from Proto-Indo-European or its immediate successors). 

The Samoyedic group has undergone a longer period of independent development, and its divergent vocabulary could be caused by mechanisms of replacement such as language contact. (The Finno-Ugric group is usually dated to approximately 4000 years ago, the Samoyedic a little over 2000.) Proponents of the traditional binary division note, however, that the invocation of extensive contact influence on vocabulary is at odds with the grammatical conservatism of Samoyedic.

The consonant "*š" (voiceless postalveolar fricative, ) has not been conclusively shown to occur in the traditional Proto-Uralic lexicon, but it is attested in some of the Proto-Finno-Ugric material. Another feature attested in the Finno-Ugric vocabulary is that "*i" now behaves as a neutral vowel with respect to front-back vowel harmony, and thus there are roots such as "*niwa-" "to remove the hair from hides".

Regular sound changes proposed for this stage are few and remain open to interpretation. Sammallahti (1988) proposes five, following Janhunen's (1981) reconstruction of Proto-Finno-Permic:
Sammallahti (1988) further reconstructs sound changes "*oo", "*ee" → "*a", "*ä" (merging with original "*a", "*ä") for the development from Proto-Finno-Ugric to Proto-Ugric. Similar sound laws are required for other languages as well. Thus, the origin and raising of long vowels may actually belong at a later stage, and the development of these words from Proto-Uralic to Proto-Ugric can be summarized as simple loss of "*x" (if it existed in the first place at all; vowel length only surfaces consistently in the Baltic-Finnic languages.) The proposed raising of "*o" has been alternately interpreted instead as a lowering "*u" → "*o" in Samoyedic (PU *"lumi" → "*lomə" → Proto-Samoyedic "*jom").

Janhunen (2007, 2009) notes a number of derivational innovations in Finno-Ugric, including "*ńoma" "hare" → "*ńoma-la", (vs. Samoyedic "*ńomå"), "*pexli" "side" → "*peel-ka" → "*pelka" "thumb", though involving Proto-Uralic derivational elements.

The Finno-Ugric group is not typologically distinct from Uralic as a whole: the most widespread structural features among the group all extend to the Samoyedic languages as well.

Modern linguistic research has shown that Volgaic languages is a geographical classification rather than a linguistic one, because the Mordvinic languages are more closely related to the Finno-Lappic languages than the Mari languages.

The relation of the Finno-Permic and the Ugric groups is adjudged remote by some scholars. On the other hand, with a projected time depth of only 3,000 to 4,000 years, the traditionally accepted Finno-Ugric grouping would be far younger than many major families such as Indo-European or Semitic, and would be about the same age as, for instance, the Eastern subfamily of Nilotic. But the grouping is far from transparent or securely established. The absence of early records is a major obstacle. As for the Finno-Ugric Urheimat, most of what has been said about it is speculation.

Some linguists criticizing the Finno-Ugric genetic proposal also question the validity of the entire Uralic family, instead proposing a Ural–Altaic hypothesis, within which they believe Finno-Permic may be as distant from Ugric as from Turkic. However, this approach has been rejected by nearly all other specialists in Uralic linguistics.

One argument in favor of the Finno-Ugric grouping has come from loanwords. Several loans from the Indo-European languages are present in most or all of the Finno-Ugric languages, while being absent from Samoyedic; many others also must be for phonological reasons dated as quite old.

According to Häkkinen (1983) the alleged Proto-Finno-Ugric loanwords are disproportionally well-represented in Hungarian and the Permic languages, and disproportionally poorly represented in the Ob-Ugric languages; hence it is possible that such words have been acquired by the languages only after the initial dissolution of the Uralic family into individual dialects, and that the scarcity of loanwords in Samoyedic results from its peripheric location.

The number systems among the Finno-Ugric languages are particularly distinct from the Samoyedic languages: only the numerals "2" and "5" have cognates in Samoyedic, while also the numerals, "1", "3", "4", "6", "10" are shared by all or most Finno-Ugric languages.

Below are the numbers 1 to 10 in several Finno-Ugric languages. Forms in "italic" do not descend from the reconstructed forms.

The number '2' descends in Ugric from a front-vocalic variant *kektä.

The numbers '9' and '8' in Finnic through Mari are considered to be derived from the numbers '1' and '2' as '10–1' and '10–2'. One reconstruction is *"yk+teksa" and *"kak+teksa" respectively, where *"teksa" cf. "deka" is an Indo-European loan; notice that the difference between /t/ and /d/ is not phonemic, unlike in Indo-European. Another analysis is *"ykt-e-ksa", *"kakt-e-ksa", with *"e" being the negative verb.

100-word Swadesh lists for certain Finno-Ugric languages can be compared and contrasted at the Rosetta Project website:
Finnish, Estonian, Hungarian, Erzya.

The four largest groups that speak Finno-Ugric languages are Hungarians (14.5 million), Finns (6.5 million), Estonians (1.1 million), and Mordvins (0.85 million). Three (Hungarians, Finns, and Estonians) inhabit independent nation-states, Hungary, Finland, and Estonia, while the Mordvins have an autonomous Mordovian Republic within Russia. The traditional area of the indigenous Sami people is in Northern Fenno-Scandinavia and the Kola Peninsula in Northwest Russia and is known as Sápmi. Some other Finno-Ugric peoples have autonomous republics in Russia: Karelians (Republic of Karelia), Komi (Komi Republic), Udmurts (Udmurt Republic), Mari (Mari El Republic), and Mordvins (Moksha and Erzya; Republic of Mordovia). Khanty and Mansi peoples live in the Khanty–Mansi Autonomous Okrug of Russia, while Komi-Permyaks live in Komi-Permyak Okrug, which used to be an autonomous okrug of Russia, but today is a territory with special status within Perm Krai.

The linguistic reconstruction of the Finno-Ugric language family has led to the postulation that the ancient Proto–Finno-Ugric people were ethnically related, and that even the modern Finno-Ugric-speaking peoples are ethnically related. Such hypotheses are based on the assumption that heredity can be traced through linguistic relatedness, although it must be kept in mind that language shift and ethnic admixture, a relatively frequent and common occurrence both in recorded history and most likely also in prehistory, confuses the picture and there is no straightforward relationship, if at all, between linguistic and genetic affiliation. Still, the premise that the limited community of speakers of a proto-language must have been ethnically homogeneous remains accepted.

Modern genetic studies have shown that the Y-chromosome haplogroup N3, and sometimes N2, is almost specific though certainly not restricted to Uralic or Finno-Ugric speaking populations, especially as high frequency or primary paternal haplogroup. These haplogroups branched from haplogroup N, which probably spread north, then west and east from Northern China about 12,000–14,000 years before present from father haplogroup NO (haplogroup O being the most common Y-chromosome haplogroup in Southeast Asia).

Some of the ethnicities speaking Finno-Ugric languages are:





Notes

Further reading




</doc>
<doc id="10804" url="https://en.wikipedia.org/wiki?curid=10804" title="Finnish">
Finnish

Finnish may refer to:



</doc>
<doc id="10808" url="https://en.wikipedia.org/wiki?curid=10808" title="Latin freestyle">
Latin freestyle

Latin freestyle or Miami freestyle is a form of electronic dance music that emerged in the New York metropolitan area in the 1980s. It experienced its greatest popularity from the late 1980s until the early 1990s. It continues to be produced today and enjoys some degree of popularity, especially in urban settings. A common theme of freestyle lyricism is heartbreak in the city. The first freestyle hit is largely attributed to "Let the Music Play" by Shannon.

The music was largely made popular on radio stations such as WKTU and "pre-hip hop" Hot 97 in New York City, and it became especially popular among Italian Americans and Puerto Rican Americans in the New York metro area and Philadelphia metro area, Cuban Americans in the Miami area, and Hispanic and Latino Americans in Detroit and Los Angeles County. Notable performers in the freestyle genre include Stevie B, Corina, Lil Suzy, Timmy T, George Lamond, TKA, Noel, Company B, Exposé, Debbie Deb, Brenda K. Starr, the Cover Girls, Lisa Lisa and Cult Jam, Information Society, Pretty Poison, Sa-Fire, Shannon, Coro, Lisette Melendez, Judy Torres, Rockell, Taylor Dayne and many others.

Freestyle music developed in the early 1980s, primarily in the Hispanic communities of Upper Manhattan and The Bronx and the Italian-American communities in Brooklyn, The Bronx, and other boroughs of New York City, later spreading throughout New York's five boroughs and into New Jersey. It initially was a fusion of synthetic instrumentation and syncopated percussion of 1980s electro, as favored by fans of breakdancing. Sampling, as found in synth-pop music and hip-hop, was incorporated. Key influences include Afrika Bambaataa & Soul Sonic Force's "Planet Rock" (1982) and Shannon's "Let the Music Play" (1983), the latter was a top-ten "Billboard" Hot 100 hit. In 1984, a Latin presence was established when the first song recorded in the genre by a Latin American artist, Please Don't Go, by newcomer Nayobe (a singer from Brooklyn and of Afro-Cuban descent) was recorded and released. The song became the a success, reaching No. 23 on the "Billboard" Hot Dance Music/Club Play chart. In 1985, a Spanish version of the song was released with the title "No Te Vayas". By 1987, freestyle began getting more airplay on American pop radio stations. Songs such as "Come Go with Me" by Exposé, "Show Me" by The Cover Girls, "Fascinated" by Company B, "Silent Morning" by Noel and "Catch Me (I'm Falling)" by Pretty Poison, brought freestyle into the mainstream. House music, based partly on disco rhythms, was by 1992 challenging the relatively upbeat, syncopated freestyle sound. Pitchfork consider the Miami Mix of ABC's single "When Smokey Sings" to be proto-freestyle.

Freestyle's Top 40 Radio airplay started to really take off by 1987, and it began to disappear from the airwaves in the early 1990s as radio stations moved to Top 40-only formats. Artists such as George Lamond, Exposé, Sweet Sensation and Stevie B were still heard on mainstream radio, but other notable freestyle artists did not fare as well. Carlos Berrios and Platinum producer Frankie Cutlass appeared to have saved the style's demise by creating a new sound that was used on "Temptation" by Corina and "Together Forever" by Lisette Melendez. The songs were released in 1991, almost simultaneously, and caused a resurgence in the style when they were embraced by Top 40 radio. "Temptation" reached the number 6 spot on the "Billboard" Hot 100 Chart. These hits were followed by the success of Lisa Lisa and Cult Jam, who had been one of the earliest freestyle acts. Their records were produced by Full Force, who had also worked with UTFO and James Brown. Lisa Lisa and Cult Jam had a style that was less electro and more pop, and paved the way for artists such as Corina, Stevie B, George Lamond, Sweet Sensation and the Cover Girls to cross over into the pop market. Cross-over influences became increasingly evident when the Latin Rascals produced a remix of Duran Duran's "Notorious".

Several primarily freestyle artists released ballads during the 1980s and early 1990s that crossed over to the pop charts and charted higher than their previous work. These include "Seasons Change" by Exposé, "Thinking of You" by Sa-Fire, "One More Try" by Timmy T, "Because I Love You (The Postman Song)" by Stevie B, and "If Wishes Came True" by Sweet Sensation. Brenda K. Starr reached the Hot 100 with her ballad "I Still Believe". Freestyle shortly thereafter gave way to mainstream pop artists such as MC Hammer, Paula Abdul, Bobby Brown, New Kids on the Block, and Milli Vanilli (with some artists utilizing elements of freestyle beginning in the 1980s) using hip hop beats and electro samples in a mainstream form with slicker production and MTV-friendly videos. These artists were successful on crossover stations as well as R&B stations, and freestyle was replaced as an underground genre by newer styles such as new jack swing, trance and Eurodance. Despite this, some freestyle acts managed to garner hits well into the 1990s, with acts such as Cynthia and Rockell scoring minor hits on the "Billboard" Hot 100 as late as 1998.

Freestyle remained a largely underground genre with a sizable following in New York, but has recently seen a comeback in the cities where the music originally experienced its greatest success. New York City impresario Steve Sylvester and producer Sal Abbetiello of Fever Records launched Stevie Sly's Freestyle Party show at the Manhattan live music venue, Coda on April 1, 2004. The show featured Judy Torres, Cynthia, and the Cover Girls and was attended by several celebrity guests. The Coda show was successful, and was followed by a summer 2006 Madison Square Garden concert that showcased freestyle's most successful performers. New freestyle releases are popular with enthusiasts and newcomers alike. Miami rapper Pitbull collaborated with Miami freestyle artist Stevie B to create an updated version of Stevie B's hit, "Spring Love".

Jordin Sparks' 2009 single "S.O.S. (Let the Music Play)" nods heavily to the freestyle genre with its use of a sample from the song "Let the Music Play" by Shannon. The Sparks track directly quotes the lyrics of the Shannon single, repeating the refrain several times; however, even a cursory listen of both tracks exposes a lack of Shannon's rhythmic complexity, pace and diversity of electronic, percussive samples. There is no meter of superiority found in rhythmic complexity, but it does highlight the fact that the influence of Latin Freestyle is somewhat limited in the modern Sparks recording.

In the modern day, freestyle music continues a thriving fanbase all across the country. In cities like New York, Miami, and Los Angeles, recent concerts by freestyle artists have been extremely successful, with many events selling out.

As Latin freestyle in the late 1980s and early 1990s gradually became superseded with house music, dance-pop, and regular hip hop on one front and Spanish-language pop music with marginal Latin freestyle influences on another, "harder strain" of house music originating in New York City was known to incorporate elements of Latin freestyle and the old school hip hop sound. Principal architects of the genre were Todd Terry (early instances include "Alright Alright," and "Dum Dum Cry") and Nitro Deluxe. Deluxe's "This Brutal House," fusing Latin percussion and the New York electro sound of Man Parrish with brash house music, proved to have an impact on the United Kingdom's club music scene, presaging the early 1990s British rave scene.

Freestyle features a dance tempo with stress on beats two and four; syncopation with a bass line, and a louder bass drum, lead synth, or percussion, and optional stabs of synthesized brass or orchestral samples; sixteenth-note hi-hats; a chord progression that lasts eight, 16, or 32 beats and is usually in a minor key; relatively complex, upbeat melodies with singing, verses, and a chorus; and themes about a city, broken heart, love, or dancing. Freestyle music in general is heavily influenced by electronic instrumentation upon an upbeat dance tempo. Often the Latin clave rhythm is present in many songs, such as Amoretto "Clave Rocks" by Rae Serrano aka Amoretto. The tempo is almost always between 110 and 130 beats per minute (BPM), and is typically 118 BPM. Keyboard parts are influenced by House music, and often contain many short melodies and countermelodies.

The genre was recognized as a subgenre of hip-hop in the mid-1980s. It was dominated by "hard" electro beats of the type used primarily at the time in hip-hop music. Freestyle was more appreciated in larger cities.

The origin of the name "freestyle" is disputed. One theory is that the term refers to the mixing techniques of DJs who spun this form of music in its pre-house incarnations. Freestyle's syncopated beat structures required that DJs incorporate aspects of both electronic and hip-hop techniques, as they had to mix, or had more freedom to mix, more quickly and responsively to the individual songs. A second explanation is that the music allows for a greater degree of freedom of dance expression than other music of the time, and each dancer is free to create his or her own style. Yet another story holds that the freestyle name evolved in Miami over confusion between two tracks produced by Tony "Pretty Boy" Butler: "Freestyle Express" by Freestyle and Debbie Deb's "When I Hear Music." The sound became synonymous with Butler's production, and the name of the group he was in, Freestyle, became the genre's name. The group was named for the members' love for BMX Freestyle Bike racing.

"Let the Music Play" by Shannon, is often named as the genre's first hit, and its sound, called "The Shannon Sound", as the foundation of the genre. Others like DJ Lex and Triple Beam Records contend that Afrika Bambaataa's "Planet Rock" was the first freestyle song produced. "Let the Music Play" eventually became freestyle's biggest hit, and still receives frequent airplay. Its producers Chris Barbosa and Mark Liggett changed and redefined the electro funk sound with the addition of Latin-American rhythms and a syncopated drum-machine sound.

Many early or popular freestyle artists and DJs, such as Jellybean, Tony Torres, Raul Soto, Roman Ricardo, Lil Suzy, and Nocera, were of Puerto Rican or Italian ancestry, which was one reason for the style's popularity among Puerto Rican Americans and Italian Americans in the New York City area and Philadelphia.

The new sound rejuvenated the funk, soul and hip hop club scenes in New York City. While many neighborhood clubs closed their doors permanently, Manhattan clubs that played freestyle music began to thrive. Records like "Play At Your Own Risk" by Planet Patrol, "One More Shot" by C Bank, "Al-Naafiyish (The Soul)" by Hashim, and "I.O.U." by Freeez became hits. Established European artists such as New Order helped to inspire the original freestyle sound, then incorporated freestyle elements into their own productions.

Producers from around the world began to replicate the sound in productions that were more radio-friendly. Records such as "Let Me Be the One" by Sa-Fire, "I Remember What You Like" by Jenny Burton, "Running" by Information Society, "Give Me Tonight" by Shannon and "It Works For Me" by Pam Russo enjoyed heavy New York radio airplay.

The production team of Tony Moran and Albert Cabrera, known as the Latin Rascals, created original music for radio station WKTU that included freestyle classics like 1984's "Arabian Nights", and later hip-hop oriented projects such as the Cover Girls' "Show Me". Tony Moran later formed his own project, Concept of One, and the duo continued to produce freestyle artists into the early 1990s.

Freestyle continues to have a strong following in New York. Freestyle has begun to regain airtime in clubs across the nation . Interest in freestyle increased as reggaeton's popularity waned. Coro performed in WKTU's well-received "Beatstock" concert in 2006, and the 2008 "Freestyle Extravaganza" concert sold out Madison Square Garden.

In March 2013, Radio City Music Hall hosted the very first freestyle concert. Top freestyle artists included in the line-up were TKA, Safire, Judy Torres, Cynthia, Cover Girls, Lisa Lisa, Shannon, Noel, and Lisette Melendez. Originally scheduled as a one-night event, a second night was added shortly after the first night was sold out in a matter of days.

Radio stations nationwide began to play hits by artists like TKA, Sweet Sensation, Connie, Exposé, and Sa-Fire on the same playlists as Michael Jackson and Madonna. "(You Are My) All and All" by Joyce Sims became the first freestyle record to cross over into the R&B market, and was one of the first to reach the European market. Radio station WPOW/Power 96 was noted for exposing freestyle to South Florida in the mid-'80s through the early '90s, as well as mixing in some local Miami bass into its playlist.
'Pretty Tony' Butler produced several hits on Miami's Jam-Packed Records, including Debbie Deb's "When I Hear Music" and "Lookout Weekend", and Trinere's "I'll Be All You'll Ever Need" and "They're Playing Our Song". Company B, Stevie B, Paris By Air, Linear, Will to Power and Exposé's later hits defined Miami freestyle. Tolga Katas is credited as one of the first persons to create a hit record entirely on a computer, and produced Stevie B's "Party Your Body", "In My Eyes" and "Dreamin' of Love". Katas' record label Futura Records was an incubator for artists such as Linear, who achieved international success after a move from Futura to Atlantic Records. Many labels expected New York freestyle and Miami freestyle to have the same audience and thought that the same promotional strategy would work for both genres, which often led to poor results for the New York–based freestyle. New York freestyle retained a raw edge and underground sound even in its most polished forms. It used minor chords that made the tracks darker and more moody, and its lyrics tended to be about unrequited love or more somber themes that dealt with the reality of what inner city teens were experiencing emotionally.

The groundbreaking "Nightime" by Pretty Poison featuring red headed diva Jade Starling in 1984 initially put Philadelphia on the freestyle map. Their follow-up "Catch Me I'm Falling" was a worldwide hit and brought freestyle to American Bandstand, Soul Train, Solid Gold and the Arsenio Hall Show. "Catch Me I'm Falling" broke on the street during the summer of 1987 and was the #1 single at WCAU (98 Hot Hits) and #2 at WUSL (Power 99) during the first two weeks of July. Virgin Records was quick to sign Pretty Poison helping to usher in the avalanche of other major label signings from the exploding freestyle scene.
Several freestyle acts followed on the heels of Pretty Poison emerging from the metropolitan Philadelphia, PA area in the early 1990s, benefiting from both the clubs and the overnight success of then-Dance friendly Rhythmic Top 40 WIOQ. Artists such as Dulaio Twins, D.T.U. (Doin' The Ultimate), Full Afekt, Denine, Marré and T.P.E. (The Philadelphia Experiment) enjoyed regional success. Anthony Ponzio and Anthony Santosusso of D.T.U. teamed up with DJ Mike Ferullo in 1993 to form Tazmania Records, and T.P.E.'s Adam Marano formed Viper-7 Records. The two labels produced radio hits by such artists as Collage and Denine that would lead the resurgence of the freestyle genre in the mid-1990s. Tazmania closed in the late 1990s, while Viper-7 is now known as the Viper Music Network and covers a broad spectrum of music genres.

Freestyle experienced another resurgence of popularity in the late 2000s, as older, well-known freestyle artists, producers and record labels released new music, and old and new freestyle artists performed at Philadelphia-area bars and night clubs. Tazmania Records reopened in 2009 and began to release new music. The "Tazmania Freestyle" compilation album "Overloaded" featured some of the biggest acts from their past, such as Pure Pleazure, Stefanie Bennett, Sammy C and Samantha, but the label has since shifted focus toward pop and house. Previously announced Viper Music Network projects have failed to materialize.

Freestyle had a recognizable following in California, particularly in Los Angeles, the San Francisco Bay Area, and San Diego. California's large Latino community enjoyed the sounds of the East Coast Latin club scene, and a number of California artists became popular among freestyle fans on the East Coast. Northern California freestyle, mainly from San Francisco and San Jose, leans towards a high-tempo dance beat similar to Hi-NRG. Most freestyle in California emerged from the Bay Area and Los Angeles regions.

California's large Filipino American community also embraced freestyle music during the late 1980s and early 1990s. Jaya was one of the first Filipina-American freestyle singers, and reached number 44 in 1990 with "If You Leave Me Now". Subsequent Filipino-American freestyle artists include Jocelyn Enriquez, Buffy, Korell, Damien Bautista, One Voice, Kuya, Sharyn Maceren, and others.

Timmy T, Bernadette, Caleb-B, SF Spanish Fly, Daize, Angelina, One Voice, M:G, Stephanie Fastro & The S Factor are from the Bay Area, and San Diego artists Gustavo , Alex , Jose (Jojo) Santos, Robert Romo of the group Internal Affairs, F. Felix, Leticia, and Frankie J were popular freestyle artists from southern California.

Freestyle's popularity spread outward from the Greater Toronto Area's Italian, Hispanic/Latino and Greek populations in the late 1980s and early 1990s. It was showcased alongside house music in various Toronto nightclubs, but by the mid-1990s was replaced almost entirely by house music.

Lil' Suzy released several 12-inch singles and performed live on the Canadian live dance music television program "Electric Circus". Montreal singer Nancy Martinez's 1986 single "For Tonight" would become the first Canadian freestyle single to reach the Top 40 on the Billboard Hot 100 chart, while the Montreal girl group reached the Canadian chart with "Ole Ole" in 2000.

Performers and producers associated with the style also came from around the world, including Turkish-American Murat Konar (the writer of Information Society's "Running"), Paul Lekakis from Greece, Asian artist Leonard (Leon Youngboy) who released the song "Youngboys", and British musicians including Freeez, Paul Hardcastle, Samantha Fox, and even Robin Gibb of the Bee Gees, who also adopted the freestyle sound in his 1984 album "Secret Agent", having worked with producer Chris Barbosa. Several British new wave and synthpop bands also teamed up with freestyle producers or were influenced by the genre, and released freestyle songs or remixes. These include Duran Duran whose song "Notorious" was remixed by the Latin Rascals, and whose album "Big Thing" contained several freestyle inspired songs such as "All She Wants Is"; New Order who teamed up with Arthur Baker, producing and co-writing the track "Confusion"; Erasure and the Der Deutsche mixes of their song "Blue Savannah"; and the Pet Shop Boys, whose song "Domino Dancing" was produced by Miami-based freestyle producer Lewis Martineé. Australian act I'm Talking utilized freestyle elements into their singles “Trust Me” and “Do You Wanna Be?,” both becoming top ten hits in their native Australia.



</doc>
<doc id="10810" url="https://en.wikipedia.org/wiki?curid=10810" title="Fantasy (psychology)">
Fantasy (psychology)

Fantasy in a psychological sense refers to two different possible aspects of the mind, the conscious, and the unconscious.

A fantasy is a situation imagined by an individual that expresses certain desires or aims on the part of its creator. Fantasies sometimes involve situations that are highly unlikely; or they may be quite realistic. Fantasies can also be sexual in nature. Another, more basic meaning of fantasy is something which is not 'real,' as in perceived explicitly by any of the senses, but exists as an imagined situation of object to subject.

In everyday life, individuals often find their thoughts "pursue a series of fantasies concerning things they wish they could do or wish they had done ... fantasies of control or of sovereign choice ... daydreams."

George Eman Vaillant in his study of defence mechanisms took as a central example of "an immature defence ... "fantasy" — living in a 'Walter Mitty' dream world where you imagine you are successful and popular, instead of making real efforts to make friends and succeed at a job." Fantasy, when pushed to the extreme, is a common trait of narcissistic personality disorder; and Vaillant found that "not one person who used fantasy a lot had any close friends."

Other researchers and theorists find that fantasy has beneficial elements — providing "small regressions and compensatory wish fulfilments which are recuperative in effect." Research by Deirdre Barrett reports that people differ radically in the vividness, as well as frequency of fantasy, and that those who have the most elaborately developed fantasy life are often the people who make productive use of their imaginations in art, literature, or by being especially creative and innovative in more traditional professions.

For Freud, a fantasy is constructed around multiple, often repressed wishes, and employs disguise to mask and mark the very defensive processes by which desire is enacted. The subject's desire to maintain distance from the repressed wish and simultaneously experience it opens up a type of third person syntax allowing for multiple entry into the fantasy. Therefore, in fantasy, vision is multiplied—it becomes possible to see from more than one position at the same time, to see oneself and to see oneself seeing oneself, to divide vision and dislocate subjectivity. This radical omission of the “I” position creates space for all those processes that depend upon such a center, including not only identification but also the field and organization of vision itself.

For Freud, sexuality is linked from the very beginning to an object of fantasy. However, “the object to be rediscovered is not the lost object, but its substitute by displacement; the lost object is the object of self-preservation, of hunger, and the object one seeks to re-find in sexuality is an object displaced in relation to that first object.” This initial scene of fantasy is created out of the frustrated infants’ deflection away from the instinctual need for milk and nourishment towards a phantasmization of the mothers’ breast, which is in close proximity to the instinctual need. Now bodily pleasure is derived from the sucking of the mother's breast itself. The mouth that was the original source of nourishment is now the mouth that takes pleasure in its own sucking. This substitution of the breast for milk and the breast for a phantasmic scene represents a further level of mediation which is increasingly psychic. The child cannot experience the pleasure of milk without the psychic re-inscription of the scene in the mind. “The finding of an object is in fact a re-finding of it.” It is in the movement and constant restaging away from the instinct that desire is constituted and mobilized.

A similarly positive view of fantasy was taken by Sigmund Freud who considered fantasy () a defence mechanism. He considered that men and women "cannot subsist on the scanty satisfaction which they can extort from reality. 'We simply cannot do without auxiliary constructions,' as Theodor Fontane once said ... [without] dwelling on imaginary wish fulfillments." As childhood adaptation to the reality principle developed, so too "one species of thought activity was split off; it was kept free from reality-testing and remained subordinated to the pleasure principle alone. This activity is "fantasying" ... continued as "day-dreaming"." He compared such phantasising to the way a "nature reserve preserves its original state where everything ... including what is useless and even what is noxious, can grow and proliferate there as it pleases."

Daydreams for Freud were thus a valuable resource. "These day-dreams are cathected with a large amount of interest; they are carefully cherished by the subject and usually concealed with a great deal of sensitivity ... such phantasies may be unconscious just as well as conscious." He considered these fantasies to include a great deal of the true constitutional essence of a personality, and that the energetic man "is one who succeeds by his efforts in turning his wishful phantasies into reality," whereas the artist "can transform his phantasies into artistic creations instead of into symptoms ... the doom of neurosis."

In the context of occurrences of the mental disorder known as schizophrenia, individuals who exhibit symptoms fulfilling this particular classification might be experiencing fantasies as part of the diagnosis (Shneidman, E. S. 1948). Scientific investigation into activity of the so-called default network within the brain (Randy Buckner et al. 2008) has shown individuals diagnosed with schizophrenia have high levels ("...overactive...") of activity within their brains.

In a study of eighty individuals diagnosed with schizophrenia, it was found one quarter of men who had committed a contact crime against women were motivated by sexually orientated fantasy (A.D. Smith 2008).

Melanie Klein extended Freud's concept of fantasy to cover the developing child's relationship to a world of internal objects. In her thought, this kind of "play activity inside the person is known as 'unconscious fantasy'. And these phantasies are often very violent and aggressive. They are different from ordinary day-dreams or 'fantasies'."

The term "fantasy" became a central issue with the development of the Kleinian group as a distinctive strand within the British Psycho-Analytical Society, and was at the heart of the so-called controversial discussions of the wartime years. "A paper by Susan Isaacs (1952) on 'The nature and function of Phantasy' ... has been generally accepted by the Klein group in London as a fundamental statement of their position." As a defining feature, "Kleinian psychoanalysts regard the unconscious as made up of phantasies of relations with objects. These are thought of as primary and innate, and as the mental representations of instincts ... the psychological equivalents in the mind of defence mechanisms."

Isaacs considered that "unconscious phantasies exert a continuous influence throughout life, both in normal and neurotic people, the difference lying in the specific character of the dominant phantasies." Most schools of psychoanalytic thought would now accept that both in analysis and life, we perceive reality through a veil of unconscious fantasy. Isaacs however claimed that "Freud's 'hallucinatory wish-fulfilment' and his 'introjection' and 'projection' are the basis of the fantasy life," and how far unconscious fantasy was a genuine development of Freud's ideas, how far it represented the formation of a new psychoanalytic paradigm, is perhaps the key question of the controversial discussions.

Lacan engaged from early on with "the phantasies revealed by Melanie Klein ... the "imago" of the mother ... this shadow of the "bad internal objects"" — with the Imaginary. Increasingly, however, it was Freud's idea of fantasy as a kind of "screen-memory, representing something of more importance with which it was in some way connected" that was for him of greater importance. Lacan came to believe that "the phantasy is never anything more than the screen that conceals something quite primary, something determinate in the function of repetition."

Phantasies thus both link to and block off the individual's unconscious, his kernel or real core: ""subject and real are to be situated on either side of the split, in the resistance of the phantasy"", which thus comes close to the centre of the individual's personality and its splits and conflicts. "The subject situates himself as determined by the phantasy ... whether in the dream or in any of the more or less well-developed forms of day-dreaming;" and as a rule "a subject's fantasies are close variations on a single theme ... the 'fundamental fantasy' ... minimizing the variations in meaning which might otherwise cause a problem for desire."

The goal of therapy thus became ""la traversee du fantasme", the crossing over, traversal, or traversing of the fundamental fantasy." For Lacan, "The traversing of fantasy involves the subject's assumption of a new position with respect to the Other as language and the Other as desire ... a utopian moment beyond neurosis." The question he was left with was "What, then, does he who has passed through the experience ... who has traversed the radical phantasy ... become?."

The postmodern intersubjectivity of the 21st century has seen a new interest in fantasy as a form of interpersonal communication. Here, we are told, "We need to go beyond the pleasure principle, the reality principle, and repetition compulsion to ... the "fantasy principle" - not, as Freud did, reduce fantasies to wishes ... [but consider] all other imaginable emotions" and thus envisage emotional fantasies as a possible means of moving beyond stereotypes to more nuanced forms of personal and social relating.

Such a perspective "sees emotions as central to developing fantasies about each other that are not determined by collective 'typifications'."

Two characteristics of someone with narcissistic personality disorder are:



</doc>
<doc id="10814" url="https://en.wikipedia.org/wiki?curid=10814" title="Surnames by country">
Surnames by country

Surname conventions and laws vary around the world. This article gives an overview of surnames around the world.

In Argentina, normally only one family name, the father's paternal family name, is used and registered, as in English-speaking countries. However, it is possible to use both the paternal and maternal name. For example, if "Ana Laura Melachenko" and "Emanuel Darío Guerrero" had a daughter named "Adabel Anahí", her full name could be "Adabel Anahí Guerrero Melachenko". Women, however, do not change their family names upon marriage and continue to use their birth family names instead of their husband's family names. However, women have traditionally, and some still choose to use the old Spanish custom of adjoining ""de"" and her husband's surname to her own name. For example, if Paula Segovia marries Felipe Cossia, she might keep her birth name or become "Paula Segovia de Cossia" or "Paula Cossia".

There are some province offices where a married woman can use only her birth name, and some others where she has to use the complete name, for legal purposes. The Argentine Civilian Code states both uses are correct, but police offices and passports are issued with the complete name. Today most women prefer to maintain their birth name given that "de" can be interpreted as meaning they belong to their husbands.

When Eva Duarte married Juan Domingo Perón, she could be addressed as Eva Duarte de Perón, but the preferred style was Eva Perón, or the familiar and affectionate "Evita" (little Eva).

Combined names come from old traditional families and are considered one last name, but are rare. Although Argentina is a Spanish-speaking country, it is also composed of other varied European influences, such as Italian, French, Russian, German, etc.

Children typically use their fathers' last names only. Some state offices have started to use both last names, in the traditional father then mother order, to reduce the risk of a person being mistaken for others using the same name combinations, e.g. if Eva Duarte and Juan Perón had a child named Juan, he might be misidentified if he were called "Juan Perón", but not if he was known as Juan Perón Duarte.

In early 2008, some new legislation is under consideration that will place the mother's last name ahead the father's last name, as it is done in Portuguese-speaking countries and only optionally in Spain, despite Argentina being a Spanish-speaking country.

In Chile, marriage has no effect at all on either of the spouses' names, so people keep their birth names for all their life, no matter how many times marital status, theirs or that of their parents, may change. However, in some upper-class circles or in older couples, even though considered to be old-fashioned, it is still customary for a wife to use her husband's name as reference, as in "Doña María Inés de Ramírez" (literally Lady María Inés (wife) of Ramírez).

Children will always bear the surname of the father followed by that of the mother, but if there is no known father and the mother is single, the children can bear either both of her mother's surnames or the mother's first surname followed by any of the surnames of the mother's parents or grandparents, or the child may bear the mother's first surname twice in a row.

France 
Belgium 
Canadian 

There are about 1,000,000 different family names in German. German family names most often derive from given names, geographical names, occupational designations, bodily attributes or even traits of character. Hyphenations notwithstanding, they mostly consist of a single word; in those rare cases where the family name is linked to the given names by particles such as "von" or "zu", they usually indicate noble ancestry. Not all noble families used these names (see Riedesel), while some farm families, particularly in Westphalia, used the particle "von" or "zu" followed by their farm or former farm's name as a family name (see "Meyer zu Erpen").

Family names in German-speaking countries are usually positioned last, after all given names. There are exceptions, however: in parts of Austria and Bavaria and the Alemannic-speaking areas, the family name is regularly put in front of the first given name. Also in many – especially rural – parts of Germany, to emphasize family affiliation there is often an inversion in colloquial use, in which the family name becomes a possessive: "Rüters Erich", for example, would be Erich of the Rüter family.

In Germany today, upon marriage, both partners can choose to keep their birth name or choose either partner's name as the common name. In the latter case the partner whose name wasn't chosen can keep his birth name hyphenated to the new name (e.g. "Schmidt" and "Meyer" choose to marry under the name "Meyer". The former "Schmidt" can choose to be called "Meyer", "Schmidt-Meyer" or "Meyer-Schmidt"), but any children will only get the single common name. In the case that both partners keep their birth name they must decide on one of the two family names for all their future children. (German name)

Changing one's family name for reasons other than marriage, divorce or adoption is possible only if the application is approved by the responsible government agency. Permission will usually be granted if:

Otherwise, name changes will normally not be granted.

The Netherlands and Belgium (Flanders)

In Scandinavia, family names often, but certainly not always, originate from a patronymic. In Denmark and Norway, the corresponding ending is "-sen", as in "Karlsen". Names ending with "dotter/datter" (daughter), such as "Olofsdotter", are rare but occurring, and only apply to women. Today, the patronymic names are passed on similarly to family names in other Western countries, and a person's father does not have to be called Karl if he or she has the surname Karlsson. However, in 2006 Denmark reinstated patronymic and matronymic surnames as an option. Thus, parents Karl Larsen and Anna Hansen can name a son Karlssøn or Annasøn and a daughter Karlsdatter or Annasdatter.

Before the 19th century there was the same system in Scandinavia as in Iceland today. Noble families, however, as a rule adopted a family name, which could refer to a presumed or real forefather (e.g. Earl Birger Magnusson "Folkunge" ) or to the family's coat of arms (e.g. King Gustav Eriksson "Vasa"). In many surviving family noble names, such as "Silfversparre" ("silver chevron"; in modern spelling, "Silver-") or "Stiernhielm" ("star-helmet"; in modernized spelling, "stjärnhjälm"), the spelling is obsolete, but since it applies to a name, remains unchanged. (Some names from relatively modern times also use archaic or otherwise aberrant spelling as a stylistic trait; e.g. "-quist" instead of standard "-kvist" "twig" or "-grén" instead of standard "-gren", "branch".)

Later on, people from the Scandinavian middle classes, particularly artisans and town dwellers, adopted names in a similar fashion to that of the nobility. Family names joining two elements from nature such as the Swedish "Bergman" ("mountain man"), "Holmberg" ("island mountain"), "Lindgren" ("linden branch"), "Sandström" ("sand stream") and "Åkerlund" ("field meadow") were quite frequent and remain common today. The same is true for similar Norwegian and Danish names.
Another common practice was to adopt one's place of origin as a middle or surname.

Even more important a driver of change was the need, for administrative purposes, to develop a system under which each individual had a "stable" name from birth to death. In the old days, people would be known by their name, patronymic and the farm they lived at. This last element would change if a person got a new job, bought a new farm, or otherwise came to live somewhere else. (This is part of the origin, in this part of the world, of the custom of women changing their names upon marriage. Originally it indicated, basically, a change of address, and from older times, there are numerous examples of men doing the same thing). The many patronymic names may derive from the fact that people who moved from the country to the cities, also gave up the name of the farm they came from. As a worker, you passed by your father's name, and this name passed on to the next generation as a family name. Einar Gerhardsen, the Norwegian prime minister, used a true patronym, as his father was named Gerhard Olsen (Gerhard, the son of Ola). Gerhardsen passed his own patronym on to his children as a family name. This has been common in many working-class families. The tradition of keeping the farm name as a family name got stronger during the first half of the 20th century in Norway.

These names often indicated the place of residence of the family. For this reason, Denmark and Norway have a very high incidence of last names derived from those of farms, many signified by the suffixes like "-bø", "-rud", "-heim/-um", "-land" or "-set" (these being examples from Norway). In Denmark, the most common suffix is "-gaard" — the modern spelling is "gård" in Danish and can be either "gård" or "gard" in Norwegian, but as in Sweden, archaic spelling persists in surnames. The most well-known example of this kind of surname is probably "Kierkegaard" (combined by the words "kirke/kierke" (= church) and "gaard" (= farm) meaning "the farm located by the Church". It is, however, a common misunderstanding that the name relates to its direct translation: churchyard/cemetery), but many others could be cited. It should also be noted that, since the names in question are derived from the original owners' domiciles, the possession of this kind of name is no longer an indicator of affinity with others who bear it.

In many cases, names were taken from the nature around them. In Norway, for instance, there is an abundance of surnames based on coastal geography, with suffixes like "-strand", "-øy", "-holm", "-vik", "-fjord" or "-nes". Like the names derived from farms, most of these family names reflected the family's place of residence at the time the family name was "fixed", however. A family name such as Swedish "Dahlgren" is derived from "dahl" meaning valley and "gren" meaning branch; or similarly "Upvall" meaning "upper-valley"; It depends on the Scandinavian country, language, and dialect.

In Scandinavia family names often, but certainly not always, originate from a patronymic. Later on, people from the Scandinavian middle classes, particularly artisans and town dwellers, adopted surnames in a similar fashion to that of the gentry. Family names joining two elements from nature such as the Swedish "Bergman" ("mountain man"), "Holmberg" ("island mountain"), "Lindgren" ("linden branch"), "Sandström" ("sand stream") and "Åkerlund" ("field grove") were quite frequent and remain common today.

Finland including Karelia and Estonia was the eastern part of The Kingdom of Sweden from its unification around 1100 - 1200 AD until the year 1809 when Finland was conquered by Russia. During the Russian revolution 1917, Finland proclaimed the republic Finland and Sweden and many European countries rapidly acknowledged the new nation Finland. Finland has mainly Finnish (increasing) and Swedish (decreasing) surnames and first names. There are two predominant surname traditions among the "Finnish" in Finland: the West Finnish and the East Finnish. The surname traditions of "Swedish-speaking" farmers, fishermen and craftsmen resembles the West Finnish tradition, while smaller populations of "Sami" and "Romani people" have traditions of their own. Finland was exposed to a very small immigration from Russia, so Russian names barely exists.

Until the mid 20th Century, Finland was a predominantly agrarian society, and the names of West Finns were based on their association with a particular area, farm, or homestead, e.g. "Jaakko Jussila" ("Jaakko from the farm of Jussi"). On the other hand, the East Finnish surname tradition dates back to the 13th century. There, the Savonians pursued slash-and-burn agriculture which necessitated moving several times during a person's lifetime. This in turn required the families to have surnames, which were in wide use among the common folk as early as the 13th century. By the mid-16th century, the East Finnish surnames had become hereditary. Typically, the oldest East Finnish surnames were formed from the first names of the patriarchs of the families, e.g. "Ikävalko", "Termonen", "Pentikäinen". In the 16th, 17th, and 18th centuries, new names were most often formed by adding the name of the former or current place of living (e.g. "Puumalainen" < Puumala). In the East Finnish tradition, the women carried the family name of their fathers in female form (e.g. "Puumalatar" < "Puumalainen"). By the 19th century, this practice fell into disuse due to the influence of the West-European surname tradition.

In Western Finland, agrarian names dominated, and the last name of the person was usually given according to the farm or holding they lived on. In 1921, surnames became compulsory for all Finns. At this point, the agrarian names were usually adopted as surnames. A typical feature of such names is the addition of prefixes "Ala-" (Sub-) or "Ylä-" (Up-), giving the location of the holding along a waterway in relation of the main holding. (e.g. "Yli-Ojanperä", "Ala-Verronen"). The Swedish speaking farmers along the coast of Österbotten usually used two surnames – one which pointed out the father's name (e.g. "Eriksson", "Andersson", "Johansson") and one which related to the farm or the land their family or bigger family owned or had some connection to (e.g. "Holm", "Fant", "Westergård", "Kloo"). So a full name could be "Johan Karlsson Kvist", for his daughter "Elvira Johansdotter Kvist", and when she married a man with the Ahlskog farm, Elvira kept the first surname Johansdotter but changed the second surname to her husbands (e.g. "Elvira Johansdotter Ahlskog"). During the 20th century they started to drop the -son surname while they kept the second. So in Western Finland the Swedish speaking had names like "Johan Varg", "Karl Viskas", "Sebastian Byskata" and "Elin Loo", while the Swedes in Sweden at the other side of the Baltic Sea kept surnames ending with "-son" ((e.g. "Johan Eriksson", "Thor Andersson", "Anna-Karin Johansson").

A third tradition of surnames was introduced in south Finland by the Swedish-speaking upper and middle classes, which used typical German and Swedish surnames. By custom, all Finnish-speaking persons who were able to get a position of some status in urban or learned society, discarded their Finnish name, adopting a Swedish, German or (in the case of clergy) Latin surname. In the case of enlisted soldiers, the new name was given regardless of the wishes of the individual.

In the late 19th and early 20th century, the overall modernization process, and especially the political movement of fennicization, caused a movement for adoption of Finnish surnames. At that time, many persons with a Swedish or otherwise foreign surname changed their family name to a Finnish one. The features of nature with endings "-o/ö", "-nen" ("Meriö" < "Meri" "sea", "Nieminen" < "Niemi" "point") are typical of the names of this era, as well as more or less direct translations of Swedish names ("Paasivirta" < "Hällström").

In 21st-century Finland, the use of surnames follows the German model. Every person is legally obligated to have a first and last name. At most, three first names are allowed. The Finnish married couple may adopt the name of either spouse, or either spouse (or both spouses) may decide to use a double name. The parents may choose either surname or the double surname for their children, but all siblings must share the same surname. All persons have the right to change their surname once without any specific reason. A surname that is un-Finnish, contrary to the usages of the Swedish or Finnish languages, or is in use by any person residing in Finland cannot be accepted as the new name, unless valid family reasons or religious or national customs give a reason for waiving this requirement. However, persons may change their surname to any surname that has ever been used by their ancestors if they can prove such claim. Some immigrants have had difficulty naming their children, as they must choose from an approved list based on the family's household language.

In the Finnish language, both the root of the surname and the first name can be modified by consonant gradation regularly when inflected to a case.

In Iceland, most people have no family name; a person's last name is most commonly a patronymic, i.e. derived from the father's first name. For example, when a man called "Karl" has a daughter called "Anna" and a son called "Magnús", their full names will typically be "Anna Karlsdóttir" ("Karl's daughter") and "Magnús Karlsson" ("Karl's son"). The name is not changed upon marriage.

Slavic countries are noted for having masculine and feminine versions for many (but not all) of their names. In most countries the use of a feminine form is obligatory in official documents as well as in other communication, except for foreigners. In some countries only the male form figures in official use (Bosnia and Herzegovina, Croatia, Montenegro, Serbia, Slovenia), but in communication (speech, print) a feminine form is often used.

In Slovenia the last name of a female is the same as the male form in official use (identification documents, letters). In speech and descriptive writing (literature, newspapers) a female form of the last name is regularly used.

If the name has no suffix, it may or may not have a feminine version. Sometimes it has the ending changed (such as the addition of "-a"). In the Czech Republic and Slovakia, suffixless names, such as those of German origin, are feminized by adding "-ová" (for example, "Schusterová").

Bulgarian names usually consist of three components – given name, patronymic (based on father's name), family name.

Given names have many variations, but the most common names have Christian/Greek (e.g. Maria, Ivan, Christo, Peter, Pavel), Slavic (Ognyan, Miroslav, Tihomir) or Protobulgarian (Krum, Asparukh) (pre-Christian) origin.
Father's names normally consist of the father's first name and the "-ov" (male) or "-ova" (female) or "-ovi" (plural) suffix.

Family names usually also end with the "-ov", "-ev" (male) or "-ova", "-eva" (female) or "-ovi", "-evi" (plural) suffix.

In many cases (depending on the name root) the suffixes can be also "-ski" (male and plural) or "-ska" (female); "-ovski", "-evski" (male and plural) or "-ovska", "-evska" (female); "-in" (male) or "-ina" (female) or "-ini" (plural); etc.

The meaning of the suffixes is similar to the English word "of", expressing membership in/belonging to a family.
For example, the family name Ivanova means a person belonging to the Ivanovi family.

A father's name Petr*ov* means son of Peter.

Regarding the different meaning of the suffixes, "-ov", "-ev"/"-ova", "-eva" are used for expressing relationship to the father and "-in"/"-ina" for relationship to the mother (often for orphans whose father is dead).

Names of Czech people consist of given name ("křestní jméno") and surname ("příjmení"). Usage of the second or middle name is not common. Feminine names are usually derived from masculine ones by a suffix "-ová" ("Nováková") or "-á" for names being originally adjectives ("Veselá"), sometimes with a little change of original name's ending ("Sedláčková" from "Sedláček" or "Svobodová" from "Svoboda"). Women usually change their family names when they get married. The family names are usually nouns ("Svoboda", "Král", "Růžička", "Dvořák", "Beneš"), adjectives ("Novotný", "Černý", "Veselý") or past participles of verbs ("Pospíšil"). There are also a couple of names with more complicated origin which are actually complete sentences ("Skočdopole", "Hrejsemnou" or "Vítámvás"). The most common Czech family name is "Novák" / "Nováková".

In addition, many Czechs and some Slovaks have German surnames due to mixing between the ethnic groups over the past thousand years. Deriving women's names from German and other foreign names is often problematic since foreign names do not suit Czech language rules, although most commonly "-ová" is simply added ("Schmidtová"; umlauts are often, but not always, dropped, e.g. "Müllerová"), or the German name is respelled with Czech spelling ("Šmitová"). Hungarian names, which can be found fairly commonly among Slovaks, can also be either left unchanged (Hungarian "Nagy", fem. "Nagyová") or respelled according to Czech/Slovak orthography (masc. "Naď", fem. "Naďová").

In Poland and most of the former Polish–Lithuanian Commonwealth, surnames first appeared during the late Middle Ages. They initially denoted the differences between various people living in the same town or village and bearing the same name. The conventions were similar to those of English surnames, using occupations, patronymic descent, geographic origins, or personal characteristics. Thus, early surnames indicating occupation include "Karczmarz" ("innkeeper"), "Kowal" ("blacksmith"), "Złotnik" ("gold smith") and "Bednarczyk" ("young cooper"), while those indicating patronymic descent include "Szczepaniak" ("Son of "Szczepan"), "Józefowicz" ("Son of "Józef"), and "Kaźmirkiewicz" ("Son of "Kazimierz""). Similarly, early surnames like "Mazur" ("the one from Mazury") indicated geographic origin, while ones like "Nowak" ("the new one"), "Biały" ("the pale one"), and "Wielgus" ("the big one") indicated personal characteristics.

In the early 16th century, ( the Polish Renaissance), toponymic names became common, especially among the nobility. Initially, the surnames were in a form of "[first name] "z" ("de", "of") [location]". Later, most surnames were changed to adjective forms, e.g. "Jakub Wiślicki" ("James of Wiślica") and "Zbigniew Oleśnicki" (""Zbigniew" of Oleśnica"), with masculine suffixes "-ski", "-cki", "-dzki" and "-icz" or respective feminine suffixes "-ska", "-cka", "-dzka" and "-icz" on the east of Polish–Lithuanian Commonwealth. Names formed this way are adjectives grammatically, and therefore change their form depending on sex; for example, "Jan Kowalski" and "Maria Kowalska" collectively use the plural "Kowalscy".

Names with masculine suffixes "-ski", "-cki", and "-dzki", and corresponding feminine suffixes "-ska", "-cka", and "-dzka" became associated with noble origin. Many people from lower classes successively changed their surnames to fit this pattern. This produced many "Kowalski"s, "Bednarski"s, "Kaczmarski"s and so on.

A separate class of surnames derive from the names of noble clans. These are used either as separate names or the first part of a double-barrelled name. Thus, persons named "Jan Nieczuja" and "Krzysztof Nieczuja-Machocki" might be related. Similarly, after World War I and World War II, many members of Polish underground organizations adopted their war-time pseudonyms as the first part of their surnames. "Edward Rydz" thus became Marshal of Poland "Edward Śmigły-Rydz" and "Zdzisław Jeziorański" became "Jan Nowak-Jeziorański".

A full Russian name consists of personal (given) name, patronymic, and family name (surname).

Most Russian family names originated from patronymics, that is, father's name usually formed by adding the adjective suffix "-ov(a)" or "-ev(a)". Contemporary patronymics, however, have a substantive suffix "-ich" for masculine and the adjective suffix "-na" for feminine.

For example, the proverbial triad of most common Russian surnames follows:

Feminine forms of these surnames have the ending "-a":

Such a pattern of name formation is not unique to Russia or even to the Eastern and Southern Slavs in general; quite common are also names derived from professions, places of origin, and personal characteristics, with various suffixes (e.g. "-in(a)" and "-sky (-skaya)").

Professions:

Places of origin:

Personal characteristics:

A considerable number of "artificial" names exists, for example, those given to seminary graduates; such names were based on Great Feasts of the Orthodox Church or Christian virtues.

Great Orthodox Feasts:

Christian virtues:

Many freed serfs were given surnames after those of their former owners. For example, a serf of the Demidov family might be named "Demidovsky", which translates roughly as "belonging to Demidov" or "one of Demidov's bunch".

Grammatically, Russian family names follow the same rules as other nouns or adjectives (names ending with "-oy", "-aya" are grammatically adjectives), with exceptions: some names do not change in different cases and have the same form in both genders (for example, "Sedykh", "Lata").

Surnames of some South Slavic groups such as Serbs, Croats, Montenegrins, and Bosniaks traditionally end with the suffixes "-ić" and "-vić" (often transliterated to English and other western languages as "ic", "ich", "vic" or "vich". The v is added in the case of a name to which "-ić" is appended would otherwise end with a vowel, to avoid double vowels with the "i" in "-ić".) These are a diminutive indicating descent i.e. "son of". In some cases the family name was derived from a profession (e.g. blacksmith – "Kovač" → "Kovačević").

An analogous ending is also common in Slovenia. As the Slovenian language does not have the softer consonant "ć", in Slovene words and names only "č" is used. So that people from the former Yugoslavia need not change their names, in official documents "ć" is also allowed (as well as "Đ / đ"). Thus, one may have two surname variants, e.g.: Božič, Tomšič (Slovenian origin or assimilated) and Božić, Tomšić (roots from the Serbo-Croat language continuum area). Slovene names ending in -ič do not necessarily have a patrimonial origin.

In general family names in all of these countries follow this pattern with some family names being typically Serbian, some typically Croat and yet others being common throughout the whole linguistic region.

Children usually inherit their fathers' family name. In an older naming convention which was common in Serbia up until the mid-19th century, a person's name would consist of three distinct parts: the person's given name, the patronymic derived from the father's personal name, and the family name, as seen, for example, in the name of the language reformer Vuk Stefanović Karadžić.

Official family names do not have distinct male or female forms, except in Macedonia, though a somewhat archaic unofficial form of adding suffixes to family names to form female form persists, with "-eva", implying "daughter of" or "female descendant of" or "-ka", implying "wife of" or "married to". In Slovenia the feminine form of a surname ("-eva" or "-ova") is regularly used in non-official communication (speech, print), but not for official IDs or other legal documents.

Bosniak Muslim names follow the same formation pattern but are usually derived from proper names of Islamic origin, often combining archaic Islamic or feudal Turkish titles i.e. Mulaomerović, Šabanović, Hadžihafizbegović, etc. Also related to Islamic influence is the prefix "Hadži-" found in some family names. Regardless of religion, this prefix was derived from the honorary title which a distinguished ancestor earned by making a pilgrimage to either Christian or Islamic holy places; Hadžibegić, being a Bosniak Muslim example, and Hadžiantić an Orthodox Christian one.

In Croatia where tribal affiliations persisted longer, Lika, Herzegovina etc., originally a family name, came to signify practically all people living in one area, clan land or holding of the nobles. The Šubić family owned land around the Zrin River in the Central Croatian region of Banovina. The surname became Šubić Zrinski, the most famous being Nikola Šubić Zrinski.

In Montenegro and Herzegovina, family names came to signify all people living within one clan or bratstvo. As there exists a strong tradition of inheriting personal names from grandparents to grandchildren, an additional patronymic usually using suffix "-ov" had to be introduced to make distinctions between two persons bearing the same personal name and the same family name and living within same area. A noted example is Marko Miljanov Popović, i.e. Marko, son of Miljan, from Popović family.

Due to discriminatory laws in the Austro-Hungarian Empire, some Serb families of Vojvodina discarded the suffix "-ić" in an attempt to mask their ethnicity and avoid heavy taxation.

The prefix "Pop-" in Serbian names indicates descent from a priest, for example Gordana Pop Lazić, i.e. descendent of Pop Laza.

Some Serbian family names include prefixes of Turkish origin, such as "Uzun-" meaning tall, or "Kara-", black. Such names were derived from nicknames of family ancestors. A famous example is Karađorđević, descendents of Đorđe Petrović, known as Karađorđe or Black Đorđe.

Among the Bulgarians, another South Slavic people, the typical surname suffix is "-ov" (Ivanov, Kovachev), although other popular suffixes also exist.

In North Macedonia, the most popular suffix today is "-ski".

Slovenes have a great variety of surnames, most of them differentiated according to region. Surnames ending in -ič are less frequent than among Croats and Serbs. There are typically Slovenian surnames ending in -ič, such as Blažič, Stanič, Marušič. Many Slovenian surnames, especially in the Slovenian Littoral, end in -čič (Gregorčič, Kocijančič, Miklavčič, etc.), which is uncommon for other South Slavic peoples (except the neighboring Croats, e.g. Kovačić, Jelačić, Kranjčić, etc.). On the other hand, surname endings in -ski and -ov are rare, they can denote a noble origin (especially for the -ski, if it completes a toponym) or a foreign (mostly Czech) origin. One of the most typical Slovene surname endings is -nik (Rupnik, Pučnik, Plečnik, Pogačnik, Podobnik) and other used surname endings are -lin (Pavlin, Mehlin, Ahlin, Ferlin), -ar (Mlakar, Ravnikar, Smrekar Tisnikar) and -lj (Rugelj, Pucelj, Bagatelj, Bricelj). Many Slovenian surnames are linked to Medieval rural settlement patterns. Surnames like Novak (literally, "the new one") or Hribar (from "hrib", hill) were given to the peasants settled in newly established farms, usually in high mountains. Peasant families were also named according to the owner of the land which they cultivated: thus, the surname Kralj (King) or Cesar (Emperor) was given to those working on royal estates, Škof (Bishop) or Vidmar to those working on ecclesiastical lands, etc. Many Slovenian surnames are named after animals (Medved – bear, Volk, Vovk or Vouk – wolf, Golob – pigeon, Lisjak – fox, Orel – eagle, Zajc or Zajec – rabbit, etc.). Many are named after neighbouring peoples: Horvat, Hrovat, or Hrovatin (Croat), Furlan (Friulian), Nemec (German), Lah (Italian), Vogrin, Vogrič or Vogrinčič (Hungarian), Vošnjak (Bosnian), Čeh (Czech), Turk (Turk), or different Slovene regions: Kranjc, Kranjec or Krajnc (from Carniola), Kraševec (from the Kras), Korošec (from Carinthia), Kočevar or Hočevar (from the Gottschee county).

In Slovenia last name of a female is the same as the male form in official use (identification documents, letters). In speech and descriptive writing (literature, newspapers) a female form of the last name is regularly used. Examples: Novak (m.) & Novakova (f.), Kralj (m.) & Kraljeva (f.), Mali (m.) ˛& Malijeva (f.) (this is different from the Czech principle, though forexample "mali" in Slovenian also means "small" the last name is not changed grammatically to "mala".) So we have Maja Novak on the ID card and Maja Novakova in communication; Tjaša Mali and Tjaša Malijeva; respectively. Diminutive forms of last names for females are also available: Novakovka, Kraljevka. As for pronunciation, in Slovenian there is some leeway regarding accentuation. Depending on the region or local usage, you may have either Nóvak & Nóvakova or, more frequently, Novák & Novákova. Accent marks are normally not used.

Ukrainian and Belarusian names evolved from the same Old East Slavic and Ruthenian language (western Rus’) origins. Ukrainian and Belarusian names share many characteristics with family names from other Slavic cultures. Most prominent are the shared root words and suffixes. For example, the root "koval" (blacksmith) compares to the Polish "kowal", and the root "bab" (woman) is shared with Polish, Slovakian, and Czech. The suffix "-vych" (son of) corresponds to the South Slavic "-vic", the Russian "-vich", and the Polish "-wicz", while "-sky", "-ski", and "-ska" are shared with both Polish and Russian, and "-ak" with Polish.

However some suffixes are more uniquely characteristic to Ukrainian and Belarusian names, especially: "-chuk" (Western Ukraine), "-enko" (all other Ukraine) (both son of), "-ko" (little [masculine]), "-ka" (little [feminine]), "-shyn", and "-uk". See, for example, Mihalko, Ukrainian Presidents Leonid Kravchuk, and Viktor Yushchenko, Belarusian President Alexander Lukashenko, or former Soviet diplomat Andrei Gromyko. Such Ukrainian and Belarusian names can also be found in Russia, Poland, or even other Slavic countries (e.g. Croatian general Zvonimir Červenko), but are due to importation by Ukrainian, Belarusian, or Rusyn ancestors.

The given name is always followed by the father's first name, then the father's family surname.
Some surnames have a prefix of "ibn"- meaning son of ("ould"- in Mauritania)
The surnames follow similar rules defining a relation to a clan, family, place etc.
Some Arab countries have differences due to historic rule by the Ottoman Empire or due to being a different minority.

A large number of Arabic last names start with "Al-" which means "The"

Arab States of the Persian Gulf.
Names mainly consist of the person's name followed by the father's first name connected by the word "ibn" or "bin" (meaning "son of"). The last name either refers to the name of the tribe the person belongs to, or to the region, city, or town he/she originates from. In exceptional cases, members of the royal families or ancient tribes mainly, the title (usually H.M./H.E., Prince, or Sheikh) is included in the beginning as a prefix, and the first name can be followed by four names, his father, his grandfather, and great – grandfather, as a representation of the purity of blood and to show the pride one has for his ancestry.

In Arabic-speaking Levantine countries (Jordan, Lebanon, Palestine, Syria) it's common to have family names associated with a certain profession or craft, such as "Al-Haddad"/"Haddad" which means "Blacksmith" or "Al-Najjar"/"Najjar" which means "Carpenter".

In India, surnames are placed as last names or before first names, which often denote: village of origin, caste, clan, office of authority their ancestors held, or trades of their ancestors.

The largest variety of surnames is found in the states of Maharashtra and Goa, which numbers more than the rest of India together. Here surnames are placed last, the order being: the given name, followed by the father's name, followed by the family name. The majority of surnames are derived from the place where the family lived, with the 'ker' (Marathi) or 'Kar'(Konkani) suffix, for example, Mumbaiker, Puneker, Aurangabadker or Tendulkar, Parrikar, Mangeshkar, Mahendrakar. Another common variety found in Maharashtra and Goa are the ones ending in 'e'. These are usually more archaic than the 'Kar's and usually denote medieval clans or professions like Rane, Salunkhe, Gupte, Bhonsle, Ranadive, Rahane, Hazare, Apte, Satpute, Shinde, Sathe, Londhe, Salve, Kale, Gore, Godbole, etc.

In Andhra Pradesh and Tamil Nadu, surnames usually denote family names. It is easy to track family history and the caste they belonged to using a surname.

It is a common in Kerala and some other parts of South India that the spouse adopts her husband's first name instead of his family or surname name after marriage.

India is a country with numerous distinct cultural and linguistic groups. Thus, Indian surnames, where formalized, fall into seven general types.

Surnames are based on:

The convention is to write the first name followed by middle names and surname. It is common to use the father's first name as the middle name or last name even though it is not universal. In some Indian states like Maharashtra, official documents list the family name first, followed by a comma and the given names.

Traditionally, wives take the surname of their husband after marriage. In modern times, in urban areas at least, this practice is not universal and some wives either suffix their husband's surname or do not alter their surnames at all. In some rural areas, particularly in North India, wives may also take a new first name after their nuptials. Children inherit their surnames from their father.

Jains generally use Jain, Shah, Firodia, Singhal or Gupta as their last names.
Sikhs generally use the words "Singh" ("lion") and "Kaur" ("princess") as surnames added to the otherwise unisex first names of men and women, respectively. It is also common to use a different surname after Singh in which case Singh or Kaur are used as middle names (Montek Singh Ahluwalia, Surinder Kaur Badal). The tenth Guru of Sikhism ordered (Hukamnama) that any man who considered himself a Sikh must use "Singh" in his name and any woman who considered herself a Sikh must use "Kaur" in her name. Other middle names or honorifics that are sometimes used as surnames include Kumar, Dev, Lal, and Chand.

The modern-day spellings of names originated when families translated their surnames to English, with no standardization across the country. Variations are regional, based on how the name was translated from the local language to English in the 18th, 19th or 20th centuries during British rule. Therefore, it is understood in the local traditions that Agrawal and Aggarwal represent the same name derived from Uttar Pradesh and Punjab respectively. Similarly, Tagore derives from Bengal while Thakur is from Hindi-speaking areas. The officially recorded spellings tended to become the standard for that family. In the modern times, some states have attempted standardization, particularly where the surnames were corrupted because of the early British insistence of shortening them for convenience. Thus Bandopadhyay became Banerji, Mukhopadhay became Mukherji, Chattopadhyay became Chatterji, etc. This coupled with various other spelling variations created several surnames based on the original surnames. The West Bengal Government now insists on re-converting all the variations to their original form when the child is enrolled in school.

Some parts of Sri Lanka, Thailand, Nepal, Myanmar, and Indonesia have similar patronymic customs to those of India.

Nepali surnames are divided into three origins; Indo-Aryan languages, Tibeto-Burman languages and indigenous origins. Surnames of Khas community contains toponyms as Ghimire, Dahal, Pokharel, Sapkota from respective villages, occupational names as (Adhikari, Bhandari, Karki, Thapa). Many Khas surnames includes suffix as -wal, -al as in Katwal, Silwal, Dulal, Khanal, Khulal, Rijal. Kshatriya titles such as Bista, Kunwar, Rana, Rawat, Rawal, Dhami, Shah, Thakuri, Chand, were taken as surnames by various Kshetri and Thakuris. Khatri Kshetris share surnames with mainstream Pahari Bahuns. Other popular Chhetri surnames include Basnyat, Bogati, Budhathoki, Khadka, Khandayat, Mahat, Raut. Similarly, Brahmin surnames such as Acharya, Bhatta, Joshi, Pandit, Sharma, Upadhyay were taken by Pahari Bahuns. Jaisi Bahuns bear distinct surnames as Kattel, Banstola, Jaisi, Padhya and share surnames with mainstream Bahuns. Other Bahun surnames include Aryal, Bhattarai, Banskota, Chaulagain, Devkota, Dhakal, Gyawali, Koirala, Mainali, Pandey, Panta, Paudel, Regmi, Subedi, Tiwari, Upreti, Lamsal, and Dhungel. Many Indian immigrants into Pahari zone are assimilated under Khas peoples and they carried ancestral clan names as Marhatta, Rathaur, Chauhan. Khas-Dalits surnames include Kami, Bishwakarma or B.K., Damai, Mijar, Dewal, Pariyar, Ranapaheli, Sarki. Newar groups of multiethnic background bears both Indo-Aryan surnames (like Shrestha, Joshi, Pradhan) and indigenous surnames like Maharjan, Dangol. Magars bear surnames derived from Khas peoples such as Baral, Budhathoki, Lamichhane, Thapa and indigenous origins as Dura, Gharti, Pun, Pulami. Other Himalayan Mongoloid castes bears Tibeto-Burmese surnames like Gurung, Tamang, Thakali, Sherpa. Various Kiranti ethnic group contains many Indo-Aryan surnames of Khas origin which were awarded by the government of Khas peoples. These surnames are Rai, Subba, Jimmi, Dewan depending upon job and position hold by them. Terai community consists both Indo-Aryan and Indigenous origin surnames. Terai Brahmins bears surnames as Jha, Mishra, Pandit, Tiwari. Terai Rajput and other Kshatriya groups bears the surnames Chauhan, Singh, Rajput, Verma, Pal. Marwari surnames like Agrawal, Jain, Khandelwal, Maheshwari, Tapadia are also common. Nepalese Muslims bears Islamic surnames such as Ali, Ansari, Begum, Khan, Mohammad, Pathan. Other common Terai surnames are Yadav, Mahato, Kamat, Thakur, Dev, Chaudhary, Kayastha.

Pakistani surnames are basically divided in three categories: Arab naming convention, tribal or caste names and ancestral names.

Family names indicating Arab ancestry, e.g. Shaikh, Siddiqui, Abbasi, Syed, Zaidi, Khawaja, Naqvi, Farooqi, Osmani, Alavi, Hassani, and Husseini.

People claiming Afghan ancestry include those with family names ځاځي dzādzi Durrani, Gardezi, Suri, Yousafzai, Afridi, Mullagori, Mohmand, Khattak, Wazir, Mehsud, Niazi.

Family names indicating Turkish heritage include Mughal,(cheema) Baig or Beg, Pasha, Barlas, and Seljuki.

People claiming Indian ancestry include those with family names Barelwi, Lakhnavi, Delhvi, Bilgrami and Rajput.

People claiming Iranian ancestry include those with family names Agha, Bukhari, Firdausi, Ghazali, Gilani, Hamadani, Isfahani, Kashani, Kermani, Khorasani, Farooqui, Mir, Mirza, Montazeri, Nishapuri, Noorani, Kayani, Qizilbash, Saadi, Sabzvari, Shirazi, Sistani, Suhrawardi, Yazdani, Zahedi, and Zand.

Tribal names include Abro Afaqi, Afridi, Khogyani(Khakwani), Amini,[Ansari] Ashrafkhel, Awan, Bajwa, Baloch, Barakzai, Baranzai, Bhatti, Bhutto, Ranjha, Bijarani, Bizenjo, Brohi, Khetran, Bugti, Butt, Farooqui, Gabol, Ghaznavi, Ghilzai, Gichki, Gujjar, Jamali, Jamote, Janjua, Jatoi, Jutt Joyo, Junejo, Karmazkhel, Kayani, Khar, Khattak, Khuhro, Lakhani, Leghari, Lodhi, Magsi, Malik, Mandokhel, Mayo, Marwat, Mengal, Mughal, Palijo, Paracha, Panhwar, Phul, Popalzai, Qureshi & qusmani, Rabbani, Raisani, Rakhshani, Sahi, Swati, Soomro, Sulaimankhel, Talpur, Talwar, Thebo, Yousafzai, and Zamani.

Family names indicating Turkish/ Kurd ancestry, Dogar.
In Pakistan, the official paperwork format regarding personal identity is as follows:

So and so, son of so and so, of such and such tribe or clan and religion and resident of such and such place. For example, Amir Khan s/o Fakeer Khan, tribe Mughal Kayani or Chauhan Rajput, Follower of religion Islam, resident of Village Anywhere, Tehsil Anywhere, District.

A large number of Muslim Rajputs have retained their surnames such as Chauhan, Rathore, Parmar, and Janjua.

In modern Chinese, Japanese, Korean, and Vietnamese, the family name is placed before the given names, although this order may not be observed in translation. Generally speaking, Chinese, Korean, and Vietnamese names do "not" alter their order in English (Mao Zedong, Kim Jong-il, Ho Chi Minh) and Japanese names do (Kenzaburō Ōe). However, numerous exceptions exist, particularly for people born in English-speaking countries such as Yo-Yo Ma. This is sometimes systematized: in all Olympic events, the athletes of the People's Republic of China list their names in the Chinese ordering, while Chinese athletes representing other countries, such as the United States, use the Western ordering. (In Vietnam, the system is further complicated by the cultural tradition of addressing people by their given name, usually with an honorific. For example, Phan Văn Khải is "properly" addressed as Mr. Khải, even though Phan is his family name.)

Chinese family names have many types of origins, some claiming dates as early as the legendary Yellow Emperor (2nd millennium BC):

In history, some changed their surnames due to a naming taboo (from Zhuang 莊 to Yan 嚴 during the era of Liu Zhuang 劉莊) or as an award by the Emperor (Li was often to senior officers during Tang dynasty).

In modern times, some Chinese adopt an English name in addition to their native given names: e.g., adopted the English name Martin Lee. Particularly in Hong Kong and Singapore, the convention is to write both names together: Martin Lee Chu-ming. Owing to the confusion this can cause, a further convention is sometimes observed of capitalizing the surname: Martin L Chu-ming. Sometimes, however, the Chinese given name is forced into the Western system as a middle name ("Martin Chu-ming Lee"); less often, the English given name is forced into the Chinese system ("Lee Chu-ming Martin").

In Japan, the civil law forces a common surname for every married couple, unless in a case of international marriage. In most cases, women surrender their surnames upon marriage, and use the surnames of their husbands. However, a convention that a man uses his wife's family name if the wife is an only child is sometimes observed. A similar tradition called "ru zhui" (入贅) is common among Chinese when the bride's family is wealthy and has no son but wants the heir to pass on their assets under the same family name. The Chinese character "zhui" (贅) carries a money radical (貝), which implies that this tradition was originally based on financial reasons. All their offspring carry the mother's family name. If the groom is the first born with an obligation to carry his own ancestor's name, a compromise may be reached in that the first male child carries the mother's family name while subsequent offspring carry the father's family name. The tradition is still in use in many Chinese communities outside mainland China, but largely disused in China because of social changes from communism. Due to the economic reform in the past decade, accumulation and inheritance of personal wealth made a comeback to the Chinese society. It is unknown if this financially motivated tradition would also come back to mainland China.

In Chinese, Korean, and Singaporean cultures, women keep their own surnames, while the family as a whole is referred to by the surnames of the husbands.

In Hong Kong, some women would be known to the public with the surnames of their husbands preceding their own surnames, such as Anson Chan Fang On Sang. Anson is an English given name, On Sang is the given name in Chinese, Chan is the surname of Anson's husband, and Fang is her own surname. A name change on legal documents is not necessary. In Hong Kong's English publications, her family names would have been presented in small cap letters to resolve ambiguity, e.g. Anson C F On Sang in full or simply Anson Chan in short form.

In Macau, some people have their names in Portuguese spelt with some Portuguese style, such as "Carlos do Rosario Tchiang".

Chinese women in Canada, especially Hongkongers in Toronto, would preserve their maiden names before the surnames of their husbands when written in English, for instance Rosa Chan Leung, where Chan is the maiden name, and Leung is the surname of the husband.

In Chinese, Korean, and Vietnamese, surnames are predominantly monosyllabic (written with one character), though a small number of common disyllabic (or written with two characters) surnames exists (e.g. the Chinese name "Ouyang", the Korean name "Jegal" and the Vietnamese name "Phan-Tran").

Many Chinese, Korean, and Vietnamese surnames are of the same origin, but simply pronounced differently and even transliterated differently overseas in Western nations. For example, the common Chinese surnames Chen, Chan, Chin, Cheng and Tan, the Korean surname Jin, as well as the Vietnamese surname Trần are often all the same exact character 陳. The common Korean surname Kim is also the common Chinese surname Jin, and written 金. The common Mandarin surnames Lin or Lim (林) is also one and the same as the common Cantonese or Vietnamese surname "Lam" and Korean family name Lim (written/pronounced as Im in South Korea). There are people with the surname of Hayashi (林) in Japan too. The common Chinese surname 李, translated to English as Lee, is, in Chinese, the same character but transliterated as Li according to pinyin convention. Lee is also a common surname of Koreans, and the character is identical.

40% of all Vietnamese have the surname Nguyen. This may be because when a new dynasty took power in Vietnam it was custom to adopt that dynasty's surname. The last dynasty in Vietnam was the Nguyen dynasty, so as a result many people have this surname.

In several Northeast Bantu languages such as Kamba, Taita and Kikuyu in Kenya the word "wa" (meaning "of") is inserted before the surname, for instance, Mugo wa Kibiru (Kikuyu) and Mekatilili wa Menza (Mijikenda).

In Burundi and Rwanda, most, if not all surnames have God in it, for example Hakizimana (meaning God cures), Nshimirimana (I thank God) or Havyarimana/Habyarimana (God gives birth). But not all surnames end with the suffix -imana. Irakoze is one of these (technically meaning Thank God, though it is hard to translate it correctly in English or probably any other language). Surnames are often different among immediate family members, as parents frequently choose unique surnames for each child, and women keep their maiden names when married. Surnames are placed before given names and frequently written in capital letters, e.g. HAKIZIMANA Jacques.

The patronymic custom in most of the Horn of Africa gives children the father's first name as their surname. The family then gives the child its first name. Middle names are unknown. So, for example, a person's name might be "Bereket Mekonen ". In this case, "Bereket " is the first name and "Mekonen" is the surname, and also the first name of the father.

The paternal grandfather's name is often used if there is a requirement to identify a person further, for example, in school registration. Also, different cultures and tribes use the father's or grandfather's given name as the family's name. For example, some Oromos use Warra Ali to mean families of Ali, where Ali, is either the householder, a father or grandfather.

In Ethiopia, the customs surrounding the bestowal and use of family names is as varied and complex as the cultures to be found there. There are so many cultures, nations or tribes, that currently there can be no one formula whereby to demonstrate a clear pattern of Ethiopian family names. In general, however, Ethiopians use their father's name as a surname in most instances where identification is necessary, sometimes employing both father's and grandfather's names together where exigency dictates.

Many people in Eritrea have Italian surnames, but all of these are owned by Eritreans of Italian descent.

A full Albanian name consists of a given name (), patronymic () and family name (), for example "Agron Mark Gjoni". The patronymic is simply the given name of the individual's father, with no suffix added. The family name is typically a noun in the definite form or at the very least ends with a vowel or -j (an approximant close to -i). Many traditional last names end with -aj (previously -anj), which is more prevalent in certain regions of Albania and Kosovo.

Proper names in Albanian are fully declinable like any noun (e.g. "Marinelda", genitive case "i/e Marineldës" "of Marinelda").

Armenian surnames almost always have the ending () transliterated into English as -yan or -ian (spelled -ean (եան) in Western Armenian and pre-Soviet Eastern Armenian, of Ancient Armenian or Iranian origin, presumably meaning "son of"), though names with that ending can also be found among Persians and a few other nationalities. Armenian surnames can derive from a geographic location, profession, noble rank, personal characteristic or personal name of an ancestor. Armenians in the diaspora sometimes adapt their surnames to help assimilation. In Russia, many have changed -yan to -ov (or -ova for women). In Turkey, many have changed the ending to -oğlu (also meaning "son of"). In English and French-speaking countries, many have shortened their name by removing the ending (for example Charles Aznavour). In ancient Armenia, many noble names ended with the locative -t'si (example, Khorenatsi) or -uni (Bagratuni). Several modern Armenian names also have a Turkish suffix which appears before -ian/-yan: -lian denotes a placename; -djian denotes a profession. Some Western Armenian names have a particle Der, while their Eastern counterparts have Ter. This particle indicates an ancestor who was a priest (Armenian priests can choose to marry or remain celibate, but married priests cannot become a bishop). Thus someone named Der Bedrosian (Western) or Ter Petrosian (Eastern) is a descendent of an Armenian priest. The convention is still in use today: the children of a priest named Hagop Sarkisian would be called Der Sarkisian. Other examples of Armenian surnames: Adonts, Sakunts, Vardanyants, Rshtuni.

Traditional Azeri surnames usually end with "-lı", "-lu", (Turkic for 'with' or 'belonging to'), "-oğlu", "-qızı" (Turkic for 'son of' and 'daughter of'), "-zade" (Persian for 'born of'). Azerbaijanis of Iranian descent traditionally use suffixes such as '-pour' or '-zadeh', meaning 'born of' with their father's name. It is, however, more usual for them to use the name of the city in which their ancestors lived (e.g. Tabrizpour for those from Tabriz) or their occupation (e.g. Damirchizadeh for blacksmiths). Also, due to it being a part of the Russian Empire, many last names carry Slavic endings of "-ov" for men and "-ova" for women. 

Most eastern Georgian surnames end with the suffix of "-shvili", (e.g. Kartveli'shvili) Georgian for "child" or "offspring". Western Georgian surnames most commonly have the suffix "-dze", (e.g. ) Georgian for "son". Megrelian surnames usually end in "-ia", "-ua" or "-ava". Other location-specific endings exist: In Svaneti "-iani", meaning "belonging to", or "hailing from", is common. In the eastern Georgian highlands common endings are "uri" and "uli". Some noble family names end in "eli", meaning "of (someplace)".
In Georgian, the surname is not normally used as the polite form of address; instead, the given name is used together with a title. For instance, Nikoloz Kartvelishvili is politely addressed as "bat'ono Nikoloz" "My Lord. Nikoloz".

Greek surnames are most commonly patronymics. Occupation, characteristic, or ethnic background and location/origin-based surnames names also occur; they are sometimes supplemented by nicknames.

Commonly, Greek male surnames end in -s, which is the common ending for Greek masculine proper nouns in the nominative case. Exceptionally, some end in -ou, indicating the genitive case of this proper noun for patronymic reasons.

Although surnames are static today, dynamic and changing patronym usage survives in middle names in Greece where the genitive of the father's first name is commonly the middle name.

Because of their codification in the Modern Greek state, surnames have Katharevousa forms even though Katharevousa is no longer the official standard. Thus, the Ancient Greek name Eleutherios forms the Modern Greek proper name Lefteris, and former vernacular practice (prefixing the surname to the proper name) was to call John Eleutherios Leftero-giannis.

Modern practice is to call the same person Giannis Eleftheriou: the proper name is vernacular (and not Ioannis), but the surname is an archaic genitive. However, children are almost always baptised with the archaic form of the name so in official matters the child will be referred to as Ioannis Eleftheriou and not Giannis Eleftheriou.

Female surnames are most often in the Katharevousa genitive case of a male name. This is an innovation of the Modern Greek state; Byzantine practice was to form a feminine counterpart of the male surname (e.g. masculine Palaiologos, Byzantine feminine Palaiologina, Modern feminine Palaiologou).

In the past, women would change their surname when married to that of their husband (again in genitive case) signifying the transfer of "dependence" from the father to the husband. In earlier Modern Greek society, women were named with -aina as a feminine suffix on the husband's first name: "Giorgaina", "Mrs George", "Wife of George". Nowadays, a woman's legal surname does not change upon marriage, though she can use the husband's surname socially. Children usually receive the paternal surname, though in rare cases, if the bride and groom have agreed before the marriage, the children can receive the maternal surname.

Some surnames are prefixed with Papa-, indicating ancestry from a priest, e.g. "Papageorgiou", the "son of a priest named George". Others, like Archi- and Mastro- signify "boss" and "tradesman" respectively.

Prefixes such as Konto-, Makro-, and Chondro- describe body characteristics, such as "short", "tall/long" and "fat". "Gero-" and "Palaio-" signify "old" or "wise".

Other prefixes include Hadji- (Χαντζή- or Χαντζι-) which was an honorific deriving from the Arabic Hadj or pilgrimage, and indicate that the person had made a pilgrimage (in the case of Christians, to Jerusalem) and Kara- which is attributed to the Turkish word for "black" deriving from the Ottoman Empire era. The Turkish suffix -oglou (derived from a patronym, "-oğlu" in Turkish) can also be found. Although they are of course more common among Greece's Muslim minority, they still can be found among the Christian majority, often Greeks or Karamanlides who were pressured to leave Turkey after the Turkish Republic was founded (since Turkish surnames only date to the founding of the Republic, when Atatürk made them compulsory).

Arvanitic surnames also exist; an example is "Tzanavaras" or "Tzavaras", from the Arvanitic word "çanavar" or "çavar" meaning "brave" ("pallikari" in Greek).

Most Greek patronymic suffixes are diminutives, which vary by region. The most common Hellenic patronymic suffixes are:

Others, less common, are:

Either the surname or the given name may come first in different contexts; in newspapers and in informal uses, the order is "given name + surname", while in official documents and forums (tax forms, registrations, military service, school forms), the surname is often listed or said first.

In Hungarian, like Asian languages but unlike most other European ones (see French and German above for exceptions), the family name is placed before the given names. This usage does not apply to non-Hungarian names, for example "Tony Blair" will remain "Tony Blair" when written in Hungarian texts.

Names of Hungarian individuals, however, appear in Western order in English writing.

Indonesians comprise more than 300 ethnic groups. Not all of these groups traditionally have surnames, and in the populous Java surnames are not common at all – regardless of which one of the six officially recognized religions the name carrier profess. For instance, a Christian Javanese woman named "Agnes Mega Rosalin" has three forenames and no surname. "Agnes" is her Christian name, but "Mega" can be the first name she use and the name which she is addressed with. "Rosalin" is only a middle name. Nonetheless, Indonesians are well aware of the custom of family names, which is known as "marga" or "fam", and such names have become a specific kind of identifier. People can tell what a person's heritage is by his or her family or clan name.

Javanese people are the majority in Indonesia, and most do not have any surname. There are some individuals, especially old generation, who have only one name, such as "Suharto" and "Sukarno". These are not only common with the Javanese but also with other Indonesian ethnic groups who do not have the tradition of surnames. If, however, they are Muslims, they might opt to follow Arabic naming customs, but Indonesian Muslims don't automatically follow Arabic name traditions.

In conjunction with migration to Europe or America, Indonesians without surnames often adopt a surname based on some family name or middle name. The forms for visa application many Western countries use, has a square for writing the last name which cannot be left unfilled by the applicant.

Most Chinese Indonesians substituted their Chinese surnames with Indonesian-sounding surnames due to political pressure from 1965 to 1998 under Suharto's regime.

Persian last names may be:

Suffixes include: -an (plural suffix), -i ("of"), -zad/-zadeh ("born of"), -pur ("son of"), -nejad ("from the race of"), -nia ("descendant of"), -mand ("having or pertaining to"), -vand ("succeeding"), -far ("holder of"), -doost ("-phile"), -khah ("seeking of"), -manesh ("having the manner of"), -ian/-yan, -gar and -chi ("whose vocation pertains").

An example is names of geographical locations plus "-i": Irani ("Iranian"), Gilani ("of Gilan province"), Tabrizi ("of the city of Tabriz").

Another example is last names that indicate relation to religious groups such as Zoroastrian (e.g. Goshtaspi, Namiranian, Azargoshasp), Jewish (e.g. Yaghubian [Jacobean], Hayyem [Life], Shaul [Saul]) or Muslim (e.g. Alavi, Islamnia, Montazeri)

Last names are arbitrary; their holder need not to have any relation with their meaning.

Traditionally in Iran, the wife does not take her husband's surname, although children take the surname of their father. Individual reactions notwithstanding, it is possible to call a married woman by her husband's surname. This is facilitated by the fact that English words "Mrs.", "Miss", "Woman", "Lady" and "Wife (of)" in a polite context are all translated into "خانم" (Khaanom). Context, however, is important: "خانم گلدوست" (Khaanom Goldust) may, for instance, refer to the daughter of Mr. Goldust instead of his wife.
When most of Iranian surnames are used with a name, the name will be ended with a suffix _E or _ie (of) such as Hasan_e roshan (Hasan is name and roshan is surname) that means Hasan of Roshan or Mosa_ie saiidi (Muses of saiidi). The _e is not for surname and it is difficult to say it is a part of surname.

Italy has around 350,000 surnames. Most of them derive from the following sources: patronym or ilk (e.g. "Francesco di Marco", "Francis, son of Mark" or "Eduardo de Filippo", "Edward belonging to the family of Philip"), occupation (e.g. "Enzo Ferrari", "Heinz (of the) Blacksmiths"), personal characteristic (e.g. nicknames or pet names like "Dario Forte", "Darius the Strong"), geographic origin (e.g. "Elisabetta Romano", "Elisabeth from Rome") and objects (e.g. "Carlo Sacchi", "Charles Bags"). The two most common Italian family names, "Russo" and "Rossi", mean the same thing, "Red", possibly referring to a hair color that would have been very distinctive in Italy.

Both Western and Eastern orders are used for full names: the given name usually comes first, but the family name may come first in administrative settings; lists are usually indexed according to the last name.

Since 1975, women have kept their own surname when married, but until recently (2000) they could have added the surname of the husband according to the civil code, although it was a very seldom-used practice. In recent years, the husband's surname cannot be used in any official situation. In some unofficial situations, sometimes both surnames are written (the proper first), sometimes separated by "in" (e.g. "Giuseppina Mauri in Crivelli") or, in case of widows, "ved." ("vedova").

Latvian male surnames usually end in "-s", "-š" or "-is" whereas the female versions of the same names end in "-a" or "-e" or "s" in both unmarried and married women.

Before the emancipation from serfdom (1817 in Courland, 1819 in Vidzeme, 1861 in Latgale) only noblemen, free craftsmen or people living in towns had surnames. Therefore, the oldest Latvian surnames originate from German or Low German, reflecting the dominance of German as an official language in Latvia till the 19th century. Examples: "Meijers/Meijere" (German: "Meier", farm administrator; akin to Mayor), "Millers/Millere" (German: "Müller", miller), "Šmits/Šmite" (German: "Schmidt", smith), "Šulcs/Šulca" (German: "Schulze", constable), "Ulmanis" (German: "Ullmann", a person from Ulm), "Godmanis" (a God-man), "Pētersons" (son of Peter). Some Latvian surnames, mainly from Latgale are of Polish or Belorussian origin by changing the final "-ski/-cki" to "-skis/-ckis", "-czyk" to "-čiks" or "-vich/-wicz" to "-vičs", such as "Sokolovkis/Sokolovska", "Baldunčiks/Baldunčika" or "Ratkevičs/Ratkeviča".

Most Latvian peasants received their surnames in 1826 (in Vidzeme), in 1835 (in Courland), and in 1866 (in Latgale). Diminutives were the most common form of family names. Examples: "Kalniņš/Kalniņa" (small hill), "Bērziņš/Bērziņa" (small birch).

Nowadays many Latvians of Slavic descent have surnames of Russian, Belarusian, or Ukrainian origin, for example "Volkovs/Volkova" or "Antoņenko".

Libya's names and surnames have a strong Islamic/Arab nature, with some Turkish influence from Ottoman Empire rule of nearly 400 years.
Amazigh, Touareg and other minorities also have their own name/surname traditions.
Due to its location as a trade route and the different cultures that had their impact on Libya throughout history, one can find names that could have originated in neighboring countries, including clan names from the Arabian Peninsula, and Turkish names derived from military rank or status ("Basha", "Agha").

Lithuanian names follow the Baltic distinction between male and female suffixes of names, although the details are different. Male surnames usually end in "-a", "-as", "-aitis", "-ys", "-ius", or "-us", whereas the female versions change these suffixes to "-aitė, -ytė, -iūtė", and "-utė" respectively (if unmarried), "-ienė" (if married), or "-ė" (not indicating the marital status). Some Lithuanians have names of Polish or another Slavic origin, which are made to conform to Lithuanian by changing the final "-ski" to "-skas", such as "Sadauskas", with the female version bein -"skaitė" (if unmarried), "-skienė" (if married), or "-skė" (not indicating the marital status).

Different cultures have their impact on the demographics of the Maltese islands, and this is evident in the various surnames Maltese citizens bear nowadays. There are very few "Maltese" surnames per se: the few that originate from Maltese places of origin include "Chircop" (Kirkop), "Lia" (Lija), "Balzan" (Balzan), "Valletta" (Valletta), and "Sciberras" (Xebb ir-Ras Hill, on which Valletta was built). The village of Munxar, Gozo is characterised by the majority of its population having one of two surnames, either "Curmi" or "de Brincat". In Gozo, the surnames "Bajada" and "Farrugia" are also common.
Sicilian and Italian surnames are common due to the close vicinity to Malta. Sicilian Italians were the first to colonise the Maltese islands. Common examples include "Azzopardi", "Bonello", "Cauchi", "Farrugia", "Gauci", "Rizzo", "Schembri", "Tabone", "Vassallo", "Vella".
Common examples include "Depuis", "Montfort", "Monsenuier", "Tafel".
English surnames exist for a number of reasons, but mainly due to migration as well as Malta forming a part of the British Empire in the 19th century and most of the 20th. Common examples include "Bone", "Harding", "Atkins", "Mattocks", "Smith", "Jones", "Woods", "Turner".
Arabic surnames occur in part due to the early presence of the Arabs in Malta. Common examples include "Sammut", "Camilleri", "Zammit", and "Xuereb".
Common surnames of Spanish origin include "Abela", "Galdes", "Herrera", and "Guzman".
Surnames from foreign countries from the Middle Ages include German,
such as "von Brockdorff", "Hyzler", and "Schranz".
Many of the earliest Maltese surnames are Sicilian Greek, e.g. "Cilia", "Calleia", "Brincat", "Cauchi". Much less common are recent surnames from Greece; examples include "Dacoutros", and "Trakosopoulos"
The original Jewish community of Malta and Gozo has left no trace of their presence on the islands since they were expelled in January 1493.
In line with the practice in other Christian, European states, women generally assume their husband's surname after legal marriage, and this is passed on to any children the couple may bear. Some women opt to retain their old name, for professional/personal reasons, or combine their surname with that of their husband.

Mongolians do not use surnames in the way that most Westerners, Chinese or Japanese do. Since the socialist period, patronymics – then called "ovog", now called "etsgiin ner" – are used instead of a surname. If the father's name is unknown, a matronymic is used. The patro- or matronymic is written before the given name. Therefore, if a man with given name Tsakhia has a son, and gives the son the name Elbegdorj, the son's full name is Tsakhia Elbegdorj. Very frequently, the patronymic is given in genitive case, i.e. Tsakhiagiin Elbegdorj. However, the patronymic is rather insignificant in everyday use and usually just given as an initial – Ts. Elbegdorj. People are normally just referred to and addressed by their given name (Elbegdorj "guai" – Mr. Elbegdorj), and if two people share a common given name, they are usually just kept apart by their initials, not by the full patronymic.

Since 2000, Mongolians have been officially using clan names – "ovog", the same word that had been used for the patronymics before – on their IDs. Many people chose the names of the ancient clans and tribes such Borjigin, Besud, Jalair, etc. Also many extended families chose the names of the native places of their ancestors. Some chose the names of their most ancient known ancestor. Some just decided to pass their own given names (or modifications of their given names) to their descendants as clan names. Some chose other attributes of their lives as surnames. Gürragchaa chose Sansar (Cosmos). Clan names precede the patronymics and given names, e.g. Besud Tsakhiagiin Elbegdorj. These clan names have a significance and are included in Mongolian passports.

People from Myanmar or Burmese, have no family names. This, to some, is the only known Asian people having no family names at all. Some of those from Myanmar or Burma, who are familiar with European or American cultures, began to put to their younger generations with a family name – adopted from the notable ancestors. For example, Ms. Aung San Suu Kyi is the daughter of the late Father of Independence General Aung San; Hayma Ne Win, is the daughter of the famous actor Kawleikgyin Ne Win etc.

Until the middle of the 19th century, there was no standardization of surnames in the Philippines. There were native Filipinos without surnames, others whose surnames deliberately did not match that of their families, as well as those who took certain surnames simply because they had a certain prestige, usually ones related to the Roman Catholic religion, such as de los Santos ("of the saints") and de la Cruz ("of the cross").

On 21 November 1849, the Spanish Governor-General of the Philippines, Narciso Clavería y Zaldúa, decreed an end to these arbitrary practices, the systematic distribution of surnames to Filipinos without prior surnames and the universal implementation of the Spanish naming system. This produced the "Catálogo alfabético de apellidos" ("Alphabetical Catalogue of Surnames"), which listed permitted surnames with origins in Spanish, Filipino, and Hispanicised Chinese words, names, and numbers. Thus, many Spanish-sounding Filipino surnames are not surnames common to the rest of the Hispanophone world. Surnames with connections to nobility, either Spanish or local, however, was explicitly prohibited, and only allowed to be retained by families with noble status or having the surname used in three consecutive generations. The book contained many words coming from Spanish and the Philippine languages such as Tagalog, as well as many Basque surnames such as Zuloaga or Aguirre.

The colonial authorities implemented this decree because too many (early) Christianized Filipinos assumed religious names. There soon were too many people surnamed "de los Santos", "de la Cruz", "del Rosario" ("of the Rosary"), "Bautista" ("Baptist"), et cetera, which made it difficult for the Spanish colonists to control the Filipino people, and most importantly, to collect taxes. These extremely common names were also banned by the decree unless the name has been used by a family for at least four generations. This Spanish naming custom also countered the native custom before the Spanish period, wherein siblings assumed different surnames. Clavería's decree was enforced to different degrees in different parts of the colony.

Because of this implementation of Spanish naming customs, of the arrangement "given name - paternal surname - maternal surname", in the Philippines, a Spanish surname does not necessarily denote Spanish ancestry.

In practice, the application of this decree varied from municipality to municipality. Most municipalities received surnames starting with only one initial letter, but some are assigned surnames starting with two or three initial letters. For example, the majority of residents of the island of Banton in the province of Romblon have surnames starting with F such as "Fabicon", "Fallarme", "Fadrilan", and "Ferran". Other examples are case of Batangas, Batangas (present-day Batangas City), where most residents bear surnames starting with the letters A, B, and C, such as "Abacan", "Albayalde", "Almarez", "Andal", "Arce", "Arceo", "Arguelles", "Arrieta", "Babasa", "Balmes", "Basco", "Baylosis", "Berberabe", "Biscocho", "Blanco", "Borbon", "Calingasan", "Caringal", "Chavez", "Cuenca", and "Custodio" (in addition to some bearing native Tagalog surnames, such as "Dimaano", "Dimacuha", "Macatangay", "Malabanan", and "Marasigan"), and Argao, Cebu, where most residents bear surnames starting with "VI" and "Al", such as "Villaluz", "Villaflor", "Villamor", "Villanueva", "Albo", "Alcain", "Alcarez", "Algones", etc.

Thus, although perhaps a majority of Filipinos have Spanish surnames, such a surname does not indicate Spanish ancestry. In addition, most Filipinos currently do not use Spanish accented letters in their Spanish derived names. The lack of accents in Filipino Spanish has been attributed to the lack of accents on the predominantly American typewriters after the US gained control of the Philippines.

The vast majority of Filipinos follow a naming system in the American order (i.e. given name - middle name - surname), which is the reverse of the Spanish naming order (i.e. given name - paternal surname - maternal surname). Children take the mother's surname as their middle name, followed by their father's as their surname; for example, a son of Juan de la Cruz and his wife María Agbayani may be David Agbayani de la Cruz. Women usually take the surnames of their husband upon marriage, and consequently lose their maiden middle names; so upon her marriage to David de la Cruz, the full name of Laura Yuchengco Macaraeg would become Laura Macaraeg de la Cruz. Their maiden last names automatically become their middle names upon marriage.

There are other sources for surnames. Many Filipinos also have Chinese-derived surnames, which in some cases could indicate Chinese ancestry. Many Hispanicised Chinese numerals and other Hispanicised Chinese words, however, were also among the surnames in the "Catálogo alfabético de apellidos". For those whose surname may indicate Chinese ancestry, analysis of the surname may help to pinpoint when those ancestors arrived in the Philippines. A Hispanicised Chinese surname such as Cojuangco suggests an 18th-century arrival while a Chinese surname such as Lim suggests a relatively recent immigration. Some Chinese surnames such as Tiu-Laurel are composed of the immigrant Chinese ancestor's surname as well as the name of that ancestor's godparent on receiving Christian baptism.

In the predominantly Muslim areas of the southern Philippines, adoption of surnames was influenced by Islamic religious terms. As a result, surnames among Filipino Muslims are largely Arabic-based, and include such surnames as Hassan and Haradji.

There are also Filipinos who, to this day, have no surnames at all, particularly if they come from indigenous cultural communities.

Prior to the establishment of the Philippines as a US territory during the earlier part of the 20th century, Filipinos usually followed Iberian naming customs. However, upon the promulgation of the Family Code of 1987, Filipinos formalized adopting the American system of using their surnames.

A common Filipino name will consist of the given name (mostly 2 given names are given), the initial letter of the mother's maiden name and finally the father's surname (i.e. Lucy Anne C. de Guzman). Also, women are allowed to retain their maiden name or use both her and her husband's surname as a double-barreled surname, separated by a dash. This is common in feminist circles or when the woman holds a prominent office (e.g. Gloria Macapagal-Arroyo, Miriam Defensor Santiago). In more traditional circles, especially those who belong to the prominent families in the provinces, the custom of the woman being addressed as "Mrs. Husband's Full Name" is still common.

For widows, who chose to marry again, two norms are in existence. For those who were widowed before the Family Code, the full name of the woman remains while the surname of the deceased husband is attached. That is, Maria Andres, who was widowed by Ignacio Dimaculangan will have the name Maria Andres viuda de Dimaculangan. If she chooses to marry again, this name will still continue to exist while the surname of the new husband is attached. Thus, if Maria marries Rene de los Santos, her new name will be Maria Andres viuda de Dimaculangan de los Santos.

However, a new norm is also in existence. The woman may choose to use her husband's surname to be one of her middle names. Thus, Maria Andres viuda de Dimaculangan de los Santos may also be called Maria A.D. de los Santos.

Children will however automatically inherit their father's surname if they are considered legitimate. If the child is born out of wedlock, the mother will automatically pass her surname to the child, unless the father gives a written acknowledgment of paternity. The father may also choose to give the child both his parents' surnames if he wishes (that is Gustavo Paredes, whose parents are Eulogio Paredes and Juliana Angeles, while having Maria Solis as a wife, may name his child Kevin S. Angeles-Paredes.

In some Tagalog regions, the norm of giving patronyms, or in some cases matronyms, is also accepted. These names are of course not official, since family names in the Philippines are inherited. It is not uncommon to refer to someone as Juan anak ni Pablo (John, the son of Paul) or Juan apo ni Teofilo (John, the grandson of Theophilus).

In Romania, like in most of Europe, it is customary for a child to take his father's family name, and a wife to take her husband's last name. However, this is not compulsory - spouses and parents are allowed to choose other options too, as the law is flexible (see Art. 282, Art. 449 Art. 450. of the Civil Code of Romania).

Until the 19th century, the names were primarily of the form "[given name] [father's name] [grandfather's name]". The few exceptions are usually famous people or the nobility (boyars). The name reform introduced around 1850 had the names changed to a western style, most likely imported from France, consisting of a given name followed by a family name.

As such, the name is called "prenume" (French "prénom"), while the family name is called "nume" or, when otherwise ambiguous, "nume de familie" ("family name"). Although not mandatory, middle names are common.

Historically, when the family name reform was introduced in the mid-19th century, the default was to use a patronym, or a matronym when the father was dead or unknown. A common convention was to append the suffix "-escu" to the father's name, e.g. "Anghelescu" (""Anghel's" child") and "Petrescu" (""Petre's" child"). (The "-escu" seems to come from Latin "-iscum", thus being cognate with Italian "-esco" and French "-esque".) Another common convention was to append the suffix "-eanu" to the name of the place of origin, e.g. "Munteanu" ("from the mountains") and "Moldoveanu" ("from "Moldova""). These uniquely Romanian suffixes strongly identify ancestral nationality.

There are also descriptive family names derived from occupations, nicknames, and events, e.g. "Botezatu" ("baptised"), "Barbu" ("bushy bearded"), "Prodan" ("foster"), "Bălan" ("blond"), "Fieraru" ("smith"), "Croitoru" ("tailor"), "Păcuraru" ("shepherd").

Romanian family names remain the same regardless of the sex of the person.

Although given names appear before family names in most Romanian contexts, official documents invert the order, ostensibly for filing purposes. Correspondingly, Romanians occasionally introduce themselves with their family names first, e.g. a student signing a test paper in school.

Romanians bearing names of non-Romanian origin often adopt Romanianised versions of their ancestral surnames. For example, "Jurovschi" for Polish "Żurowski", or Popovici for Serbian Popović (“son of a priest”), which preserves the original pronunciation of the surname through transliteration. In some cases, these changes were mandated by the state.

In Turkey, following the Surname Law imposed in 1934 in the context of Atatürk's Reforms, every family living in Turkey was given a family name. The surname was generally selected by the elderly people of the family and could be any Turkish word (or a permitted word for families belonging to official minority groups).

Some of the most common family names in Turkey are "Yılmaz" ('undaunted'), "Doğan" ('falcon'), "Şahin" ('hawk'), "Yıldırım" ('thunderbolt'), "Şimşek" ('lightning'), "Öztürk" ('purely Turkish').

Patronymic surnames do not necessarily refer to ancestry, or in most cases cannot be traced back historically. The most usual Turkish patronymic suffix is "–oğlu"; "–ov(a)", "–yev(a)" and "–zade" also occur in the surnames of Azeri or other Turkic descendants.

Official minorities like Armenians, Greeks, and Jews have surnames in their own mother languages.
The Armenian families living in Turkey usually have Armenian surnames and generally have the suffix "–yan", "–ian", or, using Turkish spelling, "-can". Greek descendants usually have Greek surnames which might have Greek suffixes like "–ou", "–aki(s)", "–poulos/poulou", "–idis/idou", "–iadis/iadou" or prefixes like "papa–".
The Sephardic Jews who were expelled from Spain and settled in Turkey in 1492 have both Jewish/Hebrew surnames, and Spanish surnames, usually indicating their native regions, cities or villages back in Spain, like "De Leon" or "Toledano".

However these minorities increasingly tend to "Turkicize" their surnames or replace their original surnames with Turkish surnames altogether to avoid being recognized and discriminated against.


</doc>
<doc id="10815" url="https://en.wikipedia.org/wiki?curid=10815" title="Franc">
Franc

The franc (F, Fr., ₣) is the name of several currency units. The French franc was the currency of France until the euro was adopted in 1999 (by law, 2002 de facto). The Swiss franc is a major world currency today due to the prominence of Swiss financial institutions. The name is said to derive from the Latin inscription "francorum rex" (Style of the French sovereign: "King of the Franks") used on early French coins and until the 18th century, or from the French "franc", meaning "frank" (and "free" in certain contexts, such as "coup franc", "free kick" ).

The countries that use francs include Switzerland, Liechtenstein, and most of Francophone Africa. Before the introduction of the euro, francs were also used in France, Belgium and Luxembourg, while Andorra and Monaco accepted the French franc as legal tender (Monegasque franc). The franc was also used within the French Empire's colonies, including Algeria and Cambodia. The franc is sometimes Italianised or Hispanicised as the "franco", for instance in Luccan franco.

One franc is typically divided into 100 centimes. The French franc symbol was F or Fr.; the symbol ₣ was suggested in France by Prime Minister Édouard Balladur, but it was never used. For practical reasons, the banks and the financial markets used the abbreviation FF for the French franc in order to distinguish it from the Belgian franc (FB), the Luxembourgish franc (FL or FLux), et cetera. In the Luxembourgish language, the word for franc is "Frang", plural form "Frangen".

The franc was originally a French gold coin of 3.87 g minted in 1360 on the occasion of the release of King John II ("the good"), held by the English since his capture at the Battle of Poitiers four years earlier. It was equivalent to one "livre tournois" (Tours pound).

The French franc was the name of a gold coin issued in France from 1360 until 1380, then a silver coin issued between 1575 and 1641. The franc finally became the national currency from 1795 until 1999 (franc coins and notes were legal tender until 2002). Though abolished as a legal coin by Louis XIII in 1641 in favor of the gold louis and silver écu, the term franc continued to be used in common parlance for the livre tournois. The franc was also minted for many of the former French colonies, such as Morocco, Algeria, French West Africa, and others. Today, after independence, many of these countries continue to use the franc as their standard denomination.

The value of the French franc was locked to the euro at 1 euro = 6.55957 FRF on 31 December 1998, and after the introduction of the euro notes and coins, ceased to be legal tender after 28 February 2002, although they were still exchangeable at banks until 19 February 2012.

Fourteen African countries use the franc CFA (in west Africa, "Communauté financière africaine"; in equatorial Africa, "Coopération financière en Afrique centrale"), originally (1945) worth 1.7 French francs and then from 1948, 2 francs (from 1960: 0.02 new franc) but after January 1994 worth only 0.01 French franc. Therefore, from January 1999, 1 CFA franc is equivalent to €0.00152449.

A separate (franc CFP) circulates in France's Pacific territories, worth €0.0084 (formerly 0.055 French franc).

In 1981, The Comoros established an arrangement with the French government similar to that of the CFA franc. Originally, 50 Comorian francs were worth 1 French franc. In January 1994, the rate was changed to 75 Comorian francs to the French franc. Since 1999, the currency has been pegged to the euro.

The conquest of most of western Europe by Revolutionary and Napoleonic France led to the franc's wide circulation. Following independence from the Kingdom of the Netherlands, the new Kingdom of Belgium in 1832 adopted its own Belgian franc, equivalent to the French one, followed by Luxembourg adopting the Luxembourgish franc in 1848 and Switzerland in 1850. Newly unified Italy adopted the lira on a similar basis in 1862.

In 1865, France, Belgium, Switzerland and Italy created the Latin Monetary Union (to be joined by Spain and Greece in 1868): each would possess a national currency unit (franc, lira, peseta, drachma) worth 4.5 g of silver or of gold (fine), all freely exchangeable at a rate of 1:1. In the 1870s the gold value was made the fixed standard, a situation which was to continue until 1914.

In 1926 Belgium as well as France experienced depreciation and an abrupt collapse of confidence, leading to the introduction of a new gold currency for international transactions, the "belga" of 5 francs, and the country's withdrawal from the monetary union, which ceased to exist at the end of the year. The 1921 monetary union of Belgium and Luxembourg survived, however, forming the basis for full economic union in 1932.

Like the French franc, the Belgo-Luxemburgish franc ceased to exist on 1 January 1999, when it became fixed at 1 EUR = 40.3399 BEF/LUF, thus a franc was worth €0.024789. Old franc coins and notes lost their legal tender status on 28 February 2002.

1 Luxembourgish franc was equal to 1 Belgian franc. Belgian francs were legal tender inside Luxembourg, and Luxembourgish francs were legal tender in the whole of Belgium. (In reality, Luxembourgish francs were only accepted as means of payment by shops and businesses in the Belgian province of Luxembourg adjacent to the independent Grand Duchy of Luxembourg, this for historical reasons.)

The equivalent name of the Belgian franc in Dutch, Belgium's other official language, was "Belgische Frank". As mentioned before, in Luxembourg the franc was called "Frang" (plural "Frangen").

The Swiss franc (ISO code: CHF or 756), which appreciated significantly against the new European currency from April to September 2000, remains one of the world's strongest currencies, worth today around five-sixths of a euro. The Swiss franc is used in Switzerland and in Liechtenstein. Liechtenstein retains the ability to mint its own currency, the Liechtenstein franc, which it does from time to time for commemorative or emergency purposes.

The name of the country "Swiss Confederation" is found on some of the coins in Latin ("Confoederatio Helvetica"), as Switzerland has four official languages, all of which are used on the notes. The denomination is abbreviated "Fr." on the coins which is the abbreviation in all four languages.

The Saar franc, linked at par to the French franc, was introduced in the Saar Protectorate in 1948. On 1 January 1957, the territory joined the Federal Republic of Germany, nevertheless, in its new member state of Saarland, the Saar franc continued to be the currency until 6 July 1959.

The name of the Saar franc in German, the main official language in the Protectorate, was "Franken". Coins displaying German inscriptions and the coat of arms of the Protectorate were circulated and used together with French francs. As banknotes, only French franc bills existed.

Countries which formerly used a franc:

Collectivities which use a franc:




</doc>
<doc id="10819" url="https://en.wikipedia.org/wiki?curid=10819" title="Federal Reserve">
Federal Reserve

The Federal Reserve System (also known as the Federal Reserve or simply the Fed) is the central banking system of the United States of America. It was created on December 23, 1913, with the enactment of the Federal Reserve Act, after a series of financial panics (particularly the panic of 1907) led to the desire for central control of the monetary system in order to alleviate financial crises. Over the years, events such as the Great Depression in the 1930s and the Great Recession during the 2000s have led to the expansion of the roles and responsibilities of the Federal Reserve System.

The U.S. Congress established three key objectives for monetary policy in the Federal Reserve Act: maximizing employment, stabilizing prices, and moderating long-term interest rates. The first two objectives are sometimes referred to as the Federal Reserve's dual mandate. Its duties have expanded over the years, and currently also include supervising and regulating banks, maintaining the stability of the financial system, and providing financial services to depository institutions, the U.S. government, and foreign official institutions. The Fed conducts research into the economy and provides numerous publications, such as the Beige Book and the FRED database.

The Federal Reserve System is composed of several layers. It is governed by the presidentially appointed board of governors or Federal Reserve Board (FRB). Twelve regional Federal Reserve Banks, located in cities throughout the nation, regulate and oversee privately owned commercial banks. Nationally chartered commercial banks are required to hold stock in, and can elect some of the board members of, the Federal Reserve Bank of their region. The Federal Open Market Committee (FOMC) sets monetary policy. It consists of all seven members of the board of governors and the twelve regional Federal Reserve Bank presidents, though only five bank presidents vote at a time (the president of the New York Fed and four others who rotate through one-year voting terms). There are also various advisory councils. Thus, the Federal Reserve System has both public and private components. It has a structure unique among central banks, and is also unusual in that the United States Department of the Treasury, an entity outside of the central bank, prints the currency used.

The federal government sets the salaries of the board's seven governors, and it receives all the system's annual profits, after a statutory dividend of 6% on member banks' capital investment is paid, and an account surplus is maintained. In 2015, the Federal Reserve earned net income of $100.2 billion and transferred $97.7 billion to the U.S. Treasury. Although an instrument of the US Government, the Federal Reserve System considers itself "an independent central bank because its monetary policy decisions do not have to be approved by the President or anyone else in the executive or legislative branches of government, it does not receive funding appropriated by the Congress, and the terms of the members of the board of governors span multiple presidential and congressional terms."

The primary motivation for creating the Federal Reserve System was to address banking panics. Other purposes are stated in the Federal Reserve Act, such as "to furnish an elastic currency, to afford means of rediscounting commercial paper, to establish a more effective supervision of banking in the United States, and for other purposes". Before the founding of the Federal Reserve System, the United States underwent several financial crises. A particularly severe crisis in 1907 led Congress to enact the Federal Reserve Act in 1913. Today the Federal Reserve System has responsibilities in addition to ensuring the stability of the financial system.

Current functions of the Federal Reserve System include:

Banking institutions in the United States are required to hold reservesamounts of currency and deposits in other banksequal to only a fraction of the amount of the bank's deposit liabilities owed to customers. This practice is called fractional-reserve banking. As a result, banks usually invest the majority of the funds received from depositors. On rare occasions, too many of the bank's customers will withdraw their savings and the bank will need help from another institution to continue operating; this is called a bank run. Bank runs can lead to a multitude of social and economic problems. The Federal Reserve System was designed as an attempt to prevent or minimize the occurrence of bank runs, and possibly act as a lender of last resort when a bank run does occur. Many economists, following Nobel laureate Milton Friedman, believe that the Federal Reserve inappropriately refused to lend money to small banks during the bank runs of 1929; Friedman argued that this contributed to the Great Depression.

Because some banks refused to clear checks from certain other banks during times of economic uncertainty, a check-clearing system was created in the Federal Reserve System. It is briefly described in "The Federal Reserve SystemPurposes and Functions" as follows:

In the United States, the Federal Reserve serves as the lender of last resort to those institutions that cannot obtain credit elsewhere and the collapse of which would have serious implications for the economy. It took over this role from the private sector "clearing houses" which operated during the Free Banking Era; whether public or private, the availability of liquidity was intended to prevent bank runs.

Through its discount window and credit operations, Reserve Banks provide liquidity to banks to meet short-term needs stemming from seasonal fluctuations in deposits or unexpected withdrawals. Longer term liquidity may also be provided in exceptional circumstances. The rate the Fed charges banks for these loans is called the discount rate (officially the primary credit rate).

By making these loans, the Fed serves as a buffer against unexpected day-to-day fluctuations in reserve demand and supply. This contributes to the effective functioning of the banking system, alleviates pressure in the reserves market and reduces the extent of unexpected movements in the interest rates. For example, on September 16, 2008, the Federal Reserve Board authorized an $85 billion loan to stave off the bankruptcy of international insurance giant American International Group (AIG).

In its role as the central bank of the United States, the Fed serves as a banker's bank and as the government's bank. As the banker's bank, it helps to assure the safety and efficiency of the payments system. As the government's bank or fiscal agent, the Fed processes a variety of financial transactions involving trillions of dollars. Just as an individual might keep an account at a bank, the U.S. Treasury keeps a checking account with the Federal Reserve, through which incoming federal tax deposits and outgoing government payments are handled. As part of this service relationship, the Fed sells and redeems U.S. government securities such as savings bonds and Treasury bills, notes and bonds. It also issues the nation's coin and paper currency. The U.S. Treasury, through its Bureau of the Mint and Bureau of Engraving and Printing, actually produces the nation's cash supply and, in effect, sells the paper currency to the Federal Reserve Banks at manufacturing cost, and the coins at face value. The Federal Reserve Banks then distribute it to other financial institutions in various ways. During the Fiscal Year 2013, the Bureau of Engraving and Printing delivered 6.6 billion notes at an average cost of 5.0 cents per note.

Federal funds are the reserve balances (also called Federal Reserve Deposits) that private banks keep at their local Federal Reserve Bank. These balances are the namesake reserves of the Federal Reserve System. The purpose of keeping funds at a Federal Reserve Bank is to have a mechanism for private banks to lend funds to one another. This market for funds plays an important role in the Federal Reserve System as it is what inspired the name of the system and it is what is used as the basis for monetary policy. Monetary policy is put into effect partly by influencing how much interest the private banks charge each other for the lending of these funds.

Federal reserve accounts contain federal reserve credit, which can be converted into federal reserve notes. Private banks maintain their bank reserves in federal reserve accounts.

The Federal Reserve regulates private banks. The system was designed out of a compromise between the competing philosophies of privatization and government regulation. In 2006 Donald L. Kohn, vice chairman of the board of governors, summarized the history of this compromise:

The balance between private interests and government can also be seen in the structure of the system. Private banks elect members of the board of directors at their regional Federal Reserve Bank while the members of the board of governors are selected by the President of the United States and confirmed by the Senate.

The Federal Banking Agency Audit Act, enacted in 1978 as Public Law 95-320 and 31 U.S.C. section 714 establish that the board of governors of the Federal Reserve System and the Federal Reserve banks may be audited by the Government Accountability Office (GAO).

The GAO has authority to audit check-processing, currency storage and shipments, and some regulatory and bank examination functions, however, there are restrictions to what the GAO may audit. Under the Federal Banking Agency Audit Act, 31 U.S.C. section 714(b), audits of the Federal Reserve Board and Federal Reserve banks do not include (1) transactions for or with a foreign central bank or government or non-private international financing organization; (2) deliberations, decisions, or actions on monetary policy matters; (3) transactions made under the direction of the Federal Open Market Committee; or (4) a part of a discussion or communication among or between members of the board of governors and officers and employees of the Federal Reserve System related to items (1), (2), or (3). See Federal Reserve System Audits: Restrictions on GAO's Access (GAO/T-GGD-94-44), statement of Charles A. Bowsher.

The board of governors in the Federal Reserve System has a number of supervisory and regulatory responsibilities in the U.S. banking system, but not complete responsibility. A general description of the types of regulation and supervision involved in the U.S. banking system is given by the Federal Reserve:

There is a very strong economic consensus in favor of independence from political influence.

The board of directors of each Federal Reserve Bank District also has regulatory and supervisory responsibilities. If the board of directors of a district bank has judged that a member bank is performing or behaving poorly, it will report this to the board of governors. This policy is described in United States Code:

The Federal Reserve plays a role in the U.S. payments system. The twelve Federal Reserve Banks provide banking services to depository institutions and to the federal government. For depository institutions, they maintain accounts and provide various payment services, including collecting checks, electronically transferring funds, and distributing and receiving currency and coin. For the federal government, the Reserve Banks act as fiscal agents, paying Treasury checks; processing electronic payments; and issuing, transferring, and redeeming U.S. government securities.

In the Depository Institutions Deregulation and Monetary Control Act of 1980, Congress reaffirmed that the Federal Reserve should promote an efficient nationwide payments system. The act subjects all depository institutions, not just member commercial banks, to reserve requirements and grants them equal access to Reserve Bank payment services.
The Federal Reserve plays a role in the nation's retail and wholesale payments systems by providing financial services to depository institutions. Retail payments are generally for relatively small-dollar amounts and often involve a depository institution's retail clientsindividuals and smaller businesses. The Reserve Banks' retail services include distributing currency and coin, collecting checks, and electronically transferring funds through the automated clearinghouse system. By contrast, wholesale payments are generally for large-dollar amounts and often involve a depository institution's large corporate customers or counterparties, including other financial institutions. The Reserve Banks' wholesale services include electronically transferring funds through the Fedwire Funds Service and transferring securities issued by the U.S. government, its agencies, and certain other entities through the Fedwire Securities Service.

The Federal Reserve System has a "unique structure that is both public and private" and is described as "independent within the government" rather than "independent of government". The System does not require public funding, and derives its authority and purpose from the Federal Reserve Act, which was passed by Congress in 1913 and is subject to Congressional modification or repeal. The four main components of the Federal Reserve System are (1) the board of governors, (2) the Federal Open Market Committee, (3) the twelve regional Federal Reserve Banks, and (4) the member banks throughout the country.

The seven-member board of governors is a federal agency. It is charged with the overseeing of the 12 District Reserve Banks and setting national monetary policy. It also supervises and regulates the U.S. banking system in general.
Governors are appointed by the President of the United States and confirmed by the Senate for staggered 14-year terms. One term begins every two years, on February 1 of even-numbered years, and members serving a full term cannot be renominated for a second term. "[U]pon the expiration of their terms of office, members of the Board shall continue to serve until their successors are appointed and have qualified." The law provides for the removal of a member of the board by the president "for cause". The board is required to make an annual report of operations to the Speaker of the U.S. House of Representatives.

The chair and vice chair of the board of governors are appointed by the president from among the sitting governors. They both serve a four-year term and they can be renominated as many times as the president chooses, until their terms on the board of governors expire.

The current members of the board of governors are as follows:

In late December 2011, President Barack Obama nominated Jeremy C. Stein, a Harvard University finance professor and a Democrat, and Jerome Powell, formerly of Dillon Read, Bankers Trust and The Carlyle Group and a Republican. Both candidates also have Treasury Department experience in the Obama and George H. W. Bush administrations respectively.

"Obama administration officials [had] regrouped to identify Fed candidates after Peter Diamond, a Nobel Prize-winning economist, withdrew his nomination to the board in June [2011] in the face of Republican opposition. Richard Clarida, a potential nominee who was a Treasury official under George W. Bush, pulled out of consideration in August [2011]", one account of the December nominations noted. The two other Obama nominees in 2011, Janet Yellen and Sarah Bloom Raskin, were confirmed in September. One of the vacancies was created in 2011 with the resignation of Kevin Warsh, who took office in 2006 to fill the unexpired term ending January 31, 2018, and resigned his position effective March 31, 2011. In March 2012, U.S. Senator David Vitter (R, LA) said he would oppose Obama's Stein and Powell nominations, dampening near-term hopes for approval. However, Senate leaders reached a deal, paving the way for affirmative votes on the two nominees in May 2012 and bringing the board to full strength for the first time since 2006 with Duke's service after term end. Later, on January 6, 2014, the United States Senate confirmed Yellen's nomination to be chair of the Federal Reserve Board of Governors; she is slated to be the first woman to hold the position and will become chair on February 1, 2014. Subsequently, President Obama nominated Stanley Fischer to replace Yellen as the Vice Chair.

In April 2014, Stein announced he was leaving to return to Harvard May 28 with four years remaining on his term. At the time of the announcement, the FOMC "already is down three members as it awaits the Senate confirmation of ... Fischer and Lael Brainard, and as [President] Obama has yet to name a replacement for ... Duke. ... Powell is still serving as he awaits his confirmation for a second term."

Allan R. Landon, former president and CEO of the Bank of Hawaii, was nominated in early 2015 by President Obama to the board.

In July 2015, President Obama nominated University of Michigan economist Kathryn M. Dominguez to fill the second vacancy on the board. The Senate had not yet acted on Landon's confirmation by the time of the second nomination.

Daniel Tarullo submitted his resignation from the board on February 10, 2017, effective on or around April 5, 2017.

The Federal Open Market Committee (FOMC) consists of 12 members, seven from the board of governors and 5 of the regional Federal Reserve Bank presidents. The FOMC oversees and sets policy on open market operations, the principal tool of national monetary policy. These operations affect the amount of Federal Reserve balances available to depository institutions, thereby influencing overall monetary and credit conditions. The FOMC also directs operations undertaken by the Federal Reserve in foreign exchange markets. The FOMC must reach consensus on all decisions. The president of the Federal Reserve Bank of New York is a permanent member of the FOMC; the presidents of the other banks rotate membership at two- and three-year intervals. All Regional Reserve Bank presidents contribute to the committee's assessment of the economy and of policy options, but only the five presidents who are then members of the FOMC vote on policy decisions. The FOMC determines its own internal organization and, by tradition, elects the chair of the board of governors as its chair and the president of the Federal Reserve Bank of New York as its vice chair. Formal meetings typically are held eight times each year in Washington, D.C. Nonvoting Reserve Bank presidents also participate in Committee deliberations and discussion. The FOMC generally meets eight times a year in telephone consultations and other meetings are held when needed.

There is very strong consensus among economists against politicising the FOMC.

The Federal Advisory Council, composed of twelve representatives of the banking industry, advises the board on all matters within its jurisdiction.

There are 12 Federal Reserve Banks, each of which is responsible for member banks located in its district. They are located in Boston, New York, Philadelphia, Cleveland, Richmond, Atlanta, Chicago, St. Louis, Minneapolis, Kansas City, Dallas, and San Francisco. The size of each district was set based upon the population distribution of the United States when the Federal Reserve Act was passed.

The charter and organization of each Federal Reserve Bank is established by law and cannot be altered by the member banks. Member banks, do however, elect six of the nine members of the Federal Reserve Banks' boards of directors.

Each regional Bank has a president, who is the chief executive officer of their Bank. Each regional Reserve Bank's president is nominated by their Bank's board of directors, but the nomination is contingent upon approval by the board of governors. Presidents serve five-year terms and may be reappointed.

Each regional Bank's board consists of nine members. Members are broken down into three classes: A, B, and C. There are three board members in each class. Class A members are chosen by the regional Bank's shareholders, and are intended to represent member banks' interests. Member banks are divided into three categories: large, medium, and small. Each category elects one of the three class A board members. Class B board members are also nominated by the region's member banks, but class B board members are supposed to represent the interests of the public. Lastly, class C board members are appointed by the board of governors, and are also intended to represent the interests of the public.

The Federal Reserve Banks have an intermediate legal status, with some features of private corporations and some features of public federal agencies. The United States has an interest in the Federal Reserve Banks as tax-exempt federally created instrumentalities whose profits belong to the federal government, but this interest is not proprietary. In "Lewis v. United States", the United States Court of Appeals for the Ninth Circuit stated that: "The Reserve Banks are not federal instrumentalities for purposes of the FTCA [the Federal Tort Claims Act], but are independent, privately owned and locally controlled corporations." The opinion went on to say, however, that: "The Reserve Banks have properly been held to be federal instrumentalities for some purposes." Another relevant decision is "Scott v. Federal Reserve Bank of Kansas City", in which the distinction is made between Federal Reserve Banks, which are federally created instrumentalities, and the board of governors, which is a federal agency.

Regarding the structural relationship between the twelve Federal Reserve banks and the various commercial (member) banks, political science professor Michael D. Reagan has written that:

A member bank is a private institution and owns stock in its regional Federal Reserve Bank. All nationally chartered banks hold stock in one of the Federal Reserve Banks. State chartered banks may choose to be members (and hold stock in their regional Federal Reserve bank) upon meeting certain standards.

The amount of stock a member bank must own is equal to 3% of its combined capital and surplus. However, holding stock in a Federal Reserve bank is not like owning stock in a publicly traded company. These stocks cannot be sold or traded, and member banks do not control the Federal Reserve Bank as a result of owning this stock. From the profits of the Regional Bank of which it is a member, a member bank receives a dividend equal to 6% of its purchased stock. The remainder of the regional Federal Reserve Banks' profits is given over to the United States Treasury Department. In 2015, the Federal Reserve Banks made a profit of $100.2 billion and distributed $2.5 billion in dividends to member banks as well as returning $97.7 billion to the U.S. Treasury.

About 38% of U.S. banks are members of their regional Federal Reserve Bank.

An external auditor selected by the audit committee of the Federal Reserve System regularly audits the Board of Governors and the Federal Reserve Banks. The GAO will audit some activities of the Board of Governors. These audits do not cover "most of the Fed's monetary policy actions or decisions, including discount window lending (direct loans to financial institutions), open-market operations and any other transactions made under the direction of the Federal Open Market Committee" ...[nor may the GAO audit] "dealings with foreign governments and other central banks."

The annual and quarterly financial statements prepared by the Federal Reserve System conform to a basis of accounting that is set by the Federal Reserve Board and does not conform to Generally Accepted Accounting Principles (GAAP) or government Cost Accounting Standards (CAS). The financial reporting standards are defined in the Financial Accounting Manual for the Federal Reserve Banks. The cost accounting standards are defined in the Planning and Control System Manual. , the Federal Reserve Board has been publishing unaudited financial reports for the Federal Reserve banks every quarter.

November 7, 2008, Bloomberg L.P. News brought a lawsuit against the board of governors of the Federal Reserve System to force the board to reveal the identities of firms for which it has provided guarantees during the financial crisis of 2007–2008. Bloomberg, L.P. won at the trial court and the Fed's appeals were rejected at both the United States Court of Appeals for the Second Circuit and the U.S. Supreme Court. The data was released on March 31, 2011.

The term "monetary policy" refers to the actions undertaken by a central bank, such as the Federal Reserve, to influence the availability and cost of money and credit to help promote national economic goals. What happens to money and credit affects interest rates (the cost of credit) and the performance of an economy. The Federal Reserve Act of 1913 gave the Federal Reserve authority to set monetary policy in the United States.

The Federal Reserve sets monetary policy by influencing the federal funds rate, which is the rate of interbank lending of excess reserves. The rate that banks charge each other for these loans is determined in the interbank market and the Federal Reserve influences this rate through the three "tools" of monetary policy described in the "Tools" section below. The federal funds rate is a short-term interest rate that the FOMC focuses on, which affects the longer-term interest rates throughout the economy. The Federal Reserve summarized its monetary policy in 2005:

Effects on the quantity of reserves that banks used to make loans influence the economy. Policy actions that add reserves to the banking system encourage lending at lower interest rates thus stimulating growth in money, credit, and the economy. Policy actions that absorb reserves work in the opposite direction. The Fed's task is to supply enough reserves to support an adequate amount of money and credit, avoiding the excesses that result in inflation and the shortages that stifle economic growth.

There are three main tools of monetary policy that the Federal Reserve uses to influence the amount of reserves in private banks:

The Federal Reserve System implements monetary policy largely by targeting the federal funds rate. This is the interest rate that banks charge each other for overnight loans of federal funds, which are the reserves held by banks at the Fed. This rate is actually determined by the market and is not explicitly mandated by the Fed. The Fed therefore tries to align the effective federal funds rate with the targeted rate by adding or subtracting from the money supply through open market operations. The Federal Reserve System usually adjusts the federal funds rate target by 0.25% or 0.50% at a time.

Open market operations allow the Federal Reserve to increase or decrease the amount of money in the banking system as necessary to balance the Federal Reserve's dual mandates. Open market operations are done through the sale and purchase of United States Treasury security, sometimes called "Treasury bills" or more informally "T-bills" or "Treasuries". The Federal Reserve buys Treasury bills from its primary dealers. The purchase of these securities affects the federal funds rate, because primary dealers have accounts at depository institutions.

The Federal Reserve education website describes open market operations as follows:

To smooth temporary or cyclical changes in the money supply, the desk engages in repurchase agreements (repos) with its primary dealers. Repos are essentially secured, short-term lending by the Fed. On the day of the transaction, the Fed deposits money in a primary dealer's reserve account, and receives the promised securities as collateral. When the transaction matures, the process unwinds: the Fed returns the collateral and charges the primary dealer's reserve account for the principal and accrued interest. The term of the repo (the time between settlement and maturity) can vary from 1 day (called an overnight repo) to 65 days.

The Federal Reserve System also directly sets the "discount rate", which is the interest rate for "discount window lending", overnight loans that member banks borrow directly from the Fed. This rate is generally set at a rate close to 100 basis points above the target federal funds rate. The idea is to encourage banks to seek alternative funding before using the "discount rate" option. The equivalent operation by the European Central Bank is referred to as the "marginal lending facility".

Both the discount rate and the federal funds rate influence the prime rate, which is usually about 3 percentage points higher than the federal funds rate.

Another instrument of monetary policy adjustment employed by the Federal Reserve System is the fractional reserve requirement, also known as the required reserve ratio. The required reserve ratio sets the balance that the Federal Reserve System requires a depository institution to hold in the Federal Reserve Banks, which depository institutions trade in the federal funds market discussed above. The required reserve ratio is set by the board of governors of the Federal Reserve System. The reserve requirements have changed over time and some history of these changes is published by the Federal Reserve.

As a response to the financial crisis of 2008, the Federal Reserve now makes interest payments on depository institutions' required and excess reserve balances. The payment of interest on excess reserves gives the central bank greater opportunity to address credit market conditions while maintaining the federal funds rate close to the target rate set by the FOMC.

In order to address problems related to the subprime mortgage crisis and United States housing bubble, several new tools have been created. The first new tool, called the Term Auction Facility, was added on December 12, 2007. It was first announced as a temporary tool but there have been suggestions that this new tool may remain in place for a prolonged period of time. Creation of the second new tool, called the Term Securities Lending Facility, was announced on March 11, 2008. The main difference between these two facilities is that the Term Auction Facility is used to inject cash into the banking system whereas the Term Securities Lending Facility is used to inject treasury securities into the banking system. Creation of the third tool, called the Primary Dealer Credit Facility (PDCF), was announced on March 16, 2008. The PDCF was a fundamental change in Federal Reserve policy because now the Fed is able to lend directly to primary dealers, which was previously against Fed policy. The differences between these three new facilities is described by the Federal Reserve:

Some measures taken by the Federal Reserve to address this mortgage crisis have not been used since the Great Depression. The Federal Reserve gives a brief summary of these new facilities:

A fourth facility, the Term Deposit Facility, was announced December 9, 2009, and approved April 30, 2010, with an effective date of June 4, 2010. The Term Deposit Facility allows Reserve Banks to offer term deposits to institutions that are eligible to receive earnings on their balances at Reserve Banks. Term deposits are intended to facilitate the implementation of monetary policy by providing a tool by which the Federal Reserve can manage the aggregate quantity of reserve balances held by depository institutions. Funds placed in term deposits are removed from the accounts of participating institutions for the life of the term deposit and thus drain reserve balances from the banking system.

The Term Auction Facility is a program in which the Federal Reserve auctions term funds to depository institutions. The creation of this facility was announced by the Federal Reserve on December 12, 2007, and was done in conjunction with the Bank of Canada, the Bank of England, the European Central Bank, and the Swiss National Bank to address elevated pressures in short-term funding markets. The reason it was created is that banks were not lending funds to one another and banks in need of funds were refusing to go to the discount window. Banks were not lending money to each other because there was a fear that the loans would not be paid back. Banks refused to go to the discount window because it is usually associated with the stigma of bank failure. Under the Term Auction Facility, the identity of the banks in need of funds is protected in order to avoid the stigma of bank failure. Foreign exchange swap lines with the European Central Bank and Swiss National Bank were opened so the banks in Europe could have access to U.S. dollars. Federal Reserve Chairman Ben Bernanke briefly described this facility to the U.S. House of Representatives on January 17, 2008:

It is also described in the "Term Auction Facility FAQ"

The Term Securities Lending Facility is a 28-day facility that will offer Treasury general collateral to the Federal Reserve Bank of New York's primary dealers in exchange for other program-eligible collateral. It is intended to promote liquidity in the financing markets for Treasury and other collateral and thus to foster the functioning of financial markets more generally. Like the Term Auction Facility, the TSLF was done in conjunction with the Bank of Canada, the Bank of England, the European Central Bank, and the Swiss National Bank. The resource allows dealers to switch debt that is less liquid for U.S. government securities that are easily tradable. The currency swap lines with the European Central Bank and Swiss National Bank were increased.

The Primary Dealer Credit Facility (PDCF) is an overnight loan facility that will provide funding to primary dealers in exchange for a specified range of eligible collateral and is intended to foster the functioning of financial markets more generally. This new facility marks a fundamental change in Federal Reserve policy because now primary dealers can borrow directly from the Fed when this used to be prohibited.

, the Federal Reserve banks will pay interest on reserve balances (required and excess) held by depository institutions. The rate is set at the lowest federal funds rate during the reserve maintenance period of an institution, less 75bp. , the Fed has lowered the spread to a mere 35 bp.

The Term Deposit Facility is a program through which the Federal Reserve Banks will offer interest-bearing term deposits to eligible institutions. By removing "excess deposits" from participating banks, the overall level of reserves available for lending is reduced, which should result in increased market interest rates, acting as a brake on economic activity and inflation. The Federal Reserve has stated that:

The Federal Reserve initially authorized up to five "small-value offerings are designed to ensure the effectiveness of TDF operations and to provide eligible institutions with an opportunity to gain familiarity with term deposit procedures." After three of the offering auctions were successfully completed, it was announced that small-value auctions would continue on an ongoing basis.

The Term Deposit Facility is essentially a tool available to reverse the efforts that have been employed to provide liquidity to the financial markets and to reduce the amount of capital available to the economy. As stated in Bloomberg News:

Chairman Ben S. Bernanke, testifying before House Committee on Financial Services, described the Term Deposit Facility and other facilities to Congress in the following terms:

The Asset Backed Commercial Paper Money Market Mutual Fund Liquidity Facility (ABCPMMMFLF) was also called the AMLF. The Facility began operations on September 22, 2008, and was closed on February 1, 2010.

All U.S. depository institutions, bank holding companies (parent companies or U.S. broker-dealer affiliates), or U.S. branches and agencies of foreign banks were eligible to borrow under this facility pursuant to the discretion of the FRBB.

Collateral eligible for pledge under the Facility was required to meet the following criteria:

On October 7, 2008, the Federal Reserve further expanded the collateral it will loan against to include commercial paper using the new Commercial Paper Funding Facility (CPFF). The action made the Fed a crucial source of credit for non-financial businesses in addition to commercial banks and investment firms. Fed officials said they'll buy as much of the debt as necessary to get the market functioning again. They refused to say how much that might be, but they noted that around $1.3 trillion worth of commercial paper would qualify. There was $1.61 trillion in outstanding commercial paper, seasonally adjusted, on the market , according to the most recent data from the Fed. That was down from $1.70 trillion in the previous week. Since the summer of 2007, the market has shrunk from more than $2.2 trillion. This program lent out a total $738 billion before it was closed. Forty-five out of 81 of the companies participating in this program were foreign firms. Research shows that Troubled Asset Relief Program (TARP) recipients were twice as likely to participate in the program than other commercial paper issuers who did not take advantage of the TARP bailout. The Fed incurred no losses from the CPFF.

A little-used tool of the Federal Reserve is the "quantitative policy". With that the Federal Reserve actually buys back corporate bonds and mortgage backed securities held by banks or other financial institutions. This in effect puts money back into the financial institutions and allows them to make loans and conduct normal business.
The bursting of the United States housing bubble prompted the Fed to buy mortgage-backed securities for the first time in November 2008. Over six weeks, a total of $1.25 trillion were purchased in order to stabilize the housing market, about one-fifth of all U.S. government-backed mortgages.

The first attempt at a national currency was during the American Revolutionary War. In 1775, the Continental Congress, as well as the states, began issuing paper currency, calling the bills "Continentals". The Continentals were backed only by future tax revenue, and were used to help finance the Revolutionary War. Overprinting, as well as British counterfeiting, caused the value of the Continental to diminish quickly. This experience with paper money led the United States to strip the power to issue Bills of Credit (paper money) from a draft of the new Constitution on August 16, 1787, as well as banning such issuance by the various states, and limiting the states' ability to make anything but gold or silver coin legal tender on August 28.

In 1791, the government granted the First Bank of the United States a charter to operate as the U.S. central bank until 1811. The First Bank of the United States came to an end under President Madison because Congress refused to renew its charter. The Second Bank of the United States was established in 1816, and lost its authority to be the central bank of the U.S. twenty years later under President Jackson when its charter expired. Both banks were based upon the Bank of England. Ultimately, a third national bank, known as the Federal Reserve, was established in 1913 and still exists to this day.

The first U.S. institution with central banking responsibilities was the First Bank of the United States, chartered by Congress and signed into law by President George Washington on February 25, 1791, at the urging of Alexander Hamilton. This was done despite strong opposition from Thomas Jefferson and James Madison, among numerous others. The charter was for twenty years and expired in 1811 under President Madison, because Congress refused to renew it.

In 1816, however, Madison revived it in the form of the Second Bank of the United States. Years later, early renewal of the bank's charter became the primary issue in the reelection of President Andrew Jackson. After Jackson, who was opposed to the central bank, was reelected, he pulled the government's funds out of the bank. Jackson was the only President to completely pay off the debt. The bank's charter was not renewed in 1836.
From 1837 to 1862, in the Free Banking Era there was no formal central bank.
From 1846 to 1921, an Independent Treasury System ruled.
From 1863 to 1913, a system of national banks was instituted by the 1863 National Banking Act during which series of bank panics, in 1873, 1893, and 1907 occurred

The main motivation for the third central banking system came from the Panic of 1907, which caused a renewed desire among legislators, economists, and bankers for an overhaul of the monetary system. During the last quarter of the 19th century and the beginning of the 20th century, the United States economy went through a series of financial panics. According to many economists, the previous national banking system had two main weaknesses: an inelastic currency and a lack of liquidity. In 1908, Congress enacted the Aldrich–Vreeland Act, which provided for an emergency currency and established the National Monetary Commission to study banking and currency reform. The National Monetary Commission returned with recommendations which were repeatedly rejected by Congress. A revision crafted during a secret meeting on Jekyll Island by Senator Aldrich and representatives of the nation's top finance and industrial groups later became the basis of the Federal Reserve Act. The House voted on December 22, 1913, with 298 voting yes to 60 voting no. The Senate voted 4325 on December 23, 1913. President Woodrow Wilson signed the bill later that day.

The head of the bipartisan National Monetary Commission was financial expert and Senate Republican leader Nelson Aldrich. Aldrich set up two commissions – one to study the American monetary system in depth and the other, headed by Aldrich himself, to study the European central banking systems and report on them.

In early November 1910, Aldrich met with five well known members of the New York banking community to devise a central banking bill. Paul Warburg, an attendee of the meeting and longtime advocate of central banking in the U.S., later wrote that Aldrich was "bewildered at all that he had absorbed abroad and he was faced with the difficult task of writing a highly technical bill while being harassed by the daily grind of his parliamentary duties". After ten days of deliberation, the bill, which would later be referred to as the "Aldrich Plan", was agreed upon. It had several key components, including a central bank with a Washington-based headquarters and fifteen branches located throughout the U.S. in geographically strategic locations, and a uniform elastic currency based on gold and commercial paper. Aldrich believed a central banking system with no political involvement was best, but was convinced by Warburg that a plan with no public control was not politically feasible. The compromise involved representation of the public sector on the Board of Directors.

Aldrich's bill met much opposition from politicians. Critics charged Aldrich of being biased due to his close ties to wealthy bankers such as J. P. Morgan and John D. Rockefeller, Jr., Aldrich's son-in-law. Most Republicans favored the Aldrich Plan, but it lacked enough support in Congress to pass because rural and western states viewed it as favoring the "eastern establishment". In contrast, progressive Democrats favored a reserve system owned and operated by the government; they believed that public ownership of the central bank would end Wall Street's control of the American currency supply. Conservative Democrats fought for a privately owned, yet decentralized, reserve system, which would still be free of Wall Street's control.

The original Aldrich Plan was dealt a fatal blow in 1912, when Democrats won the White House and Congress. Nonetheless, President Woodrow Wilson believed that the Aldrich plan would suffice with a few modifications. The plan became the basis for the Federal Reserve Act, which was proposed by Senator Robert Owen in May 1913. The primary difference between the two bills was the transfer of control of the Board of Directors (called the Federal Open Market Committee in the Federal Reserve Act) to the government. The bill passed Congress on December 23, 1913, on a mostly partisan basis, with most Democrats voting "yea" and most Republicans voting "nay".

Key laws affecting the Federal Reserve have been:

The Federal Reserve records and publishes large amounts of data. A few websites where data is published are at the board of governors' Economic Data and Research page, the board of governors' statistical releases and historical data page, and at the St. Louis Fed's FRED (Federal Reserve Economic Data) page. The Federal Open Market Committee (FOMC) examines many economic indicators prior to determining monetary policy.

Some criticism involves economic data compiled by the Fed. The Fed sponsors much of the monetary economics research in the U.S., and Lawrence H. White objects that this makes it less likely for researchers to publish findings challenging the status quo.

The net worth of households and nonprofit organizations in the United States is published by the Federal Reserve in a report titled "Flow of Funds". At the end of the third quarter of fiscal year 2012, this value was $64.8 trillion. At the end of the first quarter of fiscal year 2014, this value was $95.5 trillion.

The most common measures are named M0 (narrowest), M1, M2, and M3. In the United States they are defined by the Federal Reserve as follows:

The Federal Reserve stopped publishing M3 statistics in March 2006, saying that the data cost a lot to collect but did not provide significantly useful information. The other three money supply measures continue to be provided in detail.
The Personal consumption expenditures price index, also referred to as simply the PCE price index, is used as one measure of the value of money. It is a United States-wide indicator of the average increase in prices for all domestic personal consumption. Using a variety of data including United States Consumer Price Index and U.S. Producer Price Index prices, it is derived from the largest component of the gross domestic product in the BEA's National Income and Product Accounts, personal consumption expenditures.

One of the Fed's main roles is to maintain price stability, which means that the Fed's ability to keep a low inflation rate is a long-term measure of their success. Although the Fed is not required to maintain inflation within a specific range, their long run target for the growth of the PCE price index is between 1.5 and 2 percent. There has been debate among policy makers as to whether the Federal Reserve should have a specific inflation targeting policy.

Most mainstream economists favor a low, steady rate of inflation. Low (as opposed to zero or negative) inflation may reduce the severity of economic recessions by enabling the labor market to adjust more quickly in a downturn, and reduce the risk that a liquidity trap prevents monetary policy from stabilizing the economy. The task of keeping the rate of inflation low and stable is usually given to monetary authorities.

One of the stated goals of monetary policy is maximum employment. The unemployment rate statistics are collected by the Bureau of Labor Statistics, and like the PCE price index are used as a barometer of the nation's economic health.

The Federal Reserve is self-funded. The vast majority (90%+) of Fed revenues come from open market operations, specifically the interest on the portfolio of Treasury securities as well as "capital gains/losses" that may arise from the buying/selling of the securities and their derivatives as part of Open Market Operations. The balance of revenues come from sales of financial services (check and electronic payment processing) and discount window loans. The board of governors (Federal Reserve Board) creates a budget report once per year for Congress. There are two reports with budget information. The one that lists the complete balance statements with income and expenses as well as the net profit or loss is the large report simply titled, "Annual Report". It also includes data about employment throughout the system. The other report, which explains in more detail the expenses of the different aspects of the whole system, is called "Annual Report: Budget Review". These detailed comprehensive reports can be found at the board of governors' website under the section "Reports to Congress"

One of the keys to understanding the Federal Reserve is the Federal Reserve balance sheet (or balance statement). In accordance with Section 11 of the Federal Reserve Act, the board of governors of the Federal Reserve System publishes once each week the "Consolidated Statement of Condition of All Federal Reserve Banks" showing the condition of each Federal Reserve bank and a consolidated statement for all Federal Reserve banks. The board of governors requires that excess earnings of the Reserve Banks be transferred to the Treasury as interest on Federal Reserve notes.

Below is the balance sheet (in billions of dollars):

In addition, the balance sheet also indicates which assets are held as collateral against Federal Reserve Notes.

The Federal Reserve System has faced various criticisms since its inception in 1913. Critique of the organization and system has come from sources such as writers, journalists, economists, and financial institutions as well as politicians and various government employees. Criticisms include lack of transparency, doubt of efficacy due to what is seen by some as poor historical performance and traditionalist concerns about the debasement of the value of the dollar. From the beginning, the Federal Reserve has been the subject of many popular conspiracy theories, that typically link the Fed to numerous other supposed conspiracies.





</doc>
<doc id="10821" url="https://en.wikipedia.org/wiki?curid=10821" title="Francium">
Francium

Francium is a chemical element with the symbol Fr and atomic number 87. It used to be known as eka-caesium. It is extremely radioactive; its most stable isotope, francium-223 (originally called actinium K after the natural decay chain it appears in), has a half-life of only 22 minutes. It is the second-most electropositive element, behind only caesium, and is the second rarest naturally occurring element (after astatine). The isotopes of francium decay quickly into astatine, radium, and radon. The electronic structure of a francium atom is [Rn] 7s, and so the element is classed as an alkali metal.

Bulk francium has never been viewed. Because of the general appearance of the other elements in its periodic table column, it is assumed that francium would appear as a highly reactive metal, if enough could be collected together to be viewed as a bulk solid or liquid. Obtaining such a sample is highly improbable, since the extreme heat of decay caused by its short half-life would immediately vaporize any viewable quantity of the element.

Francium was discovered by Marguerite Perey in France (from which the element takes its name) in 1939. It was the last element first discovered in nature, rather than by synthesis. Outside the laboratory, francium is extremely rare, with trace amounts found in uranium and thorium ores, where the isotope francium-223 continually forms and decays. As little as 20–30 g (one ounce) exists at any given time throughout the Earth's crust; the other isotopes (except for francium-221) are entirely synthetic. The largest amount produced in the laboratory was a cluster of more than 300,000 atoms.

Francium is one of the most unstable of the naturally occurring elements: its longest-lived isotope, francium-223, has a half-life of only 22 minutes. The only comparable element is astatine, whose most stable natural isotope, astatine-219 (the alpha daughter of francium-223), has a half-life of 56 seconds, although synthetic astatine-210 is much longer-lived with a half-life of 8.1 hours. All isotopes of francium decay into astatine, radium, or radon. Francium-223 also has a shorter half-life than the longest-lived isotope of each synthetic element up to and including element 105, dubnium.

Francium is an alkali metal whose chemical properties mostly resemble those of caesium. A heavy element with a single valence electron, it has the highest equivalent weight of any element. Liquid francium—if created—should have a surface tension of 0.05092 N/m at its melting point. Francium's melting point was calculated to be around 27 °C (80 °F, 300 K). The melting point is uncertain because of the element's extreme rarity and radioactivity. The estimated boiling point of 677 °C (1250 °F, 950 K) is also uncertain; the estimate 598 °C (1108 °F, 871 K) has also been suggested.

Linus Pauling estimated the electronegativity of francium at 0.7 on the Pauling scale, the same as caesium; the value for caesium has since been refined to 0.79, but there are no experimental data to allow a refinement of the value for francium. Francium has a slightly higher ionization energy than caesium, 392.811(4) kJ/mol as opposed to 375.7041(2) kJ/mol for caesium, as would be expected from relativistic effects, and this would imply that caesium is the less electronegative of the two. Francium should also have a higher electron affinity than caesium and the Fr ion should be more polarizable than the Cs ion. The CsFr molecule is predicted to have francium at the negative end of the dipole, unlike all known heterodiatomic alkali metal molecules. Francium superoxide (FrO) is expected to have a more covalent character than its lighter congeners; this is attributed to the 6p electrons in francium being more involved in the francium–oxygen bonding.

Francium coprecipitates with several caesium salts, such as caesium perchlorate, which results in small amounts of francium perchlorate. This coprecipitation can be used to isolate francium, by adapting the radiocaesium coprecipitation method of Lawrence E. Glendenin and Nelson. It will additionally coprecipitate with many other caesium salts, including the iodate, the picrate, the tartrate (also rubidium tartrate), the chloroplatinate, and the silicotungstate. It also coprecipitates with silicotungstic acid, and with perchloric acid, without another alkali metal as a carrier, which provides other methods of separation. Nearly all francium salts are water-soluble.

There are 34 known isotopes of francium ranging in atomic mass from 199 to 232. Francium has seven metastable nuclear isomers. Francium-223 and francium-221 are the only isotopes that occur in nature, though the former is far more common.

Francium-223 is the most stable isotope, with a half-life of 21.8 minutes, and it is highly unlikely that an isotope of francium with a longer half-life will ever be discovered or synthesized. Francium-223 is the fifth product of the actinium decay series as the daughter isotope of actinium-227. Francium-223 then decays into radium-223 by beta decay (1.149 MeV decay energy), with a minor (0.006%) alpha decay path to astatine-219 (5.4 MeV decay energy).

Francium-221 has a half-life of 4.8 minutes. It is the ninth product of the neptunium decay series as a daughter isotope of actinium-225. Francium-221 then decays into astatine-217 by alpha decay (6.457 MeV decay energy).

The least stable ground state isotope is francium-215, with a half-life of 0.12 μs: it undergoes a 9.54 MeV alpha decay to astatine-211. Its metastable isomer, francium-215m, is less stable still, with a half-life of only 3.5 ns.

Due to its instability and rarity, there are no commercial applications for francium. It has been used for research purposes in the fields of chemistry
and of atomic structure. Its use as a potential diagnostic aid for various cancers has also been explored, but this application has been deemed impractical.

Francium's ability to be synthesized, trapped, and cooled, along with its relatively simple atomic structure, has made it the subject of specialized spectroscopy experiments. These experiments have led to more specific information regarding energy levels and the coupling constants between subatomic particles. Studies on the light emitted by laser-trapped francium-210 ions have provided accurate data on transitions between atomic energy levels which are fairly similar to those predicted by quantum theory.

As early as 1870, chemists thought that there should be an alkali metal beyond caesium, with an atomic number of 87. It was then referred to by the provisional name "eka-caesium". Research teams attempted to locate and isolate this missing element, and at least four false claims were made that the element had been found before an authentic discovery was made.

Soviet chemist D. K. Dobroserdov was the first scientist to claim to have found eka-caesium, or francium. In 1925, he observed weak radioactivity in a sample of potassium, another alkali metal, and incorrectly concluded that eka-caesium was contaminating the sample (the radioactivity from the sample was from the naturally occurring potassium radioisotope, potassium-40). He then published a thesis on his predictions of the properties of eka-caesium, in which he named the element "russium" after his home country. Shortly thereafter, Dobroserdov began to focus on his teaching career at the Polytechnic Institute of Odessa, and he did not pursue the element further.

The following year, English chemists Gerald J. F. Druce and Frederick H. Loring analyzed X-ray photographs of manganese(II) sulfate. They observed spectral lines which they presumed to be of eka-caesium. They announced their discovery of element 87 and proposed the name "alkalinium", as it would be the heaviest alkali metal.

In 1930, Fred Allison of the Alabama Polytechnic Institute claimed to have discovered element 87 when analyzing pollucite and lepidolite using his magneto-optical machine. Allison requested that it be named "virginium" after his home state of Virginia, along with the symbols Vi and Vm. In 1934, H.G. MacPherson of UC Berkeley disproved the effectiveness of Allison's device and the validity of his discovery.

In 1936, Romanian physicist Horia Hulubei and his French colleague Yvette Cauchois also analyzed pollucite, this time using their high-resolution X-ray apparatus. They observed several weak emission lines, which they presumed to be those of element 87. Hulubei and Cauchois reported their discovery and proposed the name "moldavium", along with the symbol Ml, after Moldavia, the Romanian province where Hulubei was born. In 1937, Hulubei's work was criticized by American physicist F. H. Hirsh Jr., who rejected Hulubei's research methods. Hirsh was certain that eka-caesium would not be found in nature, and that Hulubei had instead observed mercury or bismuth X-ray lines. Hulubei insisted that his X-ray apparatus and methods were too accurate to make such a mistake. Because of this, Jean Baptiste Perrin, Nobel Prize winner and Hulubei's mentor, endorsed moldavium as the true eka-caesium over Marguerite Perey's recently discovered francium. Perey took pains to be accurate and detailed in her criticism of Hulubei's work, and finally she was credited as the sole discoverer of element 87. All other previous purported discoveries of element 87 were ruled out due to francium's very limited half-life.

Eka-caesium was discovered in on 7 January 1939 by Marguerite Perey of the Curie Institute in Paris, when she purified a sample of actinium-227 which had been reported to have a decay energy of 220 keV. Perey noticed decay particles with an energy level below 80 keV. Perey thought this decay activity might have been caused by a previously unidentified decay product, one which was separated during purification, but emerged again out of the pure actinium-227. Various tests eliminated the possibility of the unknown element being thorium, radium, lead, bismuth, or thallium. The new product exhibited chemical properties of an alkali metal (such as coprecipitating with caesium salts), which led Perey to believe that it was element 87, produced by the alpha decay of actinium-227. Perey then attempted to determine the proportion of beta decay to alpha decay in actinium-227. Her first test put the alpha branching at 0.6%, a figure which she later revised to 1%.

Perey named the new isotope "actinium-K" (it is now referred to as francium-223) and in 1946, she proposed the name "catium" (Cm) for her newly discovered element, as she believed it to be the most electropositive cation of the elements. Irène Joliot-Curie, one of Perey's supervisors, opposed the name due to its connotation of "cat" rather than "cation"; furthermore, the symbol coincided with that which had since been assigned to curium. Perey then suggested "francium", after France. This name was officially adopted by the International Union of Pure and Applied Chemistry (IUPAC) in 1949, becoming the second element after gallium to be named after France. It was assigned the symbol Fa, but this abbreviation was revised to the current Fr shortly thereafter. Francium was the last element discovered in nature, rather than synthesized, following hafnium and rhenium. Further research into francium's structure was carried out by, among others, Sylvain Lieberman and his team at CERN in the 1970s and 1980s.

Fr is the result of the alpha decay of Ac and can be found in trace amounts in uranium minerals. In a given sample of uranium, there is estimated to be only one francium atom for every 1 × 10 uranium atoms. It is also calculated that there is at most 30 g of francium in the Earth's crust at any given time.

Francium can be synthesized by a fusion reaction when a gold-197 target is bombarded with a beam of oxygen-18 atoms from a linear accelerator in a process originally developed at in the physics department at the State University of New York at Stony Brook in 1995. Depending on the energy of the oxygen beam, the reaction can yield francium isotopes with masses of 209, 210, and 211.

The francium atoms leave the gold target as ions, which are neutralized by collision with yttrium and then isolated in a magneto-optical trap (MOT) in a gaseous unconsolidated state. Although the atoms only remain in the trap for about 30 seconds before escaping or undergoing nuclear decay, the process supplies a continual stream of fresh atoms. The result is a steady state containing a fairly constant number of atoms for a much longer time. The original apparatus could trap up to a few thousand atoms, while a later improved design could trap over 300,000 at a time. Sensitive measurements of the light emitted and absorbed by the trapped atoms provided the first experimental results on various transitions between atomic energy levels in francium. Initial measurements show very good agreement between experimental values and calculations based on quantum theory. The research project using this production method relocated to TRIUMF in 2012, where over 10 francium atoms have been held at a time, including large amounts of Fr in addition to Fr and Fr.

Other synthesis methods include bombarding radium with neutrons, and bombarding thorium with protons, deuterons, or helium ions.

Fr can also be isolated from samples of its parent Ac, the francium being milked via elution with NHCl–CrO from an actinium-containing cation exchanger and purified by passing the solution through a silicon dioxide compound loaded with barium sulfate.

In 1996, the Stony Brook group trapped 3000 atoms in their MOT, which was enough for a video camera to capture the light given off by the atoms as they fluoresce. Francium has not been synthesized in amounts large enough to weigh.



</doc>
<doc id="10822" url="https://en.wikipedia.org/wiki?curid=10822" title="Fermium">
Fermium

Fermium is a synthetic element with the symbol Fm and atomic number 100. It is an actinide and the heaviest element that can be formed by neutron bombardment of lighter elements, and hence the last element that can be prepared in macroscopic quantities, although pure fermium metal has not yet been prepared. A total of 19 isotopes are known, with Fm being the longest-lived with a half-life of 100.5 days.

It was discovered in the debris of the first hydrogen bomb explosion in 1952, and named after Enrico Fermi, one of the pioneers of nuclear physics. Its chemistry is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. Owing to the small amounts of produced fermium and all of its isotopes having relatively short half-lives, there are currently no uses for it outside basic scientific research.

Fermium was first discovered in the fallout from the 'Ivy Mike' nuclear test (1 November 1952), the first successful test of a hydrogen bomb. Initial examination of the debris from the explosion had shown the production of a new isotope of plutonium, : this could only have formed by the absorption of six neutrons by a uranium-238 nucleus followed by two β decays. At the time, the absorption of neutrons by a heavy nucleus was thought to be a rare process, but the identification of raised the possibility that still more neutrons could have been absorbed by the uranium nuclei, leading to new elements.

Element 99 (einsteinium) was quickly discovered on filter papers which had been flown through the cloud from the explosion (the same sampling technique that had been used to discover ). It was then identified in December 1952 by Albert Ghiorso and co-workers at the University of California at Berkeley. They discovered the isotope Es (half-life 20.5 days) that was made by the capture of 15 neutrons by uranium-238 nuclei – which then underwent seven successive beta decays:

Some U atoms, however, could capture another amount of neutrons (most likely, 16 or 17).

The discovery of fermium () required more material, as the yield was expected to be at least an order of magnitude lower than that of element 99, and so contaminated coral from the Enewetak atoll (where the test had taken place) was shipped to the University of California Radiation Laboratory in Berkeley, California, for processing and analysis. About two months after the test, a new component was isolated emitting high-energy α-particles (7.1 MeV) with a half-life of about a day. With such a short half-life, it could only arise from the β decay of an isotope of einsteinium, and so had to be an isotope of the new element 100: it was quickly identified as Fm ().

The discovery of the new elements, and the new data on neutron capture, was initially kept secret on the orders of the U.S. military until 1955 due to Cold War tensions. Nevertheless, the Berkeley team was able to prepare elements 99 and 100 by civilian means, through the neutron bombardment of plutonium-239, and published this work in 1954 with the disclaimer that it was not the first studies that had been carried out on the elements. The "Ivy Mike" studies were declassified and published in 1955.

The Berkeley team had been worried that another group might discover lighter isotopes of element 100 through ion-bombardment techniques before they could publish their classified research, and this proved to be the case. A group at the Nobel Institute for Physics in Stockholm independently discovered the element, producing an isotope later confirmed to be Fm ("t" = 30 minutes) by bombarding a target with oxygen-16 ions, and published their work in May 1954. Nevertheless, the priority of the Berkeley team was generally recognized, and with it the prerogative to name the new element in honour of the recently deceased Enrico Fermi, the developer of the first artificial self-sustained nuclear reactor.

There are 20 isotopes of fermium listed in N 2016, with atomic weights of 241 to 260, of which Fm is the longest-lived with a half-life of 100.5 days. Fm has a half-life of 3 days, while Fm of 5.3 h, Fm of 25.4 h, Fm of 3.2 h, Fm of 20.1 h, and Fm of 2.6 hours. All the remaining ones have half-lives ranging from 30 minutes to less than a millisecond.
The neutron-capture product of fermium-257, Fm, undergoes spontaneous fission with a half-life of just 370(14) microseconds; Fm and Fm are also unstable with respect to spontaneous fission ("t" = 1.5(3) s and 4 ms respectively). This means that neutron capture cannot be used to create nuclides with a mass number greater than 257, unless carried out in a nuclear explosion. As Fm is an α-emitter, decaying to Cf, and no known fermium isotopes undergo beta minus decay to the next element, mendelevium, fermium is also the last element that can be prepared by a neutron-capture process. Because of this impediment in forming heavier isotopes, these short-lived isotopes Fm constitute the so-called "fermium gap." 

Fermium is produced by the bombardment of lighter actinides with neutrons in a nuclear reactor. Fermium-257 is the heaviest isotope that is obtained via neutron capture, and can only be produced in picogram quantities. The major source is the 85 MW High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory in Tennessee, USA, which is dedicated to the production of transcurium ("Z" > 96) elements. Lower mass fermium isotopes are available in greater quantities, however, these isotopes (254 and 255) are very short lived. In a "typical processing campaign" at Oak Ridge, tens of grams of curium are irradiated to produce decigram quantities of californium, milligram quantities of berkelium and einsteinium and picogram quantities of fermium. However, nanogram quantities of fermium can be prepared for specific experiments. The quantities of fermium produced in 20–200 kiloton thermonuclear explosions is believed to be of the order of milligrams, although it is mixed in with a huge quantity of debris; 4.0 picograms of Fm was recovered from 10 kilograms of debris from the "Hutch" test (16 July 1969). The Hutch experiment produced an estimated total of 250 micrograms of Fm.

After production, the fermium must be separated from other actinides and from lanthanide fission products. This is usually achieved by ion-exchange chromatography, with the standard process using a cation exchanger such as Dowex 50 or T eluted with a solution of ammonium α-hydroxyisobutyrate. Smaller cations form more stable complexes with the α-hydroxyisobutyrate anion, and so are preferentially eluted from the column. A rapid fractional crystallization method has also been described.

Although the most stable isotope of fermium is Fm, with a half-life of 100.5 days, most studies are conducted on Fm ("t" = 20.07(7) hours), since this isotope can be easily isolated as required as the decay product of Es ("t" = 39.8(12) days).

The analysis of the debris at the 10-megaton "Ivy Mike" nuclear test was a part of long-term project, one of the goals of which was studying the efficiency of production of transuranium elements in high-power nuclear explosions. The motivation for these experiments was as follows: synthesis of such elements from uranium requires multiple neutron capture. The probability of such events increases with the neutron flux, and nuclear explosions are the most powerful neutron sources, providing densities of the order 10 neutrons/cm within a microsecond, i.e. about 10 neutrons/(cm·s). In comparison, the flux of the HFIR reactor is 5 neutrons/(cm·s). A dedicated laboratory was set up right at Enewetak Atoll for preliminary analysis of debris, as some isotopes could have decayed by the time the debris samples reached the U.S. The laboratory was receiving samples for analysis, as soon as possible, from airplanes equipped with paper filters which flew over the atoll after the tests. Whereas it was hoped to discover new chemical elements heavier than fermium, those were not found after a series of megaton explosions conducted between 1954 and 1956 at the atoll.
The atmospheric results were supplemented by the underground test data accumulated in the 1960s at the Nevada Test Site, as it was hoped that powerful explosions conducted in confined space might result in improved yields and heavier isotopes. Apart from traditional uranium charges, combinations of uranium with americium and thorium have been tried, as well as a mixed plutonium-neptunium charge. They were less successful in terms of yield that was attributed to stronger losses of heavy isotopes due to enhanced fission rates in heavy-element charges. Isolation of the products was found to be rather problematic, as the explosions were spreading debris through melting and vaporizing rocks under the great depth of 300–600 meters, and drilling to such depth in order to extract the products was both slow and inefficient in terms of collected volumes.

Among the nine underground tests, which were carried between 1962 and 1969 and codenamed Anacostia (5.2 kilotons, 1962), Kennebec (<5 kilotons, 1963), Par (38 kilotons, 1964), Barbel (<20 kilotons, 1964), Tweed (<20 kilotons, 1965), Cyclamen (13 kilotons, 1966), Kankakee (20-200 kilotons, 1966), Vulcan (25 kilotons, 1966) and Hutch (20-200 kilotons, 1969), the last one was most powerful and had the highest yield of transuranium elements. In the dependence on the atomic mass number, the yield showed a saw-tooth behavior with the lower values for odd isotopes, due to their higher fission rates. The major practical problem of the entire proposal was however collecting the radioactive debris dispersed by the powerful blast. Aircraft filters adsorbed only about 4 of the total amount and collection of tons of corals at Enewetak Atoll increased this fraction by only two orders of magnitude. Extraction of about 500 kilograms of underground rocks 60 days after the Hutch explosion recovered only about 10 of the total charge. The amount of transuranium elements in this 500-kg batch was only 30 times higher than in a 0.4 kg rock picked up 7 days after the test. This observation demonstrated the highly nonlinear dependence of the transuranium elements yield on the amount of retrieved radioactive rock. In order to accelerate sample collection after explosion, shafts were drilled at the site not after but before the test, so that explosion would expel radioactive material from the epicenter, through the shafts, to collecting volumes near the surface. This method was tried in the Anacostia and Kennebec tests and instantly provided hundreds kilograms of material, but with actinide concentration 3 times lower than in samples obtained after drilling; whereas such method could have been efficient in scientific studies of short-lived isotopes, it could not improve the overall collection efficiency of the produced actinides.

Although no new elements (apart from einsteinium and fermium) could be detected in the nuclear test debris, and the total yields of transuranium elements were disappointingly low, these tests did provide significantly higher amounts of rare heavy isotopes than previously available in laboratories. So 6 atoms of Fm could be recovered after the Hutch detonation. They were then used in the studies of thermal-neutron induced fission of Fm and in discovery of a new fermium isotope Fm. Also, the rare Cm isotope was synthesized in large quantities, which is very difficult to produce in nuclear reactors from its progenitor Cm – the half-life of Cm (64 minutes) is much too short for months-long reactor irradiations, but is very "long" on the explosion timescale.

Because of the short half-life of all isotopes of fermium, any primordial fermium, that is fermium that could be present on the Earth during its formation, has decayed by now. Synthesis of fermium from naturally occurring actinides uranium and thorium in the Earth crust requires multiple neutron capture, which is an extremely unlikely event. Therefore, most fermium is produced on Earth in scientific laboratories, high-power nuclear reactors, or in nuclear weapons tests, and is present only within a few months from the time of the synthesis. The transuranic elements from americium to fermium did occur naturally in the natural nuclear fission reactor at Oklo, but no longer do so.

The chemistry of fermium has only been studied in solution using tracer techniques, and no solid compounds have been prepared. Under normal conditions, fermium exists in solution as the Fm ion, which has a hydration number of 16.9 and an acid dissociation constant of 1.6 (p"K" = 3.8). Fm forms complexes with a wide variety of organic ligands with hard donor atoms such as oxygen, and these complexes are usually more stable than those of the preceding actinides. It also forms anionic complexes with ligands such as chloride or nitrate and, again, these complexes appear to be more stable than those formed by einsteinium or californium. It is believed that the bonding in the complexes of the later actinides is mostly ionic in character: the Fm ion is expected to be smaller than the preceding An ions because of the higher effective nuclear charge of fermium, and hence fermium would be expected to form shorter and stronger metal–ligand bonds.

Fermium(III) can be fairly easily reduced to fermium(II), for example with samarium(II) chloride, with which fermium(II) coprecipitates. In the precipitate, the compound fermium(II) chloride (FmCl) was produced, though it was not purified or studied in isolation. The electrode potential has been estimated to be similar to that of the ytterbium(III)/(II) couple, or about −1.15 V with respect to the standard hydrogen electrode, a value which agrees with theoretical calculations. The Fm/Fm couple has an electrode potential of −2.37(10) V based on polarographic measurements.

Although few people come in contact with fermium, the International Commission on Radiological Protection has set annual exposure limits for the two most stable isotopes. For fermium-253, the ingestion limit was set at 10 becquerels (1 Bq is equivalent to one decay per second), and the inhalation limit at 10 Bq; for fermium-257, at 10 Bq and 4000 Bq respectively.




</doc>
<doc id="10823" url="https://en.wikipedia.org/wiki?curid=10823" title="Frédéric Chopin">
Frédéric Chopin

Frédéric François Chopin (; ; ; 1 March 181017 October 1849) was a Polish composer and virtuoso pianist of the Romantic era who wrote primarily for solo piano. He has maintained worldwide renown as a leading musician of his era, one whose "poetic genius was based on a professional technique that was without equal in his generation."

Chopin was born Fryderyk Franciszek Chopin in the Duchy of Warsaw and grew up in Warsaw, which in 1815 became part of Congress Poland. A child prodigy, he completed his musical education and composed his earlier works in Warsaw before leaving Poland at the age of 20, less than a month before the outbreak of the November 1830 Uprising. At 21, he settled in Paris. Thereafter—in the last 18 years of his life—he gave only 30 public performances, preferring the more intimate atmosphere of the salon. He supported himself by selling his compositions and by giving piano lessons, for which he was in high demand. Chopin formed a friendship with Franz Liszt and was admired by many of his other musical contemporaries (including Robert Schumann). In 1835, Chopin obtained French citizenship. After a failed engagement to Maria Wodzińska from 1836 to 1837, he maintained an often troubled relationship with the French writer Amantine Dupin (known by her pen name, George Sand). A brief and unhappy visit to Majorca with Sand in 1838–39 would prove one of his most productive periods of composition. In his final years, he was supported financially by his admirer Jane Stirling, who also arranged for him to visit Scotland in 1848. For most of his life, Chopin was in poor health. He died in Paris in 1849 at the age of 39, probably of pericarditis aggravated by tuberculosis.

All of Chopin's compositions include the piano. Most are for solo piano, though he also wrote two piano concertos, a few chamber pieces, and some 19 songs set to Polish lyrics. His piano writing was technically demanding and expanded the limits of the instrument: his own performances were noted for their nuance and sensitivity. Chopin invented the concept of the instrumental "ballade". His major piano works also include mazurkas, waltzes, nocturnes, polonaises, études, impromptus, scherzos, preludes and sonatas, some published only posthumously. Among the influences on his style of composition were Polish folk music, the classical tradition of J.S. Bach, Mozart, and Schubert, and the atmosphere of the Paris salons of which he was a frequent guest. His innovations in style, harmony, and musical form, and his association of music with nationalism, were influential throughout and after the late Romantic period.

Chopin's music, his status as one of music's earliest superstars, his (indirect) association with political insurrection, his high-profile love-life, and his early death have made him a leading symbol of the Romantic era. His works remain popular, and he has been the subject of numerous films and biographies of varying historical fidelity.

Fryderyk Chopin was born in Żelazowa Wola, 46 kilometres () west of Warsaw, in what was then the Duchy of Warsaw, a Polish state established by Napoleon. The parish baptismal record gives his birthday as 22 February 1810, and cites his given names in the Latin form "Fridericus Franciscus" (in Polish, he was "Fryderyk Franciszek"). However, the composer and his family used the birthdate 1 March, which is now generally accepted as the correct date.
Fryderyk's father, Nicolas Chopin, was a Frenchman from Lorraine who had emigrated to Poland in 1787 at the age of sixteen. Nicolas tutored children of the Polish aristocracy, and in 1806 married Tekla Justyna Krzyżanowska, a poor relative of the Skarbeks, one of the families for whom he worked. Fryderyk was baptized on Easter Sunday, 23 April 1810, in the same church where his parents had married, in Brochów. His eighteen-year-old godfather, for whom he was named, was Fryderyk Skarbek, a pupil of Nicolas Chopin. Fryderyk was the couple's second child and only son; he had an elder sister, Ludwika (1807–1855), and two younger sisters, Izabela (1811–1881) and Emilia (1812–1827). Nicolas was devoted to his adopted homeland, and insisted on the use of the Polish language in the household.

In October 1810, six months after Fryderyk's birth, the family moved to Warsaw, where his father acquired a post teaching French at the Warsaw Lyceum, then housed in the Saxon Palace. Fryderyk lived with his family in the Palace grounds. The father played the flute and violin; the mother played the piano and gave lessons to boys in the boarding house that the Chopins kept. Chopin was of slight build, and even in early childhood was prone to illnesses.

Fryderyk may have had some piano instruction from his mother, but his first professional music tutor, from 1816 to 1821, was the Czech pianist Wojciech Żywny. His elder sister Ludwika also took lessons from Żywny, and occasionally played duets with her brother. It quickly became apparent that he was a child prodigy. By the age of seven Fryderyk had begun giving public concerts, and in 1817 he composed two polonaises, in G minor and B-flat major. His next work, a polonaise in A-flat major of 1821, dedicated to Żywny, is his earliest surviving musical manuscript.

In 1817 the Saxon Palace was requisitioned by Warsaw's Russian governor for military use, and the Warsaw Lyceum was reestablished in the Kazimierz Palace (today the rectorate of Warsaw University). Fryderyk and his family moved to a building, which still survives, adjacent to the Kazimierz Palace. During this period, Fryderyk was sometimes invited to the Belweder Palace as playmate to the son of the ruler of Russian Poland, Grand Duke Constantine Pavlovich of Russia; he played the piano for Constantine Pavlovich and composed a march for him. Julian Ursyn Niemcewicz, in his dramatic eclogue, ""Nasze Przebiegi"" ("Our Discourses", 1818), attested to "little Chopin's" popularity.

From September 1823 to 1826, Chopin attended the Warsaw Lyceum, where he received organ lessons from the Czech musician Wilhelm Würfel during his first year. In the autumn of 1826 he began a three-year course under the Silesian composer Józef Elsner at the Warsaw Conservatory, studying music theory, figured bass, and composition. Throughout this period he continued to compose and to give recitals in concerts and salons in Warsaw. He was engaged by the inventors of the "aeolomelodicon" (a combination of piano and mechanical organ), and on this instrument in May 1825 he performed his own improvisation and part of a concerto by Moscheles. The success of this concert led to an invitation to give a recital on a similar instrument (the "aeolopantaleon") before Tsar Alexander I, who was visiting Warsaw; the Tsar presented him with a diamond ring. At a subsequent aeolopantaleon concert on 10 June 1825, Chopin performed his Rondo Op. 1. This was the first of his works to be commercially published and earned him his first mention in the foreign press, when the Leipzig "Allgemeine Musikalische Zeitung" praised his "wealth of musical ideas".

During 1824–28 Chopin spent his vacations away from Warsaw, at a number of locales. In 1824 and 1825, at Szafarnia, he was a guest of Dominik Dziewanowski, the father of a schoolmate. Here for the first time he encountered Polish rural folk music. His letters home from Szafarnia (to which he gave the title "The Szafarnia Courier"), written in a very modern and lively Polish, amused his family with their spoofing of the Warsaw newspapers and demonstrated the youngster's literary gift.

In 1827, soon after the death of Chopin's youngest sister Emilia, the family moved from the Warsaw University building, adjacent to the Kazimierz Palace, to lodgings just across the street from the university, in the south annexe of the Krasiński Palace on Krakowskie Przedmieście, where Chopin lived until he left Warsaw in 1830. Here his parents continued running their boarding house for male students; the Chopin Family Parlour ("Salonik Chopinów") became a museum in the 20th century. In 1829 the artist Ambroży Mieroszewski executed a set of portraits of Chopin family members, including the first known portrait of the composer.

Four boarders at his parents' apartments became Chopin's intimates: Tytus Woyciechowski, Jan Nepomucen Białobłocki, Jan Matuszyński and Julian Fontana; the latter two would become part of his Paris milieu. He was friendly with members of Warsaw's young artistic and intellectual world, including Fontana, Józef Bohdan Zaleski and Stefan Witwicki. He was also attracted to the singing student Konstancja Gładkowska. In letters to Woyciechowski, he indicated which of his works, and even which of their passages, were influenced by his fascination with her; his letter of 15 May 1830 revealed that the slow movement ("Larghetto") of his Piano Concerto No. 1 (in E minor) was secretly dedicated to her – "It should be like dreaming in beautiful springtime – by moonlight." His final Conservatory report (July 1829) read: "Chopin F., third-year student, exceptional talent, musical genius."

In September 1828 Chopin, while still a student, visited Berlin with a family friend, zoologist Feliks Jarocki, enjoying operas directed by Gaspare Spontini and attending concerts by Carl Friedrich Zelter, Felix Mendelssohn and other celebrities. On an 1829 return trip to Berlin, he was a guest of Prince Antoni Radziwiłł, governor of the Grand Duchy of Posen—himself an accomplished composer and aspiring cellist. For the prince and his pianist daughter Wanda, he composed his Introduction and Polonaise brillante in C major for cello and piano, Op. 3.

Back in Warsaw that year, Chopin heard Niccolò Paganini play the violin, and composed a set of variations, "Souvenir de Paganini". It may have been this experience which encouraged him to commence writing his first Études, (1829–32), exploring the capacities of his own instrument. On 11 August, three weeks after completing his studies at the Warsaw Conservatory, he made his debut in Vienna. He gave two piano concerts and received many favourable reviews—in addition to some commenting (in Chopin's own words) that he was "too delicate for those accustomed to the piano-bashing of local artists". In one of these concerts, he premiered his Variations on "Là ci darem la mano", Op. 2 (variations on a duet from Mozart's opera "Don Giovanni") for piano and orchestra. He returned to Warsaw in September 1829, where he premiered his Piano Concerto No. 2 in F minor, Op. 21 on 17 March 1830.

Chopin's successes as a composer and performer opened the door to western Europe for him, and on 2 November 1830, he set out, in the words of Zdzisław Jachimecki, "into the wide world, with no very clearly defined aim, forever." With Woyciechowski, he headed for Austria again, intending to go on to Italy. Later that month, in Warsaw, the November 1830 Uprising broke out, and Woyciechowski returned to Poland to enlist. Chopin, now alone in Vienna, was nostalgic for his homeland, and wrote to a friend, "I curse the moment of my departure." When in September 1831 he learned, while travelling from Vienna to Paris, that the uprising had been crushed, he expressed his anguish in the pages of his private journal: "Oh God! ... You are there, and yet you do not take vengeance!" Jachimecki ascribes to these events the composer's maturing "into an inspired national bard who intuited the past, present and future of his native Poland."

When he left Warsaw in late 1830, Chopin had intended to go to Italy, but violent unrest there made that a dangerous destination. His next choice was Paris; difficulties obtaining a visa from Russian authorities resulted in him getting a transit permission from the French. In later years he would quote the passport's endorsement "Passeport en passant par Paris à Londres" ("In transit to London via Paris"), joking that he was in the city "only in passing."

Chopin arrived in Paris in late September 1831; he would never return to Poland, thus becoming one of many expatriates of the Polish Great Emigration. In France he used the French versions of his given names, and after receiving French citizenship in 1835, he travelled on a French passport. However, Chopin remained close to his fellow Poles in exile as friends and confidants and he never felt fully comfortable speaking French. Chopin's biographer Adam Zamoyski writes that he never considered himself to be French, despite his father's French origins, and always saw himself as a Pole.

In Paris, Chopin encountered artists and other distinguished figures, and found many opportunities to exercise his talents and achieve celebrity. During his years in Paris he was to become acquainted with, among many others, Hector Berlioz, Franz Liszt, Ferdinand Hiller, Heinrich Heine, Eugène Delacroix, and Alfred de Vigny. Chopin was also acquainted with the poet Adam Mickiewicz, principal of the Polish Literary Society, some of whose verses he set as songs.

Two Polish friends in Paris were also to play important roles in Chopin's life there. His fellow student at the Warsaw Conservatory, Julian Fontana, had originally tried unsuccessfully to establish himself in England; Fontana was to become, in the words of Michałowski and Samson, Chopin's "general factotum and copyist". Albert Grzymała, who in Paris became a wealthy financier and society figure, often acted as Chopin's adviser and "gradually began to fill the role of elder brother in [his] life."

At the end of 1831, Chopin received the first major endorsement from an outstanding contemporary when Robert Schumann, reviewing the Op. 2 Variations in the "Allgemeine musikalische Zeitung" (his first published article on music), declared: "Hats off, gentlemen! A genius." On 25 February 1832 Chopin gave a debut Paris concert in the "salons de MM Pleyel“ 9 rue Cadet which drew universal admiration. The critic François-Joseph Fétis wrote in the "Revue et gazette musicale": "Here is a young man who ... taking no model, has found, if not a complete renewal of piano music, ... an abundance of original ideas of a kind to be found nowhere else ..." After this concert, Chopin realized that his essentially intimate keyboard technique was not optimal for large concert spaces. Later that year he was introduced to the wealthy Rothschild banking family, whose patronage also opened doors for him to other private salons (social gatherings of the aristocracy and artistic and literary elite). By the end of 1832 Chopin had established himself among the Parisian musical elite, and had earned the respect of his peers such as Hiller, Liszt, and Berlioz. He no longer depended financially upon his father, and in the winter of 1832 he began earning a handsome income from publishing his works and teaching piano to affluent students from all over Europe. This freed him from the strains of public concert-giving, which he disliked.
Chopin seldom performed publicly in Paris. In later years he generally gave a single annual concert at the Salle Pleyel, a venue that seated three hundred. He played more frequently at salons, but preferred playing at his own Paris apartment for small groups of friends. The musicologist Arthur Hedley has observed that "As a pianist Chopin was unique in acquiring a reputation of the highest order on the basis of a minimum of public appearances—few more than thirty in the course of his lifetime." The list of musicians who took part in some of his concerts provides an indication of the richness of Parisian artistic life during this period. Examples include a concert on 23 March 1833, in which Chopin, Liszt and Hiller performed (on pianos) a concerto by J.S. Bach for three keyboards; and, on 3 March 1838, a concert in which Chopin, his pupil Adolphe Gutmann, Charles-Valentin Alkan, and Alkan's teacher Joseph Zimmermann performed Alkan's arrangement, for eight hands, of two movements from Beethoven's 7th symphony. Chopin was also involved in the composition of Liszt's "Hexameron"; he wrote the sixth (and final) variation on Bellini's theme. Chopin's music soon found success with publishers, and in 1833 he contracted with Maurice Schlesinger, who arranged for it to be published not only in France but, through his family connections, also in Germany and England.

In the spring of 1834, Chopin attended the Lower Rhenish Music Festival in Aix-la-Chapelle with Hiller, and it was there that Chopin met Felix Mendelssohn. After the festival, the three visited Düsseldorf, where Mendelssohn had been appointed musical director. They spent what Mendelssohn described as "a very agreeable day", playing and discussing music at his piano, and met Friedrich Wilhelm Schadow, director of the Academy of Art, and some of his eminent pupils such as Lessing, Bendemann, Hildebrandt and Sohn. In 1835 Chopin went to Carlsbad, where he spent time with his parents; it was the last time he would see them. On his way back to Paris, he met old friends from Warsaw, the Wodzińskis. He had made the acquaintance of their daughter Maria in Poland five years earlier, when she was eleven. This meeting prompted him to stay for two weeks in Dresden, when he had previously intended to return to Paris via Leipzig. The sixteen-year-old girl's portrait of the composer is considered, along with Delacroix's, as among Chopin's best likenesses. In October he finally reached Leipzig, where he met Schumann, Clara Wieck and Mendelssohn, who organised for him a performance of his own oratorio "St. Paul", and who considered him "a perfect musician". In July 1836 Chopin travelled to Marienbad and Dresden to be with the Wodziński family, and in September he proposed to Maria, whose mother Countess Wodzińska approved in principle. Chopin went on to Leipzig, where he presented Schumann with his G minor Ballade. At the end of 1836 he sent Maria an album in which his sister Ludwika had inscribed seven of his songs, and his 1835 Nocturne in C-sharp minor, Op. 27, No. 1. The anodyne thanks he received from Maria proved to be the last letter he was to have from her.

Although it is not known exactly when Chopin first met Franz Liszt after arriving in Paris, on 12 December 1831 he mentioned in a letter to his friend Woyciechowski that "I have met Rossini, Cherubini, Baillot, etc.—also Kalkbrenner. You would not believe how curious I was about Herz, Liszt, Hiller, etc." Liszt was in attendance at Chopin's Parisian debut on 26 February 1832 at the Salle Pleyel, which led him to remark: "The most vigorous applause seemed not to suffice to our enthusiasm in the presence of this talented musician, who revealed a new phase of poetic sentiment combined with such happy innovation in the form of his art."

The two became friends, and for many years lived in close proximity in Paris, Chopin at 38 Rue de la Chaussée-d'Antin, and Liszt at the Hôtel de France on the Rue Lafitte, a few blocks away. They performed together on seven occasions between 1833 and 1841. The first, on 2 April 1833, was at a benefit concert organized by Hector Berlioz for his bankrupt Shakespearean actress wife Harriet Smithson, during which they played George Onslow's "Sonata in F minor" for piano duet. Later joint appearances included a benefit concert for the Benevolent Association of Polish Ladies in Paris. Their last appearance together in public was for a charity concert conducted for the Beethoven Monument in Bonn, held at the Salle Pleyel and the Paris Conservatory on 25 and 26 April 1841.

Although the two displayed great respect and admiration for each other, their friendship was uneasy and had some qualities of a love-hate relationship. Harold C. Schonberg believes that Chopin displayed a "tinge of jealousy and spite" towards Liszt's virtuosity on the piano, and others have also argued that he had become enchanted with Liszt's theatricality, showmanship and success. Liszt was the dedicatee of Chopin's Op. 10 Études, and his performance of them prompted the composer to write to Hiller, "I should like to rob him of the way he plays my studies." However, Chopin expressed annoyance in 1843 when Liszt performed one of his nocturnes with the addition of numerous intricate embellishments, at which Chopin remarked that he should play the music as written or not play it at all, forcing an apology. Most biographers of Chopin state that after this the two had little to do with each other, although in his letters dated as late as 1848 he still referred to him as "my friend Liszt". Some commentators point to events in the two men's romantic lives which led to a rift between them; there are claims that Liszt had displayed jealousy of his mistress Marie d'Agoult's obsession with Chopin, while others believe that Chopin had become concerned about Liszt's growing relationship with George Sand.

In 1836, at a party hosted by Marie d'Agoult, Chopin met the French author George Sand (born [Amantine] Aurore [Lucile] Dupin). Short (under five feet, or 152 cm), dark, big-eyed and a cigar smoker, she initially repelled Chopin, who remarked, "What an unattractive person "la Sand" is. Is she really a woman?" However, by early 1837 Maria Wodzińska's mother had made it clear to Chopin in correspondence that a marriage with her daughter was unlikely to proceed. It is thought that she was influenced by his poor health and possibly also by rumours about his associations with women such as d'Agoult and Sand. Chopin finally placed the letters from Maria and her mother in a package on which he wrote, in Polish, "My tragedy". Sand, in a letter to Grzymała of June 1838, admitted strong feelings for the composer and debated whether to abandon a current affair in order to begin a relationship with Chopin; she asked Grzymała to assess Chopin's relationship with Maria Wodzińska, without realising that the affair, at least from Maria's side, was over.

In June 1837 Chopin visited London incognito in the company of the piano manufacturer Camille Pleyel where he played at a musical soirée at the house of English piano maker James Broadwood. On his return to Paris, his association with Sand began in earnest and by the end of June 1838 they had become lovers. Sand, who was six years older than the composer, and who had had a series of lovers, wrote at this time: "I must say I was confused and amazed at the effect this little creature had on me ... I have still not recovered from my astonishment, and if I were a proud person I should be feeling humiliated at having been carried away ..." The two spent a miserable winter on Majorca (8 November 1838 to 13 February 1839), where, together with Sand's two children, they had journeyed in the hope of improving the health of Chopin and that of Sand's 15-year-old son Maurice, and also to escape the threats of Sand's former lover Félicien Mallefille. After discovering that the couple were not married, the deeply traditional Catholic people of Majorca became inhospitable, making accommodation difficult to find. This compelled the group to take lodgings in a former Carthusian monastery in Valldemossa, which gave little shelter from the cold winter weather.

On 3 December 1838, Chopin complained about his bad health and the incompetence of the doctors in Majorca: "Three doctors have visited me ... The first said I was dead; the second said I was dying; and the third said I was about to die." He also had problems having his Pleyel piano sent to him, having to rely in the meantime on a piano made in Palma by Juan Bauza. The Pleyel piano finally arrived from Paris in December, just shortly before Chopin and Sand left the island. Chopin wrote to Pleyel in January 1839: "I am sending you my Preludes [(Op. 28)]. I finished them on your little piano, which arrived in the best possible condition in spite of the sea, the bad weather and the Palma customs." Chopin was also able to undertake work while in Majorca on his Ballade No. 2, Op. 38; two Polonaises, Op. 40; and the Scherzo No. 3, Op. 39.

Although this period had been productive, the bad weather had such a detrimental effect on Chopin's health that Sand determined to leave the island. To avoid further customs duties, Sand sold the piano to a local French couple, the Canuts. The group traveled first to Barcelona, then to Marseilles, where they stayed for a few months while Chopin convalesced. While in Marseilles Chopin made a rare appearance at the organ during a requiem mass for the tenor Adolphe Nourrit on 24 April 1839, playing a transcription of Franz Schubert's "lied" "Die Gestirne" (D. 444). In May 1839 they headed for the summer to Sand's estate at Nohant, where they spent most summers until 1846. In autumn they returned to Paris, where Chopin's apartment at 5 rue Tronchet was close to Sand's rented accommodation at the rue Pigalle. He frequently visited Sand in the evenings, but both retained some independence. In 1842 he and Sand moved to the Square d'Orléans, living in adjacent buildings.

On 26 July 1840 Chopin and Sand were present at the dress rehearsal of Berlioz's "Grande symphonie funèbre et triomphale", composed to commemorate the tenth anniversary of the July Revolution. Chopin was reportedly unimpressed with the composition.

During the summers at Nohant, particularly in the years 1839–43, Chopin found quiet, productive days during which he composed many works, including his Polonaise in A-flat major, Op. 53. Among the visitors to Nohant were Delacroix and the mezzo-soprano Pauline Viardot, whom Chopin had advised on piano technique and composition. Delacroix gives an account of staying at Nohant in a letter of 7 June 1842:
The hosts could not be more pleasant in entertaining me. When we are not all together at dinner, lunch, playing billiards, or walking, each of us stays in his room, reading or lounging around on a couch. Sometimes, through the window which opens on the garden, a gust of music wafts up from Chopin at work. All this mingles with the songs of nightingales and the fragrance of roses.

From 1842 onward, Chopin showed signs of serious illness. After a solo recital in Paris on 21 February 1842, he wrote to Grzymała: "I have to lie in bed all day long, my mouth and tonsils are aching so much." He was forced by illness to decline a written invitation from Alkan to participate in a repeat performance of the Beethoven Seventh Symphony arrangement at Érard's on 1 March 1843. Late in 1844, Charles Hallé visited Chopin and found him "hardly able to move, bent like a half-opened penknife and evidently in great pain", although his spirits returned when he started to play the piano for his visitor. Chopin's health continued to deteriorate, particularly from this time onwards. Modern research suggests that apart from any other illnesses, he may also have suffered from temporal lobe epilepsy.

Chopin's relations with Sand were soured in 1846 by problems involving her daughter Solange and Solange's fiancé, the young fortune-hunting sculptor Auguste Clésinger. The composer frequently took Solange's side in quarrels with her mother; he also faced jealousy from Sand's son Maurice. Chopin was utterly indifferent to Sand's radical political pursuits, while Sand looked on his society friends with disdain. As the composer's illness progressed, Sand had become less of a lover and more of a nurse to Chopin, whom she called her "third child". In letters to third parties, she vented her impatience, referring to him as a "child," a "little angel", a "sufferer" and a "beloved little corpse." In 1847 Sand published her novel "Lucrezia Floriani", whose main characters—a rich actress and a prince in weak health—could be interpreted as Sand and Chopin; the story was uncomplimentary to Chopin, who could not have missed the allusions as he helped Sand correct the printer's galleys. In 1847 he did not visit Nohant, and he quietly ended their ten-year relationship following an angry correspondence which, in Sand's words, made "a strange conclusion to nine years of exclusive friendship." The two would never meet again.

Chopin's output as a composer throughout this period declined in quantity year by year. Whereas in 1841 he had written a dozen works, only six were written in 1842 and six shorter pieces in 1843. In 1844 he wrote only the Op. 58 sonata. 1845 saw the completion of three mazurkas (Op. 59). Although these works were more refined than many of his earlier compositions, Zamoyski concludes that "his powers of concentration were failing and his inspiration was beset by anguish, both emotional and intellectual."

Chopin's public popularity as a virtuoso began to wane, as did the number of his pupils, and this, together with the political strife and instability of the time, caused him to struggle financially. In February 1848, with the cellist Auguste Franchomme, he gave his last Paris concert, which included three movements of the Cello Sonata Op. 65.
In April, during the Revolution of 1848 in Paris, he left for London, where he performed at several concerts and at numerous receptions in great houses. This tour was suggested to him by his Scottish pupil Jane Stirling and her elder sister. Stirling also made all the logistical arrangements and provided much of the necessary funding.

In London Chopin took lodgings at Dover Street, where the firm of Broadwood provided him with a grand piano. At his first engagement, on 15 May at Stafford House, the audience included Queen Victoria and Prince Albert. The Prince, who was himself a talented musician, moved close to the keyboard to view Chopin's technique. Broadwood also arranged concerts for him; among those attending were Thackeray and the singer Jenny Lind. Chopin was also sought after for piano lessons, for which he charged the high fee of one guinea per hour, and for private recitals for which the fee was 20 guineas. At a concert on 7 July he shared the platform with Viardot, who sang arrangements of some of his mazurkas to Spanish texts. On 28 August, he played at a concert in Manchester's Concert Hall, sharing the stage with Marietta Alboni and Lorenzo Salvi.

In late summer he was invited by Jane Stirling to visit Scotland, where he stayed at Calder House near Edinburgh and at Johnstone Castle in Renfrewshire, both owned by members of Stirling's family. She clearly had a notion of going beyond mere friendship, and Chopin was obliged to make it clear to her that this could not be so. He wrote at this time to Grzymała "My Scottish ladies are kind, but such bores", and responding to a rumour about his involvement, answered that he was "closer to the grave than the nuptial bed." He gave a public concert in Glasgow on 27 September, and another in Edinburgh, at the Hopetoun Rooms on Queen Street (now Erskine House) on 4 October. In late October 1848, while staying at 10 Warriston Crescent in Edinburgh with the Polish physician Adam Łyszczyński, he wrote out his last will and testament—"a kind of disposition to be made of my stuff in the future, if I should drop dead somewhere", he wrote to Grzymała.

Chopin made his last public appearance on a concert platform at London's Guildhall on 16 November 1848, when, in a final patriotic gesture, he played for the benefit of Polish refugees. By this time he was very seriously ill, weighing under 99 pounds (i.e. less than 45 kg), and his doctors were aware that his sickness was at a terminal stage.

At the end of November, Chopin returned to Paris. He passed the winter in unremitting illness, but gave occasional lessons and was visited by friends, including Delacroix and Franchomme. Occasionally he played, or accompanied the singing of Delfina Potocka, for his friends. During the summer of 1849, his friends found him an apartment in Chaillot, out of the centre of the city, for which the rent was secretly subsidised by an admirer, Princess Obreskoff. Here in June 1849 he was visited by Jenny Lind.

With his health further deteriorating, Chopin desired to have a family member with him. In June 1849 his sister Ludwika came to Paris with her husband and daughter, and in September, supported by a loan from Jane Stirling, he took an apartment at Place Vendôme 12. After 15 October, when his condition took a marked turn for the worse, only a handful of his closest friends remained with him, although Viardot remarked sardonically that "all the grand Parisian ladies considered it "de rigueur" to faint in his room."

Some of his friends provided music at his request; among them, Potocka sang and Franchomme played the cello. Chopin bequeathed his unfinished notes on a piano tuition method, "Projet de méthode", to Alkan for completion. On 17 October, after midnight, the physician leaned over him and asked whether he was suffering greatly. "No longer", he replied. He died a few minutes before two o'clock in the morning. Those present at the deathbed appear to have included his sister Ludwika, Princess Marcelina Czartoryska, Sand's daughter Solange, and his close friend Thomas Albrecht. Later that morning, Solange's husband Clésinger made Chopin's death mask and a cast of his left hand.

The funeral, held at the Church of the Madeleine in Paris, was delayed almost two weeks, until 30 October. Entrance was restricted to ticket holders as many people were expected to attend. Over 3,000 people arrived without invitations, from as far as London, Berlin and Vienna, and were excluded.

Mozart's Requiem was sung at the funeral; the soloists were the soprano Jeanne-Anaïs Castellan, the mezzo-soprano Pauline Viardot, the tenor Alexis Dupont, and the bass Luigi Lablache; Chopin's Preludes No. 4 in E minor and No. 6 in B minor were also played. The organist at the funeral was Louis Lefébure-Wély. The funeral procession to Père Lachaise Cemetery, which included Chopin's sister Ludwika, was led by the aged Prince Adam Czartoryski. The pallbearers included Delacroix, Franchomme, and Camille Pleyel. At the graveside, the "Funeral March" from Chopin's Piano Sonata No. 2 was played, in Reber's instrumentation.

Chopin's tombstone, featuring the muse of music, Euterpe, weeping over a broken lyre, was designed and sculpted by Clésinger. The expenses of the funeral and monument, amounting to 5,000 francs, were covered by Jane Stirling, who also paid for the return of the composer's sister Ludwika to Warsaw. As requested by Chopin, Ludwika took his heart (which had been removed by his doctor Jean Cruveilhier and preserved in alcohol in a vase), back to Poland in 1850. She also took a collection of two hundred letters from Sand to Chopin; after 1851 these were returned to Sand, who destroyed them.
Chopin's disease and the cause of his death have been a matter of discussion. His death certificate gave the cause of death as tuberculosis, and his physician, Cruveilhier, was then the leading French authority on this disease. Other possibilities that have been advanced have included cystic fibrosis, cirrhosis, and alpha 1-antitrypsin deficiency. A visual examination of Chopin's preserved heart (the jar was not opened), conducted in 2014 and first published in the American Journal of Medicine in 2017, suggested that the likely cause of his death was a rare case of pericarditis caused by complications of chronic tuberculosis.

Over 230 works of Chopin survive; some compositions from early childhood have been lost. All his known works involve the piano, and only a few range beyond solo piano music, as either piano concertos, songs or chamber music.

Chopin was educated in the tradition of Beethoven, Haydn, Mozart and Clementi; he used Clementi's piano method with his own students. He was also influenced by Hummel's development of virtuoso, yet Mozartian, piano technique. He cited Bach and Mozart as the two most important composers in shaping his musical outlook. Chopin's early works are in the style of the "brilliant" keyboard pieces of his era as exemplified by the works of Ignaz Moscheles, Friedrich Kalkbrenner, and others. Less direct in the earlier period are the influences of Polish folk music and of Italian opera. Much of what became his typical style of ornamentation (for example, his "fioriture") is taken from singing. His melodic lines were increasingly reminiscent of the modes and features of the music of his native country, such as drones.

Chopin took the new salon genre of the nocturne, invented by the Irish composer John Field, to a deeper level of sophistication. He was the first to write ballades and scherzi as individual concert pieces. He essentially established a new genre with his own set of free-standing preludes (Op. 28, published 1839). He exploited the poetic potential of the concept of the concert étude, already being developed in the 1820s and 1830s by Liszt, Clementi and Moscheles, in his two sets of studies (Op. 10 published in 1833, Op. 25 in 1837).

Chopin also endowed popular dance forms with a greater range of melody and expression. Chopin's mazurkas, while originating in the traditional Polish dance (the "mazurek"), differed from the traditional variety in that they were written for the concert hall rather than the dance hall; as J. Barrie Jones puts it, "it was Chopin who put the mazurka on the European musical map." The series of seven polonaises published in his lifetime (another nine were published posthumously), beginning with the Op. 26 pair (published 1836), set a new standard for music in the form. His waltzes were also written specifically for the salon recital rather than the ballroom and are frequently at rather faster tempos than their dance-floor equivalents.

Some of Chopin's well-known pieces have acquired descriptive titles, such as the "Revolutionary" Étude (Op. 10, No. 12), and the "Minute Waltz" (Op. 64, No. 1). However, with the exception of his "Funeral March", the composer never named an instrumental work beyond genre and number, leaving all potential extramusical associations to the listener; the names by which many of his pieces are known were invented by others. There is no evidence to suggest that the "Revolutionary" Étude was written with the failed Polish uprising against Russia in mind; it merely appeared at that time. The "Funeral March", the third movement of his Sonata No. 2 (Op. 35), the one case where he did give a title, was written before the rest of the sonata, but no specific event or death is known to have inspired it.

The last opus number that Chopin himself used was 65, allocated to the Cello Sonata in G minor. He expressed a deathbed wish that all his unpublished manuscripts be destroyed. At the request of the composer's mother and sisters, however, his musical executor Julian Fontana selected 23 unpublished piano pieces and grouped them into eight further opus numbers (Opp. 66–73), published in 1855. In 1857, 17 Polish songs that Chopin wrote at various stages of his life were collected and published as Op. 74, though their order within the opus did not reflect the order of composition.

Works published since 1857 have received alternative catalogue designations instead of opus numbers. The present standard musicological reference for Chopin's works is the Kobylańska Catalogue (usually represented by the initials 'KK'), named for its compiler, the Polish musicologist Krystyna Kobylańska.

Chopin's original publishers included Maurice Schlesinger and Camille Pleyel. His works soon began to appear in popular 19th-century piano anthologies. The first collected edition was by Breitkopf & Härtel (1878–1902). Among modern scholarly editions of Chopin's works are the version under the name of Paderewski published between 1937 and 1966 and the more recent Polish "National Edition", edited by Jan Ekier, both of which contain detailed explanations and discussions regarding choices and sources.

Chopin published his music in France, England and the German states due to the copyright laws of the time. As such there are often three different kinds of ‘first editions’. Each edition is different from the other, as Chopin edited them separately and at times he did some revision to the music while editing it. Furthermore, Chopin provided his publishers with varying sources, including autographs, annotated proofsheets and scribal copies. Only recently have these differences gained greater recognition.

Improvisation stands at the centre of Chopin's creative processes. However, this does not imply impulsive rambling: Nicholas Temperley writes that "improvisation is designed for an audience, and its starting-point is that audience's expectations, which include the current conventions of musical form." The works for piano and orchestra, including the two concertos, are held by Temperley to be "merely vehicles for brilliant piano playing ... formally longwinded and extremely conservative". After the piano concertos (which are both early, dating from 1830), Chopin made no attempts at large-scale multi-movement forms, save for his late sonatas for piano and for cello; "instead he achieved near-perfection in pieces of simple general design but subtle and complex cell-structure." Rosen suggests that an important aspect of Chopin's individuality is his flexible handling of the four-bar phrase as a structural unit.

J. Barrie Jones suggests that "amongst the works that Chopin intended for concert use, the four ballades and four scherzos stand supreme", and adds that "the Barcarolle Op. 60 stands apart as an example of Chopin's rich harmonic palette coupled with an Italianate warmth of melody." Temperley opines that these works, which contain "immense variety of mood, thematic material and structural detail", are based on an extended "departure and return" form; "the more the middle section is extended, and the further it departs in key, mood and theme, from the opening idea, the more important and dramatic is the reprise when it at last comes."

Chopin's mazurkas and waltzes are all in straightforward ternary or episodic form, sometimes with a coda. The mazurkas often show more folk features than many of his other works, sometimes including modal scales and harmonies and the use of drone basses. However, some also show unusual sophistication, for example Op. 63 No. 3, which includes a canon at one beat's distance, a great rarity in music.

Chopin's polonaises show a marked advance on those of his Polish predecessors in the form (who included his teachers Żywny and Elsner). As with the traditional polonaise, Chopin's works are in triple time and typically display a martial rhythm in their melodies, accompaniments and cadences. Unlike most of their precursors, they also require a formidable playing technique.

The 21 nocturnes are more structured, and of greater emotional depth, than those of Field (whom Chopin met in 1833). Many of the Chopin nocturnes have middle sections marked by agitated expression (and often making very difficult demands on the performer) which heightens their dramatic character.

Chopin's études are largely in straightforward ternary form. He used them to teach his own technique of piano playing—for instance playing double thirds (Op. 25, No. 6), playing in octaves (Op. 25, No. 10), and playing repeated notes (Op. 10, No.  7).

The preludes, many of which are very brief (some consisting of simple statements and developments of a single theme or figure), were described by Schumann as "the beginnings of studies". Inspired by J.S. Bach's "The Well-Tempered Clavier", Chopin's preludes move up the circle of fifths (rather than Bach's chromatic scale sequence) to create a prelude in each major and minor tonality. The preludes were perhaps not intended to be played as a group, and may even have been used by him and later pianists as generic preludes to others of his pieces, or even to music by other composers, as Kenneth Hamilton suggests: he has noted a recording by Ferruccio Busoni of 1922, in which the Prelude Op. 28 No. 7 is followed by the Étude Op. 10 No. 5.

The two mature piano sonatas (No. 2, Op. 35, written in 1839 and No. 3, Op. 58, written in 1844) are in four movements. In Op. 35, Chopin was able to combine within a formal large musical structure many elements of his virtuosic piano technique—"a kind of dialogue between the public pianism of the brilliant style and the German sonata principle". The sonata has been considered as showing the influences of both Bach and Beethoven. The Prelude from Bach's Suite No. 6 in D major for cello (BWV 1012) is quoted; and there are references to two sonatas of Beethoven: the Sonata Opus 111 in C minor, and the Sonata Opus 26 in A flat major, which, like Chopin's Op. 35, has a funeral march as its slow movement. The last movement of Chopin's Op. 35, a brief (75-bar) perpetuum mobile in which the hands play in unmodified octave unison throughout, was found shocking and unmusical by contemporaries, including Schumann. The Op. 58 sonata is closer to the German tradition, including many passages of complex counterpoint, "worthy of Brahms" according to the music historians Kornel Michałowski and Jim Samson.

Chopin's harmonic innovations may have arisen partly from his keyboard improvisation technique. Temperley says that in his works "novel harmonic effects frequently result from the combination of ordinary appoggiaturas or passing notes with melodic figures of accompaniment", and cadences are delayed by the use of chords outside the home key (neapolitan sixths and diminished sevenths), or by sudden shifts to remote keys. Chord progressions sometimes anticipate the shifting tonality of later composers such as Claude Debussy, as does Chopin's use of modal harmony.

In 1841, Léon Escudier wrote of a recital given by Chopin that year, "One may say that Chopin is the creator of a school of piano and a school of composition. In truth, nothing equals the lightness, the sweetness with which the composer preludes on the piano; moreover nothing may be compared to his works full of originality, distinction and grace." Chopin refused to conform to a standard method of playing and believed that there was no set technique for playing well. His style was based extensively on his use of very independent finger technique. In his "Projet de méthode" he wrote: "Everything is a matter of knowing good fingering ... we need no less to use the rest of the hand, the wrist, the forearm and the upper arm." He further stated: "One needs only to study a certain position of the hand in relation to the keys to obtain with ease the most beautiful quality of sound, to know how to play short notes and long notes, and [to attain] unlimited dexterity." The consequences of this approach to technique in Chopin's music include the frequent use of the entire range of the keyboard, passages in double octaves and other chord groupings, swiftly repeated notes, the use of grace notes, and the use of contrasting rhythms (four against three, for example) between the hands.

Jonathan Bellman writes that modern concert performance style—set in the "conservatory" tradition of late 19th- and 20th-century music schools, and suitable for large auditoria or recordings—militates against what is known of Chopin's more intimate performance technique. The composer himself said to a pupil that "concerts are never real music, you have to give up the idea of hearing in them all the most beautiful things of art." Contemporary accounts indicate that in performance, Chopin avoided rigid procedures sometimes incorrectly attributed to him, such as "always crescendo to a high note", but that he was concerned with expressive phrasing, rhythmic consistency and sensitive colouring. Berlioz wrote in 1853 that Chopin "has created a kind of chromatic embroidery ... whose effect is so strange and piquant as to be impossible to describe ... virtually nobody but Chopin himself can play this music and give it this unusual turn". Hiller wrote that "What in the hands of others was elegant embellishment, in his hands became a colourful wreath of flowers."

Chopin's music is frequently played with "rubato", "the practice in performance of disregarding strict time, 'robbing' some note-values for expressive effect". There are differing opinions as to how much, and what type, of "rubato" is appropriate for his works. Charles Rosen comments that "most of the written-out indications of rubato in Chopin are to be found in his mazurkas ... It is probable that Chopin used the older form of rubato so important to Mozart ... [where] the melody note in the right hand is delayed until after the note in the bass ... An allied form of this rubato is the arpeggiation of the chords thereby delaying the melody note; according to Chopin's pupil Karol Mikuli, Chopin was firmly opposed to this practice."

Friederike Müller, a pupil of Chopin, wrote: "[His] playing was always noble and beautiful; his tones sang, whether in full "forte" or softest "piano". He took infinite pains to teach his pupils this "legato", "cantabile" style of playing. His most severe criticism was 'He—or she—does not know how to join two notes together.' He also demanded the strictest adherence to rhythm. He hated all lingering and dragging, misplaced "rubatos", as well as exaggerated "ritardandos" ... and it is precisely in this respect that people make such terrible errors in playing his works."

With his mazurkas and polonaises, Chopin has been credited with introducing to music a new sense of nationalism. Schumann, in his 1836 review of the piano concertos, highlighted the composer's strong feelings for his native Poland, writing that "Now that the Poles are in deep mourning [after the failure of the November Uprising of 1830], their appeal to us artists is even stronger ... If the mighty autocrat in the north [i.e. Nicholas I of Russia] could know that in Chopin's works, in the simple strains of his mazurkas, there lurks a dangerous enemy, he would place a ban on his music. Chopin's works are cannon buried in flowers!" The biography of Chopin published in 1863 under the name of Franz Liszt (but probably written by Carolyne zu Sayn-Wittgenstein) states that Chopin "must be ranked first among the first musicians ... individualizing in themselves the poetic sense of an entire nation."

Some modern commentators have argued against exaggerating Chopin's primacy as a "nationalist" or "patriotic" composer. George Golos refers to earlier "nationalist" composers in Central Europe, including Poland's Michał Kleofas Ogiński and Franciszek Lessel, who utilised polonaise and mazurka forms. Barbara Milewski suggests that Chopin's experience of Polish music came more from "urbanised" Warsaw versions than from folk music, and that attempts (by Jachimecki and others) to demonstrate genuine folk music in his works are without basis. Richard Taruskin impugns Schumann's attitude toward Chopin's works as patronizing and comments that Chopin "felt his Polish patriotism deeply and sincerely" but consciously modelled his works on the tradition of Bach, Beethoven, Schubert, and Field.

A reconciliation of these views is suggested by William Atwood: "Undoubtedly [Chopin's] use of traditional musical forms like the polonaise and mazurka roused nationalistic sentiments and a sense of cohesiveness amongst those Poles scattered across Europe and the New World ... While some sought solace in [them], others found them a source of strength in their continuing struggle for freedom. Although Chopin's music undoubtedly came to him intuitively rather than through any conscious patriotic design, it served all the same to symbolize the will of the Polish people ..."

Jones comments that "Chopin's unique position as a composer, despite the fact that virtually everything he wrote was for the piano, has rarely been questioned." He also notes that Chopin was fortunate to arrive in Paris in 1831—"the artistic environment, the publishers who were willing to print his music, the wealthy and aristocratic who paid what Chopin asked for their lessons"—and these factors, as well as his musical genius, also fuelled his contemporary and later reputation. While his illness and his love-affairs conform to some of the stereotypes of romanticism, the rarity of his public recitals (as opposed to performances at fashionable Paris soirées) led Arthur Hutchings to suggest that "his lack of Byronic flamboyance [and] his aristocratic reclusiveness make him exceptional" among his romantic contemporaries, such as Liszt and Henri Herz.

Chopin's qualities as a pianist and composer were recognized by many of his fellow musicians. Schumann named a piece for him in his suite "Carnaval", and Chopin later dedicated his Ballade No. 2 in F major to Schumann. Elements of Chopin's music can be traced in many of Liszt's later works. Liszt later transcribed for piano six of Chopin's Polish songs. A less fraught friendship was with Alkan, with whom he discussed elements of folk music, and who was deeply affected by Chopin's death.

Two of Chopin's long-standing pupils, Karol Mikuli (1821–1897) and Georges Mathias, were themselves piano teachers and passed on details of his playing to their own students, some of whom (such as Raoul Koczalski) were to make recordings of his music. Other pianists and composers influenced by Chopin's style include Louis Moreau Gottschalk, Édouard Wolff (1816–1880) and Pierre Zimmermann. Debussy dedicated his own 1915 piano Études to the memory of Chopin; he frequently played Chopin's music during his studies at the Paris Conservatoire, and undertook the editing of Chopin's piano music for the publisher Jacques Durand.
Polish composers of the following generation included virtuosi such as Moritz Moszkowski, but, in the opinion of J. Barrie Jones, his "one worthy successor" among his compatriots was Karol Szymanowski (1882–1937). Edvard Grieg, Antonín Dvořák, Isaac Albéniz, Pyotr Ilyich Tchaikovsky and Sergei Rachmaninoff, among others, are regarded by critics as having been influenced by Chopin's use of national modes and idioms. Alexander Scriabin was devoted to the music of Chopin, and his early published works include nineteen mazurkas, as well as numerous études and preludes; his teacher Nikolai Zverev drilled him in Chopin's works to improve his virtuosity as a performer. In the 20th century, composers who paid homage to (or in some cases parodied) the music of Chopin included George Crumb, Bohuslav Martinů, Darius Milhaud,
Igor Stravinsky and Heitor Villa-Lobos.

Chopin's music was used in the 1909 ballet "Chopiniana", choreographed by Michel Fokine and orchestrated by Alexander Glazunov. Sergei Diaghilev commissioned additional orchestrations—from Stravinsky, Anatoly Lyadov, Sergei Taneyev and Nikolai Tcherepnin—for later productions, which used the title "Les Sylphides".

Chopin's music remains very popular and is regularly performed, recorded and broadcast worldwide. The world's oldest monographic music competition, the International Chopin Piano Competition, founded in 1927, is held every five years in Warsaw. The Fryderyk Chopin Institute of Poland lists on its website over eighty societies worldwide devoted to the composer and his music. The Institute site also lists nearly 1,500 performances of Chopin works on YouTube .

The British Library notes that "Chopin's works have been recorded by all the great pianists of the recording era." The earliest recording was an 1895 performance by Paul Pabst of the Nocturne in E major Op. 62 No. 2. The British Library site makes available a number of historic recordings, including some by Alfred Cortot, Ignaz Friedman, Vladimir Horowitz, Benno Moiseiwitsch, Ignacy Jan Paderewski, Arthur Rubinstein, Xaver Scharwenka and many others. A select discography of recordings of Chopin works by pianists representing the various pedagogic traditions stemming from Chopin is given by Methuen-Campbell in his work tracing the lineage and character of those traditions.

Numerous recordings of Chopin's works are available. On the occasion of the composer's bicentenary, the critics of "The New York Times" recommended performances by the following contemporary pianists (among many others): Martha Argerich, Vladimir Ashkenazy, Emanuel Ax, Evgeny Kissin, Murray Perahia, Maurizio Pollini and Krystian Zimerman. The Warsaw Chopin Society organizes the "Grand prix du disque de F. Chopin" for notable Chopin recordings, held every five years.

Chopin has figured extensively in Polish literature, both in serious critical studies of his life and music and in fictional treatments. The earliest manifestation was probably an 1830 sonnet on Chopin by Leon Ulrich. French writers on Chopin (apart from Sand) have included Marcel Proust and André Gide; and he has also featured in works of Gottfried Benn and Boris Pasternak. There are numerous biographies of Chopin in English (see bibliography for some of these).

Possibly the first venture into fictional treatments of Chopin's life was a fanciful operatic version of some of its events. "Chopin" was written by Giacomo Orefice and produced in Milan in 1901. All the music is derived from that of Chopin.

Chopin's life and his relations with George Sand have been fictionalized in numerous films. The 1945 biographical film "A Song to Remember" earned Cornel Wilde an Academy Award nomination as Best Actor for his portrayal of the composer. Other film treatments have included: "La valse de l'adieu" (France, 1928) by Henry Roussel, with Pierre Blanchar as Chopin; "Impromptu" (1991), starring Hugh Grant as Chopin; "La note bleue" (1991); and "" (2002).

Chopin's life was covered in a BBC documentary "Chopin – The Women Behind The Music" (2010), in a 1999 documentary by András Schiff and Mischa Scorer, and in a 2010 documentary realised by Angelo Bozzolini and Roberto Prosseda for Italian television.

Notes
Citations
Bibliography

Music scores


</doc>
<doc id="10825" url="https://en.wikipedia.org/wiki?curid=10825" title="Free Democratic Party (Germany)">
Free Democratic Party (Germany)

The Free Democratic Party (, FDP) is a liberal and classical liberal political party in Germany. The FDP is led by Christian Lindner.

The FDP was founded in 1948 by members of former liberal political parties which existed in Germany before World War II, namely the German Democratic Party and the German People's Party. For most of the German Federal Republic's history, it has held the balance of power in the Bundestag. It was a junior coalition partner to the CDU/CSU (1949–1956, 1961–1966, 1982–1998 and 2009–2013) and the Social Democratic Party of Germany (1969–1982). In the 2013 federal election, the FDP failed to win any directly elected seats in the Bundestag and came up short of the 5 percent threshold to qualify for list representation, being left without representation in the Bundestag for the first time in its history. In the 2017 federal election, the FDP regained its representation in the Bundestag, receiving 10.6% of the vote.

The FDP strongly supports human rights, civil liberties and internationalism. The party is traditionally considered centre-right. Since the 1980s, the party has firmly pushed economic liberalism and has aligned itself closely to the promotion of free markets and privatization. It is a member of Liberal International and the Alliance of Liberals and Democrats for Europe (ALDE).

The history of liberal parties in Germany dates back to 1861, when the German Progress Party was founded, being the first political party in the modern sense in Germany. From the establishment of the National Liberal Party in 1867 until the demise of the Weimar Republic in 1933, the liberal-democratic camp was divided into a "national-liberal" and a "left-liberal" line of tradition. After 1918 the national-liberal strain was represented by the German People's Party (DVP), the left-liberal one by the German Democratic Party (DDP, which merged into the German State Party in 1930). Both parties played an important role in government during the Weimar Republic era, but successively lost votes during the rise of the Nazi Party since the late-1920s. After the Nazi seizure of power, both liberal parties agreed to the Enabling Act of 1933 and subsequently dissolved themselves. During the 12 years of Hitler's rule, some former liberals collaborated with the Nazis (e.g. economy minister Hjalmar Schacht), while others resisted actively against Nazism (e.g. the Solf Circle).

Soon after World War II, the Soviet Union pushed for the creation of licensed "anti-fascist" parties in its occupation zone in East Germany. In July 1945, former DDP politicians Wilhelm Külz, Eugen Schiffer and Waldemar Koch called for the establishment of a pan-German liberal party. Their Liberal-Democratic Party (LDP) was soon licensed by the Soviet Military Administration in Germany, under the condition that the new party joined the pro-Soviet "Democratic Bloc".

In September 1945, citizens in Hamburg—including the anti-Nazi resistance circle "Association Free Hamburg"—established the "Party of Free Democrats" (PFD) as a bourgeois left-wing party and the first liberal Party in the Western occupation zones. The German Democratic Party was revived in some states of the Western occupation zones (in the Southwestern states of Württemberg-Baden and Württemberg-Hohenzollern under the name of Democratic People's Party).

Many former members of DDP and DVP however agreed to finally overcome the traditional split of German liberalism into a national-liberal and a left-liberal branch, aiming for the creation of a united liberal party. In October 1945 a liberal coalition party was founded in the state of Bremen under the name of Bremen Democratic People's Party. In January 1946, liberal state parties of the British occupation zone merged into the "Free Democratic Party of the British Zone" (FDP). A similar state party in Hesse, called Liberal-Democratic Party, was licensed by the US military government in January 1946. In the state of Bavaria, a "Free Democratic Party" was founded in May 1946.

In the first post-war state elections in 1946, liberal parties performed well in Württemberg-Baden (16.8%), Bremen (18.3%), Hamburg (18.2%) and Greater Berlin (still undivided; 9.3%). The LDP was especially strong in the October 1946 state elections of the Soviet zone—the last free parliamentary election in East Germany—obtaining an average of 24.6% (highest in Saxony-Anhalt, 29.9%, and Thuringia, 28.5%), thwarting an absolute majority of the Socialist Unity Party of Germany (SED) that was favoured by the Soviet occupation power. This disappointment to the communists however led to a change of electoral laws in the Soviet zone, cutting the autonomy of non-socialist parties like the LDP and forcing it to join the SED-dominated National Front, making it a dependent "bloc party".

The Democratic Party of Germany (DPD) was established in Rothenburg ob der Tauber on 17 March 1947 as a pan-German party of liberals from all four occupation zones. Its leaders were Theodor Heuss (representing the DVP of Württemberg-Baden in the American zone) and Wilhelm Külz (representing the LDP of the Soviet zone). However, the project failed in January 1948 as a result of disputes over Külz's pro-Soviet direction.

The Free Democratic Party was established on 11–12 December 1948 in Heppenheim, in Hesse, as an association of all 13 liberal state parties in the three Western zones of occupation. The proposed name, Liberal Democratic Party (LDP), was rejected by the delegates, who voted 64 to 25 in favour of the name Free Democratic Party (FDP).

The party's first chairman was Theodor Heuss, a member of the Democratic People's Party in Württemberg-Baden; his deputy was Franz Blücher of the FDP in the British zone. The place for the party's foundation was chosen deliberately: it was at the Heppenheim Assembly that the moderate liberals had met in October 1847 before the March Revolution. Some regard the "Heppenheim Assembly", which was held at the Hotel "Halber Mond" on 10 October 1847, as a meeting of leading liberals that was the beginning of the German Revolution of 1848-49.

The FDP was founded on 11 December 1948 through the merger of nine regional liberal parties formed in 1945 from the remnants of the pre-1933 German People's Party (DVP) and the German Democratic Party (DDP), which had been active in the Weimar Republic.

In the first elections to the Bundestag on 14 August 1949, the FDP won a vote share of 11.9 percent (with 12 direct mandates, particularly in Baden-Württemberg and Hesse), and thus obtained 52 of 402 seats. In September of the same year the FDP chairman Theodor Heuss was elected the first President of the Federal Republic of Germany. In his 1954 re-election, he received the best election result to date of a President with 871 of 1018 votes (85.6 percent) of the Federal Assembly. Adenauer was also elected on the proposal of the new German President with an extremely narrow majority as the first Chancellor. The FDP participated with the CDU/CSU and the DP in Adenauer's coalition cabinet: they had three ministers: Franz Blücher (Vice-Chancellor), Thomas Dehler (Justice) and Eberhard Wildermuth (housing).

On the most important economic, social and German national issues, the FDP agreed with their coalition partners, the CDU/CSU. However, the FDP recommended to the bourgeois voters a secular party that refused the religious schools and accused the opposition parties of clericalization. The FDP said they were known also as a consistent representative of the market economy, while the CDU was then dominated nominally from the Ahlen Programme, which allowed a Third Way between capitalism and socialism. Ludwig Erhard, the "father" of the social market economy, had his followers in the early years of the Federal Republic in the Union rather than in the FDP.

The FDP won Hesse's 1950 state election with 31.8 percent, the best result in its history, through appealing to East Germans displaced by the war by including them on their ticket.

Up to the 1950s, several of the FDP's regional organizations were to the right of the CDU/CSU, which initially had ideas of some sort of Christian socialism, and even former office-holders of the Third Reich were courted with national, patriotic values. The FDP voted in parliament at the end of 1950 against the CDU- and SPD- introduced de-nazification process. At their party conference in Munich in 1951 they demanded the release of all "so-called war criminals" and welcomed the establishment of the "Association of German soldiers" of former Wehrmacht and SS members, to advance the integration of the nationalist forces in democracy. The 1953 Naumann-Affair, named after Werner Naumann, identifies old Nazis trying to infiltrate the party, which had many right-wing and nationalist members in Hesse, North Rhine-Westphalia and Lower Saxony. After the British occupation authorities had arrested seven prominent members of the Naumann circle, the FDP federal board installed a commission of inquiry, chaired by Thomas Dehler, which particularly sharply criticized the situation in the North Rhine-Westphalian FDP. In the following years, the right wing lost power, and the extreme right increasingly sought areas of activity outside the FDP. In the 1953 federal election, the FDP received 9.5 percent of the party votes, 10.8 percent of the primary vote (with 14 direct mandates, particularly in Hamburg, Lower Saxony, Hesse, Württemberg and Bavaria) and 48 of 487 seats.

In the second term of the Bundestag, the South German Liberal democrats gained influence in the party. Thomas Dehler, a representative of a more left-liberal course took over as party and parliamentary leader. The former Minister of Justice Dehler, who in 1933 suffered persecution by the Nazis, was known for his rhetorical focus. Generally the various regional associations were independent and translated so different from country to country accents in liberal politics. After the FDP had left in early 1956, the coalition with the CDU in North Rhine-Westphalia and made with SPD and center a new state government, were a total of 16 members of parliament, including the four federal ministers from the FDP and founded the short-lived Free People's Party, which then up was involved to the end of the legislature instead of FDP in the Federal Government. The FDP first took it to the opposition.

Only one of the smaller post-war parties, the FDP survived despite many problems. In 1957 federal elections they still reached 7.7 percent of the vote to 1990 and their last direct mandate with which they had held 41 of 497 seats in the Bundestag. However, they still remained in opposition, because the Union won an absolute majority. In the following example, the FDP sat for a nuclear-free zone in Central Europe.

Even before the election Dehler was assigned as party chairman. At the federal party in Berlin at the end January 1957 relieved him Reinhold Maier. Dehler's role as Group Chairman took over after the election of the national set very Erich Mende. Mende was also chairman of the party.

In the 1961 federal elections, it achieved 12.8 percent nationwide, the best result until then, and the FDP entered a coalition with the CDU again. Although it was committed before the election to continuing to sit in any case in a government together with Adenauer, Chancellor Adenauer was again, however, to withdraw under the proviso, after two years. These events led to the FDP being nicknamed the "Umfallerpartei" ("pushover party").

In the Spiegel Affair, the FDP withdrew their ministers from the federal government. Although the coalition was renewed again under Adenauer in 1962, the FDP withdrew again on the condition in October 1963. This occurred even under the new Chancellor, Ludwig Erhard. This was for Erich Mende turn the occasion to go into the cabinet: he took the rather unimportant Federal Ministry for All-German Affairs.

In the 1965 federal elections the FDP gained 9.5 percent. The coalition with the CDU in 1966 broke on the subject of tax increases and it was followed by a grand coalition between the CDU and the SPD. The opposition also pioneered a course change to: The former foreign policy and the attitude to the eastern territories were discussed. The new chairman elected delegates in 1968 Walter Scheel, a European-oriented liberals, although it came from the national liberal camp, but with Willi Weyer and Hans-Dietrich Genscher led the new center of the party. This center strove to make the FDP coalition support both major parties. Here, the Liberals approached to by their reorientation in East Germany and politics especially of the SPD.

On 21 October 1969 began the period after the election of a Social-Liberal coalition with the SPD and the German Chancellor Willy Brandt. Walter Scheel was he who initiated the foreign policy reversal. Despite a very small majority he and Willy Brandt sat by the controversial New Ostpolitik. This policy was within the FDP quite controversial, especially since after the entry into the Federal Government defeats in state elections in North Rhine-Westphalia, Lower Saxony and Saarland on 14 June 1970 followed. In Hanover and Saarbrücken, the party left the parliament.

After the federal party congress in Bonn, just a week later supported the policy of the party leadership and Scheel had confirmed in office, founded by Siegfried party rights Zoglmann 11 July 1970 a "non-partisan" organization called the National-Liberal action on the Hohensyburgstraße - to fall with the goal of ending the left-liberal course of the party and Scheel. However, this was not. Zoglmann supported in October 1970 a disapproval resolution of opposition to Treasury Secretary Alexander Möller, Erich Mende, Heinz Starke, and did the same. A little later all three declared their withdrawal from the FDP; Mende and Strong joined the CDU, Zoglmann later founded the German Union "(Deutsche Union)", which remained a splinter party.

The foreign policy and the socio-political changes were made in 1971 by the Freiburg theses, which were as Rowohlt Paperback sold more than 100,000 times, on a theoretical basis, the FDP is committed to "social liberalism" and social reforms. Walter Scheel was first foreign minister and vice chancellor, 1974, he was then second-liberal President and paving the way for inner-party the previous interior minister Hans-Dietrich Genscher free.

From 1969 to 1974 the FDP supported the SPD Chancellor Willy Brandt, who was succeeded by Helmut Schmidt. Already by the end of the 70s there did not seem to be enough similarities between the FDP and the SPD to form a new coalition, but the CDU/CSU chancellor candidate of Franz Josef Strauss in 1980 pushed the parties to run together again. The FDP's policies, however, began to drift apart from the SPD's, especially when it came to the economy. Within the SPD, there was strong grassroots opposition to Chancellor Helmut Schmidt's policies on the NATO Double-Track Decision. However, within the FDP, the conflicts and contrasts were always greater.

In the fall of 1982, the FDP tore up its coalition agreement with the SPD and instead threw its support behind the CDU/CSU. On 1 October, the FDP and CDU/CSU were able to oust Schmidt and replace him with CDU party chairman Helmut Kohl as the new Chancellor. The coalition change resulted in severe internal conflicts, and the FDP then lost about 20 percent of its 86,500 members, as reflected in the general election in 1983 by a drop from 10.6 percent to 7.0 percent. The members went mostly to the SPD, the Greens and newly formed splinter parties, such as the left-liberal party Liberal Democrats (LD). The exiting members included the former FDP General Secretary and later EU Commissioner Günter Verheugen. At the party convention in November 1982, the Schleswig-Holstein state chairman Uwe Ronneburger challenged Hans-Dietrich Genscher as party chairman. Ronneburger received 186 of the votes—about 40 percent—and was just narrowly defeated by Genscher.

Young FDP members who did not agree with the politics of the FDP youth organization Young Democrats founded in 1980 the Young Liberals (JuLis). For a time there were two youth organizations side by side, until the JuLis became the new official youth wing of the FDP. The Young Democrats split from the FDP and were left as a party-independent youth organization.

At the time of reunification, the FDP's objective was a special economic zone in the former East Germany, but could not prevail against the CDU / CSU, as this would prevent any loss of votes in the five new federal states in the general election in 1990.

In all federal election campaigns since the 1980s, the party sided with the CDU and CSU, the main conservative parties in Germany. Following German reunification in 1990, the FDP merged with the Association of Free Democrats, a grouping of liberals from East Germany and the Liberal Democratic Party of Germany.

During the political upheavals of 1989/1990 in the GDR new liberal parties emerged, like the FDP East Germany or the German Forum Party. They formed the Liberal Democratic Party, who had previously acted as a block party on the side of the SED and with Manfred Gerlach also the last Council of State of the GDR presented, the Alliance of Free Democrats, (BFD). Within the FDP came in the following years to considerable internal discussions about dealing with the former block party. Even before the reunification of Germany united on a joint congress in Hanover, the West German FDP united with the other parties to form the first all-German party. Both party factions brought the FDP a great, albeit short-lived, increase in membership. In the first all-German Bundestag elections, the CDU/CSU/FDP center-right coalition was confirmed, the FDP received 11.0 percent of the valid votes (79 seats) and won (in Halle (Saale)) the first direct mandate since 1957.

During the 1990s, the FDP won between 6.2 and 11 percent of the vote in Bundestag elections. It last participated in the federal government by representing the junior partner in the government of Chancellor Helmut Kohl of the CDU.

In 1998, the CDU/CSU - FDP coalition lost the federal election, which ended the FDP's nearly 30-year reign in government. In its 2002 campaign the FDP made an exception to its party policy of siding with the CDU/CSU when it adopted equidistance to the CDU and SPD. From 1998 until 2009 the FDP remained in the opposition until it became part of a new center-right coalition government.

In the 2005 general election the party won 9.8 percent of the vote and 61 federal deputies, an unpredicted improvement from prior opinion polls. It is believed that this was partly due to tactical voting by CDU and Christian Social Union of Bavaria (CSU) alliance supporters who hoped for stronger market-oriented economic reforms than the CDU/CSU alliance called for. However, because the CDU did worse than predicted, the FDP and the CDU/CSU alliance were unable to form a coalition government. At other times, for example after the 2002 federal election, a coalition between the FDP and CDU/CSU was impossible primarily because of the weak results of the FDP.

The CDU/CSU parties had achieved the 3rd worst performance in German postwar history with only 35.2 percent of the votes. Therefore, the FDP wasn't able to form a coalition with its preferred partners, the CDU/CSU parties. As a result, the party was considered as a potential member of two other political coalitions, following the election. One possibility was a partnership between the FDP, the Social Democratic Party of Germany (SPD) and the Alliance 90/The Greens, known as a "traffic light coalition", named after the colors of the three parties. This coalition was ruled out, because the FDP considered the Social Democrats and the Greens insufficiently committed to market-oriented economic reform. The other possibility was a CDU-FDP-Green coalition, known as a "Jamaica coalition" because of the colours of the three parties. This coalition wasn't concluded either, since the Greens ruled out participation in any coalition with the CDU/CSU. Instead, the CDU formed a Grand coalition with the SPD, and the FDP entered the opposition. FDP leader Guido Westerwelle became the unofficial leader of the opposition by virtue of the FDP's position as the largest opposition party in the Bundestag.

In the 2009 European parliament elections, the FDP received 11% of the national vote (2,888,084 votes in total) and returned 12 MEPs.

In the national vote on 27 September 2009 the FDP increased its share of the vote by 4.8 percentage points to 14.6%, an all-time record so far. This percentage was enough to offset a decline in the CDU/CSU's vote compared to 2005, to create a CDU-FDP centre-right governing coalition in the Bundestag with a 53% majority of seats. On election night, party leader Westerwelle said his party would work to ensure that civil liberties were respected and that Germany got an "equitable tax system and better education opportunities".

The party also made gains in the two state elections held at the same time, acquiring sufficient seats for a CDU-FDP coalition in the northernmost state, Schleswig-Holstein, and gaining enough votes in left-leaning Brandenburg to clear the 5% hurdle to enter that state's parliament.

However, after reaching its best ever election result in 2009, the FDP's support collapsed. The party’s policy pledges were put on hold by Merkel as the recession of 2009 unfolded and with the onset of the European debt crisis in 2010. By the end of 2010, the party's support had dropped to as low as 5%. The FDP retained their seats in the state elections in North Rhine-Westphalia, which was held six months after the federal election, but out of the seven state elections that have been held since 2009, the FDP have lost all their seats in five of them due to failing to cross the 5% threshold.

Support for the party further eroded amid infighting and an internal rebellion over euro-area bailouts during the debt crisis.

Westerwelle stepped down as party leader in 2011 after the party was wiped out in both Saxony-Anhalt and Rhineland-Palatinate, as well as losing half its seats in Baden-Württemberg. He was replaced on 13 May 2011 by Philipp Rösler. The change in leadership failed to revive the FDP's fortunes, however, and in the next series of state elections, the party lost all its seats in Bremen, Mecklenburg-Vorpommern, and Berlin. In Berlin, the party lost nearly 75% of the support they had had in the previous election.

In March 2012, the FDP lost all their seats in Saarland. However, this was averted in the Schleswig-Holstein state elections, when they achieved 8% of the vote, which was a severe loss of seats but still over the 5% threshold. In the snap elections in North Rhine-Westphalia a week later, the FDP not only crossed the threshold, but also increased its share of the votes to 2 percentage points higher than in the previous state election. This was attributed to the local leadership of Christian Lindner.

The FDP last won a directly elected seat in 1990—the only time it has won a directly elected seat since 1957. The party's inability to win directly elected seats came back to haunt it at the 2013 election, in which it came up just short of the 5% threshold. With no directly elected seats, the FDP was shut out of the Bundestag for the first time since 1949. After the previous chairman Philipp Rösler then resigned, Christian Lindner took over the leadership of the party.

In the 2014 European parliament elections, the FDP received 3.36% of the national vote (986,253 votes in total) and returned 3 MEPs. In the 2014 Brandenburg state election the party experienced a 5.8% down-swing and lost all their representatives in the Brandenburg state parliament. In the 2014 Saxony state election, the party experienced a 5.2% down-swing, again losing all of its seats. In the 2014 Thuringian state election a similar phenomenon was repeated with the party falling below the 5% threshold following a 5.1% drop in popular vote.

The party managed to enter parliament in the 2015 Bremen state election with the party receiving 6.5% of the vote and gaining 6 seats. However, it failed to get into government as a coalition between the Social Democrats and the Greens was created. In the 2016 Mecklenburg-Vorpommern state election the party failed to get into parliament despite increasing its vote share by 0.3%. The party did manage to get into parliament in Baden-Württemberg, gaining 3% of the vote and a total of 12 seats. This represents a five-seat improvement over their previous results. In the 2016 Berlin state election the party gained 4.9% of the vote and 12 seats but still failed to get into government. A red-red-green coalition was instead formed relegating the FDP to the opposition. In the 2016 Rhineland-Palatinate state election, the party managed to enter parliament receiving 6.2% of the vote and 7 seats. It also managed to enter government under a traffic light coalition. In 2016 Saxony-Anhalt state election the party narrowly missed the 5% threshold, receiving 4.9% of the vote and therefore receiving zero seats despite a 1% swing in their favour.

The 2017 North Rhine-Westphalia state election was widely considered a test of the party's future as their chairman Christian Lindner was also leading the party in that state. The party experienced a 4% swing in its favour gaining 6 seats and entering into a coalition with the CDU with a bare majority. In the 2017 Saarland state election the party again failed to gain any seats despite a 1% swing in their favour. The party gained 3 seats and increased its vote share by 3.2% in the 2017 Schleswig-Holstein state election. This success was often credited to their state chairman Wolfgang Kubicki. They also managed to re-enter the government under a Jamaica coalition.

In the federal election of 2017 the party scored 10,7% of votes and re-entered the Bundestag, winning 80 seats.

The FDP is a liberal and classical liberal party, both in the sense of supporting "laissez-faire" and free market economic policies and in the sense of supporting liberal democracy and socially liberal policies emphasizing the minimization of government interference in individual affairs. Scholars of political science identify the FDP as closer to the CDU/CSU than to the Social Democratic Party (SPD) on economic issues, but closer to the SPD and the Greens on issues such as civil liberties, education, defense, and foreign policy.

During the 2017 German federal election, the party called for Germany to adopt an immigration channel using a Canada-style points-based immigration system; spend up to 3% of GDP on defense and international security; phase out the solidarity surcharge tax (which was first levied in 1991 to pay for the costs of absorbing East Germany after German reunification); cut taxes by 30 billion euro (twice the amount of the tax cut proposed by the CDU); and improve road infrastructure by spending 2 billion euro annually for each of the next two decades, to be funded by selling government stakes in Deutsche Bahn, Deutsche Telekom, and Deutsche Post. The FDP also called for the improvement of Germany's digital infrastructure, the establishment of a Ministry of Digital Affairs, and greater investment in education. The party also supports allowing dual citizenship (in contrast to the CDU-CSU, which opposes it), but also supports requiring third-generation immigrants to select a single nationality.

The FDP supports the legalization of cannabis in Germany, and strongly opposes proposals to heighten Internet surveillance.

The FDP supports allowing same-sex marriage in Germany.

The FDP is a pro-European party, favoring greater European integration. In its 2009 campaign manifesto, the FDP pledged support for ratification of the Lisbon Treaty as well as EU reforms aimed at enhancing transparency and democratic responsiveness, reducing bureaucracy, establishing stringent curbs on the EU budget, and fully liberalizing the Single Market.

The party tends to draw its support from professionals and self-employed Germans. It lacks consistent support from a voting bloc, such as the trade union membership that supports the SPD or the church membership that supports the CDU/CSU, and thus has historically only garnered a small group of "Stammwähler" (staunch supporters who consistently vote for the party).

The party's membership has historically been largely male; in the 1995, less than one-third of the party's members were women, and in the 1980s women fewer than one-tenth of the party's national executive committee. By the 1990s, the percentage of women on the FDP's national executive committee rose to 20%.

Below are charts of the results that the FDP has secured in each election to the federal Bundestag. Timelines showing the number of seats and percentage of party list votes won are on the right.





</doc>
<doc id="10826" url="https://en.wikipedia.org/wiki?curid=10826" title="Fax">
Fax

Fax (short for facsimile), sometimes called telecopying or telefax (the latter short for telefacsimile), is the telephonic transmission of scanned printed material (both text and images), normally to a telephone number connected to a printer or other output device. The original document is scanned with a fax machine (or a telecopier), which processes the contents (text or images) as a single fixed graphic image, converting it into a bitmap, and then transmitting it through the telephone system in the form of audio-frequency tones. The receiving fax machine interprets the tones and reconstructs the image, printing a paper copy. Early systems used direct conversions of image darkness to audio tone in a continuous or analog manner. Since the 1980s, most machines modulate the transmitted audio frequencies using a digital representation of the page which is compressed to quickly transmit areas which are all-white or all-black.

Scottish inventor Alexander Bain worked on chemical mechanical fax type devices and in 1846 was able to reproduce graphic signs in laboratory experiments. He received British patent 9745 on May 27, 1843 for his "Electric Printing Telegraph".

Frederick Bakewell made several improvements on Bain's design and demonstrated a telefax machine. The Pantelegraph was invented by the Italian physicist Giovanni Caselli. He introduced the first commercial telefax service between Paris and Lyon in 1865, some 11 years before the invention of the telephone.

In 1880, English inventor Shelford Bidwell constructed the "scanning phototelegraph" that was the first telefax machine to scan any two-dimensional original, not requiring manual plotting or drawing. Around 1900, German physicist Arthur Korn invented the "", widespread in continental Europe especially, since a widely noticed transmission of a wanted-person photograph from Paris to London in 1908, used until the wider distribution of the radiofax. Its main competitors were the "Bélinographe" by Édouard Belin first, then since the 1930s the "Hellschreiber", invented in 1929 by German inventor Rudolf Hell, a pioneer in mechanical image scanning and transmission.

The 1888 invention of the telautograph by Elisha Gray marked a further development in fax technology, allowing users to send signatures over long distances, thus allowing the verification of identification or ownership over long distances.

On May 19, 1924, scientists of the AT&T Corporation "by a new process of transmitting pictures by electricity" sent 15 photographs by telephone from Cleveland to New York City, such photos being suitable for newspaper reproduction. Previously, photographs had been sent over the radio using this process.

The Western Union "Deskfax" fax machine, announced in 1948, was a compact machine that fit comfortably on a desktop, using special spark printer paper.

As a designer for the Radio Corporation of America (RCA), in 1924, Richard H. Ranger invented the wireless photoradiogram, or transoceanic radio facsimile, the forerunner of today’s "fax" machines. A photograph of President Calvin Coolidge sent from New York to London on November 29, 1924, became the first photo picture reproduced by transoceanic radio facsimile. Commercial use of Ranger’s product began two years later. Also in 1924, Herbert E. Ives of AT&T transmitted and reconstructed the first color facsimile, a natural-color photograph of silent film star Rudolph Valentino in period costume, using red, green and blue color separations.

Beginning in the late 1930s, the Finch Facsimile system was used to transmit a "radio newspaper" to private homes via commercial AM radio stations and ordinary radio receivers equipped with Finch's printer, which used thermal paper. Sensing a new and potentially golden opportunity, competitors soon entered the field, but the printer and special paper were expensive luxuries, AM radio transmission was very slow and vulnerable to static, and the newspaper was too small. After more than ten years of repeated attempts by Finch and others to establish such a service as a viable business, the public, apparently quite content with its cheaper and much more substantial home-delivered daily newspapers, and with conventional spoken radio bulletins to provide any "hot" news, still showed only a passing curiosity about the new medium.

By the late 1940s, radiofax receivers were sufficiently miniaturized to be fitted beneath the dashboard of Western Union's "Telecar" telegram delivery vehicles.

In the 1960s, the United States Army transmitted the first photograph via satellite facsimile to Puerto Rico from the Deal Test Site using the Courier satellite.

Radio fax is still in limited use today for transmitting weather charts and information to ships at sea. Also, it is also widely used within the medical field to transmit confidential patient information. 

In 1964, Xerox Corporation introduced (and patented) what many consider to be the first commercialized version of the modern fax machine, under the name (LDX) or Long Distance Xerography. This model was superseded two years later with a unit that would truly set the standard for fax machines for years to come. Up until this point facsimile machines were very expensive and hard to operate. In 1966, Xerox released the Magnafax Telecopiers, a smaller, 46-pound facsimile machine. This unit was far easier to operate and could be connected to any standard telephone line. This machine was capable of transmitting a letter-sized document in about six minutes. The first sub-minute, digital fax machine was developed by Dacom, which built on digital data compression technology originally developed at Lockheed for satellite communication.

By the late 1970s, many companies around the world (especially Japanese firms) had entered the fax market. Very shortly after this, a new wave of more compact, faster and efficient fax machines would hit the market. Xerox continued to refine the fax machine for years after their ground-breaking first machine. In later years it would be combined with copier equipment to create the hybrid machines we have today that copy, scan and fax. Some of the lesser known capabilities of the Xerox fax technologies included their Ethernet enabled Fax Services on their 8000 workstations in the early 1980s.

Prior to the introduction of the ubiquitous fax machine, one of the first being the Exxon Qwip in the mid-1970s, facsimile machines worked by optical scanning of a document or drawing spinning on a drum. The reflected light, varying in intensity according to the light and dark areas of the document, was focused on a photocell so that the current in a circuit varied with the amount of light. This current was used to control a tone generator (a modulator), the current determining the frequency of the tone produced. This audio tone was then transmitted using an acoustic coupler (a speaker, in this case) attached to the microphone of a common telephone handset. At the receiving end, a handset’s speaker was attached to an acoustic coupler (a microphone), and a demodulator converted the varying tone into a variable current that controlled the mechanical movement of a pen or pencil to reproduce the image on a blank sheet of paper on an identical drum rotating at the same rate.

In 1985, Hank Magnuski, founder of GammaLink, produced the first computer fax board, called GammaFax. Such boards could provide voice telephony via Analog Expansion Bus.

Although businesses usually maintain some kind of fax capability, the technology has faced increasing competition from Internet-based alternatives. In some countries, because electronic signatures on contracts are not yet recognized by law, while faxed contracts with copies of signatures are, fax machines enjoy continuing support in business. In Japan, faxes are still used extensively for cultural and graphemic reasons and are available for sending to both domestic and international recipients from over 81% of all convenience stores nationwide. Convenience-store fax machines commonly print the slightly re-sized content of the sent fax in the electronic confirmation-slip, in A4 paper size.

In many corporate environments, freestanding fax machines have been replaced by fax servers and other computerized systems capable of receiving and storing incoming faxes electronically, and then routing them to users on paper or via an email (which may be secured). Such systems have the advantage of reducing costs by eliminating unnecessary printouts and reducing the number of inbound analog phone lines needed by an office.

The once ubiquitous fax machine has also begun to disappear from the small office and home office environments. Remotely hosted fax-server services are widely available from VoIP and e-mail providers allowing users to send and receive faxes using their existing e-mail accounts without the need for any hardware or dedicated fax lines. Personal computers have also long been able to handle incoming and outgoing faxes using analog modems or ISDN, eliminating the need for a stand-alone fax machine. These solutions are often ideally suited for users who only very occasionally need to use fax services. In July 2017 the United Kingdom's National Health Service was said to be the world's largest purchaser of fax machines because the digital revolution has largely bypassed it. In June 2018 the Labour Party said that the NHS had at least 11,620 fax machines in operation and in December the Department of Health and Social Care said that no more fax machines could be bought from 2019 and that the existing ones must be replaced by secure email by 31 March 2020.

Leeds Teaching Hospitals NHS Trust, generally viewed as digitally advanced in the NHS, was engaged in a process of removing its fax machines in early 2019. This involved quite a lot of e-fax solutions because of the need to communicate with pharmacies and nursing homes which may not have access to the NHS email system and may need something in their paper records.

In 2018 two-thirds of Canadian doctors reported that they primarily used fax machines to communicate with other doctors. Faxes are seen, probably mistakenly, as safer and more secure and electronic systems are often unable to communicate with each other.

There are several indicators of fax capabilities: group, class, data transmission rate, and conformance with ITU-T (formerly CCITT) recommendations. Since the 1968 Carterphone decision, most fax machines have been designed to connect to standard PSTN lines and telephone numbers.

Group 1 and 2 faxes are sent in the same manner as a frame of analog television, with each scanned line transmitted as a continuous analog signal. Horizontal resolution depended upon the quality of the scanner, transmission line, and the printer. Analog fax machines are obsolete and no longer manufactured. ITU-T Recommendations T.2 and T.3 were withdrawn as obsolete in July 1996.


A major breakthrough in the development of the modern facsimile system was the result of digital technology, where the analog signal from scanners was digitized and then compressed, resulting in the ability to transmit high rates of data across standard phone lines. The first digital fax machine was the Dacom Rapidfax first sold in late 1960s, which incorporated digital data compression technology developed by Lockheed for transmission of images from satellites.

Group 3 and 4 faxes are digital formats, and take advantage of digital compression methods to greatly reduce transmission times.


Fax Over IP (FoIP) can transmit and receive pre-digitized documents at near realtime speeds using ITU-T recommendation T.38 to send digitised images over an IP network using JPEG compression. T.38 is designed to work with VoIP services and often supported by analog telephone adapters used by legacy fax machines that need to connect through a VoIP service. Scanned documents are limited to the amount of time the user takes to load the document in a scanner and for the device to process a digital file. The resolution can vary from as little as 150 DPI to 9600 DPI or more. This type of faxing is not related to the e-mail to fax service that still uses fax modems at least one way.

Computer modems are often designated by a particular fax class, which indicates how much processing is offloaded from the computer's CPU to the fax modem.


Several different telephone line modulation techniques are used by fax machines. They are negotiated during the fax-modem handshake, and the fax devices will use the highest data rate that both fax devices support, usually a minimum of 14.4 kbit/s for Group 3 fax.

Note that "Super Group 3" faxes use V.34bis modulation that allows a data rate of up to 33.6 kbit/s.

As well as specifying the resolution (and allowable physical size) of the image being faxed, the ITU-T T.4 recommendation specifies two compression methods for decreasing the amount of data that needs to be transmitted between the fax machines to transfer the image. The two methods defined in T.4 are:
An additional method is specified in T.6:
Later, other compression techniques were added as options to ITU-T recommendation T.30, such as the more efficient JBIG (T.82, T.85) for bi-level content, and JPEG (T.81), T.43, MRC (T.44), and T.45 for grayscale, palette, and colour content. Fax machines can negotiate at the start of the T.30 session to use the best technique implemented on both sides.

Modified Huffman (MH), specified in T.4 as the one-dimensional coding scheme, is a codebook-based run-length encoding scheme optimised to efficiently compress whitespace. As most faxes consist mostly of white space, this minimises the transmission time of most faxes. Each line scanned is compressed independently of its predecessor and successor.

Modified READ, specified as an optional two-dimensional coding scheme in T.4, encodes the first scanned line using MH. The next line is compared to the first, the differences determined, and then the differences are encoded and transmitted. This is effective as most lines differ little from their predecessor. This is not continued to the end of the fax transmission, but only for a limited number of lines until the process is reset and a new 'first line' encoded with MH is produced. This limited number of lines is to prevent errors propagating throughout the whole fax, as the standard does not provide for error-correction. This is an optional facility, and some fax machines do not use MR in order to minimise the amount of computation required by the machine. The limited number of lines is two for 'Standard' resolution faxes, and four for 'Fine' resolution faxes.

The ITU-T T.6 recommendation adds a further compression type of Modified Modified READ (MMR), which simply allows for a greater number of lines to be coded by MR than in T.4. This is because T.6 makes the assumption that the transmission is over a circuit with a low number of line errors such as digital ISDN. In this case, there is no maximum number of lines for which the differences are encoded.

In 1999, ITU-T recommendation T.30 added JBIG (ITU-T T.82) as another lossless bi-level compression algorithm, or more precisely a "fax profile" subset of JBIG (ITU-T T.85). JBIG-compressed pages result in 20% to 50% faster transmission than MMR-compressed pages, and up to 30-times faster transmission if the page includes halftone images.

JBIG performs adaptive compression, that is both the encoder and decoder collect statistical information about the transmitted image from the pixels transmitted so far, in order to predict the probability for each next pixel being either black or white. For each new pixel, JBIG looks at ten nearby, previously transmitted pixels. It counts, how often in the past the next pixel has been black or white in the same neighborhood, and estimates from that the probability distribution of the next pixel. This is fed into an arithmetic coder, which adds only a small fraction of a bit to the output sequence if the more probable pixel is then encountered.

The ITU-T T.85 "fax profile" constrains some optional features of the full JBIG standard, such that codecs do not have to keep data about more than the last three pixel rows of an image in memory at any time. This allows the streaming of "endless" images, where the height of the image may not be known until the last row is transmitted.

ITU-T T.30 allows fax machines to negotiate one of two options of the T.85 "fax profile":

A proprietary compression scheme employed on Panasonic fax machines is Matsushita Whiteline Skip (MWS). It can be overlaid on the other compression schemes, but is operative only when two Panasonic machines are communicating with one another. This system detects the blank scanned areas between lines of text, and then compresses several blank scan lines into the data space of a single character. (JBIG implements a similar technique called "typical prediction", if header flag TPBON is set to 1.)

Group 3 fax machines transfer one or a few printed or handwritten pages per minute in black-and-white (bitonal) at a resolution of 204×98 (normal) or 204×196 (fine) dots per square inch. The transfer rate is 14.4 kbit/s or higher for modems and some fax machines, but fax machines support speeds beginning with 2400 bit/s and typically operate at 9600 bit/s. The transferred image formats are called ITU-T (formerly CCITT) fax group 3 or 4. Group 3 faxes have the suffix codice_1 and the MIME type image/g3fax.

The most basic fax mode transfers in black and white only. The original page is scanned in a resolution of 1728 pixels/line and 1145 lines/page (for A4). The resulting raw data is compressed using a modified Huffman code optimized for written text, achieving average compression factors of around 20. Typically a page needs 10 s for transmission, instead of about 3 minutes for the same uncompressed raw data of 1728×1145 bits at a speed of 9600 bit/s. The compression method uses a Huffman codebook for run lengths of black and white runs in a single scanned line, and it can also use the fact that two adjacent scanlines are usually quite similar, saving bandwidth by encoding only the differences.

Fax classes denote the way fax programs interact with fax hardware. Available classes include Class 1, Class 2, Class 2.0 and 2.1, and Intel CAS. Many modems support at least class 1 and often either Class 2 or Class 2.0. Which is preferable to use depends on factors such as hardware, software, modem firmware, and expected use.

Fax machines from the 1970s to the 1990s often used direct thermal printers with rolls of thermal paper as their printing technology, but since the mid-1990s there has been a transition towards plain-paper faxes: thermal transfer printers, inkjet printers and laser printers.

One of the advantages of inkjet printing is that inkjets can affordably print in color; therefore, many of the inkjet-based fax machines claim to have color fax capability. There is a standard called ITU-T30e (formally ITU-T Recommendation T.30 Annex E ) for faxing in color; however, it is not widely supported, so many of the color fax machines can only fax in color to machines from the same manufacturer.

Stroke speed in facsimile systems is the rate at which a fixed line perpendicular to the direction of scanning is crossed in one direction by a scanning or recording spot. Stroke speed is usually expressed as a number of strokes per minute. When the fax system scans in both directions, the stroke speed is twice this number. In most conventional 20th century mechanical systems, the stroke speed is equivalent to drum speed.

As a precaution, thermal fax paper is typically not accepted in archives or as documentary evidence in some courts of law unless photocopied. This is because the image-forming coating is eradicable and brittle, and it tends to detach from the medium after a long time in storage.

One popular alternative is to subscribe to an Internet fax service, allowing users to send and receive faxes from their personal computers using an existing email account. No software, fax server or fax machine is needed. Faxes are received as attached TIFF or PDF files, or in proprietary formats that require the use of the service provider's software. Faxes can be sent or retrieved from anywhere at any time that a user can get Internet access. Some services offer secure faxing to comply with stringent HIPAA and Gramm–Leach–Bliley Act requirements to keep medical information and financial information private and secure. Utilizing a fax service provider does not require paper, a dedicated fax line, or consumable resources.

Another alternative to a physical fax machine is to make use of computer software which allows people to send and receive faxes using their own computers, utilizing fax servers and unified messaging. A virtual (email) fax can be printed out and then signed and scanned back to computer before being emailed. Also the sender can attach a digital signature to the document file.

With the surging popularity of mobile phones, virtual fax machines can now be downloaded as applications for Android and iOS. These applications make use of the phone's internal camera to scan fax documents for upload or they can import from various cloud services.





</doc>
<doc id="10827" url="https://en.wikipedia.org/wiki?curid=10827" title="Film crew">
Film crew

A film crew is a group of people, hired by a production company, for the purpose of producing a film or motion picture. The "crew" is distinguished from the "cast" as the "cast" are understood to be the actors who appear in front of the camera or provide voices for characters in the film. The "crew" is also separate from the "producers" as the "producers" are the ones who own a portion of either the film company or the film's intellectual property rights. A film crew is divided into different departments, each of which specializes in a specific aspect of the production. Film crew positions have evolved over the years, spurred by technological change, but many traditional jobs date from the early 20th century and are common across jurisdictions and film-making cultures.

Motion picture projects have three discrete stages: development, production and distribution. Within the production stage there are also three clearly defined sequential phases — pre-production, principal photography and post-production — and many film crew positions are associated with only one or two of the phases. Distinctions are also made between above-the-line personnel (such as the director, the screenwriter and the producers) who begin their involvement during the project's development stage, and the below-the-line "technical" crew involved only with the production stage.

A film director is a person who directs the making of a film. The director most often has the highest authority on a film set. Generally, a film director controls a film's artistic and dramatic aspects and visualizes the screenplay (or script) while guiding the technical crew and actors in the fulfillment of that vision. The director has a key role in choosing the cast members, production design, and the creative aspects of film-making. Under European Union law, the director is viewed as the author of the film.

The film director gives direction to the cast and crew and creates an overall vision through which a film eventually becomes realized, or noticed. Directors need to be able to mediate differences in creative visions and stay within the boundaries of the film's budget. There are many pathways to becoming a film director. Some film directors started as screenwriters, cinematographers, film editors or actors. Other film directors have attended a film school. Directors use different approaches. Some outline a general plotline and let the actors improvise dialogue, while others control every aspect, and demand that the actors and crew follow instructions precisely. Some directors also write their own screenplays or collaborate on screenplays with long-standing writing partners. Some directors edit or appear in their films, or compose the music score for their films.

"Production" is generally not considered a department as such, but rather as a series of functional groups. These include the film's producers and executive producers and production office staff such as the production manager, the production coordinator, and their assistants; the various assistant directors; the accounting staff and sometimes the locations manager and their assistants.

Producer
Executive producer

Line producer
Production assistant

Production manager
Assistant production manager
Unit manager
Production coordinator




Since the turn of the 21st century, several additional professionals are now routinely listed in the production credits on most major motion pictures.






Grips are trained lighting and rigging technicians. Their main responsibility is to work closely with the electrical department to put in the non-electrical components of lighting set-ups required for a shot, such as flags, overheads, and bounces. On the sound stage, they move and adjust major set pieces when something needs to be moved to get a camera into position. In the US and Canada they may belong to the International Alliance of Theatrical Stage Employees.



The art department in a major feature film can often number hundreds of people. Usually it is considered to include several sub-departments: the art department proper, with its art director, set designers and draftsmen; set decoration, under the set decorator; props, under the props master/mistress; construction, headed by the construction coordinator; scenic, headed by the key scenic artist; and special effects.


Within the overall art department is a sub-department, also called the art department—which can be confusing. This consists of the people who design the sets and create the graphic art.







Some actors or actresses have personal makeup artists or hair stylists.


This department oversees the mechanical effects—also called practical or physical effects—that create optical illusions during live-action shooting. It is not to be confused with the Visual effects department, which adds photographic effects during filming to be altered later during video editing in the post-production process.






Visual effects commonly refers to post-production alterations of the film's images. The on set VFX crew works to prepare shots and plates for future visual effects. This may include adding tracking markers, taking and asking for reference plates and helping the Director understand the limitations and ease of certain shots that will effect the future post production. A VFX crew can also work alongside the Special effects department for any on-set optical effects that need physical representation during filming (on camera.)









Previs is the visual planning and design of feature films. They are brought in at an early stage to visualize in complex 3d animatics how scenes in the film might look like. They work with the Director, producers, VFX supervisor and production designer on conceptualising ideas. Once in a fully designed sequence, these animatics are used by stunts, filming units and production to plan and shoot the scenes - helping with planning, resourcing and costing these shots. 

Animation film crews have many of the same roles and departments as live-action films (including directing, production, editing, camera, sound, and so on), but nearly all on-set departments (lighting, electrical, grip, sets, props, costume, hair, makeup, special effects, and stunts) were traditionally replaced with a single animation department made up of various types of animators (character, effects, in-betweeners, cleanup, and so on). In traditional animation, the nature of the medium meant that "everything" was literally flattened into the drawn lines and solid colors that became the characters, making nearly all live-action positions irrelevant. Because animation has traditionally been so labor-intensive and thus expensive, animation films normally have a separate story department in which storyboard artists painstakingly develop scenes to make sure they make sense before they are actually animated.

However, since the turn of the 21st century, modern 3D computer graphics and computer animation have made possible a level of rich detail never seen before. Many animated films now have specialized artists and animators who act as the virtual equivalent of lighting technicians, grips, costume designers, props masters, set decorators, set dressers, and cinematographers. They make artistic decisions strongly similar to those of their live-action counterparts, but implement them in a virtual space that exists only in software rather than on a physical set. There have been major breakthroughs in the simulation of hair since 2005, meaning that hairstylists have been called in since then to consult on a few animation projects.




</doc>
<doc id="10828" url="https://en.wikipedia.org/wiki?curid=10828" title="Fear">
Fear

Fear is a feeling induced by perceived danger or threat that occurs in certain types of organisms, which causes a change in metabolic and organ functions and ultimately a change in behavior, such as fleeing, hiding, or freezing from perceived traumatic events. Fear in human beings may occur in response to a certain stimulus occurring in the present, or in anticipation or expectation of a future threat perceived as a risk to body or life. The fear response arises from the perception of danger leading to confrontation with or escape from/avoiding the threat (also known as the fight-or-flight response), which in extreme cases of fear (horror and terror) can be a freeze response or paralysis.

In humans and animals, fear is modulated by the process of cognition and learning. Thus fear is judged as rational or appropriate and irrational or inappropriate. An irrational fear is called a phobia.

Psychologists such as John B. Watson, Robert Plutchik, and Paul Ekman have suggested that there is only a small set of basic or innate emotions and that fear is one of them. This hypothesized set includes such emotions as acute stress reaction, anger, angst, anxiety, fright, horror, joy, panic, and sadness. Fear is closely related to, but should be distinguished from, the emotion anxiety, which occurs as the result of threats that are perceived to be uncontrollable or unavoidable. The fear response serves survival by engendering appropriate behavioral responses, so it has been preserved throughout evolution. Sociological and organizational research also suggests that individuals’ fears are not solely dependent on their nature but are also shaped by their social relations and culture, which guide their understanding of when and how much fear to feel.

Many physiological changes in the body are associated with fear, summarized as the fight-or-flight response. An inborn response for coping with danger, it works by accelerating the breathing rate (hyperventilation), heart rate, vasoconstriction of the peripheral blood vessels leading to blushing and sanskadania of the central vessels (pooling), increasing muscle tension including the muscles attached to each hair follicle to contract and causing "goose bumps", or more clinically, piloerection (making a cold person warmer or a frightened animal look more impressive), sweating, increased blood glucose (hyperglycemia), increased serum calcium, increase in white blood cells called neutrophilic leukocytes, alertness leading to sleep disturbance and "butterflies in the stomach" (dyspepsia). This primitive mechanism may help an organism survive by either running away or fighting the danger. With the series of physiological changes, the consciousness realizes an emotion of fear.

People develop specific fears as a result of learning. This has been studied in psychology as fear conditioning, beginning with John B. Watson's Little Albert experiment in 1920, which was inspired after observing a child with an irrational fear of dogs. In this study, an 11-month-old boy was conditioned to fear a white rat in the laboratory. The fear became generalized to include other white, furry objects, such as a rabbit, dog, and even a ball of cotton.

Fear can be learned by experiencing or watching a frightening traumatic accident. For example, if a child falls into a well and struggles to get out, he or she may develop a fear of wells, heights (acrophobia), enclosed spaces (claustrophobia), or water (aquaphobia). There are studies looking at areas of the brain that are affected in relation to fear. When looking at these areas (such as the amygdala), it was proposed that a person learns to fear regardless of whether they themselves have experienced trauma, or if they have observed the fear in others. In a study completed by Andreas Olsson, Katherine I. Nearing and Elizabeth A. Phelps, the amygdala were affected both when subjects observed someone else being submitted to an aversive event, knowing that the same treatment awaited themselves, and when subjects were subsequently placed in a fear-provoking situation. This suggests that fear can develop in both conditions, not just simply from personal history.

Fear is affected by cultural and historical context. For example, in the early 20th century, many Americans feared polio, a disease that can lead to paralysis. There are consistent cross-cultural differences in how people respond to fear. Display rules affect how likely people are to show the facial expression of fear and other emotions.

Although many fears are learned, the capacity to fear is part of human nature. Many studies have found that certain fears (e.g. animals, heights) are much more common than others (e.g. flowers, clouds). These fears are also easier to induce in the laboratory. This phenomenon is known as preparedness. Because early humans that were quick to fear dangerous situations were more likely to survive and reproduce, preparedness is theorized to be a genetic effect that is the result of natural selection.

From an evolutionary psychology perspective, different fears may be different adaptations that have been useful in our evolutionary past. They may have developed during different time periods. Some fears, such as fear of heights, may be common to all mammals and developed during the mesozoic period. Other fears, such as fear of snakes, may be common to all simians and developed during the cenozoic time period. Still others, such as fear of mice and insects, may be unique to humans and developed during the paleolithic and neolithic time periods (when mice and insects become important carriers of infectious diseases and harmful for crops and stored foods).

Fear is high only if the observed risk and seriousness both are high, and it is low if risk or seriousness is low.

In a 2005 Gallup Poll (U.S.), a national sample of adolescents between the ages of 13 and 17 were asked what they feared the most. The question was open-ended and participants were able to say whatever they wanted. The top ten fears were, in order: terrorist attacks, spiders, death, failure, war, criminal or gang violence, being alone, the future, and nuclear war.

In an estimate of what people fear the most, book author Bill Tancer analyzed the most frequent online queries that involved the phrase, "fear of..." following the assumption that people tend to seek information on the issues that concern them the most. His top ten list of fears published 2008 consisted of flying, heights, clowns, intimacy, death, rejection, people, snakes, failure, and driving.

According to surveys, some of the most common fears are of demons and ghosts, the existence of evil powers, cockroaches, spiders, snakes, heights, Trypophobia, water, enclosed spaces, tunnels, bridges, needles, social rejection, failure, examinations, and public speaking.

Death anxiety is multidimensional; it covers "fears related to one's own death, the death of others, fear of the unknown after death, fear of obliteration, and fear of the dying process, which includes fear of a slow death and a painful death". Death anxiety is one's uncertainty to dying. However, there is a more severe form of having a fear of death, which is known as Thanatophobia, which is anxiety over death that becomes debilitating or keeps a person from living their life.

The Yale philosopher Shelly Kagan examined fear of death in a 2007 Yale open course by examining the following questions: Is fear of death a reasonable appropriate response? What conditions are required and what are appropriate conditions for feeling fear of death? What is meant by fear, and how much fear is appropriate? According to Kagan for fear in general to make sense, three conditions should be met:
The amount of fear should be appropriate to the size of "the bad". If the three conditions are not met, fear is an inappropriate emotion. He argues, that death does not meet the first two criteria, even if death is a "deprivation of good things" and even if one believes in a painful afterlife. Because death is certain, it also does not meet the third criterion, but he grants that the unpredictability of when one dies "may" be cause to a sense of fear.

In a 2003 study of 167 women and 121 men, aged 65–87, low self-efficacy predicted fear of the unknown after death and fear of dying for women and men better than demographics, social support, and physical health. Fear of death was measured by a "Multidimensional Fear of Death Scale" which included the 8 subscales Fear of Dying, Fear of the Dead, Fear of Being Destroyed, Fear for Significant Others, Fear of the Unknown, Fear of Conscious Death, Fear for the Body After Death, and Fear of Premature Death. In hierarchical multiple regression analysis the most potent predictors of death fears were low "spiritual health efficacy", defined as beliefs relating to one's perceived ability to generate spiritually based faith and inner strength, and low "instrumental efficacy", defined as beliefs relating to one's perceived ability to manage activities of daily living.

Psychologists have tested the hypotheses that fear of death motivates religious commitment, and that assurances about an afterlife alleviate the fear; however, empirical research on this topic has been equivocal. Religiosity can be related to fear of death when the afterlife is portrayed as time of punishment. "Intrinsic religiosity", as opposed to mere "formal religious involvement", has been found to be negatively correlated with death anxiety. In a 1976 study of people of various Christian denominations, those who were most firm in their faith, who attended religious services weekly, were the least afraid of dying. The survey found a negative correlation between fear of death and "religious concern".

In a 2006 study of white, Christian men and women the hypothesis was tested that traditional, church-centered religiousness and de-institutionalized spiritual seeking are ways of approaching fear of death in old age. Both religiousness and spirituality were related to positive psychosocial functioning, but only church-centered religiousness protected subjects against the fear of death.

Fear of the unknown or irrational fear is caused by negative thinking (worry) which arises from anxiety accompanied with a subjective sense of apprehension or dread. Irrational fear shares a common neural pathway with other fears, a pathway that engages the nervous system to mobilize bodily resources in the face of danger or threat. Many people are scared of the "unknown". The irrational fear can branch out to many areas such as the hereafter, the next ten years or even tomorrow. Chronic irrational fear has deleterious effects since the elicitor stimulus is commonly absent or perceived from delusions. Such fear can create comorbidity with the anxiety disorder umbrella. Being scared may cause people to experience anticipatory fear of what may lie ahead rather than planning and evaluating for the same. For example, "continuation of scholarly education" is perceived by many educators as a risk that may cause them fear and stress, and they would rather teach things they've been taught than go and do research. That can lead to habits such as laziness and procrastination. The ambiguity of situations that tend to be uncertain and unpredictable can cause anxiety in addition to other psychological and physical problems in some populations; especially those who engage it constantly, for example, in war-ridden places or in places of conflict, terrorism, abuse, etc. Poor parenting that instills fear can also debilitate a child's psyche development or personality. For example, parents tell their children not to talk to strangers in order to protect them. In school they would be motivated to not show fear in talking with strangers, but to be assertive and also aware of the risks and the environment in which it takes place. Ambiguous and mixed messages like this can affect their self-esteem and self-confidence. Researchers say talking to strangers isn't something to be thwarted but allowed in a parent's presence if required. Developing a sense of equanimity to handle various situations is often advocated as an antidote to irrational fear and as an essential skill by a number of ancient philosophies.

Fear of the unknown (FOTU) "may be a, or possibly the, fundamental fear".

Often laboratory studies with rats are conducted to examine the acquisition and extinction of conditioned fear responses. In 2004, researchers conditioned rats ("Rattus norvegicus") to fear a certain stimulus, through electric shock. The researchers were able to then cause an extinction of this conditioned fear, to a point that no medications or drugs were able to further aid in the extinction process. However the rats did show signs of avoidance learning, not fear, but simply avoiding the area that brought pain to the test rats. The avoidance learning of rats is seen as a conditioned response, and therefore the behavior can be unconditioned, as supported by the earlier research.

Species-specific defense reactions (SSDRs) or avoidance learning in nature is the specific tendency to avoid certain threats or stimuli, it is how animals survive in the wild. Humans and animals both share these species-specific defense reactions, such as the flight-or-fight, which also include pseudo-aggression, fake or intimidating aggression and freeze response to threats, which is controlled by the sympathetic nervous system. These SSDRs are learned very quickly through social interactions between others of the same species, other species, and interaction with the environment. These acquired sets of reactions or responses are not easily forgotten. The animal that survives is the animal that already knows what to fear and how to avoid this threat. An example in humans is the reaction to the sight of a snake, many jump backwards before cognitively realizing what they are jumping away from, and in some cases it is a stick rather than a snake.

As with many functions of the brain, there are various regions of the brain involved in deciphering fear in humans and other nonhuman species. The amygdala communicates both directions between the prefrontal cortex, hypothalamus, the sensory cortex, the hippocampus, thalamus, septum, and the brainstem. The amygdala plays an important role in SSDR, such as the ventral amygdalofugal, which is essential for associative learning, and SSDRs are learned through interaction with the environment and others of the same species. An emotional response is created only after the signals have been relayed between the different regions of the brain, and activating the sympathetic nervous systems; which controls the flight, fight, freeze, fright, and faint response. Often a damaged amygdala can cause impairment in the recognition of fear (like the human case of patient S.M.). This impairment can cause different species to lack the sensation of fear, and often can become overly confident, confronting larger peers, or walking up to predatory creatures.

Robert C. Bolles (1970), a researcher at University of Washington, wanted to understand species-specific defense reactions and avoidance learning among animals, but found that the theories of avoidance learning and the tools that were used to measure this tendency were out of touch with the natural world. He theorized the species-specific defense reaction (SSDR). There are three forms of SSDRs: flight, fight (pseudo-aggression), or freeze. Even domesticated animals have SSDRs, and in those moments it is seen that animals revert to atavistic standards and become "wild" again. Dr. Bolles states that responses are often dependent on the reinforcement of a safety signal, and not the aversive conditioned stimuli. This safety signal can be a source of feedback or even stimulus change. Intrinsic feedback or information coming from within, muscle twitches, increased heart rate, are seen to be more important in SSDRs than extrinsic feedback, stimuli that comes from the external environment. Dr. Bolles found that most creatures have some intrinsic set of fears, to help assure survival of the species. Rats will run away from any shocking event, and pigeons will flap their wings harder when threatened. The wing flapping in pigeons and the scattered running of rats are considered species-specific defense reactions or behaviors. Bolles believed that SSDRs are conditioned through Pavlovian conditioning, and not operant conditioning; SSDRs arise from the association between the environmental stimuli and adverse events. Michael S. Fanselow conducted an experiment, to test some specific defense reactions, he observed that rats in two different shock situations responded differently, based on instinct or defensive topography, rather than contextual information.

Species-specific defense responses are created out of fear, and are essential for survival. Rats that lack the gene stathmin show no avoidance learning, or a lack of fear, and will often walk directly up to cats and be eaten. Animals use these SSDRs to continue living, to help increase their chance of fitness, by surviving long enough to procreate. Humans and animals alike have created fear to know what should be avoided, and this fear can be learned through association with others in the community, or learned through personal experience with a creature, species, or situations that should be avoided. SSDRs are an evolutionary adaptation that has been seen in many species throughout the world including rats, chimpanzees, prairie dogs, and even humans, an adaptation created to help individual creatures survive in a hostile world.

Fear learning changes across the lifetime due to natural developmental changes in the brain. This includes changes in the prefrontal cortex and the amygdala.


The brain structures that are the center of most neurobiological events associated with fear are the two amygdalae, located behind the pituitary gland. Each amygdala is part of a circuitry of fear learning. They are essential for proper adaptation to stress and specific modulation of emotional learning memory. In the presence of a threatening stimulus, the amygdalae generate the secretion of hormones that influence fear and aggression. Once a response to the stimulus in the form of fear or aggression commences, the amygdalae may elicit the release of hormones into the body to put the person into a state of alertness, in which they are ready to move, run, fight, etc. This defensive response is generally referred to in physiology as the fight-or-flight response regulated by the hypothalamus, part of the limbic system. Once the person is in safe mode, meaning that there are no longer any potential threats surrounding them, the amygdalae will send this information to the medial prefrontal cortex (mPFC) where it is stored for similar future situations, which is known as memory consolidation.

Some of the hormones involved during the state of fight-or-flight include epinephrine, which regulates heart rate and metabolism as well as dilating blood vessels and air passages, norepinephrine increasing heart rate, blood flow to skeletal muscles and the release of glucose from energy stores, and cortisol which increases blood sugar, increases circulating neutrophilic leukocytes, calcium amongst other things.

After a situation which incites fear occurs, the amygdalae and hippocampus record the event through synaptic plasticity. The stimulation to the hippocampus will cause the individual to remember many details surrounding the situation. Plasticity and memory formation in the amygdala are generated by activation of the neurons in the region. Experimental data supports the notion that synaptic plasticity of the neurons leading to the lateral amygdalae occurs with fear conditioning. In some cases, this forms permanent fear responses such as posttraumatic stress disorder (PTSD) or a phobia. MRI and fMRI scans have shown that the amygdalae in individuals diagnosed with such disorders including bipolar or panic disorder are larger and wired for a higher level of fear.

Pathogens can suppress amygdala activity. Rats infected with the toxoplasmosis parasite become less fearful of cats, sometimes even seeking out their urine-marked areas. This behavior often leads to them being eaten by cats. The parasite then reproduces within the body of the cat. There is evidence that the parasite concentrates itself in the amygdala of infected rats. In a separate experiment, rats with lesions in the amygdala did not express fear or anxiety towards unwanted stimuli. These rats pulled on levers supplying food that sometimes sent out electrical shocks. While they learned to avoid pressing on them, they did not distance themselves from these shock-inducing levers.

Several brain structures other than the amygdalae have also been observed to be activated when individuals are presented with fearful vs. neutral faces, namely the occipitocerebellar regions including the fusiform gyrus and the inferior parietal / superior temporal gyri. Fearful eyes, brows and mouth seem to separately reproduce these brain responses. Scientists from Zurich studies show that the hormone oxytocin related to stress and sex reduces activity in your brain fear center.

In threatening situations insects, aquatic organisms, birds, reptiles, and mammals emit odorant substances, initially called alarm substances, which are chemical signals now called alarm pheromones ("Schreckstoff" in German). This is to defend themselves and at the same time to inform members of the same species of danger and leads to observable behavior change like freezing, defensive behavior, or dispersion depending on circumstances and species. For example, stressed rats release odorant cues that cause other rats to move away from the source of the signal.

After the discovery of pheromones in 1959, alarm pheromones were first described in 1968 in ants and earthworms, and four years later also found in mammals, both mice and rats. Over the next two decades identification and characterization of these pheromones proceeded in all manner of insects and sea animals, including fish, but it was not until 1990 that more insight into mammalian alarm pheromones was gleaned.

Earlier, in 1985, a link between odors released by stressed rats and pain perception was discovered: unstressed rats exposed to these odors developed opioid-mediated analgesia. In 1997, researchers found that bees became less responsive to pain after they had been stimulated with isoamyl acetate, a chemical smelling of banana, and a component of bee alarm pheromone. The experiment also showed that the bees' fear-induced pain tolerance was mediated by an endorphine.

By using the forced swimming test in rats as a model of fear-induction, the first mammalian "alarm substance" was found. In 1991, this "alarm substance" was shown to fulfill criteria for pheromones: well-defined behavioral effect, species specificity, minimal influence of experience and control for nonspecific arousal. Rat activity testing with the alarm pheromone, and their preference/avoidance for odors from cylinders containing the pheromone, showed that the pheromone had very low volatility.

In 1993 a connection between alarm chemosignals in mice and their immune response was found. Pheromone production in mice was found to be associated with or mediated by the pituitary gland in 1994.

In 2004, it was demonstrated that rats' alarm pheromones had different effects on the "recipient" rat (the rat perceiving the pheromone) depending which body region they were released from: Pheromone production from the face modified behavior in the recipient rat, e.g. caused sniffing or movement, whereas pheromone secreted from the rat's anal area induced autonomic nervous system stress responses, like an increase in core body temperature. Further experiments showed that when a rat perceived alarm pheromones, it increased its defensive and risk assessment behavior, and its acoustic startle reflex was enhanced.

It was not until 2011 that a link between severe pain, neuroinflammation and alarm pheromones release in rats was found: real time RT-PCR analysis of rat brain tissues indicated that shocking the footpad of a rat increased its production of proinflammatory cytokines in deep brain structures, namely of IL-1β, heteronuclear Corticotropin-releasing hormone and c-fos mRNA expressions in both the paraventricular nucleus and the bed nucleus of the stria terminalis, and it increased stress hormone levels in plasma (corticosterone).

The neurocircuit for how rats perceive alarm pheromones was shown to be related to the hypothalamus, brainstem, and amygdalae, all of which are evolutionary ancient structures deep inside or in the case of the brainstem underneath the brain away from the cortex, and involved in the fight-or-flight response, as is the case in humans.

Alarm pheromone-induced anxiety in rats has been used to evaluate the degree to which anxiolytics can alleviate anxiety in humans. For this the change in the acoustic startle reflex of rats with alarm pheromone-induced anxiety (i.e. reduction of defensiveness) has been measured. Pretreatment of rats with one of five anxiolytics used in clinical medicine was able to reduce their anxiety: namely midazolam, phenelzine (a nonselective monoamine oxidase (MAO) inhibitor), propranolol, a nonselective beta blocker, clonidine, an alpha 2 adrenergic agonist or CP-154,526, a corticotropin-releasing hormone antagonist.

Faulty development of odor discrimination impairs the perception of pheromones and pheromone-related behavior, like aggressive behavior and mating in male rats: The enzyme Mitogen-activated protein kinase 7 (MAPK7) has been implicated in regulating the development of the olfactory bulb and odor discrimination and it is highly expressed in developing rat brains, but absent in most regions of adult rat brains. Conditional deletion of the MAPK7gene in mouse neural stem cells impairs several pheromone-mediated behaviors, including aggression and mating in male mice. These behavior impairments were not caused by a reduction in the level of testosterone, by physical immobility, by heightened fear or anxiety or by depression. Using mouse urine as a natural pheromone-containing solution, it has been shown that the impairment was associated with defective detection of related pheromones, and with changes in their inborn preference for pheromones related to sexual and reproductive activities.

Lastly, alleviation of an acute fear response because a friendly peer (or in biological language: an affiliative conspecific) tends and befriends is called "social buffering". The term is in analogy to the 1985 "buffering" hypothesis in psychology, where social support has been proven to mitigate the negative health effects of alarm pheromone mediated distress. The role of a "social pheromone" is suggested by the recent discovery that olfactory signals are responsible in mediating the "social buffering" in male rats. "Social buffering" was also observed to mitigate the conditioned fear responses of honeybees. A bee colony exposed to an environment of high threat of predation did not show increased aggression and aggressive-like gene expression patterns in individual bees, but decreased aggression. That the bees did not simply habituate to threats is suggested by the fact that the disturbed colonies also decreased their foraging.

Biologists have proposed in 2012 that fear pheromones evolved as molecules of "keystone significance", a term coined in analogy to keystone species. Pheromones may determine species compositions and affect rates of energy and material exchange in an ecological community. Thus pheromones generate structure in a food web and play critical roles in maintaining natural systems.

Evidence of chemosensory alarm signals in humans has emerged slowly: Although alarm pheromones have not been physically isolated and their chemical structures have not been identified in humans so far, there is evidence for their presence. Androstadienone, for example, a steroidal, endogenous odorant, is a pheromone candidate found in human sweat, axillary hair and plasma. The closely related compound androstenone is involved in communicating dominance, aggression or competition; sex hormone influences on androstenone perception in humans showed a high testosterone level related to heightened androstenone sensitivity in men, a high testosterone level related to unhappiness in response to androstenone in men, and a high estradiol level related to disliking of androstenone in women.

A German study from 2006 showed when anxiety-induced versus exercise-induced human sweat from a dozen people was pooled and offered to seven study participants, of five able to olfactorily distinguish exercise-induced sweat from room air, three could also distinguish exercise-induced sweat from anxiety induced sweat. The acoustic startle reflex response to a sound when sensing anxiety sweat was larger than when sensing exercise-induced sweat, as measured by electromyograph analysis of the orbital muscle, which is responsible for the eyeblink component. This showed for the first time that fear chemosignals can modulate the startle reflex in humans without emotional mediation; fear chemosignals primed the recipient's "defensive behavior" prior to the subjects' conscious attention on the acoustic startle reflex level.

In analogy to the social buffering of rats and honeybees in response to chemosignals, induction of empathy by "smelling anxiety" of another person has been found in humans.

A study from 2013 provided brain imaging evidence that human responses to fear chemosignals may be gender-specific. Researchers collected alarm-induced sweat and exercise-induced sweat from donors extracted it, pooled it and presented it to 16 unrelated people undergoing functional brain MRI. While stress-induced sweat from males produced a comparably strong emotional response in both females and males, stress-induced sweat from females produced a markedly stronger arousal in women than in men. Statistical tests pinpointed this gender-specificity to the right amygdala and strongest in the superficial nuclei. Since no significant differences were found in the olfactory bulb, the response to female fear-induced signals is likely based on processing the meaning, i.e. on the emotional level, rather than the strength of chemosensory cues from each gender, i.e. the perceptual level.

An approach-avoidance task was set up where volunteers seeing either an angry or a happy cartoon face on a computer screen pushed away or pulled toward them a joystick as fast as possible. Volunteers smelling anandrostadienone, masked with clove oil scent responded faster, especially to angry faces, than those smelling clove oil only, which was interpreted as anandrostadienone-related activation of the fear system. A potential mechanism of action is, that androstadienone alters the "emotional face processing". Androstadienone is known to influence activity of the fusiform gyrus which is relevant for face recognition.

A drug treatment for fear conditioning and phobias via the amygdalae is the use of glucocorticoids. In one study, glucocorticoid receptors in the central nuclei of the amygdalae were disrupted in order to better understand the mechanisms of fear and fear conditioning. The glucocorticoid receptors were inhibited using lentiviral vectors containing Cre-recombinase injected into mice. Results showed that disruption of the glucocorticoid receptors prevented conditioned fear behavior. The mice were subjected to auditory cues which caused them to freeze normally. However, a reduction of freezing was observed in the mice that had inhibited glucocorticoid receptors.

Cognitive behavioral therapy has been successful in helping people overcome their fear. Because fear is more complex than just forgetting or deleting memories, an active and successful approach involves people repeatedly confronting their fears. By confronting their fears in a safe manner a person can suppress the "fear-triggering memories" or stimuli.

Exposure therapy has known to have helped up to 90% of people with specific phobias to significantly decrease their fear over time.

Another psychological treatment is systematic desensitization, which is a type of behavior therapy used to completely remove the fear or produce a disgusted response to this fear and replace it. The replacement that occurs will be relaxation and will occur through conditioning. Through conditioning treatments, muscle tensioning will lessen and deep breathing techniques will aid in de-tensioning.

Other methods for treating or coping with one's fear was suggested by life coach Robin Sharma. A person could keep a journal in which they write down rational thoughts regarding their fears. Journal entries are a healthy method of expressing one's fears without compromising their safety or causing uncertainty. Another suggestion is a fear ladder. To create a fear ladder, one must write down all of their fears and score them on a scale of one to ten. Next, the person addresses their phobia, starting with the lowest number.

Finding solace in religion is another method to cope with one's fear. Having something to answer your questions regarding your fears, such as, what happens after death or if there is an afterlife, can help mitigate one's fear of death because there is no room for uncertainty as their questions are answered. Religion offers a method of being able to understand and make sense of one's fears rather than ignore them.

The fear of the end of life and its existence is in other words the fear of death. The fear of death ritualized the lives of our ancestors. These rituals were designed to reduce that fear; they helped collect the cultural ideas that we now have in the present. These rituals also helped preserve the cultural ideas. The results and methods of human existence had been changing at the same time that social formation was changing.

When people are faced with their own thoughts of death, they either accept that they are dying or will die because they have lived a full life or they will experience fear. A theory was developed in response to this, which is called the Terror Management Theory. The theory states that a person's cultural worldviews (religion, values, etc.) will mitigate the terror associated with the fear of death through avoidance. To help manage their terror, they find solace in their death-denying beliefs, such as their religion. Another way people cope with their death related fears is pushing any thoughts of death into the future or by avoiding these thoughts all together through distractions. Although there are methods for one coping with the terror associated with their fear of death, not everyone suffers from these same uncertainties. People who have lived a full life, typically do not fear death because they believe that they have lived their life to the fullest.

Religions are filled with different fears that humans have had throughout many centuries. The fears aren't just metaphysical (including the problems of life and death) but are also moral. Death is seen as a boundary to another world. That world would always be different depending on how each individual lived their lives. The origins of this intangible fear are not found in the present world. In a sense we can assume that fear was a big influence on things such as morality. This assumption, however, flies in the face of concepts such as moral absolutism and moral universalism – which would hold that our morals are rooted in either the divine or natural laws of the universe, and would not be generated by any human feeling, thought or emotion.

From a theological perspective, the word "fear" encompasses more than simple fear. Robert B. Strimple says that fear includes the "... convergence of awe, reverence, adoration...". Some translations of the Bible, such as the New International Version, sometimes replace the word "fear" with "reverence".

Fear in religion can be seen throughout the years, however, the most prominent example would be The Crusades. Pope Urban II allowed for Christian mercenary troops to be sent on a mission in order to recover the Holy Lands from the Muslims. However, the message was misinterpreted and as a result, innocent people were slaughtered. Although the Crusades were meant to stay between the Muslims and the Christians, the hate spread onto the Jewish culture. Jewish people who feared for their lives, gave into the forced conversion of Christianity because they believed this would secure their safety. Other Jewish people feared betraying their God by conceding to a conversion, and instead, secured their own fate, which was death. It can also be argued that Christians feared their religion not being the primary religion, and this is why they committed mass murder.

Fear may be politically and culturally manipulated to persuade citizenry of ideas which would otherwise be widely rejected or dissuade citizenry from ideas which would otherwise be widely supported. In contexts of disasters, nation-states manage the fear not only to provide their citizens with an explanation about the event or blaming some minorities, but also to adjust their previous beliefs.

Fear can alter how a person thinks or reacts to situations because fear has the power to inhibit one's rational way of thinking. As a result, people who do not experience fear, are able to use fear as a tool to manipulate others. People who are experiencing fear, seek preservation through safety and can be manipulated by a person who is there to provide that safety that is being sought after. "When we're afraid, a manipulator can talk us out of the truth we see right in front of us. Words become more real than reality" By this, a manipulator is able to use our fear to manipulate us out the truth and instead make us believe and trust in their truth. Politicians are notorious for using fear to manipulate the people into supporting their will. Through keywords and key phrases such as, "it is for your safety," or "it is for the safety of this country," politicians invoke fear within people that their safety is at stake, and people will ultimately follow in line in order for their safety to be restored.

Fear is found and reflected in mythology and folklore as well as in works of fiction such as novels and films.

Works of dystopian and (post)apocalyptic fiction convey the fears and anxieties of societies.

The fear of the world's end is about as old as civilization itself. In a 1967 study Frank Kermode suggests that the failure of religious prophecies led to a shift in how society apprehends this ancient mode. Scientific and critical thought supplanting religious and mythical thought as well as a public emancipation may be the cause of eschatology becoming replaced by more realistic scenarios. Such might constructively provoke discussion and steps to be taken to prevent depicted catastrophes.

"The Story of the Youth Who Went Forth to Learn What Fear Was" is a German fairy tale dealing with the topic of not knowing fear.
Many stories also include characters who fear the antagonist of the plot. One important characteristic of historical and mythical heroes across cultures is to be fearless in the face of big and often lethal enemies.

In the world of athletics fear is often used as a means of motivation to not fail. This situation involves using fear in a way that increases the chances of a positive outcome. In this case the fear that is being created is initially a cognitive state to the receiver. This initial state is what generates the first response of the athlete, this response generates a possibility of fight or flight reaction by the athlete (receiver), which in turn will increase or decrease the possibility of success or failure in the certain situation for the athlete. The amount of time that the athlete has to determine this decision is small but it is still enough time for the receiver to make a determination through cognition. Even though the decision is made quickly, the decision is determined through past events that have been experienced by the athlete. The results of these past events will determine how the athlete will make his cognitive decision in the split second that he or she has.

Fear of failure as described above has been studied frequently in the field of sport psychology. Many scholars have tried to determine how often fear of failure is triggered within athletes. As well as what personalities of athletes most often choose to use this type of motivation. Studies have also been conducted to determine the success rate of this method of motivation.

Murray's Exploration in Personal (1938) was one of the first studies that actually identified fear of failure as an actual motive to avoid failure or to achieve success. His studies suggested that inavoidance, the need to avoid failure, was found in many college-aged men during the time of his research in 1938. This was a monumental finding in the field of psychology because it allowed other researchers to better clarify how fear of failure can actually be a determinant of creating achievement goals as well as how it could be used in the actual act of achievement.

In the context of sport, a model was created by R.S. Lazarus in 1991 that uses the cognitive-motivational-relational theory of emotion.
Another study was done in 2001 by Conroy, Poczwardowski, and Henschen that created five aversive consequences of failing that have been repeated over time. The five categories include (a) experiencing shame and embarrassment, (b) devaluing one's self-estimate, (c) having an uncertain future, (d) important others losing interest, (e) upsetting important others. These five categories can help one infer the possibility of an individual to associate failure with one of these threat categories, which will lead them to experiencing fear of failure.

In summary, the two studies that were done above created a more precise definition of fear of failure, which is "a dispositional tendency to experience apprehension and anxiety in evaluative situations because individuals have learned that failure is associated with aversive consequences".

People who have damage to their amygdalae, which can be caused by a rare genetic disease known as Urbach–Wiethe disease, are unable to experience fear. The disease destroys both amygdalae in late childhood. Since the discovery of the disease, there have only been 400 recorded cases. This is not debilitating; however, a lack of fear can allow someone to get into a dangerous situation they otherwise would have avoided. For example, people who possess a lack of fear would approach a known venomous snake while those who possess fear, would typically try to avoid it. Fear is an important aspect of a human being's development.

Another possibility of a lack of fear could be Antisocial Personality Disorder or more commonly known as sociopathy, which is a mental health condition where people often manipulate others. People who suffer from this disorder lack a physiological response and are capable of lying or not experiencing fear. When a person is approached by something they fear or has a phobia, the body will react in a way that conveys this fear. For example, a person who possesses Arachnophobia, or a fear of spiders, would experience a physical response such as, nausea, dizziness, or even shaking when approached by a spider. However, a person with Antisocial Personality Disorder lacks this capability and would not activate their "fight or flight" response to such fear. They also show a disregard for the safety of others, and may intentionally harm them or allow harm to come to them.




</doc>
<doc id="10830" url="https://en.wikipedia.org/wiki?curid=10830" title="Football team">
Football team

A football team is a group of players selected to play together in the various team sports known as football. Such teams could be selected to play in a match against an opposing team, to represent a football club, group, state or nation, an all-star team or even selected as a hypothetical team (such as a Dream Team or Team of the Century) and never play an actual match.

There are several varieties of football, notably association football, gridiron football, Australian rules football, Gaelic football, rugby league and rugby union. The number of players selected for each team within these varieties and their associated codes can vary substantially. Sometimes, the word "team" is limited to those who play on the field in a match and does not always include other players who may take part as replacements or emergency players. "Football squad" may be used to be inclusive of these support and reserve players.

The term football club is the most commonly used for a sports club which is an organised or incorporated body with a president, committee and a set of rules responsible for ensuring the continued playing existence of one or more teams which are selected for regular competition play (and which may participate in several different divisions or leagues). The oldest football clubs date back to the early 19th century. The words team and club are sometimes used interchangeably by supporters, although they typically refer to the team within the club playing in the highest division or competition.

The number of players that take part in the sport simultaneously, thus forming the team are:





</doc>
<doc id="10831" url="https://en.wikipedia.org/wiki?curid=10831" title="F">
F

F (named "ef" ) is the sixth letter in the modern English alphabet and the ISO basic Latin alphabet.

The origin of 'F' is the Semitic letter "vâv" (or "waw") that represented a sound like or . Graphically it originally probably depicted either a hook or a club. It may have been based on a comparable Egyptian hieroglyph such as (transliterated as ḥ(dj)): T3

The Phoenician form of the letter was adopted into Greek as a vowel, "upsilon" (which resembled its descendant 'Y' but was also the ancestor of the Roman letters 'U', 'V', and 'W'); and, with another form, as a consonant, "digamma", which indicated the pronunciation , as in Phoenician. Latin 'F,' despite being pronounced differently, is ultimately descended from digamma and closely resembles it in form.

After sound changes eliminated from spoken Greek, "digamma" was used only as a numeral. However, the Greek alphabet also gave rise to other alphabets, and some of these retained letters descended from digamma. In the Etruscan alphabet, 'F' probably represented , as in Greek, and the Etruscans formed the digraph 'FH' to represent . (At the time these letters were borrowed, there was no Greek letter that represented /f/: the Greek letter phi 'Φ' then represented an aspirated voiceless bilabial plosive , although in Modern Greek it has come to represent .) When the Romans adopted the alphabet, they used 'V' (from Greek "upsilon") not only for the vowel , but also for the corresponding semivowel , leaving 'F' available for . And so out of the various "vav" variants in the Mediterranean world, the letter F entered the Roman alphabet attached to a sound which its antecedents in Greek and Etruscan did not have. The Roman alphabet forms the basis of the alphabet used today for English and many other languages.

The lowercase 'f' is not related to the visually similar long s, 'ſ' (or medial s). The use of the "long s" largely died out by the beginning of the 19th century, mostly to prevent confusion with 'f' when using a short mid-bar (see more at: S).

In the English writing system is used to represent the sound , the voiceless labiodental fricative. It is often doubled at the end of words. Exceptionally, it represents the voiced labiodental fricative in the common word "of".

In the writing systems of other languages, commonly represents , or .

The International Phonetic Alphabet uses to represent the voiceless labiodental fricative.

An italic letter is conventionally used to denote an arbitrary function. See also f with hook (ƒ).





</doc>
<doc id="10834" url="https://en.wikipedia.org/wiki?curid=10834" title="Food preservation">
Food preservation

Food preservation prevents the growth of microorganisms (such as yeasts), or other microorganisms (although some methods work by introducing benign bacteria or fungi to the food), as well as slowing the oxidation of fats that cause rancidity. Food preservation may also include processes that inhibit visual deterioration, such as the enzymatic browning reaction in apples after they are cut during food preparation.

Many processes designed to preserve food involve more than one food preservation method. Preserving fruit by turning it into jam, for example, involves boiling (to reduce the fruit’s moisture content and to kill bacteria, etc.), sugaring (to prevent their re-growth) and sealing within an airtight jar (to prevent recontamination). Some traditional methods of preserving food have been shown to have a lower energy input and carbon footprint, when compared to modern methods.

Some methods of food preservation are known to create carcinogens. In 2015, the International Agency for Research on Cancer of the World Health Organization classified processed meat, i.e. meat that has undergone salting, curing, fermenting, and smoking, as "carcinogenic to humans".

Maintaining or creating nutritional value, texture and flavor is an important aspect of food preservation.

New techniques of food preservation became available to the home chef from the dawn of agriculture until the Industrial Revolution.

The earliest form of curing was dehydration or drying, used as early as 12,000 BC. Smoking and salting techniques improve on the drying process and add antimicrobial agents that aid in preservation. Smoke deposits a number of pyrolysis products onto the food, including the phenols syringol, guaiacol and catechol. Salt accelerates the drying process using osmosis and also inhibits the growth of several common strains of bacteria. More recently nitrites have been used to cure meat, contributing a characteristic pink colour.

Cooling preserves food by slowing down the growth and reproduction of microorganisms and the action of enzymes that causes the food to rot. The introduction of commercial and domestic refrigerators drastically improved the diets of many in the Western world by allowing food such as fresh fruit, salads and dairy products to be stored safely for longer periods, particularly during warm weather.

Before the era of mechanical refrigeration, cooling for food storage occurred in the forms of root cellars and iceboxes. Rural people often did their own ice cutting, whereas town and city dwellers often relied on the ice trade. Today, root cellaring remains popular among people who value various goals, including local food, heirloom crops, traditional home cooking techniques, family farming, frugality, self-sufficiency, organic farming, and others.

Freezing is also one of the most commonly used processes, both commercially and domestically, for preserving a very wide range of foods, including prepared foods that would not have required freezing in their unprepared state. For example, potato waffles are stored in the freezer, but potatoes themselves require only a cool dark place to ensure many months' storage. Cold stores provide large-volume, long-term storage for strategic food stocks held in case of national emergency in many countries.

Boiling liquid food items can kill any existing microbes. Milk and water are often boiled to kill any harmful microbes that may be present in them.

Heating to temperatures which are sufficient to kill microorganisms inside the food is a method used with perpetual stews. Milk is also boiled before storing to kill many microorganisms.

The earliest cultures have used sugar as a preservative, and it was commonplace to store fruit in honey. Similar to pickled foods, sugar cane was brought to Europe through the trade routes. In northern climates without sufficient sun to dry foods, preserves are made by heating the fruit with sugar. "Sugar tends to draw water from the microbes (plasmolysis). This process leaves the microbial cells dehydrated, thus killing them. In this way, the food will remain safe from microbial spoilage." Sugar is used to preserve fruits, either in an antimicrobial syrup with fruit such as apples, pears, peaches, apricots, and plums, or in crystallized form where the preserved material is cooked in sugar to the point of crystallization and the resultant product is then stored dry. This method is used for the skins of citrus fruit (candied peel), angelica, and ginger. Also, sugaring can be used in the production of jam and jelly.

Pickling is a method of preserving food in an edible, antimicrobial liquid. Pickling can be broadly classified into two categories: chemical pickling and fermentation pickling.

In chemical pickling, the food is placed in an edible liquid that inhibits or kills bacteria and other microorganisms. Typical pickling agents include brine (high in salt), vinegar, alcohol, and vegetable oil. Many chemical pickling processes also involve heating or boiling so that the food being preserved becomes saturated with the pickling agent. Common chemically pickled foods include cucumbers, peppers, corned beef, herring, and eggs, as well as mixed vegetables such as piccalilli.

In fermentation pickling, bacteria in the liquid produce organic acids as preservation agents, typically by a process that produces lactic acid through the presence of lactobacillales. Fermented pickles include sauerkraut, nukazuke, kimchi, and surströmming.

Sodium hydroxide (lye) makes food too alkaline for bacterial growth. Lye will saponify fats in the food, which will change its flavor and texture. Lutefisk uses lye in its preparation, as do some olive recipes. Modern recipes for century eggs also call for lye.

Canning involves cooking food, sealing it in sterilized cans or jars, and boiling the containers to kill or weaken any remaining bacteria as a form of sterilization. It was invented by the French confectioner Nicolas Appert. By 1806, this process was used by the French Navy to preserve meat, fruit, vegetables, and even milk. Although Appert had discovered a new way of preservation, it wasn't understood until 1864 when Louis Pasteur found the relationship between microorganisms, food spoilage, and illness.

Foods have varying degrees of natural protection against spoilage and may require that the final step occur in a pressure cooker. High-acid fruits like strawberries require no preservatives to can and only a short boiling cycle, whereas marginal vegetables such as carrots require longer boiling and addition of other acidic elements. Low-acid foods, such as vegetables and meats, require pressure canning. Food preserved by canning or bottling is at immediate risk of spoilage once the can or bottle has been opened.

Lack of quality control in the canning process may allow ingress of water or micro-organisms. Most such failures are rapidly detected as decomposition within the can causes gas production and the can will swell or burst. However, there have been examples of poor manufacture (underprocessing) and poor hygiene allowing contamination of canned food by the obligate anaerobe "Clostridium botulinum", which produces an acute toxin within the food, leading to severe illness or death. This organism produces no gas or obvious taste and remains undetected by taste or smell. Its toxin is denatured by cooking, however. Cooked mushrooms, handled poorly and then canned, can support the growth of "Staphylococcus aureus", which produces a toxin that is not destroyed by canning or subsequent reheating.

Food may be preserved by cooking in a material that solidifies to form a gel. Such materials include gelatin, agar, maize flour, and arrowroot flour. Some foods naturally form a protein gel when cooked, such as eels and elvers, and sipunculid worms, which are a delicacy in Xiamen, in the Fujian province of the People's Republic of China. Jellied eels are a delicacy in the East End of London, where they are eaten with mashed potatoes. Potted meats in aspic (a gel made from gelatin and clarified meat broth) were a common way of serving meat off-cuts in the UK until the 1950s. Many jugged meats are also jellied.

A traditional British way of preserving meat (particularly shrimp) is by setting it in a pot and sealing it with a layer of fat. Also common is potted chicken liver; jellying is one of the steps in producing traditional pâtés.

Meat can be preserved by jugging. Jugging is the process of stewing the meat (commonly game or fish) in a covered earthenware jug or casserole. The animal to be jugged is usually cut into pieces, placed into a tightly-sealed jug with brine or gravy, and stewed. Red wine and/or the animal's own blood is sometimes added to the cooking liquid. Jugging was a popular method of preserving meat up until the middle of the 20th century.

Burial of food can preserve it due to a variety of factors: lack of light, lack of oxygen, cool temperatures, pH level, or desiccants in the soil. Burial may be combined with other methods such as salting or fermentation. Most foods can be preserved in soil that is very dry and salty (thus a desiccant) such as sand, or soil that is frozen.

Many root vegetables are very resistant to spoilage and require no other preservation than storage in cool dark conditions, for example by burial in the ground, such as in a storage clamp. Century eggs are traditionally created by placing eggs in alkaline mud (or other alkaline substance), resulting in their "inorganic" fermentation through raised pH instead of spoiling. The fermentation preserves them and breaks down some of the complex, less flavorful proteins and fats into simpler, more flavorful ones. Cabbage was traditionally buried during Autumn in northern US farms for preservation. Some methods keep it crispy while other methods produce sauerkraut. A similar process is used in the traditional production of kimchi. Sometimes meat is buried under conditions that cause preservation. If buried on hot coals or ashes, the heat can kill pathogens, the dry ash can desiccate, and the earth can block oxygen and further contamination. If buried where the earth is very cold, the earth acts like a refrigerator.

In Orissa, India, it is practical to store rice by burying it underground. This method helps to store for three to six months during the dry season.

Butter and similar substances have been preserved as bog butter in Irish peat bogs for centuries.

Meat can be preserved by salting it, cooking it at or near 100 °C in some kind of fat (such as lard or tallow), and then storing it immersed in the fat. These preparations were popular in Europe before refrigerators became ubiquitous. They are still popular in France, where they are called "confit". The preparation will keep longer if stored in a cold cellar or buried in cold ground.

Some foods, such as many cheeses, wines, and beers, use specific micro-organisms that combat spoilage from other less-benign organisms. These micro-organisms keep pathogens in check by creating an environment toxic for themselves and other micro-organisms by producing acid or alcohol. Methods of fermentation include, but are not limited to, starter micro-organisms, salt, hops, controlled (usually cool) temperatures and controlled (usually low) levels of oxygen. These methods are used to create the specific controlled conditions that will support the desirable organisms that produce food fit for human consumption.

Fermentation is the microbial conversion of starch and sugars into alcohol. Not only can fermentation produce alcohol, but it can also be a valuable preservation technique. Fermentation can also make foods more nutritious and palatable. For example, drinking water in the Middle Ages was dangerous because it often contained pathogens that could spread disease. When the water is made into beer, the boiling during the brewing process kills any bacteria in the water that could make people sick. Additionally, the water now has the nutrients from the barley and other ingredients, and the microorganisms can also produce vitamins as they ferment.

Techniques of food preservation were developed in research laboratories for commercial applications.

Pasteurization is a process for preservation of liquid food. It was originally applied to combat the souring of young local wines. Today, the process is mainly applied to dairy products. In this method, milk is heated at about for 15–30 seconds to kill the bacteria present in it and cooling it quickly to to prevent the remaining bacteria from growing. The milk is then stored in sterilized bottles or pouches in cold places. This method was invented by Louis Pasteur, a French chemist, in 1862.

Vacuum-packing stores food in a vacuum environment, usually in an air-tight bag or bottle. The vacuum environment strips bacteria of oxygen needed for survival. Vacuum-packing is commonly used for storing nuts to reduce loss of flavor from oxidization. A major drawback to vacuum packaging, at the consumer level, is that vacuum sealing can deform contents and rob certain foods, such as cheese, of its flavor.

Preservative food additives can be "antimicrobial"—which inhibit the growth of bacteria or fungi, including mold—or "antioxidant", such as oxygen absorbers, which inhibit the oxidation of food constituents. Common antimicrobial preservatives include calcium propionate, sodium nitrate, sodium nitrite, sulfites (sulfur dioxide, sodium bisulfite, potassium hydrogen sulfite, etc.), and EDTA. Antioxidants include butylated hydroxyanisole (BHA) and butylated hydroxytoluene (BHT). Other preservatives include formaldehyde (usually in solution), glutaraldehyde (insecticide), ethanol, and methylchloroisothiazolinone.

Irradiation of food is the exposure of food to ionizing radiation. Multiple types of ionizing radiation can be used, including beta particles (high-energy electrons) and gamma rays (emitted from radioactive sources such as cobalt-60 or cesium-137). Irradiation can kill bacteria, molds, and insect pests, reduce the ripening and spoiling of fruits, and at higher doses induce sterility. The technology may be compared to pasteurization; it is sometimes called "cold pasteurization", as the product is not heated. Irradiation may allow lower-quality or contaminated foods to be rendered marketable.

National and international expert bodies have declared food irradiation as "wholesome"; organizations of the United Nations, such as the World Health Organization and Food and Agriculture Organization, endorse food irradiation. Consumers may have a negative view of irradiated food based on the misconception that such food is radioactive; in fact, irradiated food does not and cannot become radioactive. Activists have also opposed food irradiation for other reasons, for example, arguing that irradiation can be used to sterilize contaminated food without resolving the underlying cause of the contamination. International legislation on whether food may be irradiated or not varies worldwide from no regulation to a full ban.

Approximately 500,000 tons of food items are irradiated per year worldwide in over 40 countries. These are mainly spices and condiments, with an increasing segment of fresh fruit irradiated for fruit fly quarantine.

Pulsed electric field (PEF) electroporation is a method for processing cells by means of brief pulses of a strong electric field. PEF holds potential as a type of low-temperature alternative pasteurization process for sterilizing food products. In PEF processing, a substance is placed between two electrodes, then the pulsed electric field is applied. The electric field enlarges the pores of the cell membranes, which kills the cells and releases their contents. PEF for food processing is a developing technology still being researched. There have been limited industrial applications of PEF processing for the pasteurization of fruit juices. To date, several PEF treated juices are available on the market in Europe. Furthermore, for several years a juice pasteurization application in the US has used PEF. For cell disintegration purposes especially potato processors show great interest in PEF technology as an efficient alternative for their preheaters. Potato applications are already operational in the US and Canada. There are also commercial PEF potato applications in various countries in Europe, as well as in Australia, India, and China.

Modifying atmosphere is a way to preserve food by operating on the atmosphere around it. Salad crops that are notoriously difficult to preserve are now being packaged in sealed bags with an atmosphere modified to reduce the oxygen (O) concentration and increase the carbon dioxide (CO) concentration. There is concern that, although salad vegetables retain their appearance and texture in such conditions, this method of preservation may not retain nutrients, especially vitamins.
There are two methods for preserving grains with carbon dioxide. One method is placing a block of dry ice in the bottom and filling the can with the grain. Another method is purging the container from the bottom by gaseous carbon dioxide from a cylinder or bulk supply vessel.

Carbon dioxide prevents insects and, depending on concentration, mold and oxidation from damaging the grain. Grain stored in this way can remain edible for approximately five years.

Nitrogen gas (N) at concentrations of 98% or higher is also used effectively to kill insects in the grain through hypoxia. However, carbon dioxide has an advantage in this respect, as it kills organisms through hypercarbia and hypoxia (depending on concentration), but it requires concentrations of above 35%, or so. This makes carbon dioxide preferable for fumigation in situations where a hermetic seal cannot be maintained.

Controlled Atmospheric Storage (CA): "CA storage is a non-chemical process. Oxygen levels in the sealed rooms are reduced, usually by the infusion of nitrogen gas, from the approximate 21 percent in the air we breathe to 1 percent or 2 percent. Temperatures are kept at a constant . Humidity is maintained at 95 percent and carbon dioxide levels are also controlled. Exact conditions in the rooms are set according to the apple variety. Researchers develop specific regimens for each variety to achieve the best quality. Computers help keep conditions constant."
"Eastern Washington, where most of Washington’s apples are grown, has enough warehouse storage for 181 million boxes of fruit, according to a report done in 1997 by managers for the Washington State Department of Agriculture Plant Services Division. The storage capacity study shows that 67 percent of that space—enough for 121,008,000 boxes of apples—is CA storage."
Air-tight storage of grains (sometimes called hermetic storage) relies on the respiration of grain, insects, and fungi that can modify the enclosed atmosphere sufficiently to control insect pests. This is a method of great antiquity, as well as having modern equivalents. The success of the method relies on having the correct mix of sealing, grain moisture, and temperature.

A patented process uses fuel cells to exhaust and automatically maintain the exhaustion of oxygen in a shipping container, containing, for example, fresh fish.

This process subjects the surface of food to a "flame" of ionized gas molecules, such as helium or nitrogen. This causes micro-organisms to die off on the surface.

High-pressure food preservation or pascalization refers to the use of a food preservation technique that makes use of high pressure. "Pressed inside a vessel exerting or more, food can be processed so that it retains its fresh appearance, flavor, texture and nutrients while disabling harmful microorganisms and slowing spoilage." By 2005, the process was being used for products ranging from orange juice to guacamole to deli meats and widely sold.

Biopreservation is the use of natural or controlled microbiota or antimicrobials as a way of preserving food and extending its shelf life. Beneficial bacteria or the fermentation products produced by these bacteria are used in biopreservation to control spoilage and render pathogens inactive in food. It is a benign ecological approach which is gaining increasing attention.

Of special interest are lactic acid bacteria (LAB). Lactic acid bacteria have antagonistic properties that make them particularly useful as biopreservatives. When LABs compete for nutrients, their metabolites often include active antimicrobials such as lactic acid, acetic acid, hydrogen peroxide, and peptide bacteriocins. Some LABs produce the antimicrobial nisin, which is a particularly effective preservative.

These days, LAB bacteriocins are used as an integral part of hurdle technology. Using them in combination with other preservative techniques can effectively control spoilage bacteria and other pathogens, and can inhibit the activities of a wide spectrum of organisms, including inherently resistant Gram-negative bacteria.

Hurdle technology is a method of ensuring that pathogens in food products can be eliminated or controlled by combining more than one approach. These approaches can be thought of as "hurdles" the pathogen has to overcome if it is to remain active in the food. The right combination of hurdles can ensure all pathogens are eliminated or rendered harmless in the final product.

Hurdle technology has been defined by Leistner (2000) as an intelligent combination of hurdles that secures the microbial safety and stability as well as the organoleptic and nutritional quality and the economic viability of food products. The organoleptic quality of the food refers to its sensory properties, that is its look, taste, smell, and texture.

Examples of hurdles in a food system are high temperature during processing, low temperature during storage, increasing the acidity, lowering the water activity or redox potential, and the presence of preservatives or biopreservatives. According to the type of pathogens and how risky they are, the intensity of the hurdles can be adjusted individually to meet consumer preferences in an economical way, without sacrificing the safety of the product.




</doc>
<doc id="10835" url="https://en.wikipedia.org/wiki?curid=10835" title="Frequency modulation">
Frequency modulation

In telecommunications and signal processing, frequency modulation (FM) is the encoding of information in a carrier wave by varying the instantaneous frequency of the wave.

In analog frequency modulation, such as FM radio broadcasting of an audio signal representing voice or music, the instantaneous frequency deviation, the difference between the frequency of the carrier and its center frequency, is proportional to the modulating signal.

Digital data can be encoded and transmitted via FM by shifting the carrier's frequency among a predefined set of frequencies representing digits – for example one frequency can represent a binary 1 and a second can represent binary 0. This modulation technique is known as frequency-shift keying (FSK). FSK is widely used in modems such as fax modems, and can also be used to send Morse code. Radioteletype also uses FSK.

Frequency modulation is widely used for FM radio broadcasting. It is also used in telemetry, radar, seismic prospecting, and monitoring newborns for seizures via EEG, two-way radio systems, music synthesis, magnetic tape-recording systems and some video-transmission systems. In radio transmission, an advantage of frequency modulation is that it has a larger signal-to-noise ratio and therefore rejects radio frequency interference better than an equal power amplitude modulation (AM) signal. For this reason, most music is broadcast over FM radio.

Frequency modulation and phase modulation are the two complementary principal methods of angle modulation; phase modulation is often used as an intermediate step to achieve frequency modulation. These methods contrast with amplitude modulation, in which the amplitude of the carrier wave varies, while the frequency and phase remain constant.

If the information to be transmitted (i.e., the baseband signal) is formula_1 and the sinusoidal carrier is formula_2, where "f" is the carrier's base frequency, and "A" is the carrier's amplitude, the modulator combines the carrier with the baseband data signal to get the transmitted signal* 

where formula_4, formula_5 being the sensitivity of the frequency modulator and formula_6 being the amplitude of the modulating signal or baseband signal.

In this equation, formula_7 is the "instantaneous frequency" of the oscillator and formula_8 is the "frequency deviation", which represents the maximum shift away from "f" in one direction, assuming "x"("t") is limited to the range ±1.

While most of the energy of the signal is contained within "f" ± "f", it can be shown by Fourier analysis that a wider range of frequencies is required to precisely represent an FM signal. The frequency spectrum of an actual FM signal has components extending infinitely, although their amplitude decreases and higher-order components are often neglected in practical design problems.

Mathematically, a baseband modulating signal may be approximated by a sinusoidal continuous wave signal with a frequency "f". This method is also named as single-tone modulation. The integral of such a signal is:

In this case, the expression for y(t) above simplifies to:

where the amplitude formula_11 of the modulating sinusoid is represented by the peak deviation formula_8 (see frequency deviation).

The harmonic distribution of a sine wave carrier modulated by such a sinusoidal signal can be represented with Bessel functions; this provides the basis for a mathematical understanding of frequency modulation in the frequency domain.

As in other modulation systems, the modulation index indicates by how much the modulated variable varies around its unmodulated level. It relates to variations in the carrier frequency:

where formula_14 is the highest frequency component present in the modulating signal "x"("t"), and formula_15 is the peak frequency-deviation—i.e. the maximum deviation of the "instantaneous frequency" from the carrier frequency. For a sine wave modulation, the modulation index is seen to be the ratio of the peak frequency deviation of the carrier wave to the frequency of the modulating sine wave.

If formula_16, the modulation is called narrowband FM (NFM), and its bandwidth is approximately formula_17. Sometimes modulation index formula_18 is considered as NFM, otherwise wideband FM (WFM or FM).

For digital modulation systems, for example binary frequency shift keying (BFSK), where a binary signal modulates the carrier, the modulation index is given by:

where formula_20 is the symbol period, and formula_21 is used as the highest frequency of the modulating binary waveform by convention, even though it would be more accurate to say it is the highest "fundamental" of the modulating binary waveform. In the case of digital modulation, the carrier formula_22 is never transmitted. Rather, one of two frequencies is transmitted, either formula_23 or formula_24, depending on the binary state 0 or 1 of the modulation signal.

If formula_25, the modulation is called "wideband FM" and its bandwidth is approximately formula_26. While wideband FM uses more bandwidth, it can improve the signal-to-noise ratio significantly; for example, doubling the value of formula_15, while keeping formula_28 constant, results in an eight-fold improvement in the signal-to-noise ratio. (Compare this with chirp spread spectrum, which uses extremely wide frequency deviations to achieve processing gains comparable to traditional, better-known spread-spectrum modes).

With a tone-modulated FM wave, if the modulation frequency is held constant and the modulation index is increased, the (non-negligible) bandwidth of the FM signal increases but the spacing between spectra remains the same; some spectral components decrease in strength as others increase. If the frequency deviation is held constant and the modulation frequency increased, the spacing between spectra increases.
Frequency modulation can be classified as narrowband if the change in the carrier frequency is about the same as the signal frequency, or as wideband if the change in the carrier frequency is much higher (modulation index > 1) than the signal frequency. For example, narrowband FM (NFM) is used for two-way radio systems such as Family Radio Service, in which the carrier is allowed to deviate only 2.5 kHz above and below the center frequency with speech signals of no more than 3.5 kHz bandwidth. Wideband FM is used for FM broadcasting, in which music and speech are transmitted with up to 75 kHz deviation from the center frequency and carry audio with up to a 20 kHz bandwidth and subcarriers up to 92 kHz.

For the case of a carrier modulated by a single sine wave, the resulting frequency spectrum can be calculated using Bessel functions of the first kind, as a function of the sideband number and the modulation index. The carrier and sideband amplitudes are illustrated for different modulation indices of FM signals. For particular values of the modulation index, the carrier amplitude becomes zero and all the signal power is in the sidebands.

Since the sidebands are on both sides of the carrier, their count is doubled, and then multiplied by the modulating frequency to find the bandwidth. For example, 3 kHz deviation modulated by a 2.2 kHz audio tone produces a modulation index of 1.36. Suppose that we limit ourselves to only those sidebands that have a relative amplitude of at least 0.01. Then, examining the chart shows this modulation index will produce three sidebands. These three sidebands, when doubled, gives us (6 × 2.2 kHz) or a 13.2 kHz required bandwidth.
A rule of thumb, "Carson's rule" states that nearly all (~98 percent) of the power of a frequency-modulated signal lies within a bandwidth formula_29 of:

where formula_31, as defined above, is the peak deviation of the instantaneous frequency formula_32 from the center carrier frequency formula_33, formula_34 is the Modulation index which is the ratio of frequency deviation to highest frequency in the modulating signal and formula_14is the highest frequency in the modulating signal.
Condition for application of Carson's rule is only sinusoidal signals.

where W is the highest frequency in the modulating signal but non-sinusoidal in nature and D is the Deviation ratio which the ratio of frequency deviation to highest frequency of modulating non-sinusoidal signal.

FM provides improved signal-to-noise ratio (SNR), as compared for example with AM. Compared with an optimum AM scheme, FM typically has poorer SNR below a certain signal level called the noise threshold, but above a higher level – the full improvement or full quieting threshold – the SNR is much improved over AM. The improvement depends on modulation level and deviation. For typical voice communications channels, improvements are typically 5–15 dB. FM broadcasting using wider deviation can achieve even greater improvements. Additional techniques, such as pre-emphasis of higher audio frequencies with corresponding de-emphasis in the receiver, are generally used to improve overall SNR in FM circuits. Since FM signals have constant amplitude, FM receivers normally have limiters that remove AM noise, further improving SNR.

FM signals can be generated using either direct or indirect frequency modulation:

Many FM detector circuits exist. A common method for recovering the information signal is through a Foster-Seeley discriminator or ratio detector. A phase-locked loop can be used as an FM demodulator. "Slope detection" demodulates an FM signal by using a tuned circuit which has its resonant frequency slightly offset from the carrier. As the frequency rises and falls the tuned circuit provides a changing amplitude of response, converting FM to AM. AM receivers may detect some FM transmissions by this means, although it does not provide an efficient means of detection for FM broadcasts.

FM is also used at intermediate frequencies by analog VCR systems (including VHS) to record the luminance (black and white) portions of the video signal. Commonly, the chrominance component is recorded as a conventional AM signal, using the higher-frequency FM signal as bias. FM is the only feasible method of recording the luminance ("black and white") component of video to (and retrieving video from) magnetic tape without distortion; video signals have a large range of frequency components – from a few hertz to several megahertz, too wide for equalizers to work with due to electronic noise below −60 dB. FM also keeps the tape at saturation level, acting as a form of noise reduction; a limiter can mask variations in playback output, and the FM capture effect removes print-through and pre-echo. A continuous pilot-tone, if added to the signal – as was done on V2000 and many Hi-band formats – can keep mechanical jitter under control and assist timebase correction.

These FM systems are unusual, in that they have a ratio of carrier to maximum modulation frequency of less than two; contrast this with FM audio broadcasting, where the ratio is around 10,000. Consider, for example, a 6-MHz carrier modulated at a 3.5-MHz rate; by Bessel analysis, the first sidebands are on 9.5 and 2.5 MHz and the second sidebands are on 13 MHz and −1 MHz. The result is a reversed-phase sideband on +1 MHz; on demodulation, this results in unwanted output at 6 – 1 = 5 MHz. The system must be designed so that this unwanted output is reduced to an acceptable level.

FM is also used at audio frequencies to synthesize sound. This technique, known as FM synthesis, was popularized by early digital synthesizers and became a standard feature in several generations of personal computer sound cards.

Edwin Howard Armstrong (1890–1954) was an American electrical engineer who invented wideband frequency modulation (FM) radio.
He patented the regenerative circuit in 1914, the superheterodyne receiver in 1918 and the super-regenerative circuit in 1922. Armstrong presented his paper, "A Method of Reducing Disturbances in Radio Signaling by a System of Frequency Modulation", (which first described FM radio) before the New York section of the Institute of Radio Engineers on November 6, 1935. The paper was published in 1936.

As the name implies, wideband FM (WFM) requires a wider signal bandwidth than amplitude modulation by an equivalent modulating signal; this also makes the signal more robust against noise and interference. Frequency modulation is also more robust against signal-amplitude-fading phenomena. As a result, FM was chosen as the modulation standard for high frequency, high fidelity radio transmission, hence the term "FM radio" (although for many years the BBC called it "VHF radio" because commercial FM broadcasting uses part of the VHF band—the FM broadcast band). FM receivers employ a special detector for FM signals and exhibit a phenomenon known as the "capture effect", in which the tuner "captures" the stronger of two stations on the same frequency while rejecting the other (compare this with a similar situation on an AM receiver, where both stations can be heard simultaneously). However, frequency drift or a lack of selectivity may cause one station to be overtaken by another on an adjacent channel. Frequency drift was a problem in early (or inexpensive) receivers; inadequate selectivity may affect any tuner.

An FM signal can also be used to carry a stereo signal; this is done with multiplexing and demultiplexing before and after the FM process. The FM modulation and demodulation process is identical in stereo and monaural processes. A high-efficiency radio-frequency switching amplifier can be used to transmit FM signals (and other constant-amplitude signals). For a given signal strength (measured at the receiver antenna), switching amplifiers use less battery power and typically cost less than a linear amplifier. This gives FM another advantage over other modulation methods requiring linear amplifiers, such as AM and QAM.

FM is commonly used at VHF radio frequencies for high-fidelity broadcasts of music and speech. Analog TV sound is also broadcast using FM. Narrowband FM is used for voice communications in commercial and amateur radio settings. In broadcast services, where audio fidelity is important, wideband FM is generally used. In two-way radio, narrowband FM (NBFM) is used to conserve bandwidth for land mobile, marine mobile and other radio services.

There are reports that on October 5, 1924, Professor Mikhail A. Bonch-Bruevich, during a scientific and technical conversation in the Nizhny Novgorod Radio Laboratory, reported about his new method of telephony, based on a change in the period of oscillations. Demonstration of frequency modulation was carried out on the laboratory model.




</doc>
<doc id="10837" url="https://en.wikipedia.org/wiki?curid=10837" title="Faith and rationality">
Faith and rationality

Faith and rationality are two ideologies that exist in varying degrees of conflict or compatibility. Rationality is based on reason or facts. Faith is belief in inspiration, revelation, or authority. The word "faith" sometimes refers to a belief that is held with lack of reason or evidence, a belief that is held in spite of or against reason or evidence, or it can refer to belief based upon a degree of evidential warrant.

Although the words "faith" and "belief" are sometimes erroneously conflated and used as synonyms, "faith" properly refers to a particular type (or subset) of "belief," as defined above.

Broadly speaking, there are two categories of views regarding the relationship between faith and rationality:

The Catholic Church also has taught that true faith and correct reason can and must work together, and, viewed properly, can never be in conflict with one another, as both have their origin in God, as stated in the Papal encyclical letter issued by Pope John Paul II, "Fides et ratio" ("[On] Faith and Reason").

From at least the days of the Greek philosophers, the relationship between faith and reason has been hotly debated. Plato argued that knowledge is simply memory of the eternal. Aristotle set down rules by which knowledge could be discovered by reason.

Rationalists point out that many people hold irrational beliefs, for many reasons. There may be evolutionary causes for irrational beliefs — irrational beliefs may increase our ability to survive and reproduce. Or, according to Pascal's Wager, it may be to our advantage to have faith, because faith may promise infinite rewards, while the rewards of reason are seen by many as finite. One more reason for irrational beliefs can perhaps be explained by operant conditioning. For example, in one study by B. F. Skinner in 1948, pigeons were awarded grain at regular time intervals regardless of their behaviour. The result was that each of the pigeons developed their own idiosyncratic response which had become associated with the consequence of receiving grain.

Believers in faith — for example those who believe salvation is possible through faith alone — frequently suggest that everyone holds beliefs arrived at by faith, not reason. The belief that the universe is a sensible place and that our minds allow us to arrive at correct conclusions about it, is a belief we hold through faith. Rationalists contend that this is arrived at because they have observed the world being consistent and sensible, not because they have faith that it is.

Beliefs held "by faith" may be seen existing in a number of relationships to rationality:

St. Thomas Aquinas, the most important doctor of the Catholic Church, was the first to write a full treatment of the relationship, differences, and similarities between faith—an intellectual assent—and reason, predominately in his "Summa Theologica", "De Veritate", and "Summa contra Gentiles".

The Council of Trent's catechism—the "Roman Catechism", written during the Catholic Church's Counter-Reformation to combat Protestantism and Martin Luther's antimetaphysical tendencies.

"Dei Filius" was a dogmatic constitution of the First Vatican Council on the Roman Catholic faith. It was adopted unanimously on 24 April 1870 and was influenced by the philosophical conceptions of Johann Baptist Franzelin, who had written a great deal on the topic of faith and rationality.

Because the Roman Catholic Church does not disparage reason, but rather affirms its veracity and utility, there have been many Catholic scientists over the ages.

Twentieth-century Thomist philosopher Étienne Gilson wrote about faith and reason in his 1922 book "Le Thomisme". His contemporary Jacques Maritain wrote about it in his "The Degrees of Knowledge".

"Fides et Ratio" is an encyclical promulgated by Pope John Paul II on 14 September 1998. It deals with the relationship between faith and reason.

Pope Benedict XVI's 12 September 2006 Regensburg Lecture was about faith and reason.

Martin Luther's Theology of the Cross was a critique of the use of reason in theology as used by some in the Catholic Church. Some have asserted that Martin Luther taught that faith and reason were antithetical in the sense that questions of faith could not be illuminated by reason. Contemporary Lutheran scholarship however has found a different reality in Luther. Luther rather seeks to separate faith and reason in order to honor the separate spheres of knowledge that each understand. Bernhard Lohse for example has demonstrated in his classic work "Fides Und Ratio" that Luther ultimately sought to put the two together. More recently Hans-Peter Großhans has demonstrated that Luther's work on Biblical Criticism stresses the need for external coherence in right exegetical method. This means that for Luther it is more important that the Bible be reasonable according to the reality outside of the scriptures than that the Bible make sense to itself, that it has internal coherence. The right tool for understanding the world outside of the Bible for Luther is none other than Reason which for Luther denoted science, philosophy, history and empirical observation. Here a differing picture is presented of a Luther who deeply valued both faith and reason, and held them in dialectical partnership. Luther's concern thus in separating them is honoring their different epistemological spheres.

The view that faith underlies all rationality holds that rationality is dependent on faith for its coherence. Under this view, there is no way to comprehensively "prove" that we are actually seeing what we appear to be seeing, that what we remember actually happened, or that the laws of logic and mathematics are actually real. Instead, all beliefs depend for their coherence on "faith" in our senses, memory, and reason, because the foundations of rationalism cannot be proven by evidence or reason. Rationally, you can not prove anything you see is real, but you can prove that you yourself are real, and rationalist belief would be that you can believe that the world is consistent until something demonstrates inconsistency. This differs from faith based belief, where you believe that your world view is consistent no matter what inconsistencies the world has with your beliefs.

In this view, there are many beliefs that are held by faith alone, that rational thought would force the mind to reject. As an example, many people believe in the Biblical story of Noah's flood: that the entire Earth was covered by water for forty days. But objected that most plants cannot survive being covered by water for that length of time, a boat of that magnitude could not have been built by wood, and there would be no way for two of every animal to survive on that ship and migrate back to their place of origin. (such as penguins), Although Christian apologists offer answers to these and such issues, under the premise that such responses are insufficient, then one must choose between accepting the story on faith and rejecting reason, or rejecting the story by reason and thus rejecting faith.

Within the rationalist point of view, there remains the possibility of multiple rational explanations. For example, considering the biblical story of Noah's flood, one making rational determinations about the probability of the events does so via interpretation of modern evidence. Two observers of the story may provide different plausible explanations for the life of plants, construction of the boat, species living at the time, and migration following the flood. Some see this as meaning that a person is not strictly bound to choose between faith and reason.

American biblical scholar Archibald Thomas Robertson stated that the Greek word "pistis" used for faith in the New Testament (over two hundred forty times), and rendered "assurance" in Acts 17:31 (KJV), is "an old verb to furnish, used regularly by Demosthenes for bringing forward evidence." Likewise Tom Price (Oxford Centre for Christian Apologetics) affirms that when the New Testament talks about faith positively it only uses words derived from the Greek root [pistis] which means "to be persuaded."

In contrast to faith meaning blind trust, in the absence of evidence, even in the teeth of evidence, Alister McGrath quotes Oxford Anglican theologian W. H. Griffith-Thomas, (1861-1924), who states faith is "not blind, but intelligent" and "commences with the conviction of the mind based on adequate evidence...", which McGrath sees as "a good and reliable definition, synthesizing the core elements of the characteristic Christian understanding of faith."

Alvin Plantinga upholds that faith may be the result of evidence testifying to the reliability of the source of truth claims, but although it may involve this, he sees faith as being the result of hearing the truth of the gospel with the internal persuasion by the Holy Spirit moving and enabling him to believe. "Christian belief is produced in the believer by the internal instigation of the Holy Spirit, endorsing the teachings of Scripture, which is itself divinely inspired by the Holy Spirit. The result of the work of the Holy Spirit is faith."

The 14th Century Jewish philosopher Levi ben Gerson tried to reconcile faith and reason. He wrote, "The Torah cannot prevent us from considering to be true that which our reason urges us to believe." His contemporary Hasdai ben Abraham Crescas argued the contrary view, that reason is weak and faith strong, and that only through faith can we discover the fundamental truth that God is love, that through faith alone can we endure the suffering that is the common lot of God's chosen people.






</doc>
<doc id="10839" url="https://en.wikipedia.org/wiki?curid=10839" title="List of film institutes">
List of film institutes

Some notable institutions celebrating film, including both national film institutes and independent and non-profit organizations. For the purposes of this list, institutions that do not have their own article on Wikipedia are not considered notable.



</doc>
<doc id="10841" url="https://en.wikipedia.org/wiki?curid=10841" title="Forth">
Forth

Forth or FORTH may refer to:








</doc>
<doc id="10842" url="https://en.wikipedia.org/wiki?curid=10842" title="F wave">
F wave

In neuroscience, an F wave is the second of two voltage changes observed after electrical stimulation is applied to the skin surface above the distal region of a nerve. F waves are often used to measure nerve conduction velocity, and are particularly useful for evaluating conduction problems in the proximal region of nerves (i.e., portions of nerves near the spinal cord).

The wave is termed the "F wave" due to its initial recording in the muscles of the foot.

In a typical F wave study, a strong electrical stimulus (supramaximal stimulation) is applied to the skin surface above the distal portion of a nerve so that the impulse travels both distally (towards the muscle fiber) and proximally (back to the motor neurons of the spinal cord). (These directions are also known as orthodromic and antidromic, respectively.) When the "orthodromic" stimulus reaches the muscle fiber, it elicits a strong M-response indicative of muscle contraction. When the "antidromic" stimulus reaches the motor neuron cell bodies, a small portion of the motor neurons backfire and orthodromic wave travels back down the nerve towards the muscle. This reflected stimulus evokes small proportion of the muscle fibers causing a small, second CMAP called the F wave.

Because a different population of anterior horn cells is stimulated with each stimulation, each F wave has a slightly different shape, amplitude, and latency.

F wave properties include:


Several measurements can be done on the F responses, including minimal and maximal latencies and F wave persistence.

The minimal F wave latency is typically 25-32 ms in the upper extremities and 45-56 ms in the lower extremities.

F wave persistence is the number of F waves obtained per the number of stimulations, which is normally 80-100% (or above 50%).




</doc>
<doc id="10843" url="https://en.wikipedia.org/wiki?curid=10843" title="Fruit">
Fruit

In botany, a fruit is the seed-bearing structure in flowering plants (also known as angiosperms) formed from the ovary after flowering.

Fruits are the means by which angiosperms disseminate seeds. Edible fruits, in particular, have propagated with the movements of humans and animals in a symbiotic relationship as a means for seed dispersal and nutrition; in fact, humans and many animals have become dependent on fruits as a source of food. Accordingly, fruits account for a substantial fraction of the world's agricultural output, and some (such as the apple and the pomegranate) have acquired extensive cultural and symbolic meanings.

In common language usage, "fruit" normally means the fleshy seed-associated structures of a plant that are sweet or sour, and edible in the raw state, such as apples, bananas, grapes, lemons, oranges, and strawberries. On the other hand, in botanical usage, "fruit" includes many structures that are not commonly called "fruits", such as bean pods, corn kernels, tomatoes, and wheat grains. The section of a fungus that produces spores is also called a fruiting body.

Many common terms for seeds and fruit do not correspond to the botanical classifications. In culinary terminology, a "fruit" is usually any sweet-tasting plant part, especially a botanical fruit; a "nut" is any hard, oily, and shelled plant product; and a "vegetable" is any savory or less sweet plant product. However, in botany, a "fruit" is the ripened ovary or carpel that contains seeds, a "nut" is a type of fruit and not a seed, and a "seed" is a ripened ovule.

Examples of culinary "vegetables" and nuts that are botanically fruit include corn, cucurbits (e.g., cucumber, pumpkin, and squash), eggplant, legumes (beans, peanuts, and peas), sweet pepper, and tomato. In addition, some spices, such as allspice and chili pepper, are fruits, botanically speaking. In contrast, rhubarb is often referred to as a fruit, because it is used to make sweet desserts such as pies, though only the petiole (leaf stalk) of the rhubarb plant is edible, and edible gymnosperm seeds are often given fruit names, e.g., ginkgo nuts and pine nuts.

Botanically, a cereal grain, such as corn, rice, or wheat, is also a kind of fruit, termed a caryopsis. However, the fruit wall is very thin and is fused to the seed coat, so almost all of the edible grain is actually a seed.

The outer, often edible layer, is the "pericarp", formed from the ovary and surrounding the seeds, although in some species other tissues contribute to or form the edible portion. The pericarp may be described in three layers from outer to inner, the "epicarp", "mesocarp" and "endocarp".

Fruit that bears a prominent pointed terminal projection is said to be "beaked".

A fruit results from maturation of one or more flowers, and the gynoecium of the flower(s) forms all or part of the fruit.

Inside the ovary/ovaries are one or more ovules where the megagametophyte contains the egg cell. After double fertilization, these ovules will become seeds. The ovules are fertilized in a process that starts with pollination, which involves the movement of pollen from the stamens to the stigma of flowers. After pollination, a tube grows from the pollen through the stigma into the ovary to the ovule and two sperm are transferred from the pollen to the megagametophyte. Within the megagametophyte one of the two sperm unites with the egg, forming a zygote, and the second sperm enters the central cell forming the endosperm mother cell, which completes the double fertilization process. Later the zygote will give rise to the embryo of the seed, and the endosperm mother cell will give rise to endosperm, a nutritive tissue used by the embryo.

As the ovules develop into seeds, the ovary begins to ripen and the ovary wall, the "pericarp", may become fleshy (as in berries or drupes), or form a hard outer covering (as in nuts). In some multiseeded fruits, the extent to which the flesh develops is proportional to the number of fertilized ovules. The pericarp is often differentiated into two or three distinct layers called the "exocarp" (outer layer, also called epicarp), "mesocarp" (middle layer), and "endocarp" (inner layer). In some fruits, especially simple fruits derived from an inferior ovary, other parts of the flower (such as the floral tube, including the petals, sepals, and stamens), fuse with the ovary and ripen with it. In other cases, the sepals, petals and/or stamens and style of the flower fall off. When such other floral parts are a significant part of the fruit, it is called an "accessory fruit". Since other parts of the flower may contribute to the structure of the fruit, it is important to study flower structure to understand how a particular fruit forms.

There are three general modes of fruit development:

Plant scientists have grouped fruits into three main groups, simple fruits, aggregate fruits, and composite or multiple fruits. The groupings are not evolutionarily relevant, since many diverse plant taxa may be in the same group, but reflect how the flower organs are arranged and how the fruits develop.

Simple fruits can be either dry or fleshy, and result from the ripening of a simple or compound ovary in a flower with only one pistil. Dry fruits may be either dehiscent (they open to discharge seeds), or indehiscent (they do not open to discharge seeds). Types of dry, simple fruits, and examples of each, include:

Fruits in which part or all of the "pericarp" (fruit wall) is fleshy at maturity are "simple fleshy fruits". Types of simple, fleshy, fruits (with examples) include:
An aggregate fruit, or "etaerio", develops from a single flower with numerous simple pistils.

The pome fruits of the family Rosaceae, (including apples, pears, rosehips, and saskatoon berry) are a syncarpous fleshy fruit, a simple fruit, developing from a half-inferior ovary.

Schizocarp fruits form from a syncarpous ovary and do not really dehisce, but rather split into segments with one or more seeds; they include a number of different forms from a wide range of families. Carrot seed is an example.
Aggregate fruits form from single flowers that have multiple carpels which are not joined together, i.e. each pistil contains one carpel. Each pistil forms a fruitlet, and collectively the fruitlets are called an etaerio. Four types of aggregate fruits include etaerios of achenes, follicles, drupelets, and berries. Ranunculaceae species, including "Clematis" and "Ranunculus" have an etaerio of achenes, "Calotropis" has an etaerio of follicles, and "Rubus" species like raspberry, have an etaerio of drupelets. "Annona" have an etaerio of berries.

The raspberry, whose pistils are termed "drupelets" because each is like a small drupe attached to the receptacle. In some bramble fruits (such as blackberry) the receptacle is elongated and part of the ripe fruit, making the blackberry an "aggregate-accessory" fruit. The strawberry is also an aggregate-accessory fruit, only one in which the seeds are contained in achenes. In all these examples, the fruit develops from a single flower with numerous pistils.

A multiple fruit is one formed from a cluster of flowers (called an "inflorescence"). Each flower produces a fruit, but these mature into a single mass. Examples are the pineapple, fig, mulberry, osage-orange, and breadfruit.
In the photograph on the right, stages of flowering and fruit development in the noni or Indian mulberry ("Morinda citrifolia") can be observed on a single branch. First an inflorescence of white flowers called a head is produced. After fertilization, each flower develops into a drupe, and as the drupes expand, they become "connate" (merge) into a "multiple fleshy fruit" called a "syncarp".

Berries are another type of fleshy fruit; they are simple fruit created from a single ovary. The ovary may be compound, with several carpels. Types include (examples follow in the table below):

Some or all of the edible part of accessory fruit is not generated by the ovary. Accessory fruit can be simple, aggregate, or multiple, i.e., they can include one or more pistils and other parts from the same flower, or the pistils and other parts of many flowers.

Seedlessness is an important feature of some fruits of commerce. Commercial cultivars of bananas and pineapples are examples of seedless fruits. Some cultivars of citrus fruits (especially grapefruit, mandarin oranges, navel oranges), satsumas, table grapes, and watermelons are valued for their seedlessness. In some species, seedlessness is the result of "parthenocarpy", where fruits set without fertilization. Parthenocarpic fruit set may or may not require pollination, but most seedless citrus fruits require a stimulus from pollination to produce fruit.

Seedless bananas and grapes are triploids, and seedlessness results from the abortion of the embryonic plant that is produced by fertilization, a phenomenon known as "stenospermocarpy", which requires normal pollination and fertilization.

Variations in fruit structures largely depend on their seeds' mode of dispersal. This dispersal can be achieved by animals, explosive dehiscence, water, or wind.

Some fruits have coats covered with spikes or hooked burrs, either to prevent themselves from being eaten by animals, or to stick to the feathers, hairs, or legs of animals, using them as dispersal agents. Examples include cocklebur and unicorn plant.

The sweet flesh of many fruits is "deliberately" appealing to animals, so that the seeds held within are eaten and "unwittingly" carried away and deposited (i.e., defecated) at a distance from the parent. Likewise, the nutritious, oily kernels of nuts are appealing to rodents (such as squirrels), which hoard them in the soil to avoid starving during the winter, thus giving those seeds that remain uneaten the chance to germinate and grow into a new plant away from their parent.

Other fruits are elongated and flattened out naturally, and so become thin, like wings or helicopter blades, e.g., elm, maple, and tuliptree. This is an evolutionary mechanism to increase dispersal distance away from the parent, via wind. Other wind-dispersed fruit have tiny "parachutes", e.g., dandelion, milkweed, salsify.

Coconut fruits can float thousands of miles in the ocean to spread seeds. Some other fruits that can disperse via water are nipa palm and screw pine.

Some fruits fling seeds substantial distances (up to 100 m in sandbox tree) via explosive dehiscence or other mechanisms, e.g., impatiens and squirting cucumber.

Many hundreds of fruits, including fleshy fruits (like apple, kiwifruit, mango, peach, pear, and watermelon) are commercially valuable as human food, eaten both fresh and as jams, marmalade and other preserves. Fruits are also used in manufactured foods (e.g., cakes, cookies, ice cream, muffins, or yogurt) or beverages, such as fruit juices (e.g., apple juice, grape juice, or orange juice) or alcoholic beverages (e.g., brandy, fruit beer, or wine). Fruits are also used for gift giving, e.g., in the form of Fruit Baskets and Fruit Bouquets.

Many "vegetables" in culinary "parlance" are botanical fruits, including bell pepper, cucumber, eggplant, green bean, okra, pumpkin, squash, tomato, and zucchini. Olive fruit is pressed for olive oil. Spices like allspice, black pepper, paprika, and vanilla are derived from berries.

Fresh fruits are generally high in fiber, vitamin C, and water.

Regular consumption of fruit is generally associated with reduced risks of several diseases and functional declines associated with aging.

Because fruits have been such a major part of the human diet, various cultures have developed many different uses for fruits they do not depend on for food. For example:

For food safety, the CDC recommends proper fruit handling and preparation to reduce the risk of food contamination and foodborne illness. Fresh fruits and vegetables should be carefully selected; at the store, they should not be damaged or bruised; and precut pieces should be refrigerated or surrounded by ice.

All fruits and vegetables should be rinsed before eating. This recommendation also applies to produce with rinds or skins that are not eaten. It should be done just before preparing or eating to avoid premature spoilage.

Fruits and vegetables should be kept separate from raw foods like meat, poultry, and seafood, as well as from utensils that have come in contact with raw foods. Fruits and vegetables that are not going to be cooked should be thrown away if they have touched raw meat, poultry, seafood, or eggs.

All cut, peeled, or cooked fruits and vegetables should be refrigerated within two hours. After a certain time, harmful bacteria may grow on them and increase the risk of foodborne illness.

Fruit allergies make up about 10 percent of all food related allergies.

All fruits benefit from proper post harvest care, and in many fruits, the plant hormone ethylene causes ripening. Therefore, maintaining most fruits in an efficient cold chain is optimal for post harvest storage, with the aim of extending and ensuring shelf life.





</doc>
<doc id="10844" url="https://en.wikipedia.org/wiki?curid=10844" title="French materialism">
French materialism

French materialism is the name given to a handful of French 18th-century philosophers during the Age of Enlightenment, many of them clustered around the salon of Baron d'Holbach. Although there are important differences between them, all of them were materialists who believed that the world was made up of a single substance, matter, the motions and properties of which could be used to explain all phenomena. 

Prominent French materialists of the 18th century include:





</doc>
<doc id="10845" url="https://en.wikipedia.org/wiki?curid=10845" title="February">
February

February is the second and shortest month of the year in the Julian and Gregorian calendar with 28 days in common years and 29 days in leap years, with the quadrennial 29th day being called the "leap day". It is the first of five months to have a length of fewer than 31 days (the other four months that fall under this category are: April, June, September, and November), and the only month to have a length of fewer than 30 days, with the other seven months having 31 days. In 2019, February 28 days.

February is the third and last month of meteorological winter in the Northern Hemisphere. In the Southern Hemisphere, February is the third and last month of summer (the seasonal equivalent of August in the Northern Hemisphere, in meteorological reckoning).

February is pronounced either as or . Many people drop the first "r", replacing it with , as if it were spelled "Febuary". This comes about by analogy with "January" (), as well as by a dissimilation effect whereby having two "r"s close to each other causes one to change for ease of pronunciation.

The Roman month "Februarius" was named after the Latin term "februum", which means "purification", via the purification ritual "Februa" held on February 15 (full moon) in the old lunar Roman calendar. January and February were the last two months to be added to the Roman calendar, since the Romans originally considered winter a monthless period. They were added by Numa Pompilius about 713 BC. February remained the last month of the calendar year until the time of the decemvirs (c. 450 BC), when it became the second month. At certain intervals February was truncated to 23 or 24 days, and a 27-day intercalary month, Intercalaris, was inserted immediately after February to realign the year with the seasons.

February observances in Ancient Rome include Amburbium (precise date unknown), Sementivae (February 2), Februa (February 13–15), Lupercalia (February 13–15), Parentalia (February 13–22), Quirinalia (February 17), Feralia (February 21), Caristia (February 22), Terminalia (February 23), Regifugium (February 24), and Agonium Martiale (February 27). These days do not correspond to the modern Gregorian calendar.

Under the reforms that instituted the Julian calendar, Intercalaris was abolished, leap years occurred regularly every fourth year, and in leap years February gained a 29th day. Thereafter, it remained the second month of the calendar year, meaning the order that months are displayed (January, February, March, ..., December) within a year-at-a-glance calendar. Even during the Middle Ages, when the numbered Anno Domini year began on March 25 or December 25, the second month was February whenever all twelve months were displayed in order. The Gregorian calendar reforms made slight changes to the system for determining which years were leap years and thus contained a 29-day February.

Historical names for February include the Old English terms Solmonath (mud month) and Kale-monath (named for cabbage) as well as Charlemagne's designation Hornung. In Finnish, the month is called "helmikuu", meaning "month of the pearl"; when snow melts on tree branches, it forms droplets, and as these freeze again, they are like pearls of ice. In Polish and Ukrainian, respectively, the month is called "luty" or "лютий", meaning the month of ice or hard frost. In Macedonian the month is "sechko" (сечко), meaning month of cutting [wood]. In Czech, it is called "únor", meaning month of submerging [of river ice].

In Slovene, February is traditionally called "svečan", related to icicles or Candlemas. This name originates from "sičan", written as "svičan" in the "New Carniolan Almanac" from 1775 and changed to its final form by Franc Metelko in his "New Almanac" from 1824. The name was also spelled "sečan", meaning "the month of cutting down of trees".

In 1848, a proposal was put forward in "Kmetijske in rokodelske novice" by the Slovene Society of Ljubljana to call this month "talnik" (related to ice melting), but it did not stick. The idea was proposed by a priest, Blaž Potočnik. Another name of February in Slovene was "vesnar", after the mythological character Vesna.

Having only 28 days in common years, February is the only month of the year that can pass without a single full moon. Using Coordinated Universal Time as the basis for determining the date and time of a full moon, this last happened in 2018 and will next happen in 2037. The same is true regarding a new moon: again using Coordinated Universal Time as the basis, this last happened in 2014 and will next happen in 2033.

February is also the only month of the calendar that, once every six years and twice every 11 years consecutively, either back into the past or forward into the future, has four full 7-day weeks. In countries that start their week on a Monday, it occurs as part of a common year starting on Friday, in which February 1st is a Monday and the 28th is a Sunday; this occurred in 1965, 1971, 1982, 1993, 1999 and 2010, and occur will again in 2021. In countries that start their week on a Sunday, it occurs in a common year starting on Thursday, with the next occurrence in 2026, and previous occurrences in 1987, 1998, 2009 and 2015. The pattern is broken by a skipped leap year, but no leap year has been skipped since 1900 and no others will be skipped until 2100.

February meteor showers include the Alpha Centaurids (appearing in early February), the Beta Leonids, also known as the March Virginids (lasting from February 14 to April 25, peaking around March 20), the Delta Cancrids (appearing December 14 to February 14, peaking on January 17), the Omicron Centaurids (late January through February, peaking in mid-February), Theta Centaurids (January 23 – March 12, only visible in the southern hemisphere), Eta Virginids (February 24 and March 27, peaking around March 18), and Pi Virginids (February 13 and April 8, peaking between March 3 and March 9).

The western zodiac signs of February are Aquarius (until February 19) and Pisces (February 20 onwards).


"This list does not necessarily imply either official status nor general observance."


"This list does not necessarily imply either official status or general observance. Please note that all Baha'i, Islamic, and Jewish observances begin at the sundown prior to the date listed, and end at sundown of the date in question unless otherwise noted."

First Friday: February 1 
First Saturday: February 2
First Sunday: February 3 
First Monday: February 4
First Week of February (first Monday, ending on Sunday): February 4–10
Second Saturday: February 9
Second Sunday: February 10
11th Sunday before Pascha (Eastern Christianity): February 10 
Second Monday: February 11
Second Tuesday: February 12 
Third Friday: February 15
63 days before Pascha (Eastern Christianity): February 16

10th Sunday before Pascha in Eastern Christianity: February 17 
Week of February 22: February 17–23 
Third Monday: February 18 
2nd Monday before Clean Monday in Eastern Christianity and the following three days: February 18–21 
Third Thursday: February 21
Last Friday: February 22
Last Saturday: February 23 
2nd Sunday before Ash Wednesday (Western Christianity): February 24 
9th Sunday before Easter in Western Christianity: February 24 
Sunday before Ash Wednesday (Western Christianity): February 24 
9th Sunday before Pascha (Eastern Christianity): February 24 
Monday before Ash Wednesday (Western Christianity): February 25 
Tuesday before Ash Wednesday: February 26 
Last Tuesday: February 26
Last Thursday before Lent (Western Christianity): February 28 
Thursday of the 8th week before Pascha (Eastern Christianity): February 28
Last day of February: February 28





</doc>
<doc id="10846" url="https://en.wikipedia.org/wiki?curid=10846" title="February 1">
February 1






</doc>
<doc id="10847" url="https://en.wikipedia.org/wiki?curid=10847" title="First Lady of the United States">
First Lady of the United States

First Lady of the United States (FLOTUS) is the title held by the hostess of the White House, usually the wife of the president of the United States, concurrent with the president's term in office. Although the first lady's role has never been codified or officially defined, she figures prominently in the political and social life of the nation. Since the early 20th century, the first lady has been assisted by official staff, now known as the Office of the First Lady and headquartered in the East Wing of the White House. 

Melania Trump is the current first lady of the United States, as wife of 45th president of the United States, Donald J. Trump.

While the title was not in general use until much later, Martha Washington, the wife of George Washington, the first U.S. president (1789–1797), is considered to be the inaugural first lady of the United States. During her lifetime, she was often referred to as "Lady Washington".

Since the 1790s, the role of first lady has changed considerably. It has come to include involvement in political campaigns, management of the White House, championship of social causes, and representation of the president at official and ceremonial occasions. Because first ladies now typically publish their memoirs, which are viewed as potential sources of additional information about their husbands' administrations, and because the public is interested in these increasingly independent women in their own right, first ladies frequently remain a focus of attention long after their husbands' terms of office have ended. Additionally, over the years individual first ladies have held influence in a range of sectors, from fashion to public opinion on policy. Historically, should a president be unmarried, or a widower, the president usually asks a relative or friend to act as White House hostess.

There are four living former first ladies: Rosalynn Carter, wife of Jimmy Carter; Hillary Clinton, wife of Bill Clinton; Laura Bush; wife of George W Bush and Michelle Obama, wife of Barack Obama. , the only former first lady who has run for or held public office is Hillary Clinton.

The use of the title "First Lady" to describe the spouse or hostess of an executive began in the United States. In the early days of the republic, there was not a generally accepted title for the wife of the president. Many early first ladies expressed their own preference for how they were addressed, including the use of such titles as "Lady", "Mrs. President" and "Mrs. Presidentress"; Martha Washington was often referred to as "Lady Washington." One of the earliest uses of the term "First Lady" was applied to her in an 1838 newspaper article that appeared in the "St. Johnsbury Caledonian", the author, "Mrs. Sigourney", discussing how Martha Washington had not changed, even after her husband George became president. She wrote that "The first lady of the nation still preserved the habits of early life. Indulging in no indolence, she left the pillow at dawn, and after breakfast, retired to her chamber for an hour for the study of the scriptures and devotion".

Dolley Madison was reportedly referred to as "First Lady" in 1849 at her funeral in a eulogy delivered by President Zachary Taylor; however, no written record of this eulogy exists, nor did any of the newspapers of her day refer to her by that title. Sometime after 1849, the title began being used in Washington, D.C., social circles. One of the earliest known written examples comes from November 3, 1863, diary entry of William Howard Russell, in which he referred to gossip about "the First Lady in the Land", referring to Mary Todd Lincoln. The title first gained nationwide recognition in 1877, when newspaper journalist Mary C. Ames referred to Lucy Webb Hayes as "the First Lady of the Land" while reporting on the inauguration of Rutherford B. Hayes. The frequent reporting on Lucy Hayes' activities helped spread use of the title outside Washington. A popular 1911 comedic play about Dolley Madison by playwright Charles Nirdlinger, titled "The First Lady in the Land", popularized the title further. By the 1930s, it was in wide use. Use of the title later spread from the United States to other nations.

When Edith Wilson took control of her husband's schedule in 1919 after he had a debilitating stroke, one Republican senator labeled her "the Presidentress who had fulfilled the dream of the suffragettes by changing her title from First Lady to Acting First Man."

The wife of the vice president of the United States is sometimes referred to as the second lady of the United States (SLOTUS), but this title is much less common.

Several women (at least thirteen) who were not presidents' wives have served as first lady, as when the president was a bachelor or widower, or when the wife of the president was unable to fulfill the duties of the first lady herself. In these cases, the position has been filled by a female relative or friend of the president, such as Jefferson's daughter Martha Jefferson Randolph, Jackson's daughter-in-law Sarah Yorke Jackson and his wife's niece Emily Donelson, Taylor's daughter Mary Elizabeth Bliss, Benjamin Harrison's daughter Mary Harrison McKee, Buchanan's niece Harriet Lane, and Cleveland's sister Rose Cleveland.

The position of the first lady is not an elected one and carries only ceremonial duties. Nonetheless, first ladies have held a highly visible position in American society. The role of the first lady has evolved over the centuries. She is, first and foremost, the hostess of the White House. She organizes and attends official ceremonies and functions of state either along with, or in place of, the president. Lisa Burns identifies four successive main themes of the first ladyship: as public woman (1900–1929); as political celebrity (1932–1961); as political activist (1964–1977); and as political interloper (1980–2001).

Martha Washington created the role and hosted many affairs of state at the national capital (New York and Philadelphia). This socializing became known as "the Republican Court" and provided elite women with opportunities to play backstage political roles. Both Martha Washington and Abigail Adams were treated as if they were "ladies" of the British royal court.

Dolley Madison popularized the first ladyship by engaging in efforts to assist orphans and women, by dressing in elegant fashions and attracting newspaper coverage, and by risking her life to save iconic treasures during the War of 1812. Madison set the standard for the ladyship and her actions were the model for nearly every first lady until Eleanor Roosevelt in the 1930s. Roosevelt traveled widely and spoke to many groups, often voicing personal opinions to the left of the president's. She authored a weekly newspaper column and hosted a radio show. Jacqueline Kennedy led an effort to redecorate and restore the White House.

Many first ladies became significant fashion trendsetters. Some have exercised a degree of political influence by virtue of being an important adviser to the president.

Over the course of the 20th century, it became increasingly common for first ladies to select specific causes to promote, usually ones that are not politically divisive. It is common for the first lady to hire a staff to support these activities. Lady Bird Johnson pioneered environmental protection and beautification. Pat Nixon encouraged volunteerism and traveled extensively abroad; Betty Ford supported women's rights; Rosalynn Carter aided those with mental disabilities; Nancy Reagan founded the Just Say No drug awareness campaign; Barbara Bush promoted literacy; Hillary Clinton sought to reform the healthcare system in the U.S.; Laura Bush supported women's rights groups, and encouraged childhood literacy. Michelle Obama became identified with supporting military families and tackling childhood obesity; and Melania Trump has stated that she wants to use her position to help children, including prevention of cyberbullying and supporting children whose lives are affected by drugs.

Near the end of her husband's presidency, Clinton became the first first lady to run for political office. During the campaign, her daughter, Chelsea, took over much of the first lady's role. Victorious, Clinton served as U.S. Senator from New York from 2001 to 2009, when she resigned in order to become President Obama's Secretary of State until 2013. Clinton was the Democratic Party nominee for president in the 2016 election, but lost to Donald Trump.

The Office of the First Lady of the United States is accountable to the first lady for her to carry out her duties as hostess of the White House, and is also in charge of all social and ceremonial events of the White House. The first lady has her own staff that includes a chief of staff, press secretary, White House Social Secretary, and Chief Floral Designer. The Office of the First Lady is an entity of the White House Office, a branch of the Executive Office of the President. When First Lady Hillary Clinton decided to pursue a run for Senator of New York, she set aside her duties as first lady and moved to Chappaqua, New York to establish state residency. She resumed her duties as first lady after winning her senatorial campaign, and retained her duties as both first lady and U.S. Senator for the seventeen-day overlap before Bill Clinton's term came to an end.

Despite the significant responsibilities usually handled by the first lady, the first lady does not receive a salary. This has been criticized by both Ronald Reagan and Barack Obama.

Established in 1912, the First Ladies Collection has been one of the most popular attractions at the Smithsonian Institution. The original exhibition opened in 1914 and was one of the first at the Smithsonian to prominently feature women. Originally focused largely on fashion, the exhibition now delves deeper into the contributions of first ladies to the presidency and American society. In 2008, "First Ladies at the Smithsonian" opened at the National Museum of American History as part of its reopening year celebration. That exhibition served as a bridge to the museum's expanded exhibition on first ladies' history that opened on November 19, 2011. "The First Ladies" explores the unofficial but important position of first lady and the ways that different women have shaped the role to make their own contributions to the presidential administrations and the nation. The exhibition features 26 dresses and more than 160 other objects, ranging from those of Martha Washington to Michelle Obama, and includes White House china, personal possessions and other objects from the Smithsonian's unique collection of first ladies' materials.

Some first ladies have garnered attention for their dress and style. Jacqueline Kennedy Onassis, for instance, became a global fashion icon: her style was copied by commercial manufacturers and imitated by many young women, and she was named to the International Best Dressed List Hall of Fame in 1965. Michelle Obama has also received significant attention for her fashion choices: style writer Robin Givhan praised her in "The Daily Beast", arguing that the First Lady's style has helped to enhance the public image of the office.

, there are four living former first ladies, as identified below.
The most recent first lady to die was Barbara Bush (served 1989–1993), on April 17, 2018, at the age of 92. The greatest number of former first ladies to be alive at one time was ten, during the period from June 2, 1886 to August 23, 1887, when Sarah Yorke Jackson, Priscilla Cooper Tyler, Julia Gardiner Tyler, Sarah Childress Polk, Harriet Lane, Julia Grant, Lucy Webb Hayes, Lucretia Garfield, Mary Arthur McElroy, and Rose Cleveland were all alive and the period from March 4 to June 25, 1889, when Priscilla Cooper Tyler, Julia Gardiner Tyler, Sarah Childress Polk, Harriet Lane, Julia Grant, Lucy Webb Hayes, Lucretia Garfield, Mary Arthur McElroy, Rose Cleveland, and Frances Folsom Cleveland Preston were alive.





</doc>
<doc id="10852" url="https://en.wikipedia.org/wiki?curid=10852" title="Frank Herbert">
Frank Herbert

Franklin Patrick Herbert Jr. (October 8, 1920 – February 11, 1986) was an American science fiction writer best known for the 1965 novel "Dune" and its five sequels. Though he became famous for his long novels of fantasy, he was also a newspaper journalist, photographer, short story writer, book reviewer, ecological consultant, and lecturer.

The "Dune" saga, set in the distant future, and taking place over millennia, deals with complex themes, such as human survival and evolution, ecology, and the intersection of religion, politics, and power. "Dune" is the best-selling science fiction novel of all time, and the whole series is widely considered to be among the classics of the genre.

Frank Herbert was born on October 8, 1920, in Tacoma, Washington, to Frank Patrick Herbert Sr. and Eileen (McCarthy) Herbert. Because of a poor home environment, he ran away from home in 1938 to live with an aunt and uncle in Salem, Oregon. He enrolled in high school at Salem High School (now North Salem High School), where he graduated the next year. In 1939 he lied about his age to get his first newspaper job at the "Glendale Star". Herbert then returned to Salem in 1940 where he worked for the "Oregon Statesman" newspaper (now "Statesman Journal") in a variety of positions, including photographer.

He served in the U.S. Navy's Seabees for six months as a photographer during World War II, then he was given a medical discharge. He married Flora Parkinson in San Pedro, California, in 1940. They had a daughter, Penny (b. February 16, 1942), but divorced in 1945.

After the war, Herbert attended the University of Washington, where he met Beverly Ann Stuart at a creative writing class in 1946. They were the only students who had sold any work for publication; Herbert had sold two pulp adventure stories to magazines, the first to "Esquire" in 1945, and Stuart had sold a story to "Modern Romance" magazine. They married in Seattle, Washington on June 20, 1946, and had two sons, Brian Patrick Herbert (b. June 29, 1947, Seattle, Washington) and Bruce Calvin Herbert (b. June 26, 1951, Santa Rosa, California d. June 15, 1993, San Rafael, California, a professional photographer and gay rights activist.)

In 1949 Herbert and his wife moved to California to work on the Santa Rosa "Press-Democrat". Here they befriended the psychologists Ralph and Irene Slattery. The Slatterys introduced Herbert to the work of several thinkers who would influence his writing, including Freud, Jung, Jaspers and Heidegger; they also familiarized Herbert with Zen Buddhism.

Herbert did not graduate from the university; according to his son Brian, he wanted to study only what interested him and so did not complete the required curriculum. He returned to journalism and worked at the "Seattle Star" and the "Oregon Statesman". He was a writer and editor for the "San Francisco Examiner's" California Living magazine for a decade.

In a 1973 interview, Herbert stated that he had been reading science fiction "about ten years" before he
began writing in the genre, and he listed his favorite authors as H. G. Wells, Robert A. Heinlein, Poul Anderson and Jack Vance.

Herbert's first science fiction story, "Looking for Something", was published in the April 1952 issue of "Startling Stories", then a monthly edited by Samuel Mines. Three more of his stories appeared in 1954 issues of "Astounding Science Fiction" and "Amazing Stories". His career as a novelist began in 1955 with the serial publication of "Under Pressure" in "Astounding" from November 1955; afterward it was issued as a book by Doubleday, "The Dragon in the Sea". The story explored sanity and madness in the environment of a 21st-century submarine and predicted worldwide conflicts over oil consumption and production. It was a critical success but not a major commercial one. During this time Herbert also worked as a speechwriter for Republican senator Guy Cordon.

Herbert began researching "Dune" in 1959. He was able to devote himself wholeheartedly to his writing career because his wife returned to work full-time as an advertising writer for department stores, becoming the breadwinner during the 1960s. He later told Willis E. McNelly that the novel originated when he was supposed to do a magazine article on sand dunes in the Oregon Dunes near Florence, Oregon. He became too involved and ended up with far more raw material than needed for an article. The article was never written, but instead planted the seed that led to "Dune".

"Dune" took six years of research and writing to complete and it was much longer than commercial science fiction of the time was supposed to be. "Analog" (the renamed "Astounding", still edited by John W. Campbell) published it in two parts comprising eight installments, "Dune World" from December 1963 and "Prophet of Dune" in 1965. It was then rejected by nearly twenty book publishers. One editor prophetically wrote, "I might be making the mistake of the decade, but ...".

Sterling E. Lanier, an editor of Chilton Book Company (known mainly for its auto-repair manuals) had read the Dune serials and offered a $7,500 advance plus future royalties for the rights to publish them as a hardcover book. Herbert rewrote much of his text. "Dune" was soon a critical success. It won the Nebula Award for Best Novel in 1965 and shared the Hugo Award in 1966 with "...And Call Me Conrad" by Roger Zelazny. "Dune" was the first major ecological science fiction novel, embracing a multitude of sweeping, interrelated themes and multiple character viewpoints, a method that ran through all Herbert's mature work.

"Dune" was not an immediate bestseller. By 1968 Herbert had made $20,000 from it, far more than most science fiction novels of the time were generating, but not enough to let him take up full-time writing. However, the publication of "Dune" did open doors for him. He was the "Seattle Post-Intelligencer's" education writer from 1969 to 1972 and lecturer in general studies and interdisciplinary studies at the University of Washington (1970–1972). He worked in Vietnam and Pakistan as a social and ecological consultant in 1972. In 1973 he was director-photographer of the television show "The Tillers".

By the end of 1972, Herbert had retired from newspaper writing and become a full-time fiction writer. During the 1970s and 1980s, Herbert enjoyed considerable commercial success as an author. He divided his time between homes in Hawaii and Washington's Olympic Peninsula; his home in Port Townsend on the peninsula was intended to be an "ecological demonstration project". During this time he wrote numerous books and pushed ecological and philosophical ideas. He continued his "Dune" saga, following it with "Dune Messiah", "Children of Dune", and "God Emperor of Dune". Other highlights were "The Dosadi Experiment", "The Godmakers", "The White Plague" and the books he wrote in partnership with Bill Ransom: "The Jesus Incident", "The Lazarus Effect", and "The Ascension Factor" which were sequels to "". He also helped launch the career of Terry Brooks with a very positive review of Brooks' first novel, "The Sword of Shannara", in 1977.

Herbert's change in fortune was shadowed by tragedy. In 1974, Beverly underwent an operation for cancer. She lived ten more years, but her health was adversely affected by the surgery. During this period, Herbert was the featured speaker at the Octocon II science fiction convention held at the El Rancho Tropicana in Santa Rosa, California, in October 1978. In 1979, he met anthropologist James Funaro with whom he conceived the Contact Conference. Beverly Herbert died on February 7, 1984, the same year that "Heretics of Dune" was published; in his afterword to 1985's "", Frank Herbert wrote a eulogy for her.

In 1983, British heavy metal band Iron Maiden requested permission from Herbert's publisher to name a song on their album "Piece of Mind" after "Dune", but were told that the author had a strong distaste for their style of music. They instead titled the song "To Tame a Land".

1984 was a tumultuous year in Herbert's life. During this same year of his wife's death, his career took off with the release of David Lynch's film version of "Dune". Despite high expectations, a big-budget production design and an A-list cast, the movie drew mostly poor reviews in the United States. However, despite a disappointing response in the US, the film was a critical and commercial success in Europe and Japan.

After Beverly's death, Herbert married Theresa Shackleford in 1985, the year he published "Chapterhouse: Dune", which tied up many of the saga's story threads. This would be Herbert's final single work (the collection "Eye" was published that year, and "Man of Two Worlds" was published in 1986). He died of a massive pulmonary embolism while recovering from surgery for pancreatic cancer on February 11, 1986, in Madison, Wisconsin, age 65.

Herbert was a strong critic of the Soviet Union. He was a distant relative of the controversial Republican senator, Joseph McCarthy, whom he referred to as "Cousin Joe." Herbert was appalled to learn of McCarthy's blacklisting of suspected Communists from working in certain careers and believed that he was endangering essential freedoms of citizens of the United States. Herbert believed that governments lie to protect themselves and that, following the infamous Watergate scandal, President Richard Nixon had unwittingly taught an important lesson in not trusting government. Herbert also opposed American involvement in the US war in Vietnam. 

In "Chapterhouse: Dune", he wrote:

Frank Herbert used his science fiction novels to explore complex ideas involving philosophy, religion, psychology, politics and ecology. The underlying thrust of his work was a fascination with the question of human survival and evolution. Herbert has attracted a sometimes fanatical fan base, many of whom have tried to read everything he wrote, fiction or non-fiction, and see Herbert as something of an authority on the subject matters of his books. Indeed, such was the devotion of some of his readers that Herbert was at times asked if he was founding a cult, something he was very much against.

There are a number of key themes in Herbert's work:

Frank Herbert refrained from offering his readers formulaic answers to many of the questions he explored.

"Dune" and the "Dune" saga constitute one of the world's best-selling science fiction series and novels; "Dune" in particular has received widespread critical acclaim, winning the Nebula Award in 1965 and sharing the Hugo Award in 1966, and is frequently considered one of the best science fiction novels ever, if not the best. "Locus" subscribers voted it the all-time best SF novel in 1975, again in 1987, and the best "before 1990" in 1998.

"Dune" is considered a landmark novel for a number of reasons:


Herbert never again equalled the critical acclaim he received for "Dune". Neither his sequels to "Dune" nor any of his other books won a Hugo or Nebula Award, although almost all of them were "New York Times" Best Sellers.

Malcolm Edwards in the "Encyclopedia of Science Fiction" wrote:

Much of Herbert's work makes difficult reading. His ideas were genuinely developed concepts, not merely decorative notions, but they were sometimes embodied in excessively complicated plots and articulated in prose which did not always match the level of thinking [...] His best novels, however, were the work of a speculative intellect with few rivals in modern science fiction. 

The Science Fiction Hall of Fame inducted Herbert in 2006.

California State University, Fullerton's Pollack Library has several of Herbert's draft manuscripts of "Dune" and other works, with the author's notes, in their Frank Herbert Archives.

Beginning in 2012, Herbert's estate and WordFire Press have released four previously unpublished novels in e-book and paperback formats: "High-Opp" (2012), "Angels' Fall" (2013), "A Game of Authors" (2013), and "A Thorn in the Bush" (2014).

In recent years, Frank Herbert's son Brian Herbert and author Kevin J. Anderson have added to the "Dune" franchise, using notes left behind by Frank Herbert and discovered over a decade after his death. Brian Herbert and Anderson have written two prequel trilogies ("Prelude to Dune" and "Legends of Dune") exploring the history of the "Dune" universe before the events within "Dune", as well as two post-"Chapterhouse Dune" novels that complete the original series ("Hunters of Dune" and "Sandworms of Dune") based on Frank Herbert's own "Dune 7" outline.






</doc>
<doc id="10853" url="https://en.wikipedia.org/wiki?curid=10853" title="Fictional language">
Fictional language

Fictional languages are a subset of constructed languages, and are distinct from the former in that they have been created as part of a fictional setting (i.e. for use in a book, movie, television show, or video game). Typically they are the creation of one individual, while natural languages evolve out of a particular culture or people group. Fictional languages are also distinct from natural languages in that the former do not have native speakers.

Fictional languages are intended to be the languages of a fictional world and are often designed with the intent of giving more depth and an appearance of plausibility to the fictional worlds with which they are associated, and to have their characters communicate in a fashion which is both alien and dislocated. Within their fictional world, these languages do function as natural languages, helping to identify certain races or people groups and set these apart from others.

While some less-formed fictional languages are created as distorted versions or dialects of a pre-existing natural language, many are independently designed conlangs with their own lexicon (some more robust than others) and rules of grammar. Some of the latter are fully formed enough to be learned as a speak-able language, and many subcultures exist of those who are 'fluent' in one or more of these fictional languages. Often after the creator of a fictional language has accomplished their task, the fandom of that fictional universe will pick up where the creator left off and continue to flesh out the language, making it more like a natural language and therefore more usable.

Fictional languages are separated from artistic languages by both purpose and relative completion: a fictional language often has the least amount of grammar and vocabulary possible, and rarely extends beyond the absolutely necessary. At the same time, some others have developed languages in detail for their own sake, such as J. R. R. Tolkien's Quenya and Sindarin (two Elvish languages), "Star Trek"'s Klingon language and "Avatar"'s Na'vi language which exist as functioning, usable languages.

By analogy with the word "conlang", the term "conworld" is used to describe these fictional worlds, inhabited by fictional constructed cultures. The conworld influences vocabulary (what words the language will have for flora and fauna, articles of clothing, objects of technology, religious concepts, names of places and tribes, etc.), as well as influencing other factors such as pronouns, or how their cultures view the break-off points between colors or the gender and age of family members. Sound is also a directing factor, as creators seek to show their audience through phonology the type of race or people group to whom the language belongs.

Commercial fictional languages are those languages created for use in various commercial media, such as:


While some languages are created purely from the desire of the creator, language creation can be a profession. In 1974, Victoria Fromkin was the first person hired to create a language (Land of the Lost's Paku). Since then, notable professional language creators have included Marc Okrand (Klingon), David Peterson (Game of Thrones languages), and Paul Frommer (Na'vi).

A notable subgenre of fictional languages are alien languages, the ones that are used or might be used by putative extraterrestrial life forms. Alien languages are subject of both science fiction and scientific research. Perhaps the most fully developed fictional alien language is the Klingon language of the Star Trek universe - a fully developed constructed language.

The problem of alien language has confronted generations of science fiction writers; some have created fictional languages for their characters to use, while others have circumvented the problem through translation devices or other fantastic technology. For example, the Star Trek universe makes use of a 'universal translator', which explains why such different races, often meeting for the first time, are able to communicate with each other.

While in many cases an alien language is but an element of a fictional reality, in a number of science fiction works the core of the plot involves linguistic and psychological problems of communication between various alien species.

A further subgenre of alien languages are those that are visual, rather than auditory. Notable examples of this type are Circular Gallifreyan from BBC's Doctor Who series, and the Heptapod language from the 2016 film "Arrival".

Internet-based fictional languages are hosted along with their "conworlds" on the internet, and based at these sites, becoming known to the world through the visitors to these sites. Verdurian, the language of Mark Rosenfelder's Verduria on the planet of Almea, is a flagship Internet-based fictional language. Rosenfelder's website includes resources for other aspiring language creators. 

Many other fictional languages and their associated conworlds are created privately by their inventor, known only to the inventor and perhaps a few friends. 






</doc>
