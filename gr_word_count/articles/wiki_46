<doc id="6011" url="https://en.wikipedia.org/wiki?curid=6011" title="Chomsky hierarchy">
Chomsky hierarchy

In the formal languages of computer science and linguistics, the Chomsky hierarchy (occasionally referred to as the Chomsky-Schützenberger hierarchy) is a containment hierarchy of classes of formal grammars.
This hierarchy of grammars was described by Noam Chomsky in 1956. It is also named after Marcel-Paul Schützenberger, who played a crucial role in the development of the theory of formal languages.

A formal grammar of this type consists of a finite set of "production rules" ("left-hand side" → "right-hand side"), where each side consists of a finite sequence of the following symbols:

A formal grammar provides an axiom schema for (or "generates") a "formal language", which is a (usually infinite) set of finite-length sequences of symbols that may be constructed by applying production rules to another sequence of symbols (which initially contains just the start symbol). A rule may be applied by replacing an occurrence of the symbols on its left-hand side with those that appear on its right-hand side. A sequence of rule applications is called a "derivation". Such a grammar defines the formal language: all words consisting solely of terminal symbols which can be reached by a derivation from the start symbol.

Nonterminals are often represented by uppercase letters, terminals by lowercase letters, and the start symbol by . For example, the grammar with terminals , nonterminals , production rules
and start symbol , defines the language of all words of the form formula_1 (i.e. copies of followed by copies of ).

The following is a simpler grammar that defines the same language: 
Terminals , Nonterminals , Start symbol , Production rules

As another example, a grammar for a toy subset of English language is given by:
and start symbol . An example derivation is
Other sequences that can be derived from this grammar are: ""ideas hate great linguists"", and ""ideas generate"". While these sentences are nonsensical, they are syntactically correct. A syntactically incorrect sentence ( e.g. ""ideas ideas great hate"") cannot be derived from this grammar. See "Colorless green ideas sleep furiously" for a similar example given by Chomsky in 1957; see Phrase structure grammar and Phrase structure rules for more natural language examples and the problems of formal grammar in that area.

The following table summarizes each of Chomsky's four types of grammars, the class of language it generates, the type of automaton that recognizes it, and the form its rules must have.

Note that the set of grammars corresponding to recursive languages is not a member of this hierarchy; these would be properly between Type-0 and Type-1.

Every regular language is context-free, every context-free language is context-sensitive, every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.

Type-0 grammars include all formal grammars. They generate exactly all languages that can be recognized by a Turing machine. These languages are also known as the "recursively enumerable" or "Turing-recognizable" languages. Note that this is different from the recursive languages, which can be "decided" by an always-halting Turing machine.

Type-1 grammars generate context-sensitive languages. These grammars have rules of the form formula_2 with formula_3 a nonterminal and formula_4, formula_5 and formula_6 strings of terminals and/or nonterminals. The strings formula_4 and formula_5 may be empty, but formula_6 must be nonempty. The rule formula_10 is allowed if formula_11 does not appear on the right side of any rule. The languages described by these grammars are exactly all languages that can be recognized by a linear bounded automaton (a nondeterministic Turing machine whose tape is bounded by a constant times the length of the input.)

Type-2 grammars generate the context-free languages. These are defined by rules of the form formula_12 with formula_3 being a nonterminal and formula_6 being a string of terminals and/or nonterminals. These languages are exactly all languages that can be recognized by a non-deterministic pushdown automaton. Context-free languages—or rather its subset of deterministic context-free language—are the theoretical basis for the phrase structure of most programming languages, though their syntax also includes context-sensitive name resolution due to declarations and scope. Often a subset of grammars is used to make parsing easier, such as by an LL parser.

Type-3 grammars generate the regular languages. Such a grammar restricts its rules to a single nonterminal on the left-hand side and a right-hand side consisting of a single terminal, possibly followed by a single nonterminal (right regular). Alternatively, the right-hand side of the grammar can consist of a single terminal, possibly preceded by a single nonterminal (left regular). These generate the same languages. However, if left-regular rules and right-regular rules are combined, the language need no longer be regular. The rule formula_10 is also allowed here if formula_11 does not appear on the right side of any rule. These languages are exactly all languages that can be decided by a finite state automaton. Additionally, this family of formal languages can be obtained by regular expressions. Regular languages are commonly used to define search patterns and the lexical structure of programming languages.



</doc>
<doc id="6013" url="https://en.wikipedia.org/wiki?curid=6013" title="CRT">
CRT

CRT may refer to:









</doc>
<doc id="6014" url="https://en.wikipedia.org/wiki?curid=6014" title="Cathode-ray tube">
Cathode-ray tube

The cathode-ray tube (CRT) is a vacuum tube that contains one or more electron guns and a phosphorescent screen, and is used to display images. It modulates, accelerates, and deflects electron beam(s) onto the screen to create the images. The images may represent electrical waveforms (oscilloscope), pictures (television, computer monitor), radar targets, or other phenomena. CRTs have also been used as memory devices, in which case the visible light emitted from the fluorescent material (if any) is not intended to have significant meaning to a visual observer (though the visible pattern on the tube face may cryptically represent the stored data).

In television sets and computer monitors, the entire front area of the tube is scanned repetitively and systematically in a fixed pattern called a raster. An image is produced by controlling the intensity of each of the three electron beams, one for each additive primary color (red, green, and blue) with a video signal as a reference. In all modern CRT monitors and televisions, the beams are bent by "magnetic deflection", a varying magnetic field generated by coils and driven by electronic circuits around the neck of the tube, although electrostatic deflection is commonly used in oscilloscopes, a type of electronic test instrument.

A CRT is constructed from a glass envelope which is large, deep (i.e., long from front screen face to rear end), fairly heavy, and relatively fragile. The interior of a CRT is evacuated to approximately to , evacuation being necessary to facilitate the free flight of electrons from the gun(s) to the tube's face. The fact that it is evacuated makes handling an intact CRT potentially dangerous due to the risk of breaking the tube and causing a violent implosion that can hurl shards of glass at great velocity. As a matter of safety, the face is typically made of thick lead glass so as to be highly shatter-resistant and to block most X-ray emissions, particularly if the CRT is used in a consumer product.

Since the late 2000s, CRTs have been largely superseded by newer "flat panel" display technologies such as LCD, plasma display, and OLED displays, which in the case of LCD and OLED displays have lower manufacturing costs and power consumption, as well as significantly less weight and bulk. Flat panel displays can also be made in very large sizes; whereas was about the largest size of a CRT television, flat panels are available in and even larger sizes.

Cathode rays were discovered by Julius Plücker and Johann Wilhelm Hittorf. Hittorf observed that some unknown rays were emitted from the cathode (negative electrode) which could cast shadows on the glowing wall of the tube, indicating the rays were traveling in straight lines. In 1890, Arthur Schuster demonstrated cathode rays could be deflected by electric fields, and William Crookes showed they could be deflected by magnetic fields. In 1897, J. J. Thomson succeeded in measuring the mass of cathode rays, showing that they consisted of negatively charged particles smaller than atoms, the first "subatomic particles", which were later named "electrons". The earliest version of the CRT was known as the "Braun tube", invented by the German physicist Ferdinand Braun in 1897. It was a cold-cathode diode, a modification of the Crookes tube with a phosphor-coated screen.

The first cathode-ray tube to use a hot cathode was developed by John B. Johnson (who gave his name to the term Johnson noise) and Harry Weiner Weinhart of Western Electric, and became a commercial product in 1922.

In 1925, Kenjiro Takayanagi demonstrated a CRT television that received images with a 40-line resolution. By 1927, he improved the resolution to 100 lines, which was unrivaled until 1931. By 1928, he was the first to transmit human faces in half-tones on a CRT display. By 1935, he had invented an early all-electronic CRT television.

It was named in 1929 by inventor Vladimir K. Zworykin, who was influenced by Takayanagi's earlier work. RCA was granted a trademark for the term (for its cathode-ray tube) in 1932; it voluntarily released the term to the public domain in 1950.

The first commercially made electronic television sets with cathode-ray tubes were manufactured by Telefunken in Germany in 1934.

Flat panel displays dropped in price and started significantly displacing cathode-ray tubes in the 2000s, with LCD screens exceeding CRTs in 2008.
The last known manufacturer of (in this case, recycled) CRTs ceased in 2015.

In oscilloscope CRTs, electrostatic deflection is used, rather than the magnetic deflection commonly used with television and other large CRTs. The beam is deflected horizontally by applying an electric field between a pair of plates to its left and right, and vertically by applying an electric field to plates above and below. Televisions use magnetic rather than electrostatic deflection because the deflection plates obstruct the beam when the deflection angle is as large as is required for tubes that are relatively short for their size.

Various phosphors are available depending upon the needs of the measurement or display application. The brightness, color, and persistence of the illumination depends upon the type of phosphor used on the CRT screen. Phosphors are available with persistences ranging from less than one microsecond to several seconds. For visual observation of brief transient events, a long persistence phosphor may be desirable. For events which are fast and repetitive, or high frequency, a short-persistence phosphor is generally preferable.

When displaying fast one-shot events, the electron beam must deflect very quickly, with few electrons impinging on the screen, leading to a faint or invisible image on the display. Oscilloscope CRTs designed for very fast signals can give a brighter display by passing the electron beam through a micro-channel plate just before it reaches the screen. Through the phenomenon of secondary emission, this plate multiplies the number of electrons reaching the phosphor screen, giving a significant improvement in writing rate (brightness) and improved sensitivity and spot size as well.

Most oscilloscopes have a graticule as part of the visual display, to facilitate measurements. The graticule may be permanently marked inside the face of the CRT, or it may be a transparent external plate made of glass or acrylic plastic. An internal graticule eliminates parallax error, but cannot be changed to accommodate different types of measurements. Oscilloscopes commonly provide a means for the graticule to be illuminated from the side, which improves its visibility.

These are found in "analog phosphor storage oscilloscopes". These are distinct from "digital storage oscilloscopes" which rely on solid state digital memory to store the image.

Where a single brief event is monitored by an oscilloscope, such an event will be displayed by a conventional tube only while it actually occurs. The use of a long persistence phosphor may allow the image to be observed after the event, but only for a few seconds at best. This limitation can be overcome by the use of a direct view storage cathode-ray tube (storage tube). A storage tube will continue to display the event after it has occurred until such time as it is erased. A storage tube is similar to a conventional tube except that it is equipped with a metal grid coated with a dielectric layer located immediately behind the phosphor screen. An externally applied voltage to the mesh initially ensures that the whole mesh is at a constant potential. This mesh is constantly exposed to a low velocity electron beam from a 'flood gun' which operates independently of the main gun. This flood gun is not deflected like the main gun but constantly 'illuminates' the whole of the storage mesh. The initial charge on the storage mesh is such as to repel the electrons from the flood gun which are prevented from striking the phosphor screen.

When the main electron gun writes an image to the screen, the energy in the main beam is sufficient to create a 'potential relief' on the storage mesh. The areas where this relief is created no longer repel the electrons from the flood gun which now pass through the mesh and illuminate the phosphor screen. Consequently, the image that was briefly traced out by the main gun continues to be displayed after it has occurred. The image can be 'erased' by resupplying the external voltage to the mesh restoring its constant potential. The time for which the image can be displayed was limited because, in practice, the flood gun slowly neutralises the charge on the storage mesh. One way of allowing the image to be retained for longer is temporarily to turn off the flood gun. It is then possible for the image to be retained for several days. The majority of storage tubes allow for a lower voltage to be applied to the storage mesh which slowly restores the initial charge state. By varying this voltage a variable persistence is obtained. Turning off the flood gun and the voltage supply to the storage mesh allows such a tube to operate as a conventional oscilloscope tube.

Color tubes use three different phosphors which emit red, green, and blue light respectively. They are packed together in stripes (as in aperture grille designs) or clusters called "triads" (as in shadow mask CRTs). Color CRTs have three electron guns, one for each primary color, arranged either in a straight line or in an equilateral triangular configuration (the guns are usually constructed as a single unit). (The triangular configuration is often called "delta-gun", based on its relation to the shape of the Greek letter delta Δ.) A grille or mask absorbs the electrons that would otherwise hit the wrong phosphor. A shadow mask tube uses a metal plate with tiny holes, placed so that the electron beam only illuminates the correct phosphors on the face of the tube; the holes are tapered so that the electrons that strike the inside of any hole will be reflected back, if they are not absorbed (e.g. due to local charge accumulation), instead of bouncing through the hole to strike a random (wrong) spot on the screen. Another type of color CRT uses an aperture grille of tensioned vertical wires to achieve the same result.

Due to limitations in the dimensional precision with which CRTs can be manufactured economically, it has not been practically possible to build color CRTs in which three electron beams could be aligned to hit phosphors of respective color in acceptable coordination, solely on the basis of the geometric configuration of the electron gun axes and gun aperture positions, shadow mask apertures, etc. The shadow mask ensures that one beam will only hit spots of certain colors of phosphors, but minute variations in physical alignment of the internal parts among individual CRTs will cause variations in the exact alignment of the beams through the shadow mask, allowing some electrons from, for example, the red beam to hit, say, blue phosphors, unless some individual compensation is made for the variance among individual tubes.

Color convergence and color purity are two aspects of this single problem. Firstly, for correct color rendering it is necessary that regardless of where the beams are deflected on the screen, all three hit the same spot (and nominally pass through the same hole or slot) on the shadow mask. This is called convergence. More specifically, the convergence at the center of the screen (with no deflection field applied by the yoke) is called static convergence, and the convergence over the rest of the screen area is called dynamic convergence. The beams may converge at the center of the screen and yet stray from each other as they are deflected toward the edges; such a CRT would be said to have good static convergence but poor dynamic convergence. Secondly, each beam must only strike the phosphors of the color it is intended to strike and no others. This is called purity. Like convergence, there is static purity and dynamic purity, with the same meanings of "static" and "dynamic" as for convergence. Convergence and purity are distinct parameters; a CRT could have good purity but poor convergence, or vice versa. Poor convergence causes color "shadows" or "ghosts" along displayed edges and contours, as if the image on the screen were intaglio printed with poor registration. Poor purity causes objects on the screen to appear off-color while their edges remain sharp. Purity and convergence problems can occur at the same time, in the same or different areas of the screen or both over the whole screen, and either uniformly or to greater or lesser degrees over different parts of the screen.
The solution to the static convergence and purity problems is a set of color alignment magnets installed around the neck of the CRT. These movable weak permanent magnets are usually mounted on the back end of the deflection yoke assembly and are set at the factory to compensate for any static purity and convergence errors that are intrinsic to the unadjusted tube. Typically there are two or three pairs of two magnets in the form of rings made of plastic impregnated with a magnetic material, with their magnetic fields parallel to the planes of the magnets, which are perpendicular to the electron gun axes. Each pair of magnetic rings forms a single effective magnet whose field vector can be fully and freely adjusted (in both direction and magnitude). By rotating a pair of magnets relative to each other, their relative field alignment can be varied, adjusting the effective field strength of the pair. (As they rotate relative to each other, each magnet's field can be considered to have two opposing components at right angles, and these four components [two each for two magnets] form two pairs, one pair reinforcing each other and the other pair opposing and canceling each other. Rotating away from alignment, the magnets' mutually reinforcing field components decrease as they are traded for increasing opposed, mutually cancelling components.) By rotating a pair of magnets together, preserving the relative angle between them, the direction of their collective magnetic field can be varied. Overall, adjusting all of the convergence/purity magnets allows a finely tuned slight electron beam deflection or lateral offset to be applied, which compensates for minor static convergence and purity errors intrinsic to the uncalibrated tube. Once set, these magnets are usually glued in place, but normally they can be freed and readjusted in the field (e.g. by a TV repair shop) if necessary.

On some CRTs, additional fixed adjustable magnets are added for dynamic convergence or dynamic purity at specific points on the screen, typically near the corners or edges. Further adjustment of dynamic convergence and purity typically cannot be done passively, but requires active compensation circuits.

Dynamic color convergence and purity are one of the main reasons why until late in their history, CRTs were long-necked (deep) and had biaxially curved faces; these geometric design characteristics are necessary for intrinsic passive dynamic color convergence and purity. Only starting around the 1990s did sophisticated active dynamic convergence compensation circuits become available that made short-necked and flat-faced CRTs workable. These active compensation circuits use the deflection yoke to finely adjust beam deflection according to the beam target location. The same techniques (and major circuit components) also make possible the adjustment of display image rotation, skew, and other complex raster geometry parameters through electronics under user control.

If the shadow mask or aperture grille becomes magnetized, its magnetic field alters the paths of the electron beams. This causes errors of "color purity" as the electrons no longer follow only their intended paths, and some will hit some phosphors of colors other than the one intended. For example, some electrons from the red beam may hit blue or green phosphors, imposing a magenta or yellow tint to parts of the image that are supposed to be pure red. (This effect is localized to a specific area of the screen if the magnetization is localized.) Therefore, it is important that the shadow mask or aperture grille not be magnetized. 

Most color CRT displays, i.e. television sets and computer monitors, each have a built-in degaussing (demagnetizing) circuit, the primary component of which is a degaussing coil which is mounted around the perimeter of the CRT face inside the bezel. Upon power-up of the CRT display, the degaussing circuit produces a brief, alternating current through the degaussing coil which smoothly decays in strength (fades out) to zero over a period of a few seconds, producing a decaying alternating magnetic field from the coil. This degaussing field is strong enough to remove shadow mask magnetization in most cases. In unusual cases of strong magnetization where the internal degaussing field is not sufficient, the shadow mask may be degaussed externally with a stronger portable degausser or demagnetizer. However, an excessively strong magnetic field, whether alternating or constant, may mechanically deform (bend) the shadow mask, causing a permanent color distortion on the display which looks very similar to a magnetization effect.

The degaussing circuit is often built of a thermo-electric (not electronic) device containing a small ceramic heating element and a positive thermal coefficient (PTC) resistor, connected directly to the switched AC power line with the resistor in series with the degaussing coil. When the power is switched on, the heating element heats the PTC resistor, increasing its resistance to a point where degaussing current is minimal, but not actually zero. In older CRT displays, this low-level current (which produces no significant degaussing field) is sustained along with the action of the heating element as long as the display remains switched on. To repeat a degaussing cycle, the CRT display must be switched off and left off for at least several seconds to reset the degaussing circuit by allowing the PTC resistor to cool to the ambient temperature; switching the display-off and immediately back on will result in a weak degaussing cycle or effectively no degaussing cycle.

This simple design is effective and cheap to build, but it wastes some power continuously. Later models, especially Energy Star rated ones, use a relay to switch the entire degaussing circuit on and off, so that the degaussing circuit uses energy only when it is functionally active and needed. The relay design also enables degaussing on user demand through the unit's front panel controls, without switching the unit off and on again. This relay can often be heard clicking off at the end of the degaussing cycle a few seconds after the monitor is turned on, and on and off during a manually initiated degaussing cycle.

Vector monitors were used in early computer aided design systems and are in some late-1970s to mid-1980s arcade games such as "Asteroids".
They draw graphics point-to-point, rather than scanning a raster. Either monochrome or color CRTs can be used in vector displays, and the essential principles of CRT design and operation are the same for either type of display; the main difference is in the beam deflection patterns and circuits.

Dot pitch defines the maximum resolution of the display, assuming delta-gun CRTs. In these, as the scanned resolution approaches the dot pitch resolution, moiré appears, as the detail being displayed is finer than what the shadow mask can render. Aperture grille monitors do not suffer from vertical moiré; however, because their phosphor stripes have no vertical detail. In smaller CRTs, these strips maintain position by themselves, but larger aperture-grille CRTs require one or two crosswise (horizontal) support strips.

CRTs have a pronounced triode characteristic, which results in significant gamma (a nonlinear relationship in an electron gun between applied video voltage and beam intensity).

The Williams tube or Williams-Kilburn tube was a cathode-ray tube used to electronically store binary data. It was used in computers of the 1940s as a random-access digital storage device. In contrast to other CRTs in this article, the Williams tube was not a display device, and in fact could not be viewed since a metal plate covered its screen.

In some vacuum tube radio sets, a "Magic Eye" or "Tuning Eye" tube was provided to assist in tuning the receiver. Tuning would be adjusted until the width of a radial shadow was minimized. This was used instead of a more expensive electromechanical meter, which later came to be used on higher-end tuners when transistor sets lacked the high voltage required to drive the device. The same type of device was used with tape recorders as a recording level meter, and for various other applications including electrical test equipment.

Some displays for early computers (those that needed to display more text than was practical using vectors, or that required high speed for photographic output) used Charactron CRTs. These incorporate a perforated metal character mask (stencil), which shapes a wide electron beam to form a character on the screen. The system selects a character on the mask using one set of deflection circuits, but that causes the extruded beam to be aimed off-axis, so a second set of deflection plates has to re-aim the beam so it is headed toward the center of the screen. A third set of plates places the character wherever required. The beam is unblanked (turned on) briefly to draw the character at that position. Graphics could be drawn by selecting the position on the mask corresponding to the code for a space (in practice, they were simply not drawn), which had a small round hole in the center; this effectively disabled the character mask, and the system reverted to regular vector behavior. Charactrons had exceptionally long necks, because of the need for three deflection systems.

Nimo was the trademark of a family of small specialised CRTs manufactured by Industrial Electronics Engineers. These had 10 electron guns which produced electron beams in the form of digits in a manner similar to that of the charactron. The tubes were either simple single-digit displays or more complex 4- or 6- digit displays produced by means of a suitable magnetic deflection system. Having little of the complexities of a standard CRT, the tube required a relatively simple driving circuit, and as the image was projected on the glass face, it provided a much wider viewing angle than competitive types (e.g., nixie tubes).

Flood beam CRTs are small tubes that are arranged as pixels for large screens like Jumbotrons. The first screen using this technology was introduced by Mitsubishi Electric for the 1980 Major League Baseball All-Star Game. It differs from a normal CRT in that the electron gun within does not produce a focused controllable beam. Instead, electrons are sprayed in a wide cone across the entire front of the phosphor screen, basically making each unit act as a single light bulb. Each one is coated with a red, green or blue phosphor, to make up the color sub-pixels. This technology has largely been replaced with light emitting diode displays. Unfocused and undeflected CRTs were used as grid-controlled stroboscope lamps since 1958.

CRTs with an unphosphored front glass but with fine wires embedded in it were used as electrostatic print heads in the 1960s. The wires would pass the electron beam current through the glass onto a sheet of paper where the desired content was therefore deposited as an electrical charge pattern. The paper was then passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image.

In the late 1990s and early 2000s Philips Research Laboratories experimented with a type of thin CRT known as the "Zeus" display which contained CRT-like functionality in a flat panel display. The devices were demonstrated but never marketed.

Some CRT manufacturers, both LG Display and Samsung Display, innovated CRT technology by creating a slimmer tube. Slimmer CRT has a trade name Superslim and Ultraslim. A flat CRT has a depth. The depth of Superslim was and Ultraslim was .

Despite being a mainstay of display technology for decades, CRT-based computer monitors and televisions are now virtually a dead technology. Demand for CRT screens dropped in the late 2000s. The rapid advances and falling prices of LCD flat panel technology -- first for computer monitors, and then for televisions -- spelled doom for competing display technologies such as CRT, rear-projection, and plasma display.

Most high-end CRT production had ceased by around 2010, including high-end Sony and Panasonic product lines. In Canada and the United States, the sale and production of high-end CRT TVs ( screens) in these markets had all but ended by 2007. Just a couple of years later, inexpensive "combo" CRT TVs ( screens with an integrated VHS player) disappeared from discount stores.

Electronics retailers such as Best Buy steadily reduced store spaces for CRTs. In 2005, Sony announced that they would stop the production of CRT computer displays. Samsung did not introduce any CRT models for the 2008 model year at the 2008 Consumer Electronics Show; on 4 February 2008, they removed their 30" wide screen CRTs from their North American website and did not replace them with new models.

In the United Kingdom, DSG (Dixons), the largest retailer of domestic electronic equipment, reported that CRT models made up 80–90% of the volume of televisions sold at Christmas 2004 and 15–20% a year later, and that they were expected to be less than 5% at the end of 2006. Dixons ceased selling CRT televisions in 2006.

However, the demise of CRTs has been happening more slowly in the developing world. According to iSupply, production in units of CRTs was not surpassed by LCDs production until 4Q 2007, owing largely to CRT production at factories in China.

CRTs, despite advances, remained relatively heavy and cumbersome in comparison to other display technologies. CRT screens had much deeper cabinets compared to flat panels and rear-projection displays for a given screen size; it was largely impractical to build or use CRTs larger than . The CRT disadvantages became especially significant in light of rapid technological advancements in LCD and plasma flat-panels, which allowed them to easily surpass these size limitations while also being thin and wall-mountable (two key features that were increasingly being preferred by customers).

CRTs can emit a small amount of X-ray radiation as a result of the electron beam's bombardment of the shadow mask/aperture grille and phosphors. The amount of radiation escaping the front of the monitor is widely considered not to be harmful. The Food and Drug Administration regulations in are used to strictly limit, for instance, television receivers to 0.5 milliroentgens per hour (mR/h) (0.13 µC/(kg·h) or 36 pA/kg) at a distance of from any external surface; since 2007, most CRTs have emissions that fall well below this limit.

Older color and monochrome CRTs may have been manufactured with toxic substances, such as cadmium, in the phosphors. The rear glass tube of modern CRTs may be made from leaded glass, which represent an environmental hazard if disposed of improperly. By the time personal computers were produced, glass in the front panel (the viewable portion of the CRT) used barium rather than lead, though the rear of the CRT was still produced from leaded glass. Monochrome CRTs typically do not contain enough leaded glass to fail EPA TCLP tests. While the TCLP process grinds the glass into fine particles in order to expose them to weak acids to test for leachate, intact CRT glass does not leach (The lead is vitrified, contained inside the glass itself, similar to leaded glass crystalware).

Due to the toxins contained in CRT monitors the United States Environmental Protection Agency created rules (in October 2001) stating that CRTs must be brought to special e-waste recycling facilities. In November 2002, the EPA began fining companies that disposed of CRTs through landfills or incineration. Regulatory agencies, local and statewide, monitor the disposal of CRTs and other computer equipment.

Various states participate in the recycling of CRTs, each with their reporting requirements for collectors and recycling facilities. For example, in California the recycling of CRTs is governed by CALRecycle, the California Department of Resources Recycling and Recovery through their Payment System. . Recycling facilities that accept CRT devices from business and residential sector must obtain contact information such as address and phone number to ensure the CRTs come from a California source in order to participate in the CRT Recycling Payment System. 

In Europe, disposal of CRT televisions and monitors is covered by the WEEE Directive.

At low refresh rates (60 Hz and below), the periodic scanning of the display may produce a flicker that some people perceive more easily than others, especially when viewed with peripheral vision. Flicker is commonly associated with CRT as most televisions run at 50 Hz (PAL) or 60 Hz (NTSC), although there are some 100 Hz PAL televisions that are flicker-free. Typically only low-end monitors run at such low frequencies, with most computer monitors supporting at least 75 Hz and high-end monitors capable of 100 Hz or more to eliminate any perception of flicker. Though the 100 Hz PAL was often achieved using interleaved scanning, dividing the circuit and scan into two beams of 50 Hz. Non-computer CRTs or CRT for sonar or radar may have long persistence phosphor and are thus flicker free. If the persistence is too long on a video display, moving images will be blurred.

50 Hz/60 Hz CRTs used for television operate with horizontal scanning frequencies of 15,750 Hz (for NTSC systems) or 15,625 Hz (for PAL systems). These frequencies are at the upper range of human hearing and are inaudible to many people; however, some people (especially children) will perceive a high-pitched tone near an operating television CRT. The sound is due to magnetostriction in the magnetic core and periodic movement of windings of the flyback transformer.

This problem does not occur on 100/120 Hz TVs and on non-CGA (Color Graphics Adapter) computer displays, because they use much higher horizontal scanning frequencies (22 kHz to over 100 kHz).

High vacuum inside glass-walled cathode-ray tubes permits electron beams to fly freely—without colliding into molecules of air or other gas. If the glass is damaged, atmospheric pressure can collapse the vacuum tube into dangerous fragments which accelerate inward and then spray at high speed in all directions. The implosion energy is proportional to the evacuated volume of the CRT. Although modern cathode-ray tubes used in televisions and computer displays have epoxy-bonded face-plates or other measures to prevent shattering of the envelope, CRTs must be handled carefully to avoid personal injury.

To accelerate the electrons from the cathode to the screen with sufficient velocity, a very high voltage (EHT or Extra High Tension) is required, from a few thousand volts for a small oscilloscope CRT to tens of kV for a larger screen color TV. This is many times greater than household power supply voltage. Even after the power supply is turned off, some associated capacitors and the CRT itself may retain a charge for some time and therefore dissipate that charge suddenly through a ground such as an inattentive human grounding a capacitor discharge lead.

Under some circumstances, the signal radiated from the electron guns, scanning circuitry, and associated wiring of a CRT can be captured remotely and used to reconstruct what is shown on the CRT using a process called Van Eck phreaking. Special TEMPEST shielding can mitigate this effect. Such radiation of a potentially exploitable signal, however, occurs also with other display technologies and with electronics in general.

As electronic waste, CRTs are considered one of the hardest types to recycle. CRTs have relatively high concentration of lead and phosphors (not phosphorus), both of which are necessary for the display. There are several companies in the United States that charge a small fee to collect CRTs, then subsidize their labor by selling the harvested copper, wire, and printed circuit boards. The United States Environmental Protection Agency (EPA) includes discarded CRT monitors in its category of "hazardous household waste" but considers CRTs that have been set aside for testing to be commodities if they are not discarded, speculatively accumulated, or left unprotected from weather and other damage.

Leaded CRT glass was sold to be remelted into other CRTs, or even broken down and used in road construction.

Basics of cathode rays and discharge in low-pressure gas:

Light production by cathode rays:

Manipulating the electron beam:

Applying CRT in different display-purpose:

Miscellaneous phenomena:

Historical aspects:

Safety and precautions:




</doc>
<doc id="6015" url="https://en.wikipedia.org/wiki?curid=6015" title="Crystal">
Crystal

A crystal or crystalline solid is a solid material whose constituents (such as atoms, molecules, or ions) are arranged in a highly ordered microscopic structure, forming a crystal lattice that extends in all directions. In addition, macroscopic single crystals are usually identifiable by their geometrical shape, consisting of flat faces with specific, characteristic orientations. The scientific study of crystals and crystal formation is known as crystallography. The process of crystal formation via mechanisms of crystal growth is called crystallization or solidification.

The word "crystal" derives from the Ancient Greek word (), meaning both "ice" and "rock crystal", from (), "icy cold, frost".

Examples of large crystals include snowflakes, diamonds, and table salt. Most inorganic solids are not crystals but polycrystals, i.e. many microscopic crystals fused together into a single solid. Examples of polycrystals include most metals, rocks, ceramics, and ice. A third category of solids is amorphous solids, where the atoms have no periodic structure whatsoever. Examples of amorphous solids include glass, wax, and many plastics.

Despite the name, lead crystal, crystal glass, and related products are "not" crystals, but rather types of glass, i.e. amorphous solids.

Crystals are often used in pseudoscientific practices such as crystal therapy, and, along with gemstones, are sometimes associated with spellwork in Wiccan beliefs and related religious movements.

The scientific definition of a "crystal" is based on the microscopic arrangement of atoms inside it, called the crystal structure. A crystal is a solid where the atoms form a periodic arrangement. (Quasicrystals are an exception, see below).

Not all solids are crystals. For example, when liquid water starts freezing, the phase change begins with small ice crystals that grow until they fuse, forming a "polycrystalline" structure. In the final block of ice, each of the small crystals (called "crystallites" or "grains") is a true crystal with a periodic arrangement of atoms, but the whole polycrystal does "not" have a periodic arrangement of atoms, because the periodic pattern is broken at the grain boundaries. Most macroscopic inorganic solids are polycrystalline, including almost all metals, ceramics, ice, rocks, etc. Solids that are neither crystalline nor polycrystalline, such as glass, are called "amorphous solids", also called glassy, vitreous, or noncrystalline. These have no periodic order, even microscopically. There are distinct differences between crystalline solids and amorphous solids: most notably, the process of forming a glass does not release the latent heat of fusion, but forming a crystal does.

A crystal structure (an arrangement of atoms in a crystal) is characterized by its "unit cell", a small imaginary box containing one or more atoms in a specific spatial arrangement. The unit cells are stacked in three-dimensional space to form the crystal.

The symmetry of a crystal is constrained by the requirement that the unit cells stack perfectly with no gaps. There are 219 possible crystal symmetries, called crystallographic space groups. These are grouped into 7 crystal systems, such as cubic crystal system (where the crystals may form cubes or rectangular boxes, such as Halite (mineral) shown at right) or hexagonal crystal system (where the crystals may form hexagons, such as ordinary water ice).

Crystals are commonly recognized by their shape, consisting of flat faces with sharp angles. These shape characteristics are not "necessary" for a crystal—a crystal is scientifically defined by its microscopic atomic arrangement, not its macroscopic shape—but the characteristic macroscopic shape is often present and easy to see.

Euhedral crystals are those with obvious, well-formed flat faces. Anhedral crystals do not, usually because the crystal is one grain in a polycrystalline solid.

The flat faces (also called facets) of a euhedral crystal are oriented in a specific way relative to the underlying atomic arrangement of the crystal: they are planes of relatively low Miller index. This occurs because some surface orientations are more stable than others (lower surface energy). As a crystal grows, new atoms attach easily to the rougher and less stable parts of the surface, but less easily to the flat, stable surfaces. Therefore, the flat surfaces tend to grow larger and smoother, until the whole crystal surface consists of these plane surfaces. (See diagram on right.)

One of the oldest techniques in the science of crystallography consists of measuring the three-dimensional orientations of the faces of a crystal, and using them to infer the underlying crystal symmetry.

A crystal's habit is its visible external shape. This is determined by the crystal structure (which restricts the possible facet orientations), the specific crystal chemistry and bonding (which may favor some facet types over others), and the conditions under which the crystal formed.

By volume and weight, the largest concentrations of crystals in the Earth are part of its solid bedrock. Crystals found in rocks typically range in size from a fraction of a millimetre to several centimetres across, although exceptionally large crystals are occasionally found. , the world's largest known naturally occurring crystal is a crystal of beryl from Malakialina, Madagascar, long and in diameter, and weighing .

Some crystals have formed by magmatic and metamorphic processes, giving origin to large masses of crystalline rock. The vast majority of igneous rocks are formed from molten magma and the degree of crystallization depends primarily on the conditions under which they solidified. Such rocks as granite, which have cooled very slowly and under great pressures, have completely crystallized; but many kinds of lava were poured out at the surface and cooled very rapidly, and in this latter group a small amount of amorphous or glassy matter is common. Other crystalline rocks, the metamorphic rocks such as marbles, mica-schists and quartzites, are recrystallized. This means that they were at first fragmental rocks like limestone, shale and sandstone and have never been in a molten condition nor entirely in solution, but the high temperature and pressure conditions of metamorphism have acted on them by erasing their original structures and inducing recrystallization in the solid state.

Other rock crystals have formed out of precipitation from fluids, commonly water, to form druses or quartz veins.
Evaporites such as Halite (mineral), gypsum and some limestones have been deposited from aqueous solution, mostly owing to evaporation in arid climates.

Water-based ice in the form of snow, sea ice and glaciers is a very common manifestation of crystalline or polycrystalline matter on Earth. A single snowflake is a single crystal or a collection of crystals, while an ice cube is a polycrystal.

Many living organisms are able to produce crystals, for example calcite and aragonite in the case of most molluscs or hydroxylapatite in the case of vertebrates.

The same group of atoms can often solidify in many different ways. Polymorphism is the ability of a solid to exist in more than one crystal form. For example, water ice is ordinarily found in the hexagonal form Ice I, but can also exist as the cubic Ice I, the rhombohedral ice II, and many other forms. The different polymorphs are usually called different "phases".

In addition, the same atoms may be able to form noncrystalline phases. For example, water can also form amorphous ice, while SiO can form both fused silica (an amorphous glass) and quartz (a crystal). Likewise, if a substance can form crystals, it can also form polycrystals.

For pure chemical elements, polymorphism is known as allotropy. For example, diamond and graphite are two crystalline forms of carbon, while amorphous carbon is a noncrystalline form. Polymorphs, despite having the same atoms, may have wildly different properties. For example, diamond is among the hardest substances known, while graphite is so soft that it is used as a lubricant.

Polyamorphism is a similar phenomenon where the same atoms can exist in more than one amorphous solid form.

Crystallization is the process of forming a crystalline structure from a fluid or from materials dissolved in a fluid. (More rarely, crystals may be deposited directly from gas; see thin-film deposition and epitaxy.)

Crystallization is a complex and extensively-studied field, because depending on the conditions, a single fluid can solidify into many different possible forms. It can form a single crystal, perhaps with various possible phases, stoichiometries, impurities, defects, and habits. Or, it can form a polycrystal, with various possibilities for the size, arrangement, orientation, and phase of its grains. The final form of the solid is determined by the conditions under which the fluid is being solidified, such as the chemistry of the fluid, the ambient pressure, the temperature, and the speed with which all these parameters are changing.

Specific industrial techniques to produce large single crystals (called "boules") include the Czochralski process and the Bridgman technique. Other less exotic methods of crystallization may be used, depending on the physical properties of the substance, including hydrothermal synthesis, sublimation, or simply solvent-based crystallization.

Large single crystals can be created by geological processes. For example, selenite crystals in excess of 10 meters are found in the Cave of the Crystals in Naica, Mexico. For more details on geological crystal formation, see above.

Crystals can also be formed by biological processes, see above. Conversely, some organisms have special techniques to "prevent" crystallization from occurring, such as antifreeze proteins.

An "ideal" crystal has every atom in a perfect, exactly repeating pattern. However, in reality, most crystalline materials have a variety of crystallographic defects, places where the crystal's pattern is interrupted. The types and structures of these defects may have a profound effect on the properties of the materials.

A few examples of crystallographic defects include vacancy defects (an empty space where an atom should fit), interstitial defects (an extra atom squeezed in where it does not fit), and dislocations (see figure at right). Dislocations are especially important in materials science, because they help determine the mechanical strength of materials.

Another common type of crystallographic defect is an impurity, meaning that the "wrong" type of atom is present in a crystal. For example, a perfect crystal of diamond would only contain carbon atoms, but a real crystal might perhaps contain a few boron atoms as well. These boron impurities change the diamond's color to slightly blue. Likewise, the only difference between ruby and sapphire is the type of impurities present in a corundum crystal.
In semiconductors, a special type of impurity, called a dopant, drastically changes the crystal's electrical properties. Semiconductor devices, such as transistors, are made possible largely by putting different semiconductor dopants into different places, in specific patterns.

Twinning is a phenomenon somewhere between a crystallographic defect and a grain boundary. Like a grain boundary, a twin boundary has different crystal orientations on its two sides. But unlike a grain boundary, the orientations are not random, but related in a specific, mirror-image way.

Mosaicity is a spread of crystal plane orientations. A mosaic crystal is supposed to consist of smaller crystalline units that are somewhat misaligned with respect to each other.

In general, solids can be held together by various types of chemical bonds, such as metallic bonds, ionic bonds, covalent bonds, van der Waals bonds, and others. None of these are necessarily crystalline or non-crystalline. However, there are some general trends as follows.

Metals are almost always polycrystalline, though there are exceptions like amorphous metal and single-crystal metals. The latter are grown synthetically. (A microscopically-small piece of metal may naturally form into a single crystal, but larger pieces generally do not.) Ionic compound materials are usually crystalline or polycrystalline. In practice, large salt crystals can be created by solidification of a molten fluid, or by crystallization out of a solution. Covalently bonded solids (sometimes called covalent network solids) are also very common, notable examples being diamond and quartz. Weak van der Waals forces also help hold together certain crystals, such as crystalline molecular solids, as well as the interlayer bonding in graphite. Polymer materials generally will form crystalline regions, but the lengths of the molecules usually prevent complete crystallization—and sometimes polymers are completely amorphous.

A quasicrystal consists of arrays of atoms that are ordered but not strictly periodic. They have many attributes in common with ordinary crystals, such as displaying a discrete pattern in x-ray diffraction, and the ability to form shapes with smooth, flat faces.

Quasicrystals are most famous for their ability to show five-fold symmetry, which is impossible for an ordinary periodic crystal (see crystallographic restriction theorem).

The International Union of Crystallography has redefined the term "crystal" to include both ordinary periodic crystals and quasicrystals ("any solid having an essentially discrete diffraction diagram").

Quasicrystals, first discovered in 1982, are quite rare in practice. Only about 100 solids are known to form quasicrystals, compared to about 400,000 periodic crystals known in 2004. The 2011 Nobel Prize in Chemistry was awarded to Dan Shechtman for the discovery of quasicrystals.

Crystals can have certain special electrical, optical, and mechanical properties that glass and polycrystals normally cannot. These properties are related to the anisotropy of the crystal, i.e. the lack of rotational symmetry in its atomic arrangement. One such property is the piezoelectric effect, where a voltage across the crystal can shrink or stretch it. Another is birefringence, where a double image appears when looking through a crystal. Moreover, various properties of a crystal, including electrical conductivity, electrical permittivity, and Young's modulus, may be different in different directions in a crystal. For example, graphite crystals consist of a stack of sheets, and although each individual sheet is mechanically very strong, the sheets are rather loosely bound to each other. Therefore, the mechanical strength of the material is quite different depending on the direction of stress.

Not all crystals have all of these properties. Conversely, these properties are not quite exclusive to crystals. They can appear in glasses or polycrystals that have been made anisotropic by working or stress—for example, stress-induced birefringence.

"Crystallography" is the science of measuring the crystal structure (in other words, the atomic arrangement) of a crystal. One widely used crystallography technique is X-ray diffraction. Large numbers of known crystal structures are stored in crystallographic databases.



</doc>
<doc id="6016" url="https://en.wikipedia.org/wiki?curid=6016" title="Cytosine">
Cytosine

Cytosine (; C) is one of the four main bases found in DNA and RNA, along with adenine, guanine, and thymine (uracil in RNA). It is a pyrimidine derivative, with a heterocyclic aromatic ring and two substituents attached (an amine group at position 4 and a keto group at position 2). The nucleoside of cytosine is cytidine. In Watson-Crick base pairing, it forms three hydrogen bonds with guanine.

Cytosine was discovered and named by Albrecht Kossel and Albert Neumann in 1894 when it was hydrolyzed from calf thymus tissues. A structure was proposed in 1903, and was synthesized (and thus confirmed) in the laboratory in the same year.

In 1997 cytosine was used in an early demonstration quantum information processing when Oxford University researchers implemented the Deutsch-Jozsa algorithm on a two qubit nuclear magnetic resonance quantum computer (NMRQC).

In March 2015, NASA scientists reported the formation of cytosine, along with uracil and thymine, from pyrimidine under the space-like laboratory conditions, which is of interest because pyrimidine has been found in meteorites although its origin is unknown.

Cytosine can be found as part of DNA, as part of RNA, or as a part of a nucleotide. As cytidine triphosphate (CTP), it can act as a co-factor to enzymes, and can transfer a phosphate to convert adenosine diphosphate (ADP) to adenosine triphosphate (ATP).

In DNA and RNA, cytosine is paired with guanine. However, it is inherently unstable, and can change into uracil (spontaneous deamination). This can lead to a point mutation if not repaired by the DNA repair enzymes such as uracil glycosylase, which cleaves a uracil in DNA.

When found third in a codon of RNA, cytosine is synonymous with uracil, as they are interchangeable as the third base.
When found as the second base in a codon, the third is always interchangeable. For example, UCU, UCC, UCA and UCG are all serine, regardless of the third base.

Cytosine can also be methylated into 5-methylcytosine by an enzyme called DNA methyltransferase or be methylated and hydroxylated to make 5-hydroxymethylcytosine.
Active enzymatic deamination of cytosine or 5-methylcytosine by the APOBEC family of cytosine deaminases could have both beneficial and detrimental implications on various cellular processes as well as on organismal evolution. The implications of deamination on 5-hydroxymethylcytosine, on the other hand, remains less understood.

Cytosine has not been found in meteorites, which suggests the first strands of RNA and DNA had to look elsewhere to obtain this building block. Cytosine likely formed within some meteorite parent bodies, however did not persist within these bodies due to an effective deamination reaction into uracil.



</doc>
<doc id="6019" url="https://en.wikipedia.org/wiki?curid=6019" title="Computational chemistry">
Computational chemistry

Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids. It is necessary because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form. While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.

Examples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles.

The methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied. That system can be one molecule, a group of molecules, or a solid. Computational chemistry methods range from very approximate to highly accurate; the latter are usually feasible for small systems only. "Ab initio" methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters.

Both "ab initio" and semi-empirical approaches involve approximations. These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all. For example, most "ab initio" calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation. In principle, "ab initio" methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced. In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains. The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.

In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target.

Building on the founding discoveries and theories in the history of quantum mechanics, the first theoretical calculations in chemistry were those of Walter Heitler and Fritz London in 1927. The books that were influential in the early development of computational quantum chemistry include Linus Pauling and E. Bright Wilson's 1935 "Introduction to Quantum Mechanics – with Applications to Chemistry", Eyring, Walter and Kimball's 1944 "Quantum Chemistry", Heitler's 1945 "Elementary Wave Mechanics – with Applications to Quantum Chemistry", and later Coulson's 1952 textbook "Valence", each of which served as primary references for chemists in the decades to follow.

With the development of efficient computer technology in the 1940s, the solutions of elaborate wave equations for complex atomic systems began to be a realizable objective. In the early 1950s, the first semi-empirical atomic orbital calculations were performed. Theoretical chemists became extensive users of the early digital computers. One major advance came with the 1951 paper in Reviews of Modern Physics by Clemens C. J. Roothaan in 1951, largely on the "LCAO MO" approach (Linear Combination of Atomic Orbitals Molecular Orbitals), for many years the second-most cited paper in that journal. A very detailed account of such use in the United Kingdom is given by Smith and Sutcliffe. The first "ab initio" Hartree–Fock method calculations on diatomic molecules were performed in 1956 at MIT, using a basis set of Slater orbitals. For diatomic molecules, a systematic study using a minimum basis set and the first calculation with a larger basis set were published by Ransil and Nesbet respectively in 1960. The first polyatomic calculations using Gaussian orbitals were performed in the late 1950s. The first configuration interaction calculations were performed in Cambridge on the EDSAC computer in the 1950s using Gaussian orbitals by Boys and coworkers. By 1971, when a bibliography of "ab initio" calculations was published, the largest molecules included were naphthalene and azulene. Abstracts of many earlier developments in "ab initio" theory have been published by Schaefer.

In 1964, Hückel method calculations (using a simple linear combination of atomic orbitals (LCAO) method to determine electron energies of molecular orbitals of π electrons in conjugated hydrocarbon systems) of molecules, ranging in complexity from butadiene and benzene to ovalene, were generated on computers at Berkeley and Oxford. These empirical methods were replaced in the 1960s by semi-empirical methods such as CNDO.

In the early 1970s, efficient "ab initio" computer programs such as ATMOL, Gaussian, IBMOL, and POLYAYTOM, began to be used to speed "ab initio" calculations of molecular orbitals. Of these four programs, only Gaussian, now vastly expanded, is still in use, but many other programs are now in use. At the same time, the methods of molecular mechanics, such as MM2 force field, were developed, primarily by Norman Allinger.

One of the first mentions of the term "computational chemistry" can be found in the 1970 book "Computers and Their Role in the Physical Sciences" by Sidney Fernbach and Abraham Haskell Taub, where they state "It seems, therefore, that 'computational chemistry' can finally be more and more of a reality." During the 1970s, widely different methods began to be seen as part of a new emerging discipline of "computational chemistry". The "Journal of Computational Chemistry" was first published in 1980.

Computational chemistry has featured in several Nobel Prize awards, most notably in 1998 and 2013. Walter Kohn, "for his development of the density-functional theory", and John Pople, "for his development of computational methods in quantum chemistry", received the 1998 Nobel Prize in Chemistry. Martin Karplus, Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for "the development of multiscale models for complex chemical systems".

The term "theoretical chemistry" may be defined as a mathematical description of chemistry, whereas "computational chemistry" is usually used when a mathematical method is sufficiently well developed that it can be automated for implementation on a computer. In theoretical chemistry, chemists, physicists, and mathematicians develop algorithms and computer programs to predict atomic and molecular properties and reaction paths for chemical reactions. Computational chemists, in contrast, may simply apply existing computer programs and methodologies to specific chemical questions.

Computational chemistry has two different aspects:

Thus, computational chemistry can assist the experimental chemist or it can challenge the experimental chemist to find entirely new chemical objects.

Several major areas may be distinguished within computational chemistry:

The words "exact" and "perfect" do not apply here, as very few aspects of chemistry can be computed exactly. However, almost every aspect of chemistry can be described in a qualitative or approximate quantitative computational scheme.

Molecules consist of nuclei and electrons, so the methods of quantum mechanics apply. Computational chemists often attempt to solve the non-relativistic Schrödinger equation, with relativistic corrections added, although some progress has been made in solving the fully relativistic Dirac equation. In principle, it is possible to solve the Schrödinger equation in either its time-dependent or time-independent form, as appropriate for the problem in hand; in practice, this is not possible except for very small systems. Therefore, a great number of approximate methods strive to achieve the best trade-off between accuracy and computational cost.

Accuracy can always be improved with greater computational cost. Significant errors can present themselves in ab initio models comprising many electrons, due to the computational cost of full relativistic-inclusive methods. This complicates the study of molecules interacting with high atomic mass unit atoms, such as transitional metals and their catalytic properties. Present algorithms in computational chemistry can routinely calculate the properties of small molecules that contain up to about 40 electrons with errors for energies less than a few kJ/mol. For geometries, bond lengths can be predicted within a few picometres and bond angles within 0.5 degrees. The treatment of larger molecules that contain a few dozen atoms is computationally tractable by more approximate methods such as density functional theory (DFT).

There is some dispute within the field whether or not the latter methods are sufficient to describe complex chemical reactions, such as those in biochemistry. Large molecules can be studied by semi-empirical approximate methods. Even larger molecules are treated by classical mechanics methods that use what are called molecular mechanics (MM). In QM-MM methods, small parts of large complexes are treated quantum mechanically (QM), and the remainder is treated approximately (MM).

One molecular formula can represent more than one molecular isomer: a set of isomers. Each isomer is a local minimum on the energy surface (called the potential energy surface) created from the total energy (i.e., the electronic energy, plus the repulsion energy between the nuclei) as a function of the coordinates of all the nuclei. A stationary point is a geometry such that the derivative of the energy with respect to all displacements of the nuclei is zero. A local (energy) minimum is a stationary point where all such displacements lead to an increase in energy. The local minimum that is lowest is called the global minimum and corresponds to the most stable isomer. If there is one particular coordinate change that leads to a decrease in the total energy in both directions, the stationary point is a transition structure and the coordinate is the reaction coordinate. This process of determining stationary points is called geometry optimization.

The determination of molecular structure by geometry optimization became routine only after efficient methods for calculating the first derivatives of the energy with respect to all atomic coordinates became available. Evaluation of the related second derivatives allows the prediction of vibrational frequencies if harmonic motion is estimated. More importantly, it allows for the characterization of stationary points. The frequencies are related to the eigenvalues of the Hessian matrix, which contains second derivatives. If the eigenvalues are all positive, then the frequencies are all real and the stationary point is a local minimum. If one eigenvalue is negative (i.e., an imaginary frequency), then the stationary point is a transition structure. If more than one eigenvalue is negative, then the stationary point is a more complex one, and is usually of little interest. When one of these is found, it is necessary to move the search away from it if the experimenter is looking solely for local minima and transition structures.

The total energy is determined by approximate solutions of the time-dependent Schrödinger equation, usually with no relativistic terms included, and by making use of the Born–Oppenheimer approximation, which allows for the separation of electronic and nuclear motions, thereby simplifying the Schrödinger equation. This leads to the evaluation of the total energy as a sum of the electronic energy at fixed nuclei positions and the repulsion energy of the nuclei. A notable exception are certain approaches called direct quantum chemistry, which treat electrons and nuclei on a common footing. Density functional methods and semi-empirical methods are variants on the major theme. For very large systems, the relative total energies can be compared using molecular mechanics. The ways of determining the total energy to predict molecular structures are:

The programs used in computational chemistry are based on many different quantum-chemical methods that solve the molecular Schrödinger equation associated with the molecular Hamiltonian. Methods that do not include any empirical or semi-empirical parameters in their equations – being derived directly from theoretical principles, with no inclusion of experimental data – are called "ab initio methods". This does not imply that the solution is an exact one; they are all approximate quantum mechanical calculations. It means that a particular approximation is rigorously defined on first principles (quantum theory) and then solved within an error margin that is qualitatively known beforehand. If numerical iterative methods must be used, the aim is to iterate until full machine accuracy is obtained (the best that is possible with a finite word length on the computer, and within the mathematical and/or physical approximations made).

The simplest type of "ab initio" electronic structure calculation is the Hartree–Fock method (HF), an extension of molecular orbital theory, in which the correlated electron-electron repulsion is not specifically taken into account; only its average effect is included in the calculation. As the basis set size is increased, the energy and wave function tend towards a limit called the Hartree–Fock limit. Many types of calculations (termed post-Hartree–Fock methods) begin with a Hartree–Fock calculation and subsequently correct for electron-electron repulsion, referred to also as electronic correlation. As these methods are pushed to the limit, they approach the exact solution of the non-relativistic Schrödinger equation. To obtain exact agreement with experiment, it is necessary to include relativistic and spin orbit terms, both of which are far more important for heavy atoms. In all of these approaches, along with choice of method, it is necessary to choose a basis set. This is a set of functions, usually centered on the different atoms in the molecule, which are used to expand the molecular orbitals with the linear combination of atomic orbitals (LCAO) molecular orbital method ansatz. Ab initio methods need to define a level of theory (the method) and a basis set.

The Hartree–Fock wave function is a single configuration or determinant. In some cases, particularly for bond breaking processes, this is inadequate, and several configurations must be used. Here, the coefficients of the configurations, and of the basis functions, are optimized together.

The total molecular energy can be evaluated as a function of the molecular geometry; in other words, the potential energy surface. Such a surface can be used for reaction dynamics. The stationary points of the surface lead to predictions of different isomers and the transition structures for conversion between isomers, but these can be determined without a full knowledge of the complete surface.

A particularly important objective, called computational thermochemistry, is to calculate thermochemical quantities such as the enthalpy of formation to chemical accuracy. Chemical accuracy is the accuracy required to make realistic chemical predictions and is generally considered to be 1 kcal/mol or 4 kJ/mol. To reach that accuracy in an economic way it is necessary to use a series of post-Hartree–Fock methods and combine the results. These methods are called quantum chemistry composite methods.

Density functional theory (DFT) methods are often considered to be "ab initio methods" for determining the molecular electronic structure, even though many of the most common functionals use parameters derived from empirical data, or from more complex calculations. In DFT, the total energy is expressed in terms of the total one-electron density rather than the wave function. In this type of calculation, there is an approximate Hamiltonian and an approximate expression for the total electron density. DFT methods can be very accurate for little computational cost. Some methods combine the density functional exchange functional with the Hartree–Fock exchange term and are termed hybrid functional methods.

Semi-empirical quantum chemistry methods are based on the Hartree–Fock method formalism, but make many approximations and obtain some parameters from empirical data. They were very important in computational chemistry from the 60s to the 90s, especially for treating large molecules where the full Hartree–Fock method without the approximations were too costly. The use of empirical parameters appears to allow some inclusion of correlation effects into the methods.

Primitive semi-empirical methods were designed even before, where the two-electron part of the Hamiltonian is not explicitly included. For π-electron systems, this was the Hückel method proposed by Erich Hückel, and for all valence electron systems, the extended Hückel method proposed by Roald Hoffmann. Sometimes, Hückel methods are referred to as "completely emprirical" because they do not derive from a Hamiltonian.Yet, the term "empirical methods", or "empirical force fields" is usually used to describe Molecular Mechanics.

In many cases, large molecular systems can be modeled successfully while avoiding quantum mechanical calculations entirely. Molecular mechanics simulations, for example, use one classical expression for the energy of a compound, for instance the harmonic oscillator. All constants appearing in the equations must be obtained beforehand from experimental data or "ab initio" calculations.

The database of compounds used for parameterization, i.e., the resulting set of parameters and functions is called the force field, is crucial to the success of molecular mechanics calculations. A force field parameterized against a specific class of molecules, for instance proteins, would be expected to only have any relevance when describing other molecules of the same class.

These methods can be applied to proteins and other large biological molecules, and allow studies of the approach and interaction (docking) of potential drug molecules.

Computational chemical methods can be applied to solid state physics problems. The electronic structure of a crystal is in general described by a band structure, which defines the energies of electron orbitals for each point in the Brillouin zone. Ab initio and semi-empirical calculations yield orbital energies; therefore, they can be applied to band structure calculations. Since it is time-consuming to calculate the energy for a molecule, it is even more time-consuming to calculate them for the entire list of points in the Brillouin zone.

Once the electronic and nuclear variables are separated (within the Born–Oppenheimer representation), in the time-dependent approach, the wave packet corresponding to the nuclear degrees of freedom is propagated via the time evolution operator (physics) associated to the time-dependent Schrödinger equation (for the full molecular Hamiltonian). In the complementary energy-dependent approach, the time-independent Schrödinger equation is solved using the scattering theory formalism. The potential representing the interatomic interaction is given by the potential energy surfaces. In general, the potential energy surfaces are coupled via the vibronic coupling terms.

The most popular methods for propagating the wave packet associated to the molecular geometry are:

Molecular dynamics (MD) use either quantum mechanics, molecular mechanics or a mixture of both to calculate forces which are then used to solve Newton's laws of motion to examine the time-dependent behaviour of systems. The result of a molecular dynamics simulation is a trajectory that describes how the position and velocity of particles varies with time.

QM/MM is a hybrid method that attempts to combine the accuracy of quantum mechanics with the speed of molecular mechanics. It is useful for simulating very large molecules such as enzymes.

The atoms in molecules (QTAIM) model of Richard Bader was developed to effectively link the quantum mechanical model of a molecule, as an electronic wavefunction, to chemically useful concepts such as atoms in molecules, functional groups, bonding, the theory of Lewis pairs, and the valence bond model. Bader has demonstrated that these empirically useful chemistry concepts can be related to the topology of the observable charge density distribution, whether measured or calculated from a quantum mechanical wavefunction. QTAIM analysis of molecular wavefunctions is implemented, for example, in the AIMAll software package.

Many self-sufficient exist. Some include many methods covering a wide range, while others concentrate on a very specific range or even on one method. Details of most of them can be found in:





</doc>
<doc id="6020" url="https://en.wikipedia.org/wiki?curid=6020" title="Crash (Ballard novel)">
Crash (Ballard novel)

Crash is a novel by English author J. G. Ballard, first published in 1973. It is a story about symphorophilia; specifically car-crash sexual fetishism: its protagonists become sexually aroused by staging and participating in real car-crashes.

It was a highly controversial novel: one publisher's reader returned the verdict "This author is beyond psychiatric help. Do Not Publish!" In 1996, the novel was made into a film of the same name by David Cronenberg.

The story is told through the eyes of narrator James Ballard, named after the author himself, but it centers on the sinister figure of Dr. Robert Vaughan, a "former TV-scientist, turned nightmare angel of the expressways". Ballard meets Vaughan after being involved in a car accident himself near London Airport. Gathering around Vaughan is a group of alienated people, all of them former crash victims, who follow him in his pursuit to re-enact the crashes of celebrities and experience what the narrator calls "a new sexuality, born from a perverse technology". Vaughan's ultimate fantasy is to die in a head-on collision with movie star Elizabeth Taylor.

The Normal's 1978 song "Warm Leatherette" was inspired by the novel, as reportedly was the international hit "Cars" by Gary Numan in 1979. "Miss the Girl," a 1983 single by The Creatures, is also inspired by the novel.
The Manic Street Preachers' song "Mausoleum" from 1994's "The Holy Bible" contains the famous Ballard quote about his reasons for writing the book, "I wanted to rub the human face in its own vomit. I wanted to force it to look in the mirror." 

An apparently unauthorized adaptation of Crash called "Nightmare Angel" was filmed in 1986 by Susan Emerling and Zoe Beloff. This short film bears the credit "Inspired by J.G. Ballard."




</doc>
<doc id="6021" url="https://en.wikipedia.org/wiki?curid=6021" title="C (programming language)">
C (programming language)

C (, as in the letter "c") is a general-purpose, procedural computer programming language supporting structured programming, lexical variable scope, and recursion, while a static type system prevents unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and has found lasting use in applications previously coded in assembly language. Such applications include operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.

C was originally developed at Bell Labs by Dennis Ritchie between 1972 and 1973 to make utilities running on Unix. Later, it was applied to re-implementing the kernel of the Unix operating system. During the 1980s, C gradually gained popularity. Nowadays, it is one of the most widely used programming languages, with C compilers from various vendors available for the majority of existing computer architectures and operating systems. C has been standardized by the ANSI since 1989 (see ANSI C) and subsequently by the International Organization for Standardization.

C is an imperative procedural language. It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal runtime support. Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant C program that is written with portability in mind can be compiled for a wide variety of computer platforms and operating systems with few changes to its source code; the language has become available on various platforms, from embedded microcontrollers to supercomputers.

Like most procedural languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion. Its static type system prevents unintended operations. In C, all executable code is contained within subroutines (also called "functions", though not strictly in the sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.

The C language also exhibits the following characteristics:


While C does not include certain features found in other languages (such as object orientation and garbage collection), these can be implemented or emulated, often through the use of external libraries (e.g., the GLib Object System or the Boehm garbage collector).

Many later languages have borrowed directly or indirectly from C, including C++, C#, Unix's C shell, D, Go, Java, JavaScript, Limbo, LPC, Objective-C, Perl, PHP, Python, Rust, Swift, Verilog and SystemVerilog (hardware description languages). These languages have drawn many of their control structures and other basic features from C. Most of them (Python being a dramatic exception) also express highly similar syntax to C, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.

The origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. Thompson needed a programming language to make utilities. At first, he tried to make a Fortran compiler, but soon gave up the idea and made a new language, B, Thompson's simplified version of BCPL. However, few utilities were written in B because B was too slow, and B could not take advantage of PDP-11 features such as byte addressability.

In 1972, Ritchie started to improve B, which resulted in creating a new language C. C compiler and some utilities made by C were included in Version 2 Unix. 
At Version 4 Unix released at Nov. 1973, the Unix kernel was extensively re-implemented by C. By this time, the C language had acquired some powerful features such as codice_12 types.

Unix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system (which was written in PL/I) and Master Control Program (MCP) for the Burroughs B5000 (which was written in ALGOL) in 1961. In around 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.

In 1978, Brian Kernighan and Dennis Ritchie published the first edition of "The C Programming Language". This book, known to C programmers as "K&R", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as "K&R C". The second edition of the book covers the later ANSI C standard, described below.

K&R introduced several language features:


Even after the publication of the 1989 ANSI standard, for many years K&R C was still considered the "lowest common denominator" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.

In early versions of C, only functions that return types other than codice_30 must be declared if used before the function definition; functions used without prior declaration were presumed to return type codice_30.

For example:

The codice_30 type specifiers which are commented out could be omitted in K&R C, but are required in later standards.

Since K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.

In the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC) and some other vendors. These included:


The large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.

During the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.

In 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 "Programming Language C". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.

In 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms "C89" and "C90" refer to the same programming language.

ANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.

One of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), codice_15 pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.

C89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.

In cases where code must be compilable by either standard-conforming or K&R C-based compilers, the codice_38 macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.

After the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.

The C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as "C99". It has since been amended three times by Technical Corrigenda.

C99 introduced several new features, including inline functions, several new data types (including codice_39 and a codice_40 type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with codice_41, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.

C99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has codice_30 implicitly assumed. A standard macro codice_43 is defined with value codice_44 to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.

In 2007, work began on another revision of the C standard, informally called "C1X" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.

The C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro codice_43 is defined as codice_46 to indicate that C11 support is available.

Published in June 2018, C18 is the current standard for the C programming language. It introduces no new language features, only technical corrections and clarifications to defects in C11. The standard macro codice_43 is defined as codice_48.

Historically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.

In 2008, the C Standards Committee published a technical report extending the C language to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.

C has a formal grammar specified by the C standard. Line endings are generally not significant in C; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters codice_49 and codice_50, or (since C99) following codice_41 until the end of the line. Comments delimited by codice_49 and codice_50 do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.

C source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as codice_12, codice_35, and codice_14, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as codice_57 and codice_30 specify built-in types. Sections of code are enclosed in braces (codice_59 and codice_60, sometimes called "curly brackets") to limit the scope of declarations and to act as a single statement for control structures.

As an imperative language, C uses "statements" to specify actions. The most common statement is an "expression statement", consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by codice_61(-codice_62) conditional execution and by codice_63-codice_4, codice_4, and codice_2 iterative execution (looping). The codice_2 statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. codice_68 and codice_69 can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured codice_70 statement which branches directly to the designated label within the function. codice_5 selects a codice_72 to be executed based on the value of an integer expression.

Expressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next "sequence point"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (codice_73, codice_10, codice_75 and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.

Kernighan and Ritchie say in the Introduction of "The C Programming Language": "C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better." The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.

The basic C source character set includes the following characters:


Newline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.

Additional multi-byte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multi-national Unicode characters to be embedded portably within C source text by using codice_83 or codice_84 encoding (where the codice_85 denotes a hexadecimal character), although this feature is not yet widely implemented.

The basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.

C89 has 32 reserved words, also known as keywords, which are the words that cannot be used for any purposes other than those for which they are predefined:

C99 reserved five more words:

C11 reserved seven more words:

Most of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved word called codice_130, but this was seldom implemented, and has now been removed as a reserved word.

C supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:


C uses the operator codice_136 (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator codice_156 to test for equality. The similarity between these two operators (assignment and equality) may result in the accidental use of one in place of the other, and in many cases, the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression codice_176 might mistakenly be written as codice_177, which will be evaluated as true if codice_76 is not zero after the assignment.

The C operator precedence is not always intuitive. For example, the operator codice_156 binds more tightly than (is executed prior to) the operators codice_9 (bitwise AND) and codice_149 (bitwise OR) in expressions such as codice_182, which must be written as codice_183 if that is the coder's intent.

The "hello, world" example, which appeared in the first edition of "K&R", has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints "hello, world" to the standard output, which is usually a terminal or screen display.

The original version was:
main()

A standard-conforming "hello, world" program is:


int main(void)

The first line of the program contains a preprocessing directive, indicated by codice_184. This causes the compiler to replace that line with the entire text of the codice_185 standard header, which contains declarations for standard input and output functions such as codice_186. The angle brackets surrounding codice_185 indicate that codice_185 is located using a search strategy that prefers headers provided with the compiler to other headers having the same name, as opposed to double quotes which typically include local or project-specific header files.

The next line indicates that a function named codice_189 is being defined. The codice_189 function serves a special purpose in C programs; the run-time environment calls the codice_189 function to begin program execution. The type specifier codice_30 indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the codice_189 function, is an integer. The keyword codice_15 as a parameter list indicates that this function takes no arguments.

The opening curly brace indicates the beginning of the definition of the codice_189 function.

The next line "calls" (diverts execution to) a function named codice_186, which in this case is supplied from a system library. In this call, the codice_186 function is "passed" (provided with) a single argument, the address of the first character in the string literal codice_198. The string literal is an unnamed array with elements of type codice_57, set up automatically by the compiler with a final 0-valued character to mark the end of the array (codice_186 needs to know this). The codice_201 is an "escape sequence" that C translates to a "newline" character, which on output signifies the end of the current line. The return value of the codice_186 function is of type codice_30, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the codice_186 function succeeded.) The semicolon codice_205 terminates the statement.

The closing curly brace indicates the end of the code for the codice_189 function. According to the C99 specification and newer, the codice_189 function, unlike any other function, will implicitly return a value of codice_80 upon reaching the codice_60 that terminates the function. (Formerly an explicit codice_210 statement was required.) This is interpreted by the run-time system as an exit code indicating successful execution.

The type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal. There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (codice_14). Integer type codice_57 is often used for single-byte characters. C99 added a boolean datatype. There are also derived types including arrays, pointers, records (codice_12), and unions (codice_35).

C is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a "type cast" to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.

Some find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: "declaration reflects use".)

C's "usual arithmetic conversions" allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.

C supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be "dereferenced" to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers. Many data types, such as trees, are commonly implemented as dynamically allocated codice_12 objects linked together using pointers. Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.

A "null pointer value" explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no "next" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a "null pointer constant" can be written as codice_80, with or without explicit casting to a pointer type, or as the codice_217 macro defined by several standard headers. In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.

Void pointers (codice_218) point to objects of unspecified type, and can therefore be used as "generic" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.

Careless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.

Array types in C are traditionally of a fixed, static size specified at compile time. (The more recent C99 standard also allows a form of variable-length arrays.) However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's codice_219 function, and treat it as an array. C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.

Since arrays are always accessed (in effect) via pointers, array accesses are typically "not" checked against the underlying array size, although some compilers may provide bounds checking as an option. Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions. If bounds checking is desired, it must be done manually.

C does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting "multi-dimensional array" can be thought of as increasing in row-major order.

Multi-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional "row vector" of pointers to the columns.)

C99 introduced "variable-length arrays" which address some, but not all, of the issues with ordinary C arrays.

The subscript notation codice_220 (where codice_221 designates a pointer) is syntactic sugar for codice_222. Taking advantage of the compiler's knowledge of the pointer type, the address that codice_223 points to is not the base address (pointed to by codice_221) incremented by codice_27 bytes, but rather is defined to be the base address incremented by codice_27 multiplied by the size of an element that codice_221 points to. Thus, codice_220 designates the codice_229th element of the array.

Furthermore, in most expression contexts (a notable exception is as operand of codice_108), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.

The size of an element can be determined by applying the operator codice_108 to any dereferenced element of codice_221, as in codice_233 or codice_234, and the number of elements in a declared array codice_78 can be determined as codice_236. The latter only applies to array names: variables declared with subscripts (codice_237). Due to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (codice_219); code such as codice_239 (where codice_240 designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested. Since array name arguments to codice_108 are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same codice_108 issues as array pointers.

Thus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array "points to" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the codice_243 function, or by accessing the individual elements.

One of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:


These three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.

Where possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary. Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on codice_219 for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)

Unless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.

Another issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before codice_248 is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a "memory leak." Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)

The C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single "archive" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., codice_249, shorthand for "link the math library").

The most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, codice_185) specify the interfaces for these and other standard library facilities.

Another common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.

Since many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.

File input and output (I/O) is not part of the C language itself but instead is handled by libraries (such as the C standard library) and their associated header files (e.g. codice_185). File handling is generally implemented through high-level I/O which works through streams. A stream is from this perspective a data flow that is independent of devices, while a file is a concrete device. The high level I/O is done through the association of a stream to a file. In the C standard library, a buffer (a memory area or queue) is temporarily used to store data before it's sent to the final destination. This reduces the time spent waiting for slower devices, for example a hard drive or solid state drive. Low-level I/O functions are not part of the standard C library but are generally part of "bare metal" programming (programming that's independent of any operative system such as most but not all embedded programming). With few exceptions, implementations include low-level I/O.

A number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler. The tool lint was the first such, leading to many others.

Automated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.

There are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.

Tools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.

C is widely used for system programming in implementing operating systems and embedded system applications, because C code, when written for portability, can be used for most purposes, yet when needed, system-specific code can be used to access specific hardware addresses and to perform type punning to match externally imposed interface requirements, with a low run-time demand on system resources.

C can also be used for website programming using CGI as a "gateway" for information between the Web application, the server, and the browser. C is often chosen over interpreted languages because of its speed, stability, and near-universal availability.

One consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The reference implementations of Python, Perl and PHP, for example, are all written in C.

Because the layer of abstraction is thin and the overhead is low, C enables programmers to create efficient implementations of algorithms and data structures, useful for computationally intense programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C.

C is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.

C has also been widely used to implement end-user applications. However, such applications can also be written in newer, higher-level languages.

C has both directly and indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell. The most pervasive influence has been syntactical, all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.

Several C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.

When object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.

The C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax. C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.

Objective-C was originally a very "thin" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.

In addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C.





</doc>
<doc id="6023" url="https://en.wikipedia.org/wiki?curid=6023" title="Castle of the Winds">
Castle of the Winds

Castle of the Winds (also known as "Castle of the Winds: Vanquish the Dark Forces") is a tile-based roguelike video game for Microsoft Windows. It was developed by Rick Saada in 1989 and distributed by Epic MegaGames in 1993.
The game was given around 1998 into the public domain and provided as Freeware download by the author. Though it is secondary to its hack and slash gameplay, "Castle of the Winds" has a plot loosely based on Norse mythology, told with setting changes, unique items, and occasional passages of text. The game is composed of two parts: A Question of Vengeance, released as shareware, and Lifthransir's Bane, sold commercially. A combined license for both parts was also sold.

The game differs from most roguelikes in a number of ways. Its interface is mouse-dependent, but supports keyboard shortcuts (such as 'g' to get an item). "Castle of the Winds" also allows the player to restore saved games after dying.

Magic
The game favors the use of magic in combat, as spells are the only weapons that work from a distance. The player character automatically gains a spell with each experience level, and can permanently gain others using corresponding books, until all thirty spells available are learned. There are two opposing pairs of elements: cold vs. fire and lightning vs. acid/poison. Spells are divided into six categories: attack, defense, healing, movement, divination, and miscellaneous.

Items
"Castle of the Winds" possesses an inventory system that limits a player's load based on weight and bulk, rather than by number of items. It allows the character to use different containers, including packs, belts, chests, and bags. Other items include weapons, armor, protective clothing, purses, and ornamental jewellery. Almost every item in the game can be normal, cursed, or enchanted, with curses and enchantments working in a manner similar to "NetHack". Although items do not break with use, they may already be broken or rusted when found. Most objects that the character currently carries can be renamed.

Town Services
Wherever the player goes before entering the dungeon, there is always a town which offers the basic services of a temple for healing and curing curses, a junk store where anything can be sold for a few copper coins, a sage who can identify items and (from the second town onwards) a bank for storing the total capacity of coins to lighten the player's load. Other services that differ and vary in what they sell are outfitters, weaponsmiths, armoursmiths, magic shops and general stores.

Time
The game tracks how much time has been spent playing the game. Although story events are not triggered by the passage of time, it does determine when merchants rotate their stock. Victorious players are listed as "Valhalla's Champions" in the order of time taken, from fastest to slowest. If the player dies, they are still put on the list, but are categorized as "Dead", with their experience point total listed as at the final killing blow. The amount of time spent also determines the difficulty of the last boss.

The player begins in a tiny hamlet, near which he/she used to live. His/her farm has been destroyed and godparents killed. After clearing out an abandoned mine, the player finds a scrap of parchment that reveals the death of the player's godparents was ordered by an unknown enemy. The player then returns to the hamlet to find it pillaged, and decides to travel to Bjarnarhaven.

Once in Bjarnarhaven, the player explores the levels beneath a nearby fortress, eventually facing Hrungnir, the Hill Giant Lord, responsible for ordering the player's godparents' death. Hrungnir carries the Enchanted Amulet of Kings. Upon activating the amulet, the player is informed of his/her past by his/her dead father, after which the player is transported to the town of Crossroads, and "Part I" ends. The game can be imported or started over in "Part II".

The town of Crossroads is run by a Jarl who at first does not admit the player, but later (on up to three occasions) provides advice and rewards. The player then enters the nearby ruined titular Castle of the Winds. There the player meets his/her deceased grandfather, who instructs him/her to venture into the dungeons below, defeat Surtur, and reclaim their birthright. Venturing deeper, the player encounters monsters run rampant, a desecrated crypt, a necromancer, and the installation of various special rooms for elementals. The player eventually meets and defeats the Wolf-Man leader, Bear-Man leader, the four Jotun kings, a Demon Lord, and finally Surtur. Upon defeating Surtur and escaping the dungeons, the player sits upon the throne, completing the game.

Inspired by his love of RPGs and while learning Windows programming in the 80s, Rick Saada designed and completed Castle of the Winds. The game sold 13,500 copies. By 1998, the game's author, Rick Saada, decided to distribute the entirety of "Castle of the Winds" free of charge.

The game is public domain per Rick Saada's words: 

All terrain tiles, some landscape features, all monsters and objects, and some spell/effect graphics take the form of Windows 3.1 icons and were done by Paul Canniff. Multi-tile graphics, such as ball spells and town buildings, are bitmaps included in the executable file. No graphics use colors other than the Windows-standard 16-color palette, plus transparency. They exist in monochrome versions as well, meaning that the game will display well on monochrome monitors.

The map view is identical to the playing-field view, except for scaling to fit on one screen. A simplified map view is available to improve performance on slower computers. The latter functionality also presents a cleaner display, as the aforementioned scaling routine does not always work correctly.

Computer Gaming World rated the gameplay as good and the graphics simple but effective, while noticing the lack of audio, but regarded the game itself enjoyable.


</doc>
<doc id="6024" url="https://en.wikipedia.org/wiki?curid=6024" title="Calvinism">
Calvinism

Calvinism (also called the Reformed tradition, Reformed Christianity, Reformed Protestantism, or the Reformed faith) is a major branch of Protestantism that follows the theological tradition and forms of Christian practice set down by John Calvin and other Reformation-era theologians.

Calvinists broke from the Roman Catholic Church in the 16th century. Calvinists differ from Lutherans on the real presence of Christ in the Eucharist, theories of worship, and the use of God's law for believers, among other things. As declared in the Westminster and Second Helvetic confessions, the core doctrines are predestination and election. The term "Calvinism" can be misleading, because the religious tradition which it denotes has always been diverse, with a wide range of influences rather than a single founder. In the context of the Reformation, Huldrych Zwingli began the Reformed tradition in 1519 in the city of Zürich. His followers were instantly labeled "Zwinglians", consistent with the Catholic practice of naming heresy after its founder. Very soon, Zwingli was joined by Martin Bucer, Wolfgang Capito, William Farel, Johannes Oecolampadius and other early Reformed thinkers. The namesake of the movement, French reformer John Calvin, converted to the Reformed tradition from Roman Catholicism only in the late 1520s or early 1530s as it was already being developed. The movement was first called "Calvinism", referring to John Calvin, by Lutherans who opposed it. Many within the tradition find it either an indescriptive or an inappropriate term and would prefer the word "Reformed" to be used instead. Some Calvinists prefer the term Augustinian-Calvinism since Calvin credited his theology to Augustine of Hippo. The most important Reformed theologians include John Calvin, Huldrych Zwingli, Martin Bucer, William Farel, Heinrich Bullinger, Peter Martyr Vermigli, Theodore Beza, and John Knox. In the twentieth century, Abraham Kuyper, Herman Bavinck, B. B. Warfield, J. Gresham Machen, Karl Barth, Martyn Lloyd-Jones, Cornelius Van Til, Gordon Clark, and R. C. Sproul were influential. Contemporary Reformed theologians include J. I. Packer, John MacArthur, Timothy J. Keller, David Wells, and Michael Horton.

Reformed churches may exercise several forms of ecclesiastical polity; most are presbyterian or congregationalist, though some are episcopalian. Calvinism is largely represented by Continental Reformed, Presbyterian, and Congregationalist traditions. The biggest Reformed association is the World Communion of Reformed Churches with more than 100 million members in 211 member denominations around the world. There are more conservative Reformed federations such as the World Reformed Fellowship and the International Conference of Reformed Churches, as well as independent churches.

Calvinism is named after John Calvin. It was first used by a Lutheran theologian in 1552. It was a common practice of the Roman Catholic Church to name what it viewed as heresy after its founder. Nevertheless, the term first came out of Lutheran circles. Calvin denounced the designation himself:
Despite its negative connotation, this designation became increasingly popular in order to distinguish Calvinists from Lutherans and from newer Protestant branches that emerged later. The vast majority of churches that trace their history back to Calvin (including Presbyterians, Congregationalists, and a row of other Calvinist churches) do not use it themselves, since the designation "Reformed" is more generally accepted and preferred, especially in the English-speaking world. Moreover, these churches claim to be—in accordance with John Calvin's own words—"renewed accordingly with the true order of gospel".

Since the Arminian controversy, the Reformed tradition—as a branch of Protestantism distinguished from Lutheranism—divided into two separate groups: Arminians and Calvinists. However, it is now rare to call Arminians a part of the Reformed tradition. While the Reformed theological tradition addresses all of the traditional topics of Christian theology, the word "Calvinism" is sometimes used to refer to particular Calvinist views on soteriology and predestination, which are summarized in part by the Five Points of Calvinism. Some have also argued that Calvinism as a whole stresses the sovereignty or rule of God in all things including salvation.

First-generation Reformed theologians include Huldrych Zwingli (1484–1531), Martin Bucer (1491–1551), Wolfgang Capito (1478–1541), John Oecolampadius (1482–1531), and Guillaume Farel (1489–1565). These reformers came from diverse academic backgrounds, but later distinctions within Reformed theology can already be detected in their thought, especially the priority of scripture as a source of authority. Scripture was also viewed as a unified whole, which led to a covenantal theology of the sacraments of baptism and the Lord's Supper as visible signs of the covenant of grace. Another Reformed distinctive present in these theologians was their denial of the bodily presence of Christ in the Lord's supper. Each of these theologians also understood salvation to be by grace alone, and affirmed a doctrine of particular election (the teaching that some people are chosen by God for salvation). Martin Luther and his successor Philipp Melanchthon were undoubtedly significant influences on these theologians, and to a larger extent later Reformed theologians. The doctrine of justification by faith alone was a direct inheritance from Luther.

John Calvin (1509–64), Heinrich Bullinger (1504–75), Wolfgang Musculus (1497–1563), Peter Martyr Vermigli (1500–62), and Andreas Hyperius (1511–64) belong to the second generation of Reformed theologians. Calvin's "Institutes of the Christian Religion" (1536–59) was one of the most influential theologies of the era. Toward the middle of the 16th century, the Reformed began to commit their beliefs to confessions of faith, which would shape the future definition of the Reformed faith. The 1549 "Consensus Tigurinus" brought together those who followed Zwingli and Bullinger's memorialist theology of the Lord's supper, which taught that the supper simply serves as a reminder of Christ's death, and Calvin's view that the supper serves as a means of grace with Christ actually present, though spiritually rather than bodily. The document demonstrates the diversity as well as unity in early Reformed theology. The remainder of the 16th century saw an explosion of confessional activity. The stability and breadth of Reformed theology during this period stand in marked contrast to the bitter controversy experienced by Lutherans prior to the 1579 Formula of Concord.

Due to Calvin's missionary work in France, his programme of reform eventually reached the French-speaking provinces of the Netherlands. Calvinism was adopted in the Electorate of the Palatinate under Frederick III, which led to the formulation of the Heidelberg Catechism in 1563, and in Navarre by Jeanne d'Albret. This and the Belgic Confession were adopted as confessional standards in the first synod of the Dutch Reformed Church in 1571. Leading divines, either Calvinist or those sympathetic to Calvinism, settled in England (Martin Bucer, Peter Martyr, and Jan Łaski) and Scotland (John Knox). During the English Civil War, the Calvinistic Puritans produced the Westminster Confession, which became the confessional standard for Presbyterians in the English-speaking world. Having established itself in Europe, the movement continued to spread to other parts of the world, including North America, South Africa, and Korea.

Calvin did not live to see the foundation of his work grow into an international movement; but his death allowed his ideas to break out of their city of origin, to succeed far beyond their borders, and to establish their own distinct character.

Although much of Calvin's work was in Geneva, his publications spread his ideas of a "correctly" Reformed church to many parts of Europe. In Switzerland, some cantons are still Reformed, and some are Catholic. Calvinism became the theological system of the majority in Scotland (see John Knox), the Netherlands (see William Ames, T. J. Frelinghuysen and Wilhelmus à Brakel), some communities in Flanders, and parts of Germany (especially these adjacent to the Netherlands) in the Palatinate, Kassel and Lippe with the likes of Olevianus and his colleague Zacharias Ursinus. In Hungary and the then-independent Transylvania, Calvinism was a significant religion. In the 16th century, the Reformation gained many supporters in Eastern Hungary and Hungarian-populated regions in Transylvania. In these parts, the Reformed nobles protected the faith. Almost all Transylvanian dukes were Reformed. Today there are about 3.5 million Hungarian Reformed people worldwide. It was influential in France, Lithuania and Poland before being mostly erased due to the counter-reformational activities taken up by the monarch in each country. Calvinism gained some popularity in Scandinavia, especially Sweden, but was rejected in favor of Lutheranism after the Synod of Uppsala in 1593.

Most settlers in the American Mid-Atlantic and New England were Calvinists, including the English Puritans, the French Huguenots and Dutch settlers of New Amsterdam (New York), and the Scotch-Irish Presbyterians of the Appalachian back country. Nonconforming Protestants, Puritans, Separatists, Independents, English religious groups coming out of the English Civil War, and other English dissenters not satisfied with the degree to which the Church of England had been reformed, held overwhelmingly Reformed views. They are often cited among the primary founders of the United States of America. Dutch and French Huguenot Calvinist settlers were also the first European colonizers of South Africa, beginning in the 17th century, who became known as Boers or Afrikaners.

Sierra Leone was largely colonized by Calvinist settlers from Nova Scotia, who were largely Black Loyalists, blacks who had fought for the British during the American War of Independence. John Marrant had organized a congregation there under the auspices of the Huntingdon Connection. Some of the largest Calvinist communions were started by 19th- and 20th-century missionaries. Especially large are those in Indonesia, Korea and Nigeria. In South Korea there are 20,000 Presbyterian congregations with about 9–10 million church members, scattered in more than 100 Presbyterian denominations. In South Korea, Presbyterianism is the largest Christian denomination.
A 2011 report of the Pew Forum on Religious and Public Life estimated that members of Presbyterian or Reformed churches make up 7% of the estimated 801 million Protestants globally, or approximately 56 million people. Though the broadly defined Reformed faith is much larger, as it constitutes Congregationalist (0.5%), most of the United and uniting churches (unions of different denominations) (7.2%) and most likely some of the other Protestant denominations (38.2%). All three are distinct categories from Presbyterian or Reformed (7%) in this report.

The Reformed family of churches is one of the largest Christian denominations. According to adherents.com the Reformed/Presbyterian/Congregational/United churches represent 75 million believers worldwide.

The World Communion of Reformed Churches, which includes some United Churches (most of these are primarily Reformed; see "Uniting and united churches" for details), has 80 million believers. WCRC is the third largest Christian communion in the world, after the Roman Catholic Church and the Eastern Orthodox Churches.

Many conservative Reformed churches which are strongly Calvinistic formed the World Reformed Fellowship which has about 70 member denominations. Most are not part of the World Communion of Reformed Churches because of its ecumenical attire. The International Conference of Reformed Churches is another conservative association.

Church of Tuvalu is the only officially established state church in the Calvinist tradition in the world.

Reformed theologians believe that God communicates knowledge of himself to people through the Word of God. People are not able to know anything about God except through this self-revelation. Speculation about anything which God has not revealed through his Word is not warranted. The knowledge people have of God is different from that which they have of anything else because God is infinite, and finite people are incapable of comprehending an infinite being. While the knowledge revealed by God to people is never incorrect, it is also never comprehensive.
According to Reformed theologians, God's self-revelation is always through his son Jesus Christ, because Christ is the only mediator between God and people. Revelation of God through Christ comes through two basic channels. The first is creation and providence, which is God's creating and continuing to work in the world. This action of God gives everyone knowledge about God, but this knowledge is only sufficient to make people culpable for their sin; it does not include knowledge of the gospel. The second channel through which God reveals himself is redemption, which is the gospel of salvation from condemnation which is punishment for sin.

In Reformed theology, the Word of God takes several forms. Jesus Christ himself is the Word Incarnate. The prophecies about him said to be found in the Old Testament and the ministry of the apostles who saw him and communicated his message are also the Word of God. Further, the preaching of ministers about God is the very Word of God because God is considered to be speaking through them. God also speaks through human writers in the Bible, which is composed of texts set apart by God for self-revelation. Reformed theologians emphasize the Bible as a uniquely important means by which God communicates with people. People gain knowledge of God from the Bible which cannot be gained in any other way.

Reformed theologians affirm that the Bible is true, but differences emerge among them over the meaning and extent of its truthfulness. Conservative followers of the Princeton theologians take the view that the Bible is true and inerrant, or incapable of error or falsehood, in every place. This view is very similar to that of Catholic orthodoxy as well as modern Evangelicalism. Another view, influenced by the teaching of Karl Barth and neo-orthodoxy, is found in the Presbyterian Church (U.S.A.)'s Confession of 1967. Those who take this view believe the Bible to be the primary source of our knowledge of God, but also that some parts of the Bible may be false, not witnesses to Christ, and not normative for today's church. In this view, Christ is the revelation of God, and the scriptures witness to this revelation rather than being the revelation itself.

Reformed theologians use the concept of covenant to describe the way God enters fellowship with people in history. The concept of covenant is so prominent in Reformed theology that Reformed theology as a whole is sometimes called "covenant theology". However, sixteenth and seventeenth-century theologians developed a particular theological system called "covenant theology" or "federal theology" which many conservative Reformed churches continue to affirm today. This framework orders God's life with people primarily in two covenants: the covenant of works and the covenant of grace. The covenant of works is made with Adam and Eve in the Garden of Eden. The terms of the covenant are that God provides a blessed life in the garden on condition that Adam and Eve obey God's law perfectly. Because Adam and Eve broke the covenant by eating the forbidden fruit, they became subject to death and were banished from the garden. This sin was passed down to all mankind because all people are said to be in Adam as a covenantal or "federal" head. Federal theologians usually infer that Adam and Eve would have gained immortality had they obeyed perfectly.

A second covenant, called the covenant of grace, is said to have been made immediately following Adam and Eve's sin. In it, God graciously offers salvation from death on condition of faith in God. This covenant is administered in different ways throughout the Old and New Testaments, but retains the substance of being free of a requirement of perfect obedience.

Through the influence of Karl Barth, many contemporary Reformed theologians have discarded the covenant of works, along with other concepts of federal theology. Barth saw the covenant of works as disconnected from Christ and the gospel, and rejected the idea that God works with people in this way. Instead, Barth argued that God always interacts with people under the covenant of grace, and that the covenant of grace is free of all conditions whatsoever. Barth's theology and that which follows him has been called "monocovenantal" as opposed to the "bi-covenantal" scheme of classical federal theology. Conservative contemporary Reformed theologians, such as John Murray, have also rejected the idea of covenants based on law rather than grace. Michael Horton, however, has defended the covenant of works as combining principles of law and love.

For the most part, the Reformed tradition did not modify the medieval consensus on the doctrine of God. God's character is described primarily using three adjectives: eternal, infinite, and unchangeable. Reformed theologians such as Shirley Guthrie have proposed that rather than conceiving of God in terms of his attributes and freedom to do as he pleases, the doctrine of God is to be based on God's work in history and his freedom to live with and empower people.
Traditionally, Reformed theologians have also followed the medieval tradition going back to before the early church councils of Nicaea and Chalcedon on the doctrine of the Trinity. God is affirmed to be one God in three persons: Father, Son, and Holy Spirit. The Son (Christ) is held to be eternally begotten by the Father and the Holy Spirit eternally proceeding from the Father and Son. However, contemporary theologians have been critical of aspects of Western views here as well. Drawing on the Eastern tradition, these Reformed theologians have proposed a "social trinitarianism" where the persons of the Trinity only exist in their life together as persons-in-relationship. Contemporary Reformed confessions such as the Barmen Confession and Brief Statement of Faith of the Presbyterian Church (USA) have avoided language about the attributes of God and have emphasized his work of reconciliation and empowerment of people. Feminist theologian Letty Russell used the image of partnership for the persons of the Trinity. According to Russell, thinking this way encourages Christians to interact in terms of fellowship rather than reciprocity. Conservative Reformed theologian Michael Horton, however, has argued that social trinitarianism is untenable because it abandons the essential unity of God in favor of a community of separate beings.

Reformed theologians affirm the historic Christian belief that Christ is eternally one person with a divine and a human nature. Reformed Christians have especially emphasized that Christ truly became human so that people could be saved. Christ's human nature has been a point of contention between Reformed and Lutheran Christology. In accord with the belief that finite humans cannot comprehend infinite divinity, Reformed theologians hold that Christ's human body cannot be in multiple locations at the same time. Because Lutherans believe that Christ is bodily present in the Eucharist, they hold that Christ is bodily present in many locations simultaneously. For Reformed Christians, such a belief denies that Christ actually became human. Some contemporary Reformed theologians have moved away from the traditional language of one person in two natures, viewing it as unintelligible to contemporary people. Instead, theologians tend to emphasize Jesus' context and particularity as a first-century Jew.
John Calvin and many Reformed theologians who followed him describe Christ's work of redemption in terms of three offices: prophet, priest, and king. Christ is said to be a prophet in that he teaches perfect doctrine, a priest in that he intercedes to the Father on believers' behalf and offered himself as a sacrifice for sin, and a king in that he rules the church and fights on believers' behalf. The threefold office links the work of Christ to God's work in ancient Israel. Many, but not all, Reformed theologians continue to make use of the threefold office as a framework because of its emphasis on the connection of Christ's work to Israel. They have, however, often reinterpreted the meaning of each of the offices. For example, Karl Barth interpreted Christ's prophetic office in terms of political engagement on behalf of the poor.

Christians believe Jesus' death and resurrection makes it possible for believers to attain forgiveness for sin and reconciliation with God through the atonement. Reformed Protestants generally subscribe to a particular view of the atonement called penal substitutionary atonement, which explains Christ's death as a sacrificial payment for sin. Christ is believed to have died in place of the believer, who is accounted righteous as a result of this sacrificial payment.

In Christian theology, people are created good and in the image of God but have become corrupted by sin, which causes them to be imperfect and overly self-interested. Reformed Christians, following the tradition of Augustine of Hippo, believe that this corruption of human nature was brought on by Adam and Eve's first sin, a doctrine called original sin. Although earlier Christian authors taught the elements of physical death, moral weakness, and a sin propensity within original sin, Augustine was the first Christian to add the concept of inherited guilt ("reatus") from Adam whereby every infant is born eternally damned and humans lack any residual ability to respond to God. Reformed theologians emphasize that this sinfulness affects all of a person's nature, including their will. This view, that sin so dominates people that they are unable to avoid sin, has been called total depravity. In colloquial English, the term "total depravity" can be easily misunderstood to mean that people are absent of any goodness or unable to do any good. However the Reformed teaching is actually that while people continue to bear God's image and may do things that appear outwardly good, their sinful intentions affect all of their nature and actions so that they are not pleasing to God.

Some contemporary theologians in the Reformed tradition, such as those associated with the PC(USA)'s Confession of 1967, have emphasized the social character of human sinfulness. These theologians have sought to bring attention to issues of environmental, economic, and political justice as areas of human life that have been affected by sin.

Reformed theologians, along with other Protestants, believe salvation from punishment for sin is to be given to all those who have faith in Christ. Faith is not purely intellectual, but involves trust in God's promise to save. Protestants do not hold there to be any other requirement for salvation, but that faith alone is sufficient.

Justification is the part of salvation where God pardons the sin of those who believe in Christ. It is historically held by Protestants to be the most important article of Christian faith, though more recently it is sometimes given less importance out of ecumenical concerns. People are not on their own able even to fully repent of their sin or prepare themselves to repent because of their sinfulness. Therefore, justification is held to arise solely from God's free and gracious act.

Sanctification is the part of salvation in which God makes the believer holy, by enabling them to exercise greater love for God and for other people. The good works accomplished by believers as they are sanctified are considered to be the necessary outworking of the believer's salvation, though they do not cause the believer to be saved. Sanctification, like justification, is by faith, because doing good works is simply living as the son of God one has become.

Reformed theologians teach that sin so affects human nature that they are unable even to exercise faith in Christ by their own will. While people are said to retain will, in that they willfully sin, they are unable not to sin because of the corruption of their nature due to original sin. Reformed Christians believe that God predestined some people to be saved and others were predestined to eternal damnation. This choice by God to save some is held to be unconditional and not based on any characteristic or action on the part of the person chosen. This view is opposed to the Arminian view that God's choice of whom to save is conditional or based on his foreknowledge of who would respond positively to God.

Karl Barth reinterpreted the Reformed doctrine of predestination to apply only to Christ. Individual people are only said to be elected through their being in Christ. Reformed theologians who followed Barth, including Jürgen Moltmann, David Migliore, and Shirley Guthrie, have argued that the traditional Reformed concept of predestination is speculative and have proposed alternative models. These theologians claim that a properly trinitarian doctrine emphasizes God's freedom to love all people, rather than choosing some for salvation and others for damnation. God's justice towards and condemnation of sinful people is spoken of by these theologians as out of his love for them and a desire to reconcile them to himself.

Most objections to and attacks on Calvinism focus on the "five points of Calvinism", also called the doctrines of grace, and remembered by the mnemonic "TULIP". The five points are popularly said to summarize the Canons of Dort; however, there is no historical relationship between them, and some scholars argue that their language distorts the meaning of the Canons, Calvin's theology, and the theology of 17th-century Calvinistic orthodoxy, particularly in the language of total depravity and limited atonement. The five points were more recently popularized in the 1963 booklet "The Five Points of Calvinism Defined, Defended, Documented" by David N. Steele and Curtis C. Thomas. The origins of the five points and the acronym are uncertain, but they appear to be outlined in the Counter Remonstrance of 1611, a less known Reformed reply to the Arminians that occurred prior to the Canons of Dort. The acronym was used by Cleland Boyd McAfee as early as circa 1905. An early printed appearance of the T-U-L-I-P acronym is in Loraine Boettner's 1932 book, "The Reformed Doctrine of Predestination". The acronym was very cautiously if ever used by Calvinist apologists and theologians before the booklet by Steele and Thomas. More recently, a broad range of theologians have sought to reformulate the TULIP terminology to reflect more accurately the Canons of Dort; one recent effort has been PROOF, standing for Planned Grace, Resurrecting Grace, Outrageous Grace, Overcoming Grace, and Forever Grace.

The central assertion of these points is that God saves every person upon whom he has mercy, and that his efforts are not frustrated by the unrighteousness or inability of humans.

Reformed Christians see the Christian Church as the community with which God has made the covenant of grace, a promise of eternal life and relationship with God. This covenant extends to those under the "old covenant" whom God chose, beginning with Abraham and Sarah. The church is conceived of as both invisible and visible. The invisible church is the body of all believers, known only to God. The visible church is the institutional body which contains both members of the invisible church as well as those who appear to have faith in Christ, but are not truly part of God's elect.

In order to identify the visible church, Reformed theologians have spoken of certain marks of the Church. For some, the only mark is the pure preaching of the gospel of Christ. Others, including John Calvin, also include the right administration of the sacraments. Others, such as those following the Scots Confession, include a third mark of rightly administered church discipline, or exercise of censure against unrepentant sinners. These marks allowed the Reformed to identify the church based on its conformity to the Bible rather than the Magisterium or church tradition.

The regulative principle of worship is a teaching shared by some Calvinists and Anabaptists on how the Bible orders public worship. The substance of the doctrine regarding worship is that God institutes in the Scriptures everything he requires for worship in the Church and that everything else is prohibited. As the regulative principle is reflected in Calvin's own thought, it is driven by his evident antipathy toward the Roman Catholic Church and its worship practices, and it associates musical instruments with icons, which he considered violations of the Ten Commandments' prohibition of graven images.

On this basis, many early Calvinists also eschewed musical instruments and advocated a cappella exclusive psalmody in worship, though Calvin himself allowed other scriptural songs as well as psalms, and this practice typified presbyterian worship and the worship of other Reformed churches for some time. The original Lord's Day service designed by John Calvin was a highly liturgical service with the Creed, Alms, Confession and Absolution, the Lord's supper, Doxologies, prayers, Psalms being sung, the Lords prayer being sung, Benedictions.

Since the 19th century, however, some of the Reformed churches have modified their understanding of the regulative principle and make use of musical instruments, believing that Calvin and his early followers went beyond the biblical requirements and that such things are circumstances of worship requiring biblically rooted wisdom, rather than an explicit command. Despite the protestations of those who hold to a strict view of the regulative principle, today hymns and musical instruments are in common use, as are contemporary worship music styles with elements such as worship bands.

The Westminster Confession of Faith limits the sacraments to baptism and the Lord's Supper. Sacraments are denoted "signs and seals of the covenant of grace." Westminster speaks of "a sacramental relation, or a sacramental union, between the sign and the thing signified; whence it comes to pass that the names and effects of the one are attributed to the other." Baptism is for infant children of believers as well as believers, as it is for all the Reformed except Baptists and some Congregationalists. Baptism admits the baptized into the visible church, and in it all the benefits of Christ are offered to the baptized. On the Lord's supper, Westminster takes a position between Lutheran sacramental union and Zwinglian memorialism: "the Lord's supper really and indeed, yet not carnally and corporally, but spiritually, receive and feed upon Christ crucified, and all benefits of his death: the body and blood of Christ being then not corporally or carnally in, with, or under the bread and wine; yet, as really, but spiritually, present to the faith of believers in that ordinance as the elements themselves are to their outward senses."

The 1689 London Baptist Confession of Faith does not use the term sacrament, but describes baptism and the Lord's supper as ordinances, as do most Baptists Calvinist or otherwise. Baptism is only for those who "actually profess repentance towards God", and not for the children of believers. Baptists also insist on immersion or dipping, in contradistinction to other Reformed Christians. The Baptist Confession describes the Lord's supper as "the body and blood of Christ being then not corporally or carnally, but spiritually present to the faith of believers in that ordinance", similarly to the Westminster Confession. There is significant latitude in Baptist congregations regarding the Lord's supper, and many hold the Zwinglian view.

There are two schools of thought regarding the logical order of God's decree to ordain the fall of man: supralapsarianism (from the Latin: "supra", "above", here meaning "before" + "lapsus", "fall") and infralapsarianism (from the Latin: "infra", "beneath", here meaning "after" + "lapsus", "fall"). The former view, sometimes called "high Calvinism", argues that the Fall occurred partly to facilitate God's purpose to choose some individuals for salvation and some for damnation. Infralapsarianism, sometimes called "low Calvinism", is the position that, while the Fall was indeed planned, it was not planned with reference to who would be saved.

Supralapsarians believe that God chose which individuals to save logically prior to the decision to allow the race to fall and that the Fall serves as the means of realization of that prior decision to send some individuals to hell and others to heaven (that is, it provides the grounds of condemnation in the reprobate and the need for salvation in the elect). In contrast, infralapsarians hold that God planned the race to fall logically prior to the decision to save or damn any individuals because, it is argued, in order to be "saved", one must first need to be saved from something and therefore the decree of the Fall must precede predestination to salvation or damnation.

These two views vied with each other at the Synod of Dort, an international body representing Calvinist Christian churches from around Europe, and the judgments that came out of that council sided with infralapsarianism (Canons of Dort, First Point of Doctrine, Article 7). The Westminster Confession of Faith also teaches (in Hodge's words "clearly impl[ies]") the infralapsarian view, but is sensitive to those holding to supralapsarianism. The Lapsarian controversy has a few vocal proponents on each side today, but overall it does not receive much attention among modern Calvinists.

Amyraldism (or sometimes Amyraldianism, also known as the School of Saumur, hypothetical universalism, post redemptionism, moderate Calvinism, or four-point Calvinism) is the belief that God, prior to his decree of election, decreed Christ's atonement for all alike if they believe, but seeing that none would believe on their own, he then elected those whom he will bring to faith in Christ, thereby preserving the Calvinist doctrine of unconditional election. The efficacy of the atonement remains limited to those who believe.

Named after its formulator Moses Amyraut, this doctrine is still viewed as a variety of Calvinism in that it maintains the particularity of sovereign grace in the application of the atonement. However, detractors like B. B. Warfield have termed it "an inconsistent and therefore unstable form of Calvinism."

Hyper-Calvinism first referred to a view that appeared among the early English Particular Baptists in the 18th century. Their system denied that the call of the gospel to "repent and believe" is directed to every single person and that it is the duty of every person to trust in Christ for salvation. The term also occasionally appears in both theological and secular controversial contexts, where it usually connotes a negative opinion about some variety of theological determinism, predestination, or a version of Evangelical Christianity or Calvinism that is deemed by the critic to be unenlightened, harsh, or extreme.

The Westminster Confession of Faith says that the gospel is to be freely offered to sinners, and the Larger Catechism makes clear that the gospel is offered to the non-elect.

Neo-Calvinism, a form of Dutch Calvinism, is the movement initiated by the theologian and former Dutch prime minister Abraham Kuyper. James Bratt has identified a number of different types of Dutch Calvinism: The Seceders—split into the Reformed Church "West" and the Confessionalists; and the Neo-Calvinists—the Positives and the Antithetical Calvinists. The Seceders were largely infralapsarian and the Neo-Calvinists usually supralapsarian.

Kuyper wanted to awaken the church from what he viewed as its pietistic slumber. He declared:

No single piece of our mental world is to be sealed off from the rest and there is not a square inch in the whole domain of human existence over which Christ, who is sovereign over all, does not cry: 'Mine!' 

This refrain has become something of a rallying call for Neo-Calvinists.

Christian Reconstructionism is a fundamentalist Calvinist theonomic movement that has remained rather obscure. Founded by R. J. Rushdoony, the movement has had an important influence on the Christian Right in the United States. The movement declined in the 1990s and was declared dead in a 2008 "Church History" journal article. However, it lives on in small denominations such as the Reformed Presbyterian Church in the United States and as a minority position in other denominations. Christian Reconstructionists are usually postmillennialists and followers of the presuppositional apologetics of Cornelius Van Til. They tend to support a decentralized political order resulting in laissez-faire capitalism.

New Calvinism is a growing perspective within conservative Evangelicalism that embraces the fundamentals of 16th century Calvinism while also trying to be relevant in the present day world. In March 2009, "Time" magazine described the New Calvinism as one of the "10 ideas changing the world". Some of the major figures in this area are John Piper, Mark Driscoll, Al Mohler, Mark Dever, C. J. Mahaney, Joshua Harris, and Tim Keller. New Calvinists have been criticized for blending Calvinist soteriology with popular Evangelical positions on the sacraments and continuationism.

Calvin expressed himself on usury in a 1545 letter to a friend, Claude de Sachin, in which he criticized the use of certain passages of scripture invoked by people opposed to the charging of interest. He reinterpreted some of these passages, and suggested that others of them had been rendered irrelevant by changed conditions. He also dismissed the argument (based upon the writings of Aristotle) that it is wrong to charge interest for money because money itself is barren. He said that the walls and the roof of a house are barren, too, but it is permissible to charge someone for allowing him to use them. In the same way, money can be made fruitful.

He qualified his view, however, by saying that money should be lent to people in dire need without hope of interest, while a modest interest rate of 5% should be permitted in relation to other borrowers.

Calvin's concepts of God and man led to ideas were gradually put into practice after his death, in particular in the fields of politics and society. After the fight for independence from Spain (1579), the Netherlands, under Calvinist leadership, granted asylum to religious minorities, e.g. French Huguenots, English Independents (Congregationalists), and Jews from Spain and Portugal. The ancestors of philosopher Baruch Spinoza were Portuguese Jews. Aware of the trial against Galileo, René Descartes lived in the Netherlands, out of reach of the Inquisition. Pierre Bayle, a Reformed Frenchman, also felt safer in the Netherlands than in his home country. He was the first prominent philosopher who demanded tolerance for atheists. Hugo Grotius was able to publish a rather liberal interpretation of the Bible and his ideas about natural law. Moreover, the Calvinist Dutch authorities allowed the printing of books that could not be published elsewhere, e.g. Galileo's "Discorsi".
Alongside the liberal development of the Netherlands was the rise of modern democracy in England and North America. In the Middle Ages state and church had been closely connected. Martin Luther's doctrine of the two kingdoms separated state and church in principle. His doctrine of the priesthood of all believers raised the laity to the same level as the clergy. Going one step further, Calvin included elected laymen (church elders, presbyters) in his concept of church government. The Huguenots added synods whose members were also elected by the congregations. The other Reformed churches took over this system of church self-government which was essentially a representative democracy. Baptists, Quakers, and Methodists are organized in a similar way. These denominations and the Anglican Church were influenced by Calvin's theology in varying degrees.

Another precondition for the rise of democracy in the Anglo-American world was the fact that Calvin favored a mixture of democracy and aristocracy as the best form of government (mixed government). He appreciated the advantages of democracy. The aim of his political thought was to safeguard the rights and freedoms of ordinary men and women. In order to minimize the misuse of political power he suggested dividing it among several institutions in a system of checks and balances (separation of powers). Finally, Calvin taught that if worldly rulers rise up against God they should be put down. In this way, he and his followers stood in the vanguard of resistance to political absolutism and furthered the cause of democracy. The Congregationalists who founded Plymouth Colony (1620) and Massachusetts Bay Colony (1628) were convinced that the democratic form of government was the will of God. Enjoying self-rule they practiced separation of powers. Rhode Island, Connecticut, and Pennsylvania, founded by Roger Williams, Thomas Hooker, and William Penn, respectively, combined democratic government with freedom of religion. These colonies became safe havens for persecuted religious minorities, including Jews.
In England, Baptists Thomas Helwys and John Smyth influenced the liberal political thought of Presbyterian poet and politician John Milton and philosopher John Locke, who in turn had both a strong impact on the political development in their home country (English Civil War, Glorious Revolution) as well as in North America. The ideological basis of the American Revolution was largely provided by the radical Whigs, who had been inspired by Milton, Locke, James Harrington, Algernon Sidney, and other thinkers. The Whigs' "perceptions of politics attracted widespread support in America because they revived the traditional concerns of a Protestantism that had always verged on Puritanism." The United States Declaration of Independence, the United States Constitution and (American) Bill of Rights initiated a tradition of human and civil rights that was continued in the French Declaration of the Rights of Man and the Citizen and the constitutions of numerous countries around the world, e. g. Latin America, Japan, India, Germany, and other European countries. It is also echoed in the United Nations Charter and the Universal Declaration of Human Rights.

In the nineteenth century, the churches that were based on Calvin's theology or influenced by it were deeply involved in social reforms, e.g. the abolition of slavery (William Wilberforce, Harriet Beecher Stowe, Abraham Lincoln, and others), women suffrage, and prison reforms. Members of these churches formed co-operatives to help the impoverished masses. Henry Dunant, a Reformed pietist, founded the Red Cross and initiated the Geneva Conventions.

Some sources would view Calvinist influence as not always being solely positive. The Boers and Afrikaner Calvinists combined ideas from Calvinism and Kuyperian theology to justify apartheid in South Africa. As late as 1974, the majority of the Dutch Reformed Church in South Africa was convinced that their theological stances (including the story of the Tower of Babel) could justify apartheid. In 1990, the Dutch Reformed Church document "Church and Society" maintained that although they were changing their stance on apartheid, they believed that within apartheid and under God's sovereign guidance, "...everything was not without significance, but was of service to the Kingdom of God." These views were not universal and were condemned by many Calvinists outside South Africa. It was pressure from both outside and inside the Dutch Reformed Calvinist church which helped reverse apartheid in South Africa.

Throughout the world, the Reformed churches operate hospitals, homes for handicapped or elderly people, and educational institutions on all levels. For example, American Congregationalists founded Harvard (1636), Yale (1701), and about a dozen other colleges.










</doc>
<doc id="6026" url="https://en.wikipedia.org/wiki?curid=6026" title="Countable set">
Countable set

In mathematics, a countable set is a set with the same cardinality (number of elements) as some subset of the set of natural numbers. A countable set is either a finite set or a countably infinite set. Whether finite or infinite, the elements of a countable set can always be counted one at a time and, although the counting may never finish, every element of the set is associated with a unique natural number.

Some authors use countable set to mean "countably infinite" alone. To avoid this ambiguity, the term "at most countable" may be used when finite sets are included and "countably infinite", "enumerable", or "denumerable" otherwise.

Georg Cantor introduced the term "countable set", contrasting sets that are countable with those that are "uncountable" (i.e., "nonenumerable" or "nondenumerable"). Today, countable sets form the foundation of a branch of mathematics called "discrete mathematics".

A set is "countable" if there exists an injective function from to the natural numbers }.

If such an can be found that is also surjective (and therefore bijective), then is called "countably infinite."

In other words, a set is "countably infinite" if it has one-to-one correspondence with the natural number set, .

As noted above, this terminology is not universal. Some authors use countable to mean what is here called "countably infinite," and do not include finite sets.

Alternative (equivalent) formulations of the definition in terms of a bijective function or a surjective function can also be given. See below.

In 1874, in his first set theory article, Cantor proved that the set of real numbers is uncountable, thus showing that not all infinite sets are countable. In 1878, he used one-to-one correspondences to define and compare cardinalities. In 1883, he extended the natural numbers with his infinite ordinals, and used sets of ordinals to produce an infinity of sets having different infinite cardinalities.

A "set" is a collection of "elements", and may be described in many ways. One way is simply to list all of its elements; for example, the set consisting of the integers 3, 4, and 5 may be denoted {3, 4, 5}. This is only effective for small sets, however; for larger sets, this would be time-consuming and error-prone. Instead of listing every single element, sometimes an ellipsis ("...") is used, if the writer believes that the reader can easily guess what is missing; for example, {1, 2, 3, ..., 100} presumably denotes the set of integers from 1 to 100. Even in this case, however, it is still "possible" to list all the elements, because the set is "finite".

Some sets are "infinite"; these sets have more than "n" elements for any integer "n". For example, the set of natural numbers, denotable by {0, 1, 2, 3, 4, 5, ...}, has infinitely many elements, and we cannot use any normal number to give its size. Nonetheless, it turns out that infinite sets do have a well-defined notion of size (or more properly, of "cardinality", which is the technical term for the number of elements in a set), and not all infinite sets have the same cardinality.
To understand what this means, we first examine what it "does not" mean. For example, there are infinitely many odd integers, infinitely many even integers, and (hence) infinitely many integers overall. However, it turns out that the number of even integers, which is the same as the number of odd integers, is also the same as the number of integers overall. This is because we arrange things such that for every integer, there is a distinct even integer: ... −2→−4, −1→−2, 0→0, 1→2, 2→4, ...; or, more generally, "n"→2"n", see picture. What we have done here is arranged the integers and the even integers into a "one-to-one correspondence" (or "bijection"), which is a function that maps between two sets such that each element of each set corresponds to a single element in the other set.

However, not all infinite sets have the same cardinality. For example, Georg Cantor (who introduced this concept) demonstrated that the real numbers cannot be put into one-to-one correspondence with the natural numbers (non-negative integers), and therefore that the set of real numbers has a greater cardinality than the set of natural numbers.

A set is "countable" if: (1) it is finite, or (2) it has the same cardinality (size) as the set of natural numbers. Equivalently, a set is "countable" if it has the same cardinality as some subset of the set of natural numbers. Otherwise, it is "uncountable".

By definition a set "S" is "countable" if there exists an injective function "f" : "S" → N from "S" to the natural numbers N = {0, 1, 2, 3, ...}.

It might seem natural to divide the sets into different classes: put all the sets containing one element together; all the sets containing two elements together; ...; finally, put together all infinite sets and consider them as having the same size.
This view is not tenable, however, under the natural definition of size.

To elaborate this we need the concept of a bijection. Although a "bijection" seems a more advanced concept than a number, the usual development of mathematics in terms of set theory defines functions before numbers, as they are based on much simpler sets. This is where the concept of a bijection comes in: define the correspondence

Since every element of {"a", "b", "c"} is paired with "precisely one" element of {1, 2, 3}, "and" vice versa, this defines a bijection.

We now generalize this situation and "define" two sets as of the same size if (and only if) there is a bijection between them. For all finite sets this gives us the usual definition of "the same size". What does it tell us about the size of infinite sets?

Consider the sets "A" = {1, 2, 3, ... }, the set of positive integers and "B" = {2, 4, 6, ... }, the set of even positive integers. We claim that, under our definition, these sets have the same size, and that therefore "B" is countably infinite. Recall that to prove this we need to exhibit a bijection between them. But this is easy, using "n" ↔ 2"n", so that

As in the earlier example, every element of A has been paired off with precisely one element of B, and vice versa. Hence they have the same size. This is an example of a set of the same size as one of its proper subsets, which is impossible for finite sets.

Likewise, the set of all ordered pairs of natural numbers is countably infinite, as can be seen by following a path like the one in the picture: The resulting mapping is like this:
This mapping covers all such ordered pairs.

If each pair is treated as the numerator and denominator of a vulgar fraction, then for every positive fraction, we can come up with a distinct number corresponding to it. This representation includes also the natural numbers, since every natural number is also a fraction "N"/1. So we can conclude that there are exactly as many positive rational numbers as there are positive integers. This is true also for all rational numbers, as can be seen below.

Theorem: The Cartesian product of finitely many countable sets is countable.

This form of triangular mapping recursively generalizes to vectors of finitely many natural numbers by repeatedly mapping the first two elements to a natural number. For example, (0,2,3) maps to (5,3), which maps to 39.

Sometimes more than one mapping is useful: the set to be shown to be countably infinite is mapped onto another set, then this other set is mapped onto to the natural numbers. For example, the positive rational numbers can easily be mapped to (a subset of) the pairs of natural numbers because "p"/"q "maps to ("p", "q").

What about infinite subsets of countably infinite sets? Do these have fewer elements than N?

Theorem: Every subset of a countable set is countable. In particular, every infinite subset of a countably infinite set is countably infinite.

For example, the set of prime numbers is countable, by mapping the "n"-th prime number to "n":

What about sets being naturally "larger than" N? For instance, Z the set of all integers or Q, the set of all rational numbers, which intuitively may seem much bigger than N. But looks can be deceiving, for we assert:

Theorem: Z (the set of all integers) and Q (the set of all rational numbers) are countable.

In a similar manner, the set of algebraic numbers is countable.

These facts follow easily from a result that many individuals find non-intuitive.

Theorem: Any finite union of countable sets is countable. 
With the foresight of knowing that there are uncountable sets, we can wonder whether or not this last result can be pushed any further. The answer is "yes" and "no", we can extend it, but we need to assume a new axiom to do so.

Theorem: (Assuming the axiom of countable choice) The union of countably many countable sets is countable.

For example, given countable sets a, b, c, ...
Using a variant of the triangular enumeration we saw above:


Note that this only works if the sets a, b, c, ... are disjoint. If not, then the union is even smaller and is therefore also countable by a previous theorem.

Also note that we need the axiom of countable choice to index "all" the sets a, b, c, ... simultaneously.

Theorem: The set of all finite-length sequences of natural numbers is countable.

This set is the union of the length-1 sequences, the length-2 sequences, the length-3 sequences, each of which is a countable set (finite Cartesian product). So we are talking about a countable union of countable sets, which is countable by the previous theorem.

Theorem: The set of all finite subsets of the natural numbers is countable.

The elements of any finite subset can be ordered into a finite sequence. There are only countably many finite sequences, so also there are only countably many finite subsets.

The following theorem gives equivalent formulations in terms of a bijective function or a surjective function. A proof of this result can be found in Lang's text.

(Basic) Theorem: Let "S" be a set. The following statements are equivalent:

Corollary: Let "S" and "T" be sets.

Cantor's theorem asserts that if "A" is a set and "P"("A") is its power set, i.e. the set of all subsets of "A", then there is no surjective function from "A" to "P"("A"). A proof is given in the article Cantor's theorem. As an immediate consequence of this and the Basic Theorem above we have:

Proposition: The set "P"(N) is not countable; i.e. it is uncountable.

For an elaboration of this result see Cantor's diagonal argument.

The set of real numbers is uncountable (see Cantor's first uncountability proof), and so is the set of all infinite sequences of natural numbers.
The proofs of the statements in the above section rely upon the existence of functions with certain properties. This section presents functions commonly used in this role, but not the verifications that these functions have the required properties. The Basic Theorem and its Corollary are often used to simplify proofs. Observe that in that theorem can be replaced with any countably infinite set.

Proposition: Any finite set is countable.

Proof: Let be such a set. Two cases are to be considered: Either is empty or it isn't. 1.) The empty set is even itself a subset of the natural numbers, so it is countable. 2.) If is nonempty and finite, then by definition of finiteness there is a bijection between and the set {1, 2, ..., } for some positive natural number . This function is an injection from into .

Proposition: Any subset of a countable set is countable.

Proof: The restriction of an injective function to a subset of its domain is still injective.

Proposition: If is a countable set then } is countable.

Proof: If there is nothing to be shown. Otherwise let be an injection. Define by and 

Proposition: If and are countable sets then is countable.

Proof: Let and be injections. Define a new injection by if is in and if is in but not in .

Proposition: The Cartesian product of two countable sets and is countable.

Proof: Observe that is countable as a consequence of the definition because the function given by is injective. It then follows from the Basic Theorem and the Corollary that the Cartesian product of any two countable sets is countable. This follows because if and are countable there are surjections and . So
is a surjection from the countable set to the set and the Corollary implies is countable. This result generalizes to the Cartesian product of any finite collection of countable sets and the proof follows by induction on the number of sets in the collection.

Proposition: The integers are countable and the rational numbers are countable.

Proof: The integers are countable because the function given by if is non-negative and if is negative, is an injective function. The rational numbers are countable because the function given by is a surjection from the countable set to the rationals .

Proposition: The algebraic numbers are countable.

Proof: Per definition, every algebraic number (including complex numbers) is a root of a polynomial with integer coefficients. Given an algebraic number formula_1, let formula_2 be a polynomial with integer coefficients such that formula_1 is the "k"th root of the polynomial, where the roots are sorted by absolute value from small to big, then sorted by argument from small to big. We can define an injection (i. e. one-to-one) function given by formula_4, while formula_5 is the "n"-th prime.

Proposition: If is a countable set for each in then the union of all is also countable.

Proof: This is a consequence of the fact that for each there is a surjective function and hence the function

given by is a surjection. Since is countable, the Corollary implies that the union is countable. We use the axiom of countable choice in this proof to pick for each in a surjection from the non-empty collection of surjections from to .

A topological proof for the uncountability of the real numbers is described at finite intersection property.

If there is a set that is a standard model (see inner model) of ZFC set theory, then there is a minimal standard model ("see" Constructible universe). The Löwenheim–Skolem theorem can be used to show that this minimal model is countable. The fact that the notion of "uncountability" makes sense even in this model, and in particular that this model "M" contains elements that are:
was seen as paradoxical in the early days of set theory, see Skolem's paradox.

The minimal standard model includes all the algebraic numbers and all effectively computable transcendental numbers, as well as many other kinds of numbers.

Countable sets can be totally ordered in various ways, e.g.:

Note that in both examples of well orders here, any subset has a "least element"; and in both examples of non-well orders, "some" subsets do not have a "least element".
This is the key definition that determines whether a total order is also a well order.




</doc>
<doc id="6034" url="https://en.wikipedia.org/wiki?curid=6034" title="Cahn–Ingold–Prelog priority rules">
Cahn–Ingold–Prelog priority rules

The Cahn–Ingold–Prelog (CIP) sequence rules, named for organic chemists Robert Sidney Cahn, Christopher Kelk Ingold, and Vladimir Prelog — alternatively termed the CIP priority rules, "system", or "conventions" — are a standard process used in organic chemistry to completely and unequivocally name a stereoisomer of a molecule. The purpose of the CIP system is to assign an to each stereocenter and an "E" or "Z" descriptor to each double bond so that the configuration of the entire molecule can be specified uniquely by including the descriptors in its systematic name. A molecule may contain any number of stereocenters and any number of double bonds, and each usually gives rise to two possible isomers. A molecule with an integer "n" describing the number of its stereogenic centers will usually have 2 stereoisomers, and 2 diastereomers each having an associated pair of enantiomers. The CIP sequence rules contribute to the precise naming of every stereoisomer of every organic and organometallic molecule with all atoms of ligancy of fewer than 4 (but including ligancy of 6 as well, this term referring to the "number of neighboring atoms" bonded to a center).

The key article setting out the CIP sequence rules was published in 1966, and was followed by further refinements, before it was incorporated into the rules of the International Union of Pure and Applied Chemistry (IUPAC), the official body that defines organic nomenclature. The IUPAC presentation of the rules constitute the official, formal standard for their use, and it notes that "the method has been developed to cover all compounds with ligancy up to 4... and… [extended to the case of] ligancy 6… [as well as] for all configurations and conformations of such compounds." Nevertheless, though the IUPAC documentation presents a thorough introduction, it includes the caution that "it is essential to study the original papers, especially the 1966 paper, before using the sequence rule for other than fairly simple cases."

A recent paper argues for changes to the specific rules to accommodate for certain structures, after they found deficiencies in some of the CIP rules (sequence rules 1b and 2).

The steps for naming molecules using the CIP system are often presented as:

 and "E"/"Z" descriptors are assigned by using a system for ranking priority of the groups attached to each stereocenter. This procedure, often known as "the sequence rules", is the heart of the CIP system.

If two groups differ only in isotopes, then the larger atomic mass is used to set the priority.

If an atom A is double-bonded to an atom B, A is treated as being singly bonded to two atoms: B and a "ghost atom" that is a duplicate of B (has the same atomic number) but is not attached to anything except A. When B is replaced with a list of attached atoms, A itself, but not its "ghost", is excluded in accordance with the general principle of not doubling back along a bond that has just been followed. A triple bond is handled the same way except that A and B are each connected to two ghost atoms of the other.

If two substituents on an atom are geometric isomers of each other, the "Z"-isomer has higher priority than the "E"-isomer.

To handle a molecule containing one or more cycles, one must first expand it into a tree (called a hierarchical digraph) by traversing bonds in all possible paths starting at the stereocenter. When the traversal encounters an atom through which the current path has already passed, a ghost atom is generated in order to keep the tree finite. A single atom of the original molecule may appear in many places (some as ghosts, some not) in the tree.

After the substituents of a stereocenter have been assigned their priorities, the molecule is oriented in space so that the group with the lowest priority is pointed away from the observer. If the substituents are numbered from 1 (highest priority) to 4 (lowest priority), then the sense of rotation of a curve passing through 1, 2 and 3 distinguishes the stereoisomers. A center with a clockwise sense of rotation is an "R" ("rectus") center and a center with a counterclockwise sense of rotation is an "S" ("sinister") center. The names are derived from the Latin for 'right' and 'left', respectively.

A practical method of determining whether an enantiomer is "R" or "S" is by using the right-hand rule: one wraps the molecule with the fingers in the direction . If the thumb points in the direction of the fourth substituent, the enantiomer is "R"; otherwise, it is "S".

It is possible in rare cases that two substituents on an atom differ only in their absolute configuration ("R" or "S"). If the relative priorities of these substituents need to be established, "R" takes priority over "S". When this happens, the descriptor of the stereocenter is a lowercase letter ("r" or "s") instead of the uppercase letter normally used.

For alkenes and similar double bonded molecules, the same prioritizing process is followed for the substituents. In this case, it is the placing of the two highest priority substituents with respect to the double bond which matters. If both high priority substituents are on the same side of the double bond, i.e. in the "cis" configuration, then the stereoisomer is assigned a "Z" ("zusammen") . If by contrast they are in a "trans" configuration, then the stereoisomer is assigned an "E" ("entgegen"). In this case the identifying letters are derived from German for 'together' and 'opposite', respectively.

The following are examples of application of the nomenclature.

If a compound has more than one stereocenter each center is denoted by either "R" or "S". For example, ephedrine exists with both (1"R",2"S") and (1"S",2"R") configuration, known as enantiomers. This compound also exists with a (1"R",2"R") and (1"S",2"S") configuration. The last two stereoisomers are not ephedrine, but pseudoephedrine. All isomers are 2-methylamino-1-phenyl-1-propanol in systematic nomenclature. Pseudoephedrine is chemically distinct from ephedrine with only the three-dimensional configuration in space, as notated by the Cahn–Ingold–Prelog rules. The two compounds, ephedrine and pseudoephedrine, are diastereomers, or stereoisomers that are not enantiomers. They have different names because, as diastereomers, they have different chemical properties.

In pairs of enantiomers, all descriptors are opposite: ("R","R") and ("S","S"), or ("R","S") and ("S","R"). Diastereomers have one descriptor in common: ("R","S") and ("R","R"), or ("S","R") and ("S","S"). This holds true for compounds with more than two stereocenters; if at least one descriptor is the same in both pairs, the compounds are diastereomers. If all the stereocenters are opposite, they are enantiomers.

The relative configuration of two stereoisomers may be denoted by the descriptors "R" and "S" with an asterisk (*). ("R"*,"R"*) means two centers having identical configurations, ("R","R") or ("S","S"); ("R"*,"S"*) means two centers having opposite configurations, ("R","S") or ("S","R"). To begin, the lowest-numbered (according to IUPAC systematic numbering) stereogenic center is given the "R"* descriptor.

To designate two anomers the relative stereodescriptors alpha (α) and beta (β) are used. In the α anomer the "anomeric carbon atom" and the "reference atom" do have opposite configurations ("R","S") or ("S","R"), whereas in the β anomer they are the same ("R","R") or ("S","S").

Stereochemistry also plays a role assigning "faces" to trigonal molecules such as ketones. A nucleophile in a nucleophilic addition can approach the carbonyl group from two opposite sides or faces. When an achiral nucleophile attacks acetone, both faces are identical and there is only one reaction product. When the nucleophile attacks butanone, the faces are not identical ("enantiotopic") and a racemic product results. When the nucleophile is a chiral molecule diastereoisomers are formed. When one face of a molecule is shielded by substituents or geometric constraints compared to the other face the faces are called diastereotopic. The same rules that determine the stereochemistry of a stereocenter ("R" or "S") also apply when assigning the face of a molecular group. The faces are then called the Re"-face and Si"-face. In the example displayed on the right, the compound acetophenone is viewed from the "Re"-face. Hydride addition as in a reduction process from this side will form the ("S")-enantiomer and attack from the opposite "Si"-face will give the ("R")-enantiomer. However, one should note that adding a chemical group to the prochiral center from the "Re"-face will not always lead to an ("S")-stereocenter, as the priority of the chemical group has to be taken into account. That is, the absolute stereochemistry of the product is determined on its own and not by considering which face it was attacked from. In the above-mentioned example, if chloride ("Z" = 17) were added to the prochiral center from the "Re"-face, this would result in an ("R")-enantiomer.


</doc>
<doc id="6035" url="https://en.wikipedia.org/wiki?curid=6035" title="Celibacy">
Celibacy

Celibacy (from Latin, "cælibatus"") is the state of voluntarily being unmarried, sexually abstinent, or both, usually for religious reasons. It is often in association with the role of a religious official or devotee. In its narrow sense, the term "celibacy" is applied only to those for whom the unmarried state is the result of a sacred vow, act of renunciation, or religious conviction. In a wider sense, it is commonly understood to only mean abstinence from sexual activity.

Celibacy has existed in one form or another throughout history, in virtually all the major religions of the world, and views on it have varied. Similarly, the Romans viewed it as an aberration and legislated fiscal penalties against it, with the sole exception granted to the Vestal Virgins. The Islamic attitudes toward celibacy have been complex as well. Some Hadiths claim that Muhammad denounced celibacy, but some Sufi orders embrace it.

Classical Hindu culture encouraged asceticism and celibacy in the later stages of life, after one has met his societal obligations. Jainism, on the other hand, preached complete celibacy even for young monks and considered celibacy to be an essential behavior to attain moksha. Buddhism is similar to Jainism in this respect. There were, however, significant cultural differences in the various areas where Buddhism spread, which affected the local attitudes toward celibacy. It was not well received in China, for example, where other religions movements such as Daoism were opposed to it. A somewhat similar situation existed in Japan, where the Shinto tradition also opposed celibacy. In most native African and American Indian religious traditions, celibacy has been viewed negatively as well, although there were exceptions like periodic celibacy practiced by some Mesoamerican warriors.

The English word "celibacy" derives from the Latin "caelibatus", "state of being unmarried", from Latin , meaning "unmarried". This word derives from two Proto-Indo-European stems, * "alone" and * "living".

The words "abstinence" and "celibacy" are often used interchangeably, but are not necessarily the same thing. Sexual abstinence, also known as "continence", is abstaining from some or all aspects of sexual activity, often for some limited period of time, while celibacy may be defined as a voluntary religious vow not to marry or engage in sexual activity. Asexuality is commonly conflated with celibacy and sexual abstinence, but it is considered distinct from the two, as celibacy and sexual abstinence are behavioral and those who use those terms for themselves are generally motivated by factors such as an individual's personal or religious beliefs.

A. W. Richard Sipe, while focusing on the topic of celibacy in Catholicism, states that "the most commonly assumed definition of "celibate" is simply an unmarried or single person, and celibacy is perceived as synonymous with sexual abstinence or restraint." Sipe adds that even in the relatively uniform milieu of Catholic priests in the United States "there is simply no clear operational definition of celibacy". Elizabeth Abbott commented on the terminology in her "A History of Celibacy" (2001): "I also drafted a definition that discarded the rigidly pedantic and unhelpful distinctions between celibacy, chastity and virginity".

The concept of "new celibacy" was introduced by Gabrielle Brown in her 1980 book "The New Celibacy". In a revised version (1989) of her book, she claims that "abstinence is a response on the outside to what's going on, and celibacy is a response from the inside". According to her definition, celibacy (even short-term celibacy that is pursued for non-religious reasons) is much more than not having sex. It is more intentional than abstinence, and its goal is personal growth and empowerment. This new perspective on celibacy is echoed by several authors including Elizabeth Abbott, Wendy Keller, and Wendy Shalit.

The rule of celibacy in the Buddhist religion, whether Mahayana or Theravada, has a long history. Celibacy was advocated as an ideal rule of life for all monks and nuns by Gautama Buddha, except for Japan where it is not strictly followed due to historical and political developments following the Meiji Restoration. In Japan, celibacy was an ideal among Buddhist clerics for hundreds of years. But violations of clerical celibacy were so common for so long that, finally, in 1872, state laws made marriage legal for Buddhist clerics. Subsequently, ninety percent of Buddhist monks/clerics married. An example is Higashifushimi Kunihide, a prominent Buddhist priest of Japanese royal ancestry who was married and a father whilst serving as a monk for most of his lifetime.

Gautama, later known as the Buddha, is known for his renunciation of his wife, Princess Yasodharā, and son, Rahula. In order to pursue an ascetic life, he needed to renounce aspects of the impermanent world, including his wife and son. Later on both his wife and son joined the ascetic community and are mentioned in the Buddhist texts to have become enlightened. In another sense, a buddhavacana recorded the zen patriarch Vimalakirti as being an advocate of marital continence instead of monastic renunciation, the sutra became somewhat popular due to its brash humour as well as integrating the role of women in laity as well as spiritual life.

In the religious movement of Brahma Kumaris, celibacy is also promoted for peace and to defeat power of lust and to prepare for life in forthcoming Heaven on earth for 2,500 years when children will be created by the power of the mind even for householders to like holy brother and sister.

In this belief system, celibacy is given the utmost importance. It is said that, as per the direction of the Supreme God those lead a pure and celibate life will be successfully able to conquer the surging vices. The power of celibacy creates an unseen environment of divinity bringing peace, power, purity, prosperity and fortune. Those with the power of celibacy are eligible to claim a bright future of Golden Age of heaven / Paradise. Brahma Kumaris' concept of identifying the self as a soul, different from physical body, is deeply linked to the philosophy of celibacy. It is said that the craving for sex and impure thoughts are the reason for the whole trouble in the universe today. And celibacy is to lead the pure relationship in one's life.

When Jesus discusses marriage, he points out that there is some responsibility for a man marrying a woman (and vice versa). Not having assets of their own, women needed to be protected from the risk of their husbands' putting them on the street at whim. In those times marriage was an economic matter rather than one of love. A woman and her children could easily be rejected. Restriction of divorce was based on the necessity of protecting the woman and her position in society, not necessarily in a religious context, but an economic context. He also points out that there are those "which were made eunuchs of men: and there be eunuchs, which have made themselves eunuchs for the kingdom of heaven's sake", but in the original Greek, the word εὐνοῦχος means "castrated person". It was the custom at the time Jesus lived for priests of some ancient gods and goddesses to be castrated. In the pre-Christian period Vestals, who served the virgin goddess of the hearth, were obliged to forgo marriage, and so were some priests and servants of some ancient deities such as Isis.

There is no commandment in the New Testament that Jesus' disciples have to live in celibacy. The general view on sexuality among the early Jewish Christians was quite positive. Jesus himself does not speak in negative terms of the body in the New Testament. While the Jewish sect of essenes practiced celibacy the general practice of the Jewish community by that time prescribed marriage for everybody, and at an early age. Saint Peter, also known as Simon Peter, the Apostle was married; Jesus healed Simon Peter's mother-in-law (Matt. 8:14), and other apostles and church members among the early Jewish Christians were also married: Paul's personal friends, Priscilla and Aquila (), who were Paul's coworkers, Andronicus of Pannonia (), and Junia (), who were highly regarded among the apostles, Ananias and Sapphira (Ap 5:1), Apphia and Philemon (Phil 1: 1). According to Eusebius Church History ("Historia Ecclesiastica"), Paul the Apostle, also known as Saul of Tarsus, was also married. It was the custom in the Jewish community to marry early.

In his early writings, Paul the Apostle described marriage as a social obligation that has the potential of distracting from Christ. Sex, in turn, is not sinful but natural, and sex within marriage is both proper and necessary. In his later writings, Paul made parallels between the relations between spouses and God's relationship with the church. "Husbands love your wives even as Christ loved the church. Husbands should love their wives as their own bodies" (Ephesians 5:25–28). The early Christians lived in the belief that the End of the World would soon come upon them, and saw no point in planning new families and having children. This was why Paul encouraged both celibate and marital lifestyles among the members of the Corinthian congregation, regarding celibacy as the preferable of the two.

Paul the Apostle emphasized the importance of overcoming the desires of the flesh and saw the state of celibacy being superior to the marriage.

In the Catholic Church, a consecrated virgin, is a woman who has been consecrated by the church to a life of perpetual virginity in the service of God. According to most Christian thought, the first sacred virgin was Mary, the mother of Jesus, who was consecrated by the Holy Spirit during the Annunciation. Tradition also has it that the Apostle Matthew consecrated virgins. A number of early Christian martyrs were women or girls who had given themselves to Christ in perpetual virginity, such as Saint Agnes and Saint Lucy.

The Desert Fathers were Christian hermits, and ascetics who had a major influence on the development of Christianity and celibacy. Paul of Thebes is often credited with being the first hermit monk to go to the desert, but it was Anthony the Great who launched the movement that became the Desert Fathers. Sometime around AD 270, Anthony heard a Sunday sermon stating that perfection could be achieved by selling all of one's possessions, giving the proceeds to the poor, and following Christ.(Matt. 19.21) He followed the advice and made the further step of moving deep into the desert to seek complete solitude.

Over time, the model of Anthony and other hermits attracted many followers, who lived alone in the desert or in small groups. They chose a life of extreme asceticism, renouncing all the pleasures of the senses, rich food, baths, rest, and anything that made them comfortable. Thousands joined them in the desert, mostly men but also a handful of women. Religious seekers also began going to the desert seeking advice and counsel from the early Desert Fathers. By the time of Anthony's death, there were so many men and women living in the desert in celibacy that it was described as "a city" by Anthony's biographer. The first Conciliar document on celibacy of the Western Christian Church (Canon 33 of the Synod of Elvira, ) states that the discipline of celibacy is to refrain from the "use" of marriage, i.e. refrain from having carnal contact with one's spouse.

According to the later St. Jerome (420) celibacy is a moral virtue, consisting of living in the flesh, but outside the flesh, and so being not corrupted by it ("vivere in carne praeter carnem"). Celibacy excludes not only libidinous acts, but also sinful thoughts or desires of the flesh. Jerome referred to marriage prohibition for priests when he claimed in "Against Jovinianus" that Peter and the other apostles had been married before they were called, but subsequently gave up their marital relations.
Celibacy as a vocation may be independent from religious vows (as is the case with consecrated virgins, ascetics and hermits). In the Catholic, Orthodox and Oriental Orthodox traditions, bishops are required to be celibate. In the Eastern Christian traditions, priests and deacons are allowed to be married, yet have to remain celibate if they are unmarried at the time of ordination.

In the early Church higher clerics lived in marriages. Augustine of Hippo was one of the first to develop a theory that sexual feelings were sinful and negative. Augustine taught that the original sin of Adam and Eve was either an act of "foolishness" ("insipientia") followed by "pride" and "disobedience" to God, or else inspired by pride. The first couple disobeyed God, who had told them not to eat of the Tree of the knowledge of good and evil (Gen 2:17). The tree was a symbol of the order of creation. Self-centeredness made Adam and Eve eat of it, thus failing to acknowledge and respect the world as it was created by God, with its hierarchy of beings and values. They would not have fallen into pride and lack of wisdom, if Satan had not sown into their senses "the root of evil" ("radix Mali"). Their nature was wounded by concupiscence or libido, which affected human intelligence and will, as well as affections and desires, including sexual desire.
The sin of Adam is inherited by all human beings. Already in his pre-Pelagian writings, Augustine taught that Original Sin was transmitted by concupiscence, which he regarded as the passion of both soul and body, making humanity a "massa damnata" (mass of perdition, condemned crowd) and much enfeebling, though not destroying, the freedom of the will.

In the early 3rd century, the Canons of the Apostolic Constitutions decreed that only lower clerics might still marry after their ordination, but marriage of bishops, priests, and deacons were not allowed. Augustine's view of sexual feelings as sinful affected his view of women. For example, he considered a man's erection to be sinful, though involuntary, because it did not take place under his conscious control. His solution was to place controls on women to limit their ability to influence men. He equated flesh with woman and spirit with man.

He believed that the serpent approached Eve because she was less rational and lacked self-control, while Adam's choice to eat was viewed as an act of kindness so that Eve would not be left alone. Augustine believed sin entered the world because man (the spirit) did not exercise control over woman (the flesh). Augustine's views on women were not all negative, however. In his "Tractates on the Gospel of John", Augustine, commenting on the Samaritan woman from John 4:1–42, uses the woman as a figure of the church.

According to Raming, the authority of the "Decretum Gratiani", a collection of Roman Catholic canon law which prohibits women from leading, teaching, or being a witness, rests largely on the views of the early church fathers, especially St. Augustine. The laws and traditions founded upon St. Augustine's views of sexuality and women continue to exercise considerable influence over church doctrinal positions regarding the role of women in the church.

One explanation for the origin of obligatory celibacy is that it is based on the writings of Saint Paul, who wrote of the advantages celibacy allowed a man in serving the Lord. Celibacy was popularised by the early Christian theologians like Saint Augustine of Hippo and Origen. Another possible explanation for the origins of obligatory celibacy revolves around more practical reason, "the need to avoid claims on church property by priests' offspring". It remains a matter of Canon Law (and often a criterion for certain religious orders, especially Franciscans) that priests may not own land and therefore cannot pass it on to legitimate or illegitimate children. The land belongs to the Church through the local diocese as administered by the Local Ordinary (usually a bishop), who is often an "ex officio" corporation sole. Celibacy is viewed differently by the Catholic Church and the various Protestant communities. It includes clerical celibacy, celibacy of the consecrated life, voluntary lay celibacy, and celibacy outside of marriage.

The Protestant Reformation rejected celibate life and sexual continence for preachers. Protestant celibate communities have emerged, especially from Anglican and Lutheran backgrounds. A few minor Christian sects advocate celibacy as a better way of life. These groups included the Shakers, the Harmony Society and the Ephrata Cloister. Celibacy not only for religious and monastics (brothers/monks and sisters/nuns) but also for bishops is upheld by the Catholic Church traditions.

Many evangelicals prefer the term "abstinence" to "celibacy." Assuming everyone will marry, they focus their discussion on refraining from premarital sex and focusing on the joys of a future marriage. But some evangelicals, particularly older singles, desire a positive message of celibacy that moves beyond the "wait until marriage" message of abstinence campaigns. They seek a new understanding of celibacy that is focused on God rather than a future marriage or a lifelong vow to the Church.

There are also many Pentecostal churches which practice celibate ministry. For instance, The full-time ministers of the Pentecostal Mission are celibate. Most of them are single, married couples can become celibate.

During the first three or four centuries, no law was promulgated prohibiting clerical marriage. Celibacy was a matter of choice for bishops, priests, and deacons.

Statutes forbidding clergy from having wives were written beginning with the Council of Elvira (306) but these early statutes were not universal and were often defied by clerics and then retracted by hierarchy. The Synod of Gangra (345) condemned a false asceticism whereby worshipers boycotted celebrations presided over by married clergy." The Apostolic Constitutions () excommunicated a priest or bishop who left his wife ‘under the pretense of piety”’ (Mansi, 1:51).

"A famous letter of Synesius of Cyrene () is evidence both for the respecting of personal decision in the matter and for contemporary appreciation of celibacy. For priests and deacons clerical marriage continued to be in vogue".

"The Second Lateran Council (1139) seems to have enacted the first written law making sacred orders a diriment impediment to marriage for the universal Church." Celibacy was first required of some clerics in 1123 at the First Lateran Council. Because clerics resisted it, the celibacy mandate was restated at the Second Lateran Council (1139) and the Council of Trent (1545–64). In places, coercion and enslavement of clerical wives and children was apparently involved in the enforcement of the law. “The earliest decree in which the children [of clerics] were declared to be slaves and never to be enfranchised [freed] seems to have been a canon of the Synod of Pavia in 1018. Similar penalties were promulgated against wives and concubines (see the Synod of Melfi, 1189 can. Xii), who by the very fact of their unlawful connexion with a subdeacon or clerk of higher rank became liable to be seized by the over-lord”. Celibacy for priests continues to be a contested issue even today.

In the Roman Catholic Church, the Twelve Apostles are considered to have been the first priests and bishops of the Church. Some say the call to be eunuchs for the sake of Heaven in Matthew 19 was a call to be sexually continent and that this developed into celibacy for priests as the successors of the apostles. Others see the call to be sexually continent in Matthew 19 to be a caution for men who were too readily divorcing and remarrying.

The view of the Church is that celibacy is a reflection of life in Heaven, a source of detachment from the material world which aids in one's relationship with God. Celibacy is designed to "consecrate themselves with undivided heart to the Lord and to "the affairs of the Lord, they give themselves entirely to God and to men. It is a sign of this new life to the service of which the Church's minister is consecrated; accepted with a joyous heart celibacy radiantly proclaims the Reign of God." In contrast, Saint Peter, whom the Church considers its first Pope, was married given that he had a mother-in-law whom Christ healed (Matthew 8).

Usually, only celibate men are ordained as priests in the Latin Rite. Married clergy who have converted from other Christian denominations can be ordained Roman Catholic priests without becoming celibate. Priestly celibacy is not "doctrine" of the Church (such as the belief in the Assumption of Mary) but a matter of discipline, like the use of the vernacular (local) language in Mass or Lenten fasting and abstinence. As such, it can theoretically change at any time though it still must be obeyed by Catholics until the change were to take place. The Eastern Catholic Churches ordain both celibate and married men. However, in both the East and the West, bishops are chosen from among those who are celibate. In Ireland, several priests have fathered children, the two most prominent being Bishop Eamonn Casey and Father Michael Cleary.

The classical heritage flourished throughout the Middle Ages in both the Byzantine Greek East and the Latin West. Will Durant has made a case that certain prominent features of Plato's ideal community were discernible in the organization, dogma and effectiveness of "the" Medieval Church in Europe:

"The clergy, like Plato's guardians, were placed in authority... by their talent as shown in ecclesiastical studies and administration, by their disposition to a life of meditation and simplicity, and ... by the influence of their relatives with the powers of state and church. In the latter half of the period in which they ruled [AD 800 onwards], the clergy were as free from family cares as even Plato could desire [for such guardians]... [Clerical] Celibacy was part of the psychological structure of the power of the clergy; for on the one hand they were unimpeded by the narrowing egoism of the family, and on the other their apparent superiority to the call of the flesh added to the awe in which lay sinners held them..." "In the latter half of the period in which they ruled, the clergy were as free from family cares as even Plato could desire".

"Greater understanding of human psychology has led to questions regarding the impact of celibacy on the human development of the clergy. The realization that many non-European countries view celibacy negatively has prompted questions concerning the value of retaining celibacy as an absolute and universal requirement for ordained ministry in the Roman Catholic Church"

"The declining number of priests in active ministry, the exemption from the requirement of celibacy for married clergy who enter the Catholic Church after having been ordained in the Episcopal Church, and reported incidences of de facto nonobservance of the requirement by clergy in various parts of the world, especially in Africa and Latin America, suggests that the discussion [of celibacy] will continue"
Catholic Churches developed the less well-known institution of chaste marriage.

The reintroduction of a permanent diaconate has permitted the Church to allow married men to become deacons but they may not go on to become priests.

Some homosexual Christians choose to be celibate following their denomination's teachings on homosexuality.

In 2014, the American Association of Christian Counselors amended its code of ethics to eliminate the promotion of conversion therapy for homosexuals and encouraged them to be celibate instead.

In Hinduism, celibacy is usually associated with the "sadhus" ("holy men"), ascetics who withdraw from society and renounce all worldly ties. Celibacy, termed "brahmacharya" in Vedic scripture, is the fourth of the "yamas" and the word literally translated means "dedicated to the Divinity of Life". The word is often used in yogic practice to refer to celibacy or denying pleasure, but this is only a small part of what "brahmacharya" represents. The purpose of practicing "brahmacharya" is to keep a person focused on the purpose in life, the things that instill a feeling of peace and contentment.It is also used to cultivate occult powers and many supernatural feats,called siddhi , usually downplayed by the mainstream media and other mediums of information.

Islamic attitudes toward celibacy have been complex, Muhammad denounced it, however some Sufi orders embrace it. Islam does not promote celibacy; rather it condemns premarital sex and extramarital sex. In fact, according to Islam, marriage enables one to attain the highest form of righteousness within this sacred spiritual bond and is as such to be sought after and desired. It disagrees with the concept that marriage acts as a form of distraction in attaining nearness to God. The Qur'an (57:27) states, "But the Monasticism which they invented for themselves, We did not prescribe for them but only to please God therewith, but that they did not observe it with the right observance."

The following sayings about Muhammad also address celibacy:

"There have been people who have come to the prophet and explained how they love to be engaged in prayer and fasting for the sake of God. The Prophet Mohammed told them that, despite this being good, it is also a blessing to raise a family, to remain moderate and not to concentrate too much on one aspect as not only can this be unhealthy for an individual as well as upon society, it may also take one away from God."

Celibacy appears as a peculiarity among some Sufis.

Celibacy was practiced by women saints in Sufism. Celibacy was debated along with women's roles in Sufism in medieval times.

Celibacy, poverty, meditation, and mysticism within an ascetic context along with worship centered around Saint's tombs were promoted by the Qadiri Sufi order among Hui Muslims in China. In China, unlike other Muslim sects, the leaders (Shaikhs) of the Qadiriyya Sufi order are celibate. Unlike other Sufi orders in China, the leadership within the order is not a hereditary position, rather, one of the disciples of the celibate Shaikh is chosen by the Shaikh to succeed him . The 92-year-old celibate Shaikh Yang Shijun was the leader of the Qadiriya order in China as of 1998.

Celibacy is practiced by Haydariya Sufi dervishes.

The spiritual teacher Meher Baba stated that "[F]or the [spiritual] aspirant a life of strict celibacy is preferable to married life, if restraint comes to him easily without undue sense of self-repression. Such restraint is difficult for most persons and sometimes impossible, and for them married life is decidedly more helpful than a life of celibacy. For ordinary persons, married life is undoubtedly advisable unless they have a special aptitude for celibacy". Baba also asserted that "The value of celibacy lies in the habit of restraint and the sense of detachment and independence which it gives" and that "The aspirant must choose one of the two courses which are open to him. He must take to the life of celibacy or to the married life, and he must avoid at all costs a cheap compromise between the two. Promiscuity in sex gratification is bound to land the aspirant in a most pitiful and dangerous chaos of ungovernable lust."

In Sparta and many other Greek cities, failure to marry was grounds for loss of citizenship, and could be prosecuted as a crime. Both Cicero and Dionysius of Halicarnassus stated that Roman law forbade celibacy. There are no records of such a prosecution, nor is the Roman punishment for refusing to marry known.

Pythagoreanism was the system of esoteric and metaphysical beliefs held by Pythagoras and his followers. Pythagorean thinking was dominated by a profoundly mystical view of the world. The Pythagorean code further restricted his members from eating meat, fish, and beans which they practised for religious, ethical and ascetic reasons, in particular the idea of metempsychosis – the transmigration of souls into the bodies of other animals.
"Pythagoras himself established a small community that set a premium on study, vegetarianism, and sexual restraint or abstinence. Later philosophers believed that celibacy would be conducive to the detachment and equilibrium required by the philosopher's calling."

The tradition of sworn virgins developed out of the "Kanuni i Lekë Dukagjinit" (, or simply the "Kanun"). The "Kanun" is not a religious document – many groups follow it, including Roman Catholics, the Albanian Orthodox, and Muslims.

Women who become sworn virgins make a vow of celibacy, and are allowed to take on the social role of men: inheriting land, wearing male clothing, etc.





</doc>
<doc id="6036" url="https://en.wikipedia.org/wiki?curid=6036" title="Coalition government">
Coalition government

A coalition government in a parliamentary system is a government in which multiple political parties cooperate, reducing the dominance of any one party within that "coalition". The usual reason for this arrangement is that no party on its own can achieve a majority in the parliament. A coalition government might also be created in a time of national difficulty or crisis (for example, during wartime or economic crisis) to give a government the high degree of perceived political legitimacy or collective identity it desires while also playing a role in diminishing internal political strife. In such times, parties have formed all-party coalitions (national unity governments, grand coalitions). If a coalition collapses, a confidence vote is held or a motion of no confidence is taken.

When a general election does not produce a clear majority for a single party, parties either form coalition cabinets, supported by a parliamentary majority, or minority cabinets which may consist of one or more parties. Cabinets based on a group of parties that command a majority in parliament tend to be more stable and long-lived than minority cabinets. While the former are prone to internal struggles, they have less reason to fear votes of no confidence. Majority governments based on a single party are typically even more stable, as long as their majority can be maintained.

Countries which often operate with coalition cabinets include: the Nordic countries, the Benelux countries, Australia, Austria, Cyprus, France, Germany, Greece, India, Indonesia, Ireland, Israel, Italy, Japan, Kenya, Kosovo, Lithuania, Latvia, Lebanon, Nepal, New Zealand, Pakistan, Thailand, Trinidad and Tobago, Turkey and Ukraine. Switzerland has been ruled by a coalition of the four strongest parties in parliament from 1959 to 2008, called the "Magic Formula". Between 2010 and 2015, the United Kingdom also operated a formal coalition between the Conservative and the Liberal Democrat parties, but this was unusual: the UK usually has a single-party majority government.

In the United Kingdom, coalition governments (sometimes known as "national governments") usually have only been formed at times of national crisis. The most prominent was the National Government of 1931 to 1940. There were multi-party coalitions during both world wars. Apart from this, when no party has had a majority, minority governments normally have been formed with one or more opposition parties agreeing to vote in favour of the legislation which governments need to function: for instance the Labour government of James Callaghan formed a pact with the Liberals from March 1977 until July 1978, after a series of by-election defeats had eroded Labour's majority of three seats which had been gained at the October 1974 election. However, in the run-up to the 1997 general election, Labour opposition leader Tony Blair was in talks with Liberal Democrat leader Paddy Ashdown about forming a coalition government if Labour failed to win a majority at the election; but there proved to be no need for a coalition as Labour won the election by a landslide. The 2010 general election resulted in a hung parliament (Britain's first for 36 years), and the Conservatives, led by David Cameron, which had won the largest number of seats, formed a coalition with the Liberal Democrats in order to gain a parliamentary majority, ending 13 years of Labour government. This was the first time that the Conservatives and Lib Dems had made a power-sharing deal at Westminster. It was also the first full coalition in Britain since 1945, having been formed 70 years virtually to the day after the establishment of Winston Churchill's wartime coalition,
Labour and the Liberal Democrats have entered into a coalition three times in the Scottish Parliament and twice in the Welsh Assembly.

In Germany, for instance, coalition government is the norm, as it is rare for either the Christian Democratic Union of Germany together with their partners the Christian Social Union in Bavaria (CDU/CSU), or the Social Democratic Party of Germany (SPD), to win an unqualified majority in a national election. Thus, at the federal level, governments are formed with at least two parties. For example, Helmut Kohl's CDU governed for years in coalition with the Free Democratic Party (FDP); from 1998 to 2005 Gerhard Schröder's SPD was in power with the Greens; and from 2009 Angela Merkel, CDU/CSU was in power with the FDP.

"Grand coalitions" of the two large parties also occur, but these are relatively rare, as large parties usually prefer to associate with small ones. However, if none of the larger parties can receive enough votes to form their preferred coalition, a grand coalition might be their only choice for forming a government. This was the situation in Germany in 2005 when Angela Merkel became Chancellor: in early elections, the CDU/CSU did not garner enough votes to form a majority coalition with the FDP; similarly the SPD and Greens did not have enough votes to continue with their formerly ruling coalition. A grand coalition government was subsequently forged between the CDU/CSU and the SPD. Partnerships like these typically involve carefully structured cabinets. The CDU/CSU ended up holding the Chancellery while the SPD took the majority of cabinet posts. Parties frequently make statements ahead of elections which coalitions they categorically reject, similar to election promises or shadow cabinets in other countries.

In Germany, coalitions rarely consist of more than two parties (CDU and CSU, two allies which always form a single caucus, are in this regard considered a single party). However, in the 2010s coalitions on the state level increasingly included three different parties, often FDP, Greens and one of the major parties or "red red green" coalitions of SPD, Linkspartei and Greens. By 2016, the Greens have joined governments on the state level in eleven coalitions in seven various constellations.

Armenia became an independent state in 1991, following the collapse of the Soviet Union. Since then, many political parties were formed in it, who mainly work with each other to form coalition governments. Currently the country is governed by the My Step Alliance coalition after successfully gaining a majority in the National Assembly of Armenia following the 2018 Armenian parliamentary election.

In federal Australian politics, the conservative Liberal, National, Country Liberal and Liberal National parties are united in a coalition, known simply as the Coalition. The Coalition has become so stable, at least at the federal level, that in practice the lower house of Parliament has become a two-party house, with the Coalition and the Labor Party being the major parties. This coalition is also found in the states of New South Wales and Victoria. In South Australia and Western Australia the Liberal and National parties compete separately, while in the Northern Territory and Queensland the two parties have merged, forming the Country Liberal Party, in 1978, and the Liberal National Party, in 2008, respectively.

The other federal coalition has been:

In Belgium, where there are separate Dutch-speaking and French-speaking parties for each political grouping, coalition cabinets of up to six parties are common.

In Canada, the Great Coalition was formed in 1864 by the Clear Grits, Parti bleu, and Liberal-Conservative Party. During the First World War, Prime Minister Robert Borden attempted to form a coalition with the opposition Liberals to broaden support for controversial conscription legislation. The Liberal Party refused the offer but some of their members did cross the floor and join the government. Although sometimes referred to as a coalition government, according to the definition above, it was not. It was disbanded after the end of the war.

As a result of the 1919 Ontario election, the United Farmers of Ontario and the Labour Party, together with three independent MLAs, formed a coalition that governed Ontario until 1923.

In British Columbia, the governing Liberals formed a coalition with the opposition Conservatives in order to prevent the surging, left-wing Cooperative Commonwealth Federation from taking power in the 1941 British Columbia general election. Liberal premier Duff Pattullo refused to form a coalition with the third-place Conservatives, so his party removed him. The Liberal–Conservative coalition introduced a winner-take-all preferential voting system (the "Alternative Vote") in the hopes that their supporters would rank the other party as their second preference; however, this strategy did not take CCF second preferences into account. In the 1952 British Columbia general election, to the surprise of many, the right-wing populist BC Social Credit Party won a minority. They were able to win a majority in the subsequent election as Liberal and Conservative supporters shifted their anti-CCF vote to Social Credit.

Manitoba has had more formal coalition governments than any other province. Following gains by the United Farmer's/Progressive movement elsewhere in the country, the United Farmers of Manitoba unexpectedly won the 1921 election. Like their counterparts in Ontario, they had not expected to win and did not have a leader. They asked John Bracken, a professor in animal husbandry, to become leader and premier. Bracken changed the party's name to the Progressive Party of Manitoba. During the Great Depression, Bracken survived at a time when other premiers were being defeated by forming a coalition government with the Manitoba Liberals (eventually, the two parties would merge into the , and decades later, the party would change its name to the Manitoba Liberal Party). In 1940, Bracken formed a wartime coalition government with almost every party in the Manitoba Legislature (the Conservatives, CCF, and Social Credit; however, the CCF broke with the coalition after a few years over policy differences). The only party not included was the small, communist Labor-Progressive Party, which had a handful of seats.

In Saskatchewan, NDP premier Roy Romanow formed a formal coalition with the Saskatchewan Liberals in 1999 after being reduced to a minority. After two years, the newly elected Liberal leader Jim Melanchuk chose to withdraw from the coalition; however, 2 out of 3 members of his caucus disagreed with him and left the Liberals to run as New Democrats in the upcoming election. The Saskatchewan NDP was re-elected with a majority under its new leader Lorne Calvert, while the Saskatchewan Liberals lost their remaining seats and have not been competitive in the province since.

According to historian Christopher Moore, coalition governments in Canada became much less possible in 1919, when the leaders of parties were no longer chosen by elected MPs but instead began to be chosen by party members. That kind of leadership selection process had never been tried in any parliament system before and remains uncommon in the parliaments of the world today. According to Moore, as long as that kind of leadership selection process remains in place and concentrates power in the hands of the leader, as opposed to backbenchers, then coalition governments will be very difficult to form. Moore shows that the diffusion of power within a party tends to also lead to a diffusion of power in the parliament in which that party operates, thereby making coalitions more likely.

During the 2008–09 Canadian parliamentary dispute, two of Canada's opposition parties signed an agreement to form what would become the country's second coalition government since Confederation if the minority Conservative government was defeated on a vote of non-confidence, unseating Stephen Harper as Prime Minister. The agreement outlined a formal coalition consisting of two opposition parties, the Liberal Party and the New Democratic Party. The Bloc Québécois agreed to support the proposed coalition on confidence matters for 18 months. In the end, parliament was prorogued by the Governor General, and the coalition dispersed before parliament was reconvened.

In Denmark, all governments from 1982 until the June 2015 elections have been coalitions. The first coalition in Danish political history was formed in 1929 by Thorvald Stauning and consisted of the Social Democrats (Staunings own party) and the Social Liberals. Since then, a number of parties have participated in coalitions.

Excluding the post-WW2 Liberation Cabinet's member parties, the following parties have done so: The Centre Democrats, the Christian People's Party, the Conservative People's Party, the Justice Party, the Social Democrats, the Socialist People's Party, the Social Liberal Party, and Venstre.

In Finland, no party has had an absolute majority in the parliament since independence, and multi-party coalitions have been the norm. Finland experienced its most stable government (Lipponen I and II) since independence with a five-party governing coalition, a so-called "rainbow government". The Lipponen cabinets set the stability record and were unusual in the respect that both moderate (SDP) and radical left wing (Left Alliance) parties sat in the government with the major right-wing party (National Coalition). The Katainen cabinet was also a rainbow coalition of a total of five parties.

Since India's Independence on 15 August 1947, Indian National Congress, the major political party instrumental in Indian independence movement, ruled the nation. The first Prime Minister Jawaharlal Nehru, second PM Lal Bahadur Shastri and the third PM Indira Gandhi, all were from the Congress party. However, Raj Narain, who had unsuccessfully contested election against Indira from the constituency of Rae Bareilly in 1971, lodged a case, alleging electoral malpractices. In June 1975, Indira was found guilty and barred by High Court from holding public office for six years. In response, an ungracious Emergency was declared under the pretext of national security. The next election's result was that India's first-ever coalition government was formed at the national level under the Prime Ministership of Morarji Desai, which was also the first non-Congress national government, which existed from 24 March 1977 to 15 July 1979, headed by the Janata Party, an amalgam of political parties opposed to Emergency imposed between 1975 and 1977. As the popularity of Janata Party dwindled, Morarji Desai had to resign and Charan Singh, a rival of Desai became the fifth PM. However, due to lack of support, this coalition government did not complete its five-year term.

Congress returned to the power in 1980 under Indira Gandhi, and later under Rajiv Gandhi as the 6th PM. However, the next general election of 1989 once again brought a coalition government under National Front, which lasted until 1991, with two Prime Ministers, the second one being supported by Congress. The 1991 election resulted in a Congress led stable minority government for five years. The next 11th parliament produced three Prime Ministers in two years and forced the country back to the polls in 1998. The first successful coalition government in India which completed the whole 5-year term was the Bharatiya Janata Party (BJP) led National Democratic Alliance with Atal Bihari Vajpayee as PM from 1999 to 2004. Then another coalition, Congress led United Progressive Alliance, consisting of 13 separate parties ruled India for two terms from 2004 to 2014 with Manmohan Singh as PM. However, in the 16th general election in May 2014, BJP secured majority on its own (first party to do so since 1984 election) and National Democratic Alliance came into power, with Narendra Modi as Prime Minister. In 2019, Narendra Modi got re-elected as Prime Minister for the second time as National Democratic Alliance again secured majority in the 17th general election.

As a result of the toppling of Suharto, political freedom is significantly increased. Compared to only three parties allowed to exist in the New Order era, a total of 48 political parties participated in the 1999 election, a total of 24 parties in the 2004 election, 38 parties in the 2009 election, and 15 parties in the 2014 election. There are no majority winner of those elections and coalition governments are inevitable. The current government is a coalition of seven parties led by the PDIP and Golkar.

In Republic of Ireland, coalition governments are quite common; not since 1977 has a single party been able to form a majority government. Coalitions are the typically formed of two or more parties always consisting of one of the two biggest parties, Fianna Fáil and Fine Gael, and one or more smaller parties or independent members of parliament. The current government consists of a minority Fine Gael government, supported by a confidence and supply arrangement with Fianna Fáil.

Ireland's first coalition government was formed in 1948. Ireland has had consecutive coalition governments since the 1989 general election, excluding two brief Fianna Fáil minority administrations in 1994 and 2011 that followed the withdrawal of their coalition partners from government. Before 1989, Fianna Fáil had opposed participation in coalition governments, preferring single-party minority government instead.

Irish coalition governments have traditionally been based on one of two large blocs in Dáil Éireann: either Fianna Fáil in coalition with smaller parties or independents, or Fine Gael and the Labour Party in coalition, sometimes with smaller parties. The only exception to these traditional alliances was the 23rd Government of Ireland, comprising Fianna Fáil and the Labour Party, which ruled between 1993 and 1994. The Government of the 31st Dáil, though a traditional Fine Gael–Labour coalition, resembles a grand coalition, due to the collapse of the Fianna Fáil to third place among parties in Dáil Éireann.

A similar situation exists in Israel, which typically has at least 10 parties holding representation in the Knesset. The only faction to ever gain the majority of Knesset seats was Alignment, an alliance of the Labor Party and Mapam that held an absolute majority for a brief period from 1968 to 1969. Historically, control of the Israeli government has alternated between periods of rule by the right-wing Likud in coalition with several right-wing and religious parties and periods of rule by the center-left Labor in coalition with several left-wing parties. Ariel Sharon's formation of the centrist Kadima party in 2006 drew support from former Labor and Likud members, and Kadima ruled in coalition with several other parties.

Israel also formed a national unity government from 1984–1988. The premiership and foreign ministry portfolio were held by the head of each party for two years, and they switched roles in 1986.

Post-World War II Japan has historically been dominated by the Liberal Democratic Party, but there was a brief coalition government formed after the 1993 election following LDP's first loss of its overall House of Representatives majority since 1955, winning only 223 out of 511 seats. The LDP government was replaced by an eight-party coalition government, which consisted of all of the previous opposition parties excluding the Japanese Communist Party, who together controlled 243 seats. Every Japanese government since then has been a coalition government in one way or another.

MMP was introduced in New Zealand in the 1996 election. 
In order to get into parliament, parties need to get a total of 50% of the 121 seats in parliament – 61. Since no parties have ever gotten a full majority, they must form coalitions with other parties. For example, during the 2017 general election, Labour got 46 seats and New Zealand First got nine. The two formed a Coalition Government with confidence and supply from the Green Party which got eight seats.

Advocates of proportional representation suggest that a coalition government leads to more consensus-based politics, as a government comprising differing parties (often based on different ideologies) need to compromise about governmental policy. Another stated advantage is that a coalition government better reflects the popular opinion of the electorate within a country.

Those who disapprove of coalition governments believe that such governments have a tendency to be fractious and prone to disharmony, as their component parties hold differing beliefs and thus may not always agree on policy. Sometimes the results of an election mean that the coalitions which are mathematically most probable are ideologically infeasible, for example in Flanders or Northern Ireland. A second difficulty might be the ability of minor parties to play "kingmaker" and, particularly in close elections, gain far more power in exchange for their support than the size of their vote would otherwise justify.

Coalition governments have also been criticized for sustaining a consensus on issues when disagreement and the consequent discussion would be more fruitful. To forge a consensus, the leaders of ruling coalition parties can agree to silence their disagreements on an issue to unify the coalition against the opposition. The coalition partners, if they control the parliamentary majority, can collude to make the parliamentary discussion on the issue irrelevant by consistently disregarding the arguments of the opposition and voting against the opposition's proposals — even if there is disagreement within the ruling parties about the issue.

Powerful parties can also act in an oligocratic way to form an alliance to stifle the growth of emerging parties. Of course, such an event is rare in coalition governments when compared to two-party systems, which typically exist because of stifling of the growth of emerging parties, often through discriminatory nomination rules regulations and plurality voting systems, and so on.

A single, more powerful party can shape the policies of the coalition disproportionately. Smaller or less powerful parties can be intimidated to not openly disagree. In order to maintain the coalition, they would have to vote against their own party's platform in the parliament. If they do not, the party has to leave the government and loses executive power. However, this is contradicted by the "kingmaker" factor mentioned above.


</doc>
<doc id="6038" url="https://en.wikipedia.org/wiki?curid=6038" title="Chemical engineering">
Chemical engineering

Chemical engineering is a branch of engineering that uses principles of chemistry, physics, mathematics, biology, and economics to efficiently use, produce, design, transport and transform energy and materials. The work of chemical engineers can range from the utilisation of nano-technology and nano-materials in the laboratory to large-scale industrial processes that convert chemicals, raw materials, living cells, microorganisms, and energy into useful forms and products.

Chemical engineers are involved in many aspects of plant design and operation, including safety and hazard assessments, process design and analysis, modeling, control engineering, chemical reaction engineering, nuclear engineering, biological engineering, construction specification, and operating instructions. 

Chemical engineers typically hold a degree in Chemical Engineering or Process Engineering. Practising engineers may have professional certification and be accredited members of a professional body. Such bodies include the Institution of Chemical Engineers (IChemE) or the American Institute of Chemical Engineers (AIChE).
A degree in chemical engineering is directly linked with all of the other engineering disciplines, to various extents.

A 1996 "British Journal for the History of Science" article cites James F. Donnelly for mentioning an 1839 reference to chemical engineering in relation to the production of sulfuric acid. In the same paper however, George E. Davis, an English consultant, was credited for having coined the term. Davis also tried to found a "Society of Chemical Engineering", but instead it was named the Society of Chemical Industry (1881), with Davis as its first Secretary. The "History of Science in United States: An Encyclopedia" puts the use of the term around 1890. "Chemical engineering", describing the use of mechanical equipment in the chemical industry, became common vocabulary in England after 1850. By 1910, the profession, "chemical engineer," was already in common use in Britain and the United States.

Chemical engineering emerged upon the development of unit operations, a fundamental concept of the discipline of chemical engineering. Most authors agree that Davis invented the concept of unit operations if not substantially developed it. He gave a series of lectures on unit operations at the Manchester Technical School (later part of the University of Manchester) in 1887, considered to be one of the earliest such about chemical engineering. Three years before Davis' lectures, Henry Edward Armstrong taught a degree course in chemical engineering at the City and Guilds of London Institute. Armstrong's course failed simply because its graduates were not especially attractive to employers. Employers of the time would have rather hired chemists and mechanical engineers. Courses in chemical engineering offered by Massachusetts Institute of Technology (MIT) in the United States, Owens College in Manchester, England, and University College London suffered under similar circumstances.

Starting from 1888, Lewis M. Norton taught at MIT the first chemical engineering course in the United States. Norton's course was contemporaneous and essentially similar to Armstrong's course. Both courses, however, simply merged chemistry and engineering subjects along with product design. "Its practitioners had difficulty convincing engineers that they were engineers and chemists that they were not simply chemists." Unit operations was introduced into the course by William Hultz Walker in 1905. By the early 1920s, unit operations became an important aspect of chemical engineering at MIT and other US universities, as well as at Imperial College London. The American Institute of Chemical Engineers (AIChE), established in 1908, played a key role in making chemical engineering considered an independent science, and unit operations central to chemical engineering. For instance, it defined chemical engineering to be a "science of itself, the basis of which is ... unit operations" in a 1922 report; and with which principle, it had published a list of academic institutions which offered "satisfactory" chemical engineering courses. Meanwhile, promoting chemical engineering as a distinct science in Britain led to the establishment of the Institution of Chemical Engineers (IChemE) in 1922. IChemE likewise helped make unit operations considered essential to the discipline.

In 1940s, it became clear that unit operations alone were insufficient in developing chemical reactors. While the predominance of unit operations in chemical engineering courses in Britain and the United States continued until the 1960s, transport phenomena started to experience greater focus. Along with other novel concepts, such as process systems engineering (PSE), a "second paradigm" was defined. Transport phenomena gave an analytical approach to chemical engineering while PSE focused on its synthetic elements, such as control system and process design. Developments in chemical engineering before and after World War II were mainly incited by the petrochemical industry, however, advances in other fields were made as well. Advancements in biochemical engineering in the 1940s, for example, found application in the pharmaceutical industry, and allowed for the mass production of various antibiotics, including penicillin and streptomycin. Meanwhile, progress in polymer science in the 1950s paved way for the "age of plastics".

Concerns regarding the safety and environmental impact of large-scale chemical manufacturing facilities were also raised during this period. "Silent Spring", published in 1962, alerted its readers to the harmful effects of DDT, a potent insecticide. The 1974 Flixborough disaster in the United Kingdom resulted in 28 deaths, as well as damage to a chemical plant and three nearby villages. The 1984 Bhopal disaster in India resulted in almost 4,000 deaths. These incidents, along with other incidents, affected the reputation of the trade as industrial safety and environmental protection were given more focus. In response, the IChemE required safety to be part of every degree course that it accredited after 1982. By the 1970s, legislation and monitoring agencies were instituted in various countries, such as France, Germany, and the United States.

Advancements in computer science found applications designing and managing plants, simplifying calculations and drawings that previously had to be done manually. The completion of the Human Genome Project is also seen as a major development, not only advancing chemical engineering but genetic engineering and genomics as well. Chemical engineering principles were used to produce DNA sequences in large quantities.

Chemical engineering involves the application of several principles. Key concepts are presented below.

Chemical engineering design concerns the creation of plans, specification, and economic analyses for pilot plants, new plants or plant modifications. Design engineers often work in a consulting role, designing plants to meet clients' needs. Design is limited by a number of factors, including funding, government regulations and safety standards. These constraints dictate a plant's choice of process, materials and equipment.

Plant construction is coordinated by project engineers and project managers depending on the size of the investment. A chemical engineer may do the job of project engineer full-time or part of the time, which requires additional training and job skills or act as a consultant to the project group. In the USA the education of chemical engineering graduates from the Baccalaureate programs accredited by ABET do not usually stress project engineering education, which can be obtained by specialized training, as electives, or from graduate programs. Project engineering jobs are some of the largest employers for chemical engineers.

A unit operation is a physical step in an individual chemical engineering process. Unit operations (such as crystallization, filtration, drying and evaporation) are used to prepare reactants, purifying and separating its products, recycling unspent reactants, and controlling energy transfer in reactors. On the other hand, a unit process is the chemical equivalent of a unit operation. Along with unit operations, unit processes constitute a process operation. Unit processes (such as nitration and oxidation) involve the conversion of material by biochemical, thermochemical and other means. Chemical engineers responsible for these are called process engineers.

Process design requires the definition of equipment types and sizes as well as how they are connected together and the materials of construction. Details are often printed on a Process Flow Diagram which is used to control the capacity and reliability of a new or modified chemical factory.

Education for chemical engineers in the first college degree 3 or 4 years of study stresses the principles and practices of process design. The same skills are used in existing chemical plants to evaluate the efficiency and make recommendations for improvements.

Modeling and analysis of transport phenomena is essential for many industrial applications. Transport phenomena involve fluid dynamics, heat transfer and mass transfer, which are governed mainly by momentum transfer, energy transfer and transport of chemical species respectively. Models often involve separate considerations for macroscopic, microscopic and molecular level phenomena. Modeling of transport phenomena therefore requires an understanding of applied mathematics.

Chemical engineers "develop economic ways of using materials and energy". Chemical engineers use chemistry and engineering to turn raw materials into usable products, such as medicine, petrochemicals and plastics on a large-scale, industrial setting. They are also involved in waste management and research. Both applied and research facets could make extensive use of computers.

Chemical engineers may be involved in industry or university research where they are tasked with designing and performing experiments to create better and safer methods for production, pollution control, and resource conservation. They may be involved in designing and constructing plants as a project engineer. Chemical engineers serving as project engineers use their knowledge in selecting optimal production methods and plant equipment to minimize costs and maximize safety and profitability. After plant construction, chemical engineering project managers may be involved in equipment upgrades, process changes, troubleshooting, and daily operations in either full-time or consulting roles. 



</doc>
<doc id="6041" url="https://en.wikipedia.org/wiki?curid=6041" title="List of comedians">
List of comedians

A comedian is one who entertains through comedy, such as jokes and other forms of humour. Following is a list of comedians, comedy groups, and comedy writers.

"(sorted alphabetically by surname)"



"(sorted alphabetically by surname)"

Lists of comedians by nationality


Other related lists


</doc>
<doc id="6042" url="https://en.wikipedia.org/wiki?curid=6042" title="Compact space">
Compact space

In mathematics, and more specifically in general topology, compactness is a property that generalizes the notion of a subset of Euclidean space being closed (that is, containing all its limit points) and bounded (that is, having all its points lie within some fixed distance of each other). Examples include a closed interval, a rectangle, or a finite set of points. This notion is defined for more general topological spaces than Euclidean space in various ways.

One such generalization is that a topological space is "sequentially" compact if every infinite sequence of points sampled from the space has an infinite subsequence that converges to some point of the space. The Bolzano–Weierstrass theorem states that a subset of Euclidean space is compact in this sequential sense if and only if it is closed and bounded. Thus, if one chooses an infinite number of points in the "closed" unit interval some of those points will get arbitrarily close to some real number in that space. For instance, some of the numbers accumulate to 0 (others accumulate to 1). The same set of points would not accumulate to any point of the "open" unit interval ; so the open unit interval is not compact. Euclidean space itself is not compact since it is not bounded. In particular, the sequence of points has no subsequence that converges to any real number.

Apart from closed and bounded subsets of Euclidean space, typical examples of compact spaces include spaces consisting not of geometrical points but of functions. The term "compact" was introduced into mathematics by Maurice Fréchet in 1904 as a distillation of this concept. Compactness in this more general situation plays an extremely important role in mathematical analysis, because many classical and important theorems of 19th-century analysis, such as the extreme value theorem, are easily generalized to this situation. A typical application is furnished by the Arzelà–Ascoli theorem or the Peano existence theorem, in which one is able to conclude the existence of a function with some required properties as a limiting case of some more elementary construction.

Various equivalent notions of compactness, including sequential compactness and limit point compactness, can be developed in general metric spaces. In general topological spaces, however, different notions of compactness are not necessarily equivalent. The most useful notion, which is the standard definition of the unqualified term "compactness", is phrased in terms of the existence of finite families of open sets that "cover" the space in the sense that each point of the space lies in some set contained in the family. This more subtle notion, introduced by Pavel Alexandrov and Pavel Urysohn in 1929, exhibits compact spaces as generalizations of finite sets. In spaces that are compact in this sense, it is often possible to patch together information that holds locally—that is, in a neighborhood of each point—into corresponding statements that hold throughout the space, and many theorems are of this character.

The term compact set is sometimes a synonym for compact space, but usually refers to a compact subspace of a topological space.

In the 19th century, several disparate mathematical properties were understood that would later be seen as consequences of compactness. On the one hand, Bernard Bolzano (1817) had been aware that any bounded sequence of points (in the line or plane, for instance) has a subsequence that must eventually get arbitrarily close to some other point, called a limit point. Bolzano's proof relied on the method of bisection: the sequence was placed into an interval that was then divided into two equal parts, and a part containing infinitely many terms of the sequence was selected. The process could then be repeated by dividing the resulting smaller interval into smaller and smaller parts until it closes down on the desired limit point. The full significance of Bolzano's theorem, and its method of proof, would not emerge until almost 50 years later when it was rediscovered by Karl Weierstrass.

In the 1880s, it became clear that results similar to the Bolzano–Weierstrass theorem could be formulated for spaces of functions rather than just numbers or geometrical points. The idea of regarding functions as themselves points of a generalized space dates back to the investigations of Giulio Ascoli and Cesare Arzelà. The culmination of their investigations, the Arzelà–Ascoli theorem, was a generalization of the Bolzano–Weierstrass theorem to families of continuous functions, the precise conclusion of which was that it was possible to extract a uniformly convergent sequence of functions from a suitable family of functions. The uniform limit of this sequence then played precisely the same role as Bolzano's "limit point". Towards the beginning of the twentieth century, results similar to that of Arzelà and Ascoli began to accumulate in the area of integral equations, as investigated by David Hilbert and Erhard Schmidt. For a certain class of Green's functions coming from solutions of integral equations, Schmidt had shown that a property analogous to the Arzelà–Ascoli theorem held in the sense of mean convergence—or convergence in what would later be dubbed a Hilbert space. This ultimately led to the notion of a compact operator as an offshoot of the general notion of a compact space. It was Maurice Fréchet who, in 1906, had distilled the essence of the Bolzano–Weierstrass property and coined the term "compactness" to refer to this general phenomenon (he used the term already in his 1904 paper which led to the famous 1906 thesis).

However, a different notion of compactness altogether had also slowly emerged at the end of the 19th century from the study of the continuum, which was seen as fundamental for the rigorous formulation of analysis. In 1870, Eduard Heine showed that a continuous function defined on a closed and bounded interval was in fact uniformly continuous. In the course of the proof, he made use of a lemma that from any countable cover of the interval by smaller open intervals, it was possible to select a finite number of these that also covered it. The significance of this lemma was recognized by Émile Borel (1895), and it was generalized to arbitrary collections of intervals by Pierre Cousin (1895) and Henri Lebesgue (1904). The Heine–Borel theorem, as the result is now known, is another special property possessed by closed and bounded sets of real numbers.

This property was significant because it allowed for the passage from local information about a set (such as the continuity of a function) to global information about the set (such as the uniform continuity of a function). This sentiment was expressed by , who also exploited it in the development of the integral now bearing his name. Ultimately the Russian school of point-set topology, under the direction of Pavel Alexandrov and Pavel Urysohn, formulated Heine–Borel compactness in a way that could be applied to the modern notion of a topological space. showed that the earlier version of compactness due to Fréchet, now called (relative) sequential compactness, under appropriate conditions followed from the version of compactness that was formulated in terms of the existence of finite subcovers. It was this notion of compactness that became the dominant one, because it was not only a stronger property, but it could be formulated in a more general setting with a minimum of additional technical machinery, as it relied only on the structure of the open sets in a space.
Any finite space is trivially compact. A non-trivial example of a compact space is the (closed) unit interval of real numbers. If one chooses an infinite number of distinct points in the unit interval, then there must be some accumulation point in that interval. For instance, the odd-numbered terms of the sequence get arbitrarily close to 0, while the even-numbered ones get arbitrarily close to 1. The given example sequence shows the importance of including the boundary points of the interval, since the limit points must be in the space itself — an open (or half-open) interval of the real numbers is not compact. It is also crucial that the interval be bounded, since in the interval one could choose the sequence of points , of which no sub-sequence ultimately gets arbitrarily close to any given real number.

In two dimensions, closed disks are compact since for any infinite number of points sampled from a disk, some subset of those points must get arbitrarily close either to a point within the disc, or to a point on the boundary. However, an open disk is not compact, because a sequence of points can tend to the boundary without getting arbitrarily close to any point in the interior. Likewise, spheres are compact, but a sphere missing a point is not since a sequence of points can tend to the missing point, thereby not getting arbitrarily close to any point "within" the space. Lines and planes are not compact, since one can take a set of equally-spaced points in any given direction without approaching any point.

Various definitions of compactness may apply, depending on the level of generality. A subset of Euclidean space in particular is called compact if it is closed and bounded. This implies, by the Bolzano–Weierstrass theorem, that any infinite sequence from the set has a subsequence that converges to a point in the set. Various equivalent notions of compactness, such as sequential compactness and limit point compactness, can be developed in general metric spaces.

In general topological spaces, however, the different notions of compactness are not equivalent, and the most useful notion of compactness—originally called "bicompactness"—is defined using covers consisting of open sets (see "Open cover definition" below). That this form of compactness holds for closed and bounded subsets of Euclidean space is known as the Heine–Borel theorem. Compactness, when defined in this manner, often allows one to take information that is known locally—in a neighbourhood of each point of the space—and to extend it to information that holds globally throughout the space. An example of this phenomenon is Dirichlet's theorem, to which it was originally applied by Heine, that a continuous function on a compact interval is uniformly continuous; here, continuity is a local property of the function, and uniform continuity the corresponding global property.

Formally, a topological space is called "compact" if each of its open covers has a finite subcover. That is, is compact if for every collection of open subsets of such that

there is a finite subset of such that

Some branches of mathematics such as algebraic geometry, typically influenced by the French school of Bourbaki, use the term "quasi-compact" for the general notion, and reserve the term "compact" for topological spaces that are both Hausdorff and "quasi-compact". A compact set is sometimes referred to as a "compactum", plural "compacta".

A subset of a topological space is said to be compact if it is compact as a subspace (in the subspace topology). That is, is compact if for every arbitrary collection of open subsets of such that

there is a finite subset of such that

Compactness is a "topological" property. That is, if formula_5, with subset equipped with the subspace topology, then is compact in if and only if is compact in .

Assuming the axiom of choice, the following are equivalent:

For any subset "A" of Euclidean space R, "A" is compact if and only if it is closed and bounded; this is the Heine–Borel theorem.

As a Euclidean space is a metric space, the conditions in the next subsection also apply to all of its subsets. Of all of the equivalent conditions, it is in practice easiest to verify that a subset is closed and bounded, for example, for a closed interval or closed "n"-ball.

For any metric space ("X", "d"), the following are equivalent:

A compact metric space ("X", "d") also satisfies the following properties:

Let "X" be a topological space and C("X") the ring of real continuous functions on "X". For each "p"∈"X", the evaluation map formula_6
given by ev("f")="f"("p") is a ring homomorphism. The kernel of ev is a maximal ideal, since the residue field is the field of real numbers, by the first isomorphism theorem. A topological space "X" is pseudocompact if and only if every maximal ideal in C("X") has residue field the real numbers. For completely regular spaces, this is equivalent to every maximal ideal being the kernel of an evaluation homomorphism. There are pseudocompact spaces that are not compact, though.

In general, for non-pseudocompact spaces there are always maximal ideals "m" in C("X") such that the residue field C("X")/"m" is a (non-archimedean) hyperreal field. The framework of non-standard analysis allows for the following alternative characterization of compactness: a topological space "X" is compact if and only if every point "x" of the natural extension "*X" is infinitely close to a point "x" of "X" (more precisely, "x" is contained in the monad of "x").

A space "X" is compact if its hyperreal extension "*X" (constructed, for example, by the ultrapower construction) has the property that every point of "*X" is infinitely close to some point of "X"⊂"*X". For example, an open real interval is not compact because its hyperreal extension *(0,1) contains infinitesimals, which are infinitely close to 0, which is not a point of "X".

A continuous image of a compact space is compact.
This implies the extreme value theorem: a continuous real-valued function on a nonempty compact space is bounded above and attains its supremum. (Slightly more generally, this is true for an upper semicontinuous function.) As a sort of converse to the above statements, the pre-image of a compact space under a proper map is compact.

A closed subset of a compact space is compact, and a finite union of compact sets is compact.

The product of any collection of compact spaces is compact. (This is Tychonoff's theorem, which is equivalent to the axiom of choice.)

Every topological space "X" is an open dense subspace of a compact space having at most one point more than "X", by the Alexandroff one-point compactification. By the same construction, every locally compact Hausdorff space "X" is an open dense subspace of a compact Hausdorff space having at most one point more than "X".

A nonempty compact subset of the real numbers has a greatest element and a least element.

Let "X" be a simply ordered set endowed with the order topology. Then "X" is compact if and only if "X" is a complete lattice (i.e. all subsets have suprema and infima).







</doc>
<doc id="6045" url="https://en.wikipedia.org/wiki?curid=6045" title="Clodius">
Clodius

Clodius is an alternate form of the Roman "nomen" Claudius, a patrician "gens" that was traditionally regarded as Sabine in origin. The alternation of "o" and "au" is characteristic of the Sabine dialect. The feminine form is Clodia.

During the Late Republic, the spelling "Clodius" is most prominently associated with Publius Clodius Pulcher, a popularis politician who gave up his patrician status through an order in order to qualify for the office of tribune of the "plebs". Clodius positioned himself as a champion of the urban "plebs", supporting free grain for the poor and the right of association in guilds ("collegia"); because of this individual's ideology, "Clodius" has often been taken as a more "plebeian" spelling and a gesture of political solidarity. Clodius's two elder brothers, the Appius Claudius Pulcher who was consul in 54 BC and the C. Claudius Pulcher who was praetor in 56 BC, conducted more conventional political careers and are referred to in contemporary sources with the traditional spelling.

The view that "Clodius" represents a plebeian or politicized form has been questioned by Clodius's chief modern-era biographer. In "The Patrician Tribune", W. Jeffrey Tatum points out that the spelling is also associated with Clodius's sisters and that "the political explanation … is almost certainly wrong." A plebeian branch of the "gens", the Claudii Marcelli, retained the supposedly patrician spelling, while there is some inscriptional evidence that the "-o-" form may also have been used on occasion by close male relatives of the "patrician tribune" Clodius. Tatum argues that the use of "-o-" by the "chic" Clodia Metelli was a fashionable affectation, and that Clodius, whose perhaps inordinately loving relationship with his sister was the subject of much gossip and insinuation, was imitating his stylish sibling. The linguistic variation of "o" for "au" was characteristic of the Umbrian language, of which Sabine was a branch. Forms using "o" were considered archaic or rustic in the 50s BC, and the use of "Clodius" would have been either a whimsical gesture of pastoral fantasy, or a trendy assertion of antiquarian authenticity.

In addition to Clodius, Clodii from the Republican era include:


People using the name "Clodius" during the period of the Roman Empire include:

The Clodii Celsini continued to practice the traditional religions of antiquity in the face of Christian hegemony through at least the 4th century, when Clodius Celsinus Adelphius (see below) converted. Members of this branch include:




</doc>
<doc id="6046" url="https://en.wikipedia.org/wiki?curid=6046" title="Cicero">
Cicero

Marcus Tullius Cicero (; ; 106 BC – 7 December 43 BC) was a Roman statesman, orator, lawyer and philosopher, who served as consul in the year 63 BC. He came from a wealthy municipal family of the Roman equestrian order, and is considered one of Rome's greatest orators and prose stylists.

His influence on the Latin language was so immense that the subsequent history of prose, not only in Latin but in European languages up to the 19th century, was said to be either a reaction against or a return to his style. Cicero introduced the Romans to the chief schools of Greek philosophy and created a Latin philosophical vocabulary (with neologisms such as "evidentia", "humanitas", "qualitas", "quantitas", and "essentia") distinguishing himself as a translator and philosopher.

Though he was an accomplished orator and successful lawyer, Cicero believed his political career was his most important achievement. It was during his consulship that the second Catilinarian conspiracy attempted to overthrow the government through an attack on the city by outside forces, and Cicero suppressed the revolt by summarily and controversially executing five conspirators. During the chaotic latter half of the 1st century BC marked by civil wars and the dictatorship of Gaius Julius Caesar, Cicero championed a return to the traditional republican government. Following Julius Caesar's death, Cicero became an enemy of Mark Antony in the ensuing power struggle, attacking him in a series of speeches. He was proscribed as an enemy of the state by the Second Triumvirate and consequently executed by soldiers operating on their behalf in 43 BC after having been intercepted during an attempted flight from the Italian peninsula. His severed hands and head were then, as a final revenge of Mark Antony, displayed on The Rostra.

Petrarch's rediscovery of Cicero's letters is often credited for initiating the 14th-century Renaissance in public affairs, humanism, and classical Roman culture. According to Polish historian Tadeusz Zieliński, "the Renaissance was above all things a revival of Cicero, and only after him and through him of the rest of Classical antiquity." The peak of Cicero's authority and prestige came during the 18th-century Enlightenment, and his impact on leading Enlightenment thinkers and political theorists such as John Locke, David Hume, Montesquieu and Edmund Burke was substantial.
His works rank among the most influential in European culture, and today still constitute one of the most important bodies of primary material for the writing and revision of Roman history, especially the last days of the Roman Republic.

Cicero was born in 106 BC in Arpinum, a hill town southeast of Rome. He belonged to the "tribus" Cornelia. His father was a well-to-do member of the equestrian order and possessed good connections in Rome. However, being a semi-invalid, he could not enter public life and studied extensively to compensate. Although little is known about Cicero's mother, Helvia, it was common for the wives of important Roman citizens to be responsible for the management of the household. Cicero's brother Quintus wrote in a letter that she was a thrifty housewife.

Cicero's cognomen, or personal surname, comes from the Latin for chickpea, "cicer". Plutarch explains that the name was originally given to one of Cicero's ancestors who had a cleft in the tip of his nose resembling a chickpea. However, it is more likely that Cicero's ancestors prospered through the cultivation and sale of chickpeas. Romans often chose down-to-earth personal surnames. The famous family names of Fabius, Lentulus, and Piso come from the Latin names of beans, lentils, and peas, respectively. Plutarch writes that Cicero was urged to change this deprecatory name when he entered politics, but refused, saying that he would make "Cicero" more glorious than "Scaurus" ("Swollen-ankled") and "Catulus" ("Puppy").

During this period in Roman history, "cultured" meant being able to speak both Latin and Greek. Cicero was therefore educated in the teachings of the ancient Greek philosophers, poets and historians; as he obtained much of his understanding of the theory and practice of rhetoric from the Greek poet Archias and from the Greek rhetorician Apollonius. Cicero used his knowledge of Greek to translate many of the theoretical concepts of Greek philosophy into Latin, thus translating Greek philosophical works for a larger audience. It was precisely his broad education that tied him to the traditional Roman elite.

Cicero's interest in philosophy figured heavily in his later career and led to him providing a comprehensive account of Greek philosophy for a Roman audience, including creating a philosophical vocabulary in Latin. In 87 BC, Philo of Larissa, the head of the Academy that was founded by Plato in Athens about 300 years earlier, arrived in Rome. Cicero, "inspired by an extraordinary zeal for philosophy", sat enthusiastically at his feet and absorbed Plato's philosophy.
Cicero said of Plato's Dialogues, that if Zeus were to speak, he would use their language.

According to Plutarch, Cicero was an extremely talented student, whose learning attracted attention from all over Rome, affording him the opportunity to study Roman law under Quintus Mucius Scaevola. Cicero's fellow students were Gaius Marius Minor, Servius Sulpicius Rufus (who became a famous lawyer, one of the few whom Cicero considered superior to himself in legal matters), and Titus Pomponius. The latter two became Cicero's friends for life, and Pomponius (who later received the nickname "Atticus", and whose sister married Cicero's brother) would become, in Cicero's own words, "as a second brother", with both maintaining a lifelong correspondence.

In 79 BC, Cicero left for Greece, Asia Minor and Rhodes. This was perhaps to avoid the potential wrath of Sulla, as Plutarch claims, though Cicero himself says it was to hone his skills and improve his physical fitness. In Athens he studied philosophy with Antiochus of Ascalon, the 'Old Academic' and initiator of Middle Platonism. In Asia Minor, he met the leading orators of the region and continued to study with them. Cicero then journeyed to Rhodes to meet his former teacher, Apollonius Molon, who had previously taught him in Rome. Molon helped Cicero hone the excesses in his style, as well as train his body and lungs for the demands of public speaking. Charting a middle path between the competing Attic and Asiatic styles, Cicero would ultimately become considered second only to Demosthenes among history's orators.

Cicero married Terentia probably at the age of 27, in 79 BC. According to the upper class mores of the day it was a marriage of convenience, but lasted harmoniously for nearly 30 years. Terentia's family was wealthy, probably the plebeian noble house of Terenti Varrones, thus meeting the needs of Cicero's political ambitions in both economic and social terms. She had a half-sister named Fabia, who as a child had become a Vestal Virgin, a very great honour. Terentia was a strong willed woman and (citing Plutarch) "she took more interest in her husband's political career than she allowed him to take in household affairs."

In the 50s BC, Cicero's letters to Terentia became shorter and colder. He complained to his friends that Terentia had betrayed him but did not specify in which sense. Perhaps the marriage simply could not outlast the strain of the political upheaval in Rome, Cicero's involvement in it, and various other disputes between the two. The divorce appears to have taken place in 51 BC or shortly before. In 46 or 45 BC, Cicero married a young girl, Publilia, who had been his ward. It is thought that Cicero needed her money, particularly after having to repay the dowry of Terentia, who came from a wealthy family. This marriage did not last long.

Although his marriage to Terentia was one of convenience, it is commonly known that Cicero held great love for his daughter Tullia. When she suddenly became ill in February 45 BC and died after having seemingly recovered from giving birth to a son in January, Cicero was stunned. "I have lost the one thing that bound me to life" he wrote to Atticus. Atticus told him to come for a visit during the first weeks of his bereavement, so that he could comfort him when his pain was at its greatest. In Atticus's large library, Cicero read everything that the Greek philosophers had written about overcoming grief, "but my sorrow defeats all consolation." Caesar and Brutus as well as Servius Sulpicius Rufus sent him letters of condolence.

Cicero hoped that his son Marcus would become a philosopher like him, but Marcus himself wished for a military career. He joined the army of Pompey in 49 BC and after Pompey's defeat at Pharsalus 48 BC, he was pardoned by Caesar. Cicero sent him to Athens to study as a disciple of the peripatetic philosopher Kratippos in 48 BC, but he used this absence from "his father's vigilant eye" to "eat, drink and be merry." After Cicero's murder he joined the army of the "Liberatores" but was later pardoned by Augustus. Augustus's bad conscience for not having objected to Cicero's being put on the proscription list during the Second Triumvirate led him to aid considerably Marcus Minor's career. He became an augur, and was nominated consul in 30 BC together with Augustus. As such, he was responsible for revoking the honors of Mark Antony, who was responsible for the proscription, and could in this way take revenge. Later he was appointed proconsul of Syria and the province of Asia.

Cicero wanted to pursue a public career in politics along the steps of the Cursus honorum. In 90–88 BC, he served both Pompeius Strabo and Lucius Cornelius Sulla as they campaigned in the Social War, though he had no taste for military life, being an intellectual first and foremost. 

Cicero started his career as a lawyer around 83–81 BC. The first extant speech is a private case from 81 BC (the "pro Quinctio"), delivered when Cicero was aged 26, though he refers throughout to previous defenses he had already undertaken. His first major public case, of which a written record is still extant, was his 80 BC defense of Sextus Roscius on the charge of patricide. Taking this case was a courageous move for Cicero; patricide was considered an appalling crime, and the people whom Cicero accused of the murder, the most notorious being Chrysogonus, were favorites of Sulla. At this time it would have been easy for Sulla to have the unknown Cicero murdered. Cicero's defense was an indirect challenge to the dictator Sulla, and on the strength of his case, Roscius was acquitted. Soon after, Cicero again challenged Sulla, by criticising his disenfranchisement of Italian towns in a lost speech on behalf of a woman from Arretium.

Cicero's case in the "Pro Roscio Amerino" was divided into three parts. The first part detailed exactly the charge brought by Ericius. Cicero explained how a rustic son of a farmer, who lives off the pleasures of his own land, would not have gained anything from committing patricide because he would have eventually inherited his father's land anyway. The second part concerned the boldness and greed of two of the accusers, Magnus and Capito. Cicero told the jury that they were the more likely perpetrators of murder because the two were greedy, both for conspiring together against a fellow kinsman and, in particular, Magnus, for his boldness and for being unashamed to appear in court to support the false charges. The third part explained that Chrysogonus had immense political power, and the accusation was successfully made due to that power. Even though Chrysogonus may not have been what Cicero said he was, through rhetoric Cicero successfully made him appear to be a foreign freed man who prospered by devious means in the aftermath of the civil war. Cicero surmised that it showed what kind of a person he was and that something like murder was not beneath him.

His first office was as one of the twenty annual quaestors, a training post for serious public administration in a diversity of areas, but with a traditional emphasis on administration and rigorous accounting of public monies under the guidance of a senior magistrate or provincial commander. Cicero served as quaestor in western Sicily in 75 BC and demonstrated honesty and integrity in his dealings with the inhabitants. As a result, the grateful Sicilians asked Cicero to prosecute Gaius Verres, a governor of Sicily, who had badly plundered the province. His prosecution of Gaius Verres was a great forensic success for Cicero. Governor Gaius Verres hired the prominent lawyer of a noble family Quintus Hortensius Hortalus. After a lengthy period in Sicily collecting testimonials and evidence and persuading witnesses to come forward, Cicero returned to Rome and won the case in a series of dramatic court battles. His unique style of oratory set him apart from the flamboyant Hortensius. On the conclusion of this case, Cicero came to be considered the greatest orator in Rome. The view that Cicero may have taken the case for reasons of his own is viable. Hortensius was, at this point, known as the best lawyer in Rome; to beat him would guarantee much success and the prestige that Cicero needed to start his career. Cicero's oratorical skill is shown in his character assassination of Verres and various other techniques of persuasion used on the jury. One such example is found in the speech "Against Verres I", where he states "with you on this bench, gentlemen, with Marcus Acilius Glabrio as your president, I do not understand what Verres can hope to achieve". Oratory was considered a great art in ancient Rome and an important tool for disseminating knowledge and promoting oneself in elections, in part because there were no regular newspapers or mass media. Cicero was neither a patrician nor a plebeian noble; his rise to political office despite his relatively humble origins has traditionally been attributed to his brilliance as an orator.

Cicero grew up in a time of civil unrest and war. Sulla's victory in the first of a series of civil wars led to a new constitutional framework that undermined "libertas" (liberty), the fundamental value of the Roman Republic. Nonetheless, Sulla's reforms strengthened the position of the equestrian class, contributing to that class's growing political power. Cicero was both an Italian "eques" and a "novus homo", but more importantly he was a Roman constitutionalist. His social class and loyalty to the Republic ensured that he would "command the support and confidence of the people as well as the Italian middle classes". The "optimates" faction never truly accepted Cicero; and this undermined his efforts to reform the Republic while preserving the constitution. Nevertheless, he successfully ascended the "cursus honorum", holding each magistracy at or near the youngest possible age: quaestor in 75 BC (age 31), aedile in 69 BC (age 37), and praetor in 66 BC (age 40), when he served as president of the "Reclamation" (or extortion) Court. He was then elected consul at age 43.

Cicero was elected consul for the year 63 BC. His co-consul for the year, Gaius Antonius Hybrida, played a minor role. During his year in office, he thwarted a conspiracy centered on assassinating him and overthrowing the Roman Republic with the help of foreign armed forces, led by Lucius Sergius Catilina. Cicero procured a "senatus consultum ultimum" (a declaration of martial law) and drove Catiline from the city with four vehement speeches (the Catiline Orations), which to this day remain outstanding examples of his rhetorical style. The Orations listed Catiline and his followers' debaucheries, and denounced Catiline's senatorial sympathizers as roguish and dissolute debtors clinging to Catiline as a final and desperate hope. Cicero demanded that Catiline and his followers leave the city. At the conclusion of his first speech, Catiline hurriedly left the Senate, (which was being held in the Temple of Jupiter Stator). In his following speeches, Cicero did not directly address Catiline. He delivered the second and third orations before the people, and the last one again before the Senate. By these speeches, Cicero wanted to prepare the Senate for the worst possible case; he also delivered more evidence, against Catiline.

Catiline fled and left behind his followers to start the revolution from within while Catiline assaulted the city with an army of "moral bankrupts and honest fanatics". Catiline had attempted to involve the Allobroges, a tribe of Transalpine Gaul, in their plot, but Cicero, working with the Gauls, was able to seize letters that incriminated the five conspirators and forced them to confess in front of the Senate.

The Senate then deliberated upon the conspirators' punishment. As it was the dominant advisory body to the various legislative assemblies rather than a judicial body, there were limits to its power; however, martial law was in effect, and it was feared that simple house arrest or exile – the standard options – would not remove the threat to the state. At first Decimus Silanus spoke for the "extreme penalty"; many were swayed by Julius Caesar, who decried the precedent it would set and argued in favor of life imprisonment in various Italian towns. Cato the Younger rose in defense of the death penalty and the entire Senate finally agreed on the matter. Cicero had the conspirators taken to the Tullianum, the notorious Roman prison, where they were strangled. Cicero himself accompanied the former consul Publius Cornelius Lentulus Sura, one of the conspirators, to the Tullianum. Cicero received the honorific ""Pater Patriae"" for his efforts to suppress the conspiracy, but lived thereafter in fear of trial or exile for having put Roman citizens to death without trial.

After the conspirators were put to death, Cicero was proud of his accomplishment. Some of his political enemies argued that though the act gained Cicero popularity, he exaggerated the extent of his success. He overestimated his popularity again several years later after being exiled from Italy and then allowed back from exile. At this time, he claimed that the Republic would be restored along with him.

Many Romans at the time, led by Populares politicians Gaius Julius Caesar and patrician turned plebeian Publius Clodius Pulcher believed that Cicero's evidence against Catiline was fabricated and the witnesses were bribed. This was done in a concerted effort by Optimate politicians to prevent social changes in Rome, both the city and throughout the Roman Empire. Catiline had campaigned for and lost the office of Consul four times in as many consecutive years, the last being in 63 BC, the year of Cicero's consulship, for the year 62 with support from the Populares. Cicero, who had been elected Consul with the support of the Optimates, promoted their position as advocates of the status quo resisting social changes, especially more rights for the average inhabitants of Rome.

Just after completing his consulship, in late 62 BC, Cicero arranged the purchase of a large townhouse on the Palatine Hill previously owned by Rome's richest citizen, Marcus Licinius Crassus. It cost an exorbitant sum, 3.5 million sesterces, which required Cicero to arrange for a loan from Mark Antony based on the expected profits from Antony's proconsulship in Macedonia. In return Cicero gained a lavish house which he proudly boasted was ""in conspectu prope totius urbis"" (in sight of nearly the whole city), only a short walk away from the Roman Forum.

In 60 BC, Julius Caesar invited Cicero to be the fourth member of his existing partnership with Pompey and Marcus Licinius Crassus, an assembly that would eventually be called the First Triumvirate. Cicero refused the invitation because he suspected it would undermine the Republic.

In 58 BC, Publius Clodius Pulcher, then one of the tribunes of the plebs, introduced a law (the "Leges Clodiae") threatening exile to anyone who executed a Roman citizen without a trial. Cicero, having executed members of the Catiline Conspiracy four years previously without formal trial, and having had a public falling out with Clodius, was clearly the intended target of the law. Furthermore, many believed that Clodius acted in concert with Julius Caesar who feared that Cicero would seek to abolish many of Caesar's accomplishments while Consul in 59 BC, whom he did not support. Cicero argued that the "senatus consultum ultimum" indemnified him from punishment, and he attempted to gain the support of the senators and consuls, especially of Pompey. When help was not forthcoming, he went into exile. He arrived at Thessalonica, on 23 May 58 BC. In his absence, Clodius, who lived next door to Cicero on the Palatine, arranged for Cicero's house to be confiscated by the state, and was even able to purchase a part of the property in order to extend his own house. After demolishing Cicero's house, Clodius had the land consecrated and symbolically erected a Temple of Liberty ("aedes Libertatis") on the vacant spot.

Cicero's exile caused him to fall into depression. He wrote to Atticus: "Your pleas have prevented me from committing suicide. But what is there to live for? Don't blame me for complaining. My afflictions surpass any you ever heard of earlier". After the intervention of recently elected tribune Titus Annius Milo (probably working on the behalf of Pompey who wanted Cicero back to help him against Clodius), the senate voted in favor of recalling Cicero from exile. Clodius cast the single vote against the decree. Cicero returned to Italy on 5 August 57 BC, landing at Brundisium. He was greeted by a cheering crowd, and, to his delight, his beloved daughter Tullia. In his "Oratio De Domo Sua Ad Pontifices", Cicero convinced the College of Pontiffs to rule that the consecration of his land was invalid, thereby allowing him to regain his property and rebuild his house on the Palatine.

Cicero tried to re-enter politics, but his attack on a bill of Caesar's proved unsuccessful. The conference at Luca in 56 BC forced Cicero to recant and support the triumvirate. After this, a cowed Cicero concentrated on his literary works. It is uncertain whether he was directly involved in politics for the following few years.

In 51 BC he reluctantly accepted a promagistracy (as proconsul) in Cilicia for the year, because there were few other eligible governors available as a result of a legislative requirement enacted by Pompey in 52 BC, specifying an interval of five years between a consulship or praetorship and a provincial command. He served as proconsul of Cilicia from May 51 to November 50 BC. He was given instructions to keep nearby Cappadocia loyal to the King, Ariobarzanes III, which he achieved 'satisfactorily without war.' In 53 BC Marcus Licinius Crassus had been defeated by the Parthians at the battle of Carrhae, this opened up the Roman East for a Parthian invasion causing much unrest in Syria and Cilicia. Cicero restored calm by his mild system of government. He discovered that much of public property had been embezzled by corrupt previous governors and their staffs, and did his utmost to restore it. Thus he greatly improved the condition of the cities. He retained the civil rights of, and exempted from penalties, the men who gave the property back. Besides this, he was extremely frugal in his outlays for staff and private expenses during his governorship, and this made him highly popular among the natives. Previous governors had extorted enormous sums from the provincials in order to supply their households and bodyguard. Besides his activity in ameliorating the hard pecuniary situation of the province, Cicero was also creditably active in the military sphere. Early in his governorship he received information that prince Pacorus, son of Orodes II the king of the Parthians, had crossed the Euphrates, and was ravaging the Syrian countryside and even besieged Cassius (the interim Roman commander in Syria) in Antioch. Cicero eventually marched with two understrength legions and a large contingent of auxiliary cavalry to Cassius' relief. Pacorus and his army had already given up on besieging Antioch and were heading south through Syria, ravaging the countryside again, Cassius and his legions followed them, harrying them wherever they went, eventually ambushing and defeating them near Antigonea. Another large troop of Parthian horsemen was defeated by Cicero's cavalry who happened to run into them while scouting ahead of the main army. Cicero next defeated some robbers who were based on Mount Amanus and was hailed as imperator by his troops. Afterwards he led his army against the independent Cilician mountain tribes, besieging their fortress of Pindenissum. It took him 47 days to reduce the place, which fell in December. Then Cicero left the province to his brother Quintus, who had accompanied him on his governorship as his legate. On his way back to Rome he stopped over in Rhodes and then went to Athens, where he caught up with his old friend Titus Pomponius Atticus and met men of great learning.

The struggle between Pompey and Julius Caesar grew more intense in 50 BC. Cicero favored Pompey, seeing him as a defender of the senate and Republican tradition, but at that time avoided openly alienating Caesar. When Caesar invaded Italy in 49 BC, Cicero fled Rome. Caesar, seeking an endorsement by a senior senator, courted Cicero's favor, but even so Cicero slipped out of Italy and traveled to Dyrrachium (Epidamnos), Illyria, where Pompey's staff was situated. Cicero traveled with the Pompeian forces to Pharsalus in 48 BC, though he was quickly losing faith in the competence and righteousness of the Pompeian side. Eventually, he provoked the hostility of his fellow senator Cato, who told him that he would have been of more use to the cause of the "optimates" if he had stayed in Rome. After Caesar's victory at the Battle of Pharsalus on 9 August, Cicero returned to Rome only very cautiously. Caesar pardoned him and Cicero tried to adjust to the situation and maintain his political work, hoping that Caesar might revive the Republic and its institutions.

In a letter to Varro on c. 20 April 46 BC, Cicero outlined his strategy under Caesar's dictatorship. Cicero, however, was taken completely by surprise when the "Liberatores" assassinated Caesar on the ides of March, 44 BC. Cicero was not included in the conspiracy, even though the conspirators were sure of his sympathy. Marcus Junius Brutus called out Cicero's name, asking him to restore the republic when he lifted his bloodstained dagger after the assassination. A letter Cicero wrote in February 43 BC to Trebonius, one of the conspirators, began, "How I could wish that you had invited me to that most glorious banquet on the Ides of March"! Cicero became a popular leader during the period of instability following the assassination. He had no respect for Mark Antony, who was scheming to take revenge upon Caesar's murderers. In exchange for amnesty for the assassins, he arranged for the Senate to agree not to declare Caesar to have been a tyrant, which allowed the Caesarians to have lawful support and kept Caesar's reforms and policies intact.

Cicero and Antony now became the two leading men in Rome: Cicero as spokesman for the Senate; Antony as consul, leader of the Caesarian faction, and unofficial executor of Caesar's public will. Relations between the two, never friendly, worsened after Cicero claimed that Antony was taking liberties in interpreting Caesar's wishes and intentions. Octavian was Caesar's adopted son and heir. After he returned to Italy, Cicero began to play him against Antony. He praised Octavian, declaring he would not make the same mistakes as his father. He attacked Antony in a series of speeches he called the Philippics, after Demosthenes's denunciations of Philip II of Macedon. At the time Cicero's popularity as a public figure was unrivalled.

Cicero supported Decimus Junius Brutus Albinus as governor of Cisalpine Gaul ("Gallia Cisalpina") and urged the Senate to name Antony an enemy of the state. The speech of Lucius Piso, Caesar's father-in-law, delayed proceedings against Antony. Antony was later declared an enemy of the state when he refused to lift the siege of Mutina, which was in the hands of Decimus Brutus. Cicero's plan to drive out Antony failed. Antony and Octavian reconciled and allied with Lepidus to form the Second Triumvirate after the successive battles of Forum Gallorum and Mutina. The Triumvirate began proscribing their enemies and potential rivals immediately after legislating the alliance into official existence for a term of five years with consular "imperium". Cicero and all of his contacts and supporters were numbered among the enemies of the state, even though Octavian argued for two days against Cicero being added to the list.

Cicero was one of the most viciously and doggedly hunted among the proscribed. He was viewed with sympathy by a large segment of the public and many people refused to report that they had seen him. He was caught 7 December 43 BC leaving his villa in Formiae in a litter going to the seaside where he hoped to embark on a ship destined for Macedonia. When his killers – Herennius (a centurion) and Popilius (a tribune) – arrived, Cicero's own slaves said they had not seen him, but he was given away by Philologus, a freedman of his brother Quintus Cicero.

Cicero's last words are said to have been, "There is nothing proper about what you are doing, soldier, but do try to kill me properly." He bowed to his captors, leaning his head out of the litter in a gladiatorial gesture to ease the task. By baring his neck and throat to the soldiers, he was indicating that he would not resist. According to Plutarch, Herennius first slew him, then cut off his head. On Antony's instructions his hands, which had penned the Philippics against Antony, were cut off as well; these were nailed along with his head on the Rostra in the Forum Romanum according to the tradition of Marius and Sulla, both of whom had displayed the heads of their enemies in the Forum. Cicero was the only victim of the proscriptions who was displayed in that manner. According to Cassius Dio (in a story often mistakenly attributed to Plutarch), Antony's wife Fulvia took Cicero's head, pulled out his tongue, and jabbed it repeatedly with her hairpin in final revenge against Cicero's power of speech.

Cicero's son, Marcus Tullius Cicero Minor, during his year as a consul in 30 BC, avenged his father's death, to a certain extent, when he announced to the Senate Mark Antony's naval defeat at Actium in 31 BC by Octavian and his capable commander-in-chief, Agrippa.

Octavian is reported to have praised Cicero as a patriot and a scholar of meaning in later times, within the circle of his family. However, it was Octavian's acquiescence that had allowed Cicero to be killed, as Cicero was proscribed by the new triumvirate.

Cicero's career as a statesman was marked by inconsistencies and a tendency to shift his position in response to changes in the political climate. His indecision may be attributed to his sensitive and impressionable personality; he was prone to overreaction in the face of political and private change.
"Would that he had been able to endure prosperity with greater self-control, and adversity with more fortitude!" wrote C. Asinius Pollio, a contemporary Roman statesman and historian.

Cicero has been traditionally considered the master of Latin prose, with Quintilian declaring that Cicero was "not the name of a man, but of eloquence itself." The English words "Ciceronian" (meaning "eloquent") and "cicerone" (meaning "local guide") derive from his name. He is credited with transforming Latin from a modest utilitarian language into a versatile literary medium capable of expressing abstract and complicated thoughts with clarity. Julius Caesar praised Cicero's achievement by saying "it is more important to have greatly extended the frontiers of the Roman spirit ("ingenium") than the frontiers of the Roman empire". According to John William Mackail, "Cicero's unique and imperishable glory is that he created the language of the civilized world, and used that language to create a style which nineteen centuries have not replaced, and in some respects have hardly altered."

Cicero was also an energetic writer with an interest in a wide variety of subjects, in keeping with the Hellenistic philosophical and rhetorical traditions in which he was trained. The quality and ready accessibility of Ciceronian texts favored very wide distribution and inclusion in teaching curricula, as suggested by a graffito at Pompeii, admonishing: "You will like Cicero, or you will be whipped".
Cicero was greatly admired by influential Church Fathers such as Augustine of Hippo, who credited Cicero's lost "Hortensius" for his eventual conversion to Christianity, and St. Jerome, who had a feverish vision in which he was accused of being "follower of Cicero and not of Christ" before the judgment seat.
This influence further increased after the Early Middle Ages in Europe, which more of his writings survived than any other Latin author. Medieval philosophers were influenced by Cicero's writings on natural law and innate rights.

Petrarch's rediscovery of Cicero's letters provided the impetus for searches for ancient Greek and Latin writings scattered throughout European monasteries, and the subsequent rediscovery of classical antiquity led to the Renaissance. Subsequently, Cicero became synonymous with classical Latin to such an extent that a number of humanist scholars began to assert that no Latin word or phrase should be used unless it appeared in Cicero's works, a stance criticized by Erasmus.

His voluminous correspondence, much of it addressed to his friend Atticus, has been especially influential, introducing the art of refined letter writing to European culture. Cornelius Nepos, the 1st century BC biographer of Atticus, remarked that Cicero's letters contained such a wealth of detail "concerning the inclinations of leading men, the faults of the generals, and the revolutions in the government" that their reader had little need for a history of the period.

Among Cicero's admirers were Desiderius Erasmus, Martin Luther, and John Locke. Following the invention of Johannes Gutenberg's printing press, "De Officiis" was the second book printed in Europe, after the Gutenberg Bible. Scholars note Cicero's influence on the rebirth of religious toleration in the 17th century. 

Cicero was especially popular with the Philosophes of the 18th century, including Edward Gibbon, Diderot, David Hume, Montesquieu, and Voltaire. Gibbon wrote of his first experience reading the author's collective works thus: "I tasted the beauty of the language; I breathed the spirit of freedom; and I imbibed from his precepts and examples the public and private sense of a man...after finishing the great author, a library of eloquence and reason, I formed a more extensive plan of reviewing the Latin classics..." Voltaire called Cicero "the greatest as well as the most elegant of Roman philosophers" and even staged a play based on Cicero's role in the Catilinarian conspiracy, called "Rome Sauvée, ou Catilina", to "make young people who go to the theatre acquainted with Cicero." Voltaire was spurred to pen the drama as a rebuff to his rival Claude Prosper Jolyot de Crébillon's own play "Catilina", which had portrayed Cicero as a coward and villain who hypocritically married his own daughter to Catiline. Montesquieu produced his "Discourse on Cicero" in 1717, in which he heaped praise on the author because he rescued "philosophy from the hands of scholars, and freed it from the confusion of a foreign language". Montesquieu went on to declare that Cicero was "of all the ancients, the one who had the most personal merit, and whom I would prefer to resemble."

Across the Atlantic, Cicero the republican inspired the Founding Fathers of the United States and the revolutionaries of the French Revolution. John Adams said, "As all the ages of the world have not produced a greater statesman and philosopher united than Cicero, his authority should have great weight." Jefferson names Cicero as one of a handful of major figures who contributed to a tradition "of public right" that informed his draft of the Declaration of Independence and shaped American understandings of "the common sense" basis for the right of revolution. Camille Desmoulins said of the French republicans in 1789 that they were "mostly young people who, nourished by the reading of Cicero at school, had become passionate enthusiasts for liberty".

Jim Powell starts his book on the history of liberty with the sentence: "Marcus Tullius Cicero expressed principles that became the bedrock of liberty in the modern world."

Likewise, no other ancient personality has inspired as much venomous dislike as Cicero, especially in more modern times. His commitment to the values of the Republic accommodated a hatred of the poor and persistent opposition to the advocates and mechanisms of popular representation. Friedrich Engels referred to him as "the most contemptible scoundrel in history" for upholding republican "democracy" while at the same time denouncing land and class reforms. Cicero has faced criticism for exaggerating the democratic qualities of republican Rome, and for defending the Roman oligarchy against the popular reforms of Caesar. Michael Parenti admits Cicero's abilities as an orator, but finds him a vain, pompous and hypocritical personality who, when it suited him, could show public support for popular causes that he privately despised. Parenti presents Cicero's prosecution of the Catiline conspiracy as legally flawed at least, and possibly unlawful.

Cicero also had an influence on modern astronomy. Nicolaus Copernicus, searching for ancient views on earth motion, said that he "first ... found in Cicero that Hicetas supposed the earth to move."

Cicero was declared a righteous pagan by the Early Church, and therefore many of his works were deemed worthy of preservation. The Bogomils considered him a rare exception of a pagan saint. Subsequent Roman and medieval Christian writers quoted liberally from his works "De Re Publica" ("On the Commonwealth") and "De Legibus" ("On the Laws"), and much of his work has been recreated from these surviving fragments. Cicero also articulated an early, abstract conceptualization of rights, based on ancient law and custom. Of Cicero's books, six on rhetoric have survived, as well as parts of eight on philosophy. Of his speeches, 88 were recorded, but only 58 survive.

Cicero's great repute in Italy has led to numerous ruins being identified as having belonged to him, though none have been substantiated with absolute certainty. In Formia, two Roman-era ruins are popularly believed to be Cicero's mausoleum, the "Tomba di Cicerone", and the villa where he was assassinated in 43 BC. The latter building is centered around a central hall with Doric columns and a coffered vault, with a separate nymphaeum, on five acres of land near Formia. A modern villa was built on the site after the Rubino family purchased the land from Ferdinand II of the Two Sicilies in 1868. Cicero's supposed tomb is a 24 meter (79 feet) tall tower on an "opus quadratum" base on the ancient Via Appia outside of Formia. Some suggest that it is not in fact Cicero's tomb, but a monument built on the spot where Cicero was intercepted and assassinated while trying to reach the sea.

In Pompeii, a large villa excavated in the mid 18th century just outside the Herculaneum Gate was widely believed to have been Cicero's, who was known to have owned a holiday villa in Pompeii he called his "Pompeianum". The villa was stripped of its fine frescos and mosaics and then re-buried after 1763 – it has yet to be re-excavated. However contemporaneous descriptions of the ruins combined with Cicero's own references to his "Pompeianum" differ, making it unlikely that it is Cicero's villa.

In Rome, the location of Cicero's house has been roughly identified from excavations of the Republican-era stratum on the northwestern slope of the Palatine Hill. Cicero's "domus" has long been known to have stood in the area, according to his own descriptions and those of later authors, but there is some debate about whether it stood near the base of the hill, very close to the Roman Forum, or nearer to the summit. During his life the area was the most desirable in Rome, densely occupied with Patrician houses including the "Domus Publica" of Julius Caesar and the home of Cicero's mortal enemy Clodius. In the early Imperial era these properties fell into the possession of the Julio-Claudian dynasty and the substructures of the "Domus Tiberiana" were built over the Republican-era buildings.

Ben Jonson dramatised the conspiracy of Catiline in his play "Catiline His Conspiracy", featuring Cicero as a character. Cicero also appears as a minor character in William Shakespeare's play "Julius Caesar".

Cicero was portrayed on the motion picture screen by British actor Alan Napier in the 1953 film "Julius Caesar", based on Shakespeare's play. He has also been played by such noted actors as Michael Hordern (in "Cleopatra"), and André Morell (in the 1970 "Julius Caesar"). Most recently, Cicero was portrayed by David Bamber in the HBO series "Rome" (2005–2007) and appeared in both seasons.

In the historical novel series "Masters of Rome", Colleen McCullough presents a not-so-flattering depiction of Cicero's career, showing him struggling with an inferiority complex and vanity, morally flexible and fatally indiscreet, while his rival Julius Caesar is shown in a more approving light. Cicero is portrayed as a hero in the novel "A Pillar of Iron" by Taylor Caldwell (1965). Robert Harris' novels "Imperium", "Lustrum" (published under the name "Conspirata" in the United States) and "Dictator" comprise a three-part series based on the life of Cicero. In these novels Cicero's character is depicted in a more balanced way than in those of McCullough, with his positive traits equaling or outweighing his weaknesses (while conversely Caesar is depicted as more sinister than in McCullough). Cicero is a major recurring character in the "Roma Sub Rosa" series of mystery novels by Steven Saylor. He also appears several times as a peripheral character in John Maddox Roberts' "SPQR" series.

Samuel Barnett portrays Cicero in a 2018 audio drama series produced by Big Finish Productions 





</doc>
<doc id="6047" url="https://en.wikipedia.org/wiki?curid=6047" title="Consul">
Consul

Consul (abbrev. "cos."; Latin plural "consules") was the title of one of the two chief magistrates of the Roman Republic, and subsequently also an important title under the Roman Empire. The title was used in other European city states through antiquity and the Middle Ages, then revived in modern states, notably in the First French Republic. The related adjective is consular, from the Latin "consularis".

This usage contrasts with modern terminology, where a consul is a type of diplomat.

A consul held the highest elected political office of the Roman Republic (509 to 27 BC), and ancient Romans considered the consulship the highest level of the cursus honorum (an ascending sequence of public offices to which politicians aspired). Consuls were elected to office and held power for one year. There were always two consuls in power at any time.


It was not uncommon for an organization under Roman private law to copy the terminology of state and city institutions for its own statutory agents. The founding statute, or contract, of such an organisation was called "lex", 'law'. The people elected each year were patricians, members of the upper class.

While many cities (as in Gaul) had a double-headed chief magistracy, often another title was used, such as "Duumvir" or native styles such as "Meddix", but "consul" was used in some.

Throughout most of southern France, a consul ( or "") was an office equivalent to the of the north and roughly similar with English aldermen. The most prominent were those of Bordeaux and Toulouse, which came to be known as jurats and capitouls, respectively. The capitouls of Toulouse were granted transmittable nobility. In many other smaller towns the first consul, was the equivalent of a mayor today, assisted by a variable number of secondary consuls and jurats. His main task was to levy and collect tax.

The Dukes of Gaeta often used also the title of "consul" in its Greek form "Hypatos" (see List of Hypati and Dukes of Gaeta).

The city-state of Genoa, unlike ancient Rome, bestowed the title of "consul" on various state officials, not necessarily restricted to the highest. Among these were Genoese officials stationed in various Mediterranean ports, whose role included helping Genoese merchants and sailors in difficulties with the local authorities. This institution, with its name, was later emulated by other powers and is reflected in the modern usage of the word (see Consul (representative)).

After Napoleon Bonaparte staged a coup against the Directory government in November 1799, the French Republic adopted a constitution which conferred executive powers upon three consuls, elected for a period of ten years. In reality, the first consul, Bonaparte, dominated his two colleagues and held supreme power, soon making himself consul for life (1802) and eventually, in 1804, emperor.

The office was held by:


The short-lived Bolognese Republic, proclaimed in 1796 as a French client republic in the Central Italian city of Bologna, had a government consisting of nine consuls and its head of state was the "Presidente del Magistrato", i.e., chief magistrate, a presiding office held for four months by one of the consuls. Bologna already had consuls at some parts of its Medieval history.

The French-sponsored Roman Republic (15 February 1798 – 23 June 1800) was headed by multiple consuls:

Consular rule was interrupted by the Neapolitan occupation (27 November – 12 December 1798), which installed a Provisional Government:

Rome was occupied by France (11 July – 28 September 1799) and again by Naples (30 September 1799 – 23 June 1800), bringing an end to the Roman Republic.

Among the many petty local republics that were formed during the first year of the Greek Revolution, prior to the creation of a unified Provisional Government at the First National Assembly at Epidaurus, were:
"Note: in Greek, the term for "consul" is "hypatos" (ὕπατος), which translates as "supreme one", and hence does not necessarily imply a joint office."

In between a series of juntas and various other short-lived regimes, the young republic was governed by "consuls of the republic", with two consuls alternating in power every 4 months:

After a few presidents of the Provisional Junta, there were again consuls of the republic, 14 March 1841 – 13 March 1844 (ruling jointly, but occasionally styled "first consul", "second consul"): Carlos Antonio López Ynsfrán (b. 1792 – d. 1862) + Mariano Roque Alonzo Romero (d. 1853) (the lasts of the aforementioned juntistas, Commandant-General of the Army)
Thereafter all republican rulers were styled "president".

In modern terminology, a consul is a type of diplomat. The "American Heritage Dictionary" defines consul as "an official appointed by a government to reside in a foreign country and represent its interests there."

In most governments, the consul is the head of the consular section of an embassy, and is responsible for all consular services such as immigrant and non-immigrant visas, passports, and citizen services for expatriates living or traveling in the host country.
A less common modern usage is when the consul of one country takes a governing role in the host country.

Differently named, but same function

Modern UN System 



</doc>
<doc id="6050" url="https://en.wikipedia.org/wiki?curid=6050" title="List of equations in classical mechanics">
List of equations in classical mechanics

Classical mechanics is the branch of physics used to describe the motion of macroscopic objects. It is the most familiar of the theories of physics. The concepts it covers, such as mass, acceleration, and force, are commonly used and known. The subject is based upon a three-dimensional Euclidean space with fixed axes, called a frame of reference. The point of concurrency of the three axes is known as the origin of the particular space.

Classical mechanics utilises many equations—as well as other mathematical concepts—which relate various physical quantities to one another. These include differential equations, manifolds, Lie groups, and ergodic theory. This page gives a summary of the most important of these.

This article lists equations from Newtonian mechanics, see analytical mechanics for the more general formulation of classical mechanics (which includes Lagrangian and Hamiltonian mechanics).

Every conservative force has a potential energy. By following two principles one can consistently assign a non-relative value to "U":


In the following rotational definitions, the angle can be any angle about the specified axis of rotation. It is customary to use "θ", but this does not have to be the polar angle used in polar coordinate systems. The unit axial vector

defines the axis of rotation, formula_2 = unit vector in direction of r, formula_3 = unit vector tangential to the angle.

!
! Translation
! Rotation
!Velocity
Instantaneous:

Instantaneous:

Instantaneous:


</doc>
<doc id="6051" url="https://en.wikipedia.org/wiki?curid=6051" title="Cursus honorum">
Cursus honorum

The cursus honorum (Latin: lit. "course of honor", or more colloquially "ladder of offices") was the sequential order of public offices held by aspiring politicians in both the Roman Republic and the early Roman Empire. It was designed for men of senatorial rank. The "cursus honorum" comprised a mixture of military and political administration posts. Each office had a minimum age for election, and Lex Villia Annalis made those requirements formal. There were minimum intervals between holding successive offices and laws forbade repeating an office.

These rules were altered and flagrantly ignored in the course of the last century of the Republic. For example, Gaius Marius held consulships for five years in a row between 104 BC and 100 BC. He was consul seven times in all, also serving in 107 and 86. Officially presented as opportunities for public service, the offices often became mere opportunities for self-aggrandizement. The reforms of Sulla required a ten-year interval before holding the same office again for another term.

To have held each office at the youngest possible age ("suo anno", "in his own year") was considered a great political success. For instance, to miss out on a praetorship at 39 meant that one could not become consul at 42. Cicero expressed extreme pride not only in being a "novus homo" ("new man"; comparable to a "self-made man") who became consul even though none of his ancestors had ever served as a consul, but also in having become consul "in his year".

The "cursus honorum" began with ten years of military duty in the Roman cavalry (the "equites") or in the staff of a general who was a relative or a friend of the family. The ten years of service were intended to be mandatory in order to qualify for political office, but in practice, the rule was not always rigidly applied.

A more prestigious position was that of a military tribune. In the early Roman Republic, 24 men at the age of around 20 were elected by the Tribal Assembly to serve as commanders on a rotating basis; each commander retained six tribunes on his staff. Tribunes could also be appointed by the consuls or by military commanders in the field as necessary. After the reforms of Gaius Marius in 107 BC, the six tribunes acted as staff officers for the legionary "legatus" and were appointed tasks and command of units of troops whenever the need arose.

The subsequent steps of the "cursus honorum" were achieved by direct election every year.

The first official post was that of "quaestor". Candidates had to be at least 30 years old. However, men of patrician rank could subtract two years from this and other minimum age requirements.

Twenty quaestors served in the financial administration at Rome or as second-in-command to a governor in the provinces. They could also serve as the paymaster for a legion. A young man who obtained this job was expected to become a very important official. An additional task of all quaestors was the supervision of public games. As a quaestor, an official was allowed to wear the toga praetexta, but was not escorted by lictors, nor did he possess imperium.

At 36 years of age, proquaestor could stand for election to one of the "aedile" positions. Of these aediles, two were plebeian and two were patrician, with the patrician aediles called Curule Aediles. The plebeian aediles were elected by the Plebeian Council and the curule aediles were either elected by the Tribal Assembly or appointed by the reigning consul. The aediles had administrative responsibilities in Rome. They had to take care of the temples (whence their title, from the Latin "aedes", "temple"), organize games, and be responsible for the maintenance of the public buildings in Rome. Moreover, they took charge of Rome's water and food supplies; in their capacity as market superintendents, they served sometimes as judges in mercantile affairs.

The Aedile was the supervisor of public works; the words "edifice" and "edification" stem from the same root. He oversaw the public works, temples and markets. Therefore, the Aediles would have been in some cooperation with the current Censors, who had similar or related duties. Also they oversaw the organization of festivals and games ("ludi"), which made this a very sought-after office for a career minded politician of the late republic, as it was a good means of gaining popularity by staging spectacles.

Curule Aediles were added at a later date in the 4th century BC, and their duties do not differ substantially from plebeian aediles. However, unlike plebeian aediles, curule aediles were allowed certain symbols of rank—the "sella curulis" or 'curule chair,' for example—and only patricians could stand for election to curule aedile. This later changed, and both Plebeians and Patricians could stand for Curule Aedileship.

The elections for Curule Aedile were at first alternated between Patricians and Plebeians, until late in the 2nd century BC, when the practice was abandoned and both classes became free to run during all years.

While part of the "cursus honorum", this step was optional and not required to hold future offices. Though the office was usually held after the quaestorship and before the praetorship, there are some cases with former praetors serving as aediles.

After serving either as quaestor or as aedile, a man of 39 years could run for "praetor". The number of praetors elected varied through history, generally increasing with time. During the republic, six or eight were generally elected each year to serve judicial functions throughout Rome and other governmental responsibilities. In the absence of the consuls, a praetor would be given command of the garrison in Rome or in Italy. Also, a praetor could exercise the functions of the consuls throughout Rome, but their main function was that of a judge. They would preside over trials involving criminal acts, grant court orders and validate "illegal" acts as acts of administering justice. A praetor was escorted by six lictors, and wielded "imperium". After a term as praetor, the magistrate would serve as a provincial governor with the title of propraetor, wielding propraetor imperium, commanding the province's legions, and possessing ultimate authority within his province(s).

Two of the praetors were more prestigious than the others. The first was the Praetor Peregrinus, who was the chief judge in trials involving one or more foreigners. The other was the Praetor Urbanus, the chief judicial office in Rome. He had the power to overturn any verdict by any other courts, and served as judge in cases involving criminal charges against provincial governors. The Praetor Urbanus was not allowed to leave the city for more than ten days. If one of these two Praetors was absent from Rome, the other would perform the duties of both.

The office of "consul" was the most prestigious of all, and represented the summit of a successful career. The minimum age was 42 for plebeians and 40 for patricians. Years were identified by the names of the two consuls elected for a particular year; for instance, "M. Messalla et M. Pisone consulibus", "in the consulship of Messalla and Piso," dates an event to 61 BC. Consuls were responsible for the city's political agenda, commanded large-scale armies and controlled important provinces. The consuls served for only a year (a restriction intended to limit the amassing of power by individuals) and could only rule when they agreed, because each consul could veto the other's decision.

The consuls would alternate monthly as the chairman of the Senate. They also were the supreme commanders in the Roman army, with each being granted two legions during their consular year. Consuls also exercised the highest juridical power in the Republic, being the only office with the power to override the decisions of the Praetor Urbanus. Only laws and the decrees of the Senate or the People's assembly limited their powers, and only the veto of a fellow consul or a tribune of the plebs could supersede their decisions.

A consul was escorted by twelve lictors, held imperium and wore the toga praetexta. Because the consul was the highest executive office within the Republic, they had the power to veto any action or proposal by any other magistrate, save that of the Tribune of the Plebs. After a consulship, a consul was assigned one of the more important provinces and acted as the governor in the same way that a Propraetor did, only owning Proconsular imperium. A second consulship could only be attempted after an interval of 10 years to prevent one man holding too much power.

Although not part of the Cursus Honorum, upon completing a term as either Praetor or Consul, an officer was required to serve a term as Propraetor and Proconsul, respectively, in one of Rome's many provinces. These Propraetors and Proconsuls held near autocratic authority within their selected province or provinces. Because each governor held equal imperium to the equivalent magistrate, they were escorted by the same number of lictors (12) and could only be vetoed by a reigning Consul or Praetor. Their abilities to govern were only limited by the decrees of the Senate or the people's assemblies, and the Tribune of the Plebs was unable to veto their acts as long as the governor remained at least a mile outside of Rome.

After a term as consul, the final step in the Cursus Honorum was the office of "censor". This was the only office in the Roman Republic whose term was a period of eighteen months instead of the usual twelve. Censors were elected every five years and although the office held no military imperium, it was considered a great honour. The censors took a regular census of the people and then apportioned the citizens into voting classes on the basis of income and tribal affiliation. The censors enrolled new citizens in tribes and voting classes as well. The censors were also in charge of the membership roll of the Senate, every five years adding new senators who had been elected to the requisite offices. Censors could also remove unworthy members from the Senate. This ability was lost during the dictatorship of Sulla. Censors were also responsible for construction of public buildings and the moral status of the city.

Censors also had financial duties, in that they had to put out to tender projects that were to be financed by the state. Also, the censors were in charge of the leasing out of conquered land for public use and auction. Though this office owned no imperium, meaning no lictors for protection, they were allowed to wear the "toga praetexta".

The office of Tribune of the Plebs was an important step in the political career of plebeians. Patricians could not hold the office. The Tribune was an office first created to protect the right of the common man in Roman politics and served as the head of the Plebeian Council. In the mid-to-late Republic, however, plebeians were often just as, and sometimes more, wealthy and powerful than patricians. Those who held the office were granted sacrosanctity (the right to be legally protected from any physical harm), the power to rescue any plebeian from the hands of a patrician magistrate, and the right to veto any act or proposal of any magistrate, including another tribune of the people and the consuls. The tribune also had the power to exercise capital punishment against any person who interfered in the performance of his duties. The tribunes could even convene a Senate meeting and lay legislation before it and arrest magistrates. Their houses had to remain open for visitors even during the night, and they were not allowed to be more than a day's journey from Rome. Due to their unique power of sacrosanctity, the Tribune had no need for lictors for protection and owned no imperium, nor could they wear the toga praetexta. For a period after Sulla's reforms, a person who had held the office of Tribune of the Plebs could no longer qualify for any other office, and the powers of the tribunes were more limited, but these restrictions were subsequently lifted.

Another office not officially a step in the "cursus honorum" was the "princeps senatus", an extremely prestigious office for a patrician. The "princeps senatus" served as the leader of the Senate and was chosen to serve a five-year term by each pair of Censors every five years. Censors could, however, confirm a "princeps senatus" for a period of another five years. The "princeps senatus" was chosen from all Patricians who had served as a Consul, with former Censors usually holding the office. The office originally granted the holder the ability to speak first at session on the topic presented by the presiding magistrate, but eventually gained the power to open and close the senate sessions, decide the agenda, decide where the session should take place, impose order and other rules of the session, meet in the name of the senate with embassies of foreign countries, and write in the name of the senate letters and dispatches. This office, like the Tribune, did not own "imperium", was not escorted by lictors, and could not wear the "toga praetexta".

Of all the offices within the Roman Republic, none granted as much power and authority as the position of "dictator", known as the Master of the People. In times of emergency, the Senate would declare that a dictator was required, and the current consuls would appoint a dictator. This was the only decision that could not be vetoed by the Tribune of the Plebs. The dictator was the sole exception to the Roman legal principles of having multiple magistrates in the same office and being legally able to be held to answer for actions in office. Essentially by definition, only one dictator could serve at a time, and no dictator could ever be held legally responsible for any action during his time in office for any reason.

The dictator was the highest magistrate in degree of imperium and was attended by twenty-four lictors (as were the former Kings of Rome). Although his term lasted only six months instead of twelve (except for the Dictatorships of Sulla and Caesar), all other magistrates reported to the dictator (except for the tribunes of the plebs - although they could not veto any of the dictator's acts), granting the dictator absolute authority in both civil and military matters throughout the Republic. The Dictator was free from the control of the Senate in all that he did, could execute anyone without a trial for any reason, and could ignore any law in the performance of his duties. The Dictator was the sole magistrate under the Republic that was truly independent in discharging his duties. All of the other offices were extensions of the Senate's executive authority and thus answerable to the Senate. Since the Dictator exercised his own authority, he did not suffer this limitation, which was the cornerstone of the office's power.

When a Dictator entered office, he appointed to serve as his second-in-command a "magister equitum", the Master of the Horse, whose office ceased to exist once the Dictator left office. The magister equitum held Praetorian imperium, was attended by six lictors, and was charged with assisting the Dictator in managing the State. When the Dictator was away from Rome, the magister equitum usually remained behind to administer the city. The magister equitum, like the Dictator, had unchallengeable authority in all civil and military affairs, with his decisions only being overturned by the Dictator himself.

The Dictatorship was definitively abolished in 44 BC after the assassination of Gaius Julius Caesar (Lex Antonia).




</doc>
<doc id="6056" url="https://en.wikipedia.org/wiki?curid=6056" title="Continental drift">
Continental drift

Continental drift is the theory that the Earth's continents have moved over geologic time relative to each other, thus appearing to have "drifted" across the ocean bed. The speculation that continents might have 'drifted' was first put forward by Abraham Ortelius in 1596. The concept was independently and more fully developed by Alfred Wegener in 1912, but his theory was rejected by many for lack of any motive mechanism. Arthur Holmes later proposed mantle convection for that mechanism. The idea of continental drift has since been subsumed by the theory of plate tectonics, which explains that the continents move by riding on plates of the Earth's lithosphere.

Abraham Ortelius , Theodor Christoph Lilienthal (1756), Alexander von Humboldt (1801 and 1845), Antonio Snider-Pellegrini , and others had noted earlier that the shapes of continents on opposite sides of the Atlantic Ocean (most notably, Africa and South America) seem to fit together. W. J. Kious described Ortelius' thoughts in this way:

In 1889, Alfred Russel Wallace remarked, "It was formerly a very general belief, even amongst geologists, that the great features of the earth's surface, no less than the smaller ones, were subject to continual mutations, and that during the course of known geological time the continents and great oceans had, again and again, changed places with each other." He quotes Charles Lyell as saying, "Continents, therefore, although permanent for whole geological epochs, shift their positions entirely in the course of ages." and claims that the first to throw doubt on this was James Dwight Dana in 1849.

In his "Manual of Geology" (1863), Dana wrote, "The continents and oceans had their general outline or form defined in earliest time. This has been proved with respect to North America from the position and distribution of the first beds of the Silurian – those of the Potsdam epoch. … and this will probably prove to the case in Primordial time with the other continents also". Dana was enormously influential in America – his "Manual of Mineralogy" is still in print in revised form – and the theory became known as "Permanence theory".

This appeared to be confirmed by the exploration of the deep sea beds conducted by the Challenger expedition, 1872-6, which showed that contrary to expectation, land debris brought down by rivers to the ocean is deposited comparatively close to the shore on what is now known as the continental shelf. This suggested that the oceans were a permanent feature of the Earth's surface, and did not change places with the continents.

Apart from the earlier speculations mentioned in the previous section, the idea that the American continents had once formed a single landmass together with Europe and Asia before assuming their present shapes and positions was speculated by several scientists before Alfred Wegener's 1912 paper.
Although Wegener's theory was formed independently and was more complete than those of his predecessors, Wegener later credited a number of past authors with similar ideas:
Franklin Coxworthy (between 1848 and 1890), Roberto Mantovani (between 1889 and 1909), William Henry Pickering (1907) and Frank Bursley Taylor (1908). In addition, Eduard Suess had proposed a supercontinent Gondwana in 1885 and the Tethys Ocean in 1893, assuming a land-bridge between the present continents submerged in the form of a geosyncline, and John Perry had written an 1895 paper proposing that the earth's interior was fluid, and disagreeing with Lord Kelvin on the age of the earth.

For example: the similarity of southern continent geological formations had led Roberto Mantovani to conjecture in 1889 and 1909 that all the continents had once been joined into a supercontinent; Wegener noted the similarity of Mantovani's and his own maps of the former positions of the southern continents. In Mantovani's conjecture, this continent broke due to volcanic activity caused by thermal expansion, and the new continents drifted away from each other because of further expansion of the rip-zones, where the oceans now lie. This led Mantovani to propose an Expanding Earth theory which has since been shown to be incorrect.

Continental drift without expansion was proposed by Frank Bursley Taylor, who suggested in 1908 (published in 1910) that the continents were moved into their present positions by a process of "continental creep". In a later paper he proposed that this occurred by their being dragged towards the equator by tidal forces during the hypothesized capture of the moon in the Cretaceous, resulting in "general crustal creep" toward the equator. Although his proposed mechanism was wrong, he was the first to realize the insight that one of the effects of continental motion would be the formation of mountains, and attributed the formation of the Himalayas to the collision between the Indian subcontinent with Asia. Wegener said that of all those theories, Taylor's, although not fully developed, had the most similarities to his own. In the mid-20th century, the theory of continental drift was referred to as the "Taylor-Wegener hypothesis", although this terminology eventually fell out of common use.

Alfred Wegener first presented his hypothesis to the German Geological Society on 6 January 1912. His hypothesis was that the continents had once formed a single landmass, called Pangaea, before breaking apart and drifting to their present locations.

Wegener was the first to use the phrase "continental drift" (1912, 1915) (in German "die Verschiebung der Kontinente" – translated into English in 1922) and formally publish the hypothesis that the continents had somehow "drifted" apart. Although he presented much evidence for continental drift, he was unable to provide a convincing explanation for the physical processes which might have caused this drift. His suggestion that the continents had been pulled apart by the centrifugal pseudoforce ("Polflucht") of the Earth's rotation or by a small component of astronomical precession was rejected, as calculations showed that the force was not sufficient. The Polflucht hypothesis was also studied by Paul Sophus Epstein in 1920 and found to be implausible.

The theory of continental drift was not accepted for many years. One problem was that a plausible driving force was missing. A second problem was that Wegener's estimate of the velocity of continental motion, 250  cm/year, was implausibly high. (The currently accepted rate for the separation of the Americas from Europe and Africa is about 2.5  cm/year). It also did not help that Wegener was not a geologist. Other geologists also believed that the evidence that Wegener had provided was not sufficient. It is now accepted that the plates carrying the continents do move across the Earth's surface, although not as fast as Wegener believed; ironically one of the chief outstanding questions is the one Wegener failed to resolve: what is the nature of the forces propelling the plates?

The British geologist Arthur Holmes championed the theory of continental drift at a time when it was deeply unfashionable. He proposed in 1931 that the Earth's mantle contained convection cells which dissipated heat produced by radioactive decay and moved the crust at the surface. His "Principles of Physical Geology", ending with a chapter on continental drift, was published in 1944.

Geological maps of the time showed huge land bridges spanning the Atlantic and Indian oceans to account for the similarities of fauna and flora and the divisions of the Asian continent in the Permian era but failing to account for glaciation in India, Australia and South Africa.

Geophysicist Jack Oliver is credited with providing seismologic evidence supporting plate tectonics which encompassed and superseded continental drift with the article "Seismology and the New Global Tectonics", published in 1968, using data collected from seismologic stations, including those he set up in the South Pacific.

It is now known that there are two kinds of crust: continental crust and oceanic crust. Continental crust is inherently lighter and its composition is different from oceanic crust, but both kinds reside above a much deeper "plastic" mantle. Oceanic crust is created at spreading centers, and this, along with subduction, drives the system of plates in a chaotic manner, resulting in continuous orogeny and areas of isostatic imbalance. The theory of plate tectonics explains all this, including the movement of the continents, better than Wegener's theory.

Hans Stille and Leopold Kober opposed the idea of continental drift and worked on a "fixist" geosyncline model with Earth contraction playing a key role in the formation of orogens. Other geologists who opposed continental drift were Bailey Willis, Charles Schuchert, Rollin Chamberlin and Walther Bucher. In 1939 an international geological conference was held in Frankfurt. This conference came to be dominated by the fixists, especially as those geologists specializing in tectonics were all fixists except Willem van der Gracht. Criticism of continental drift and mobilism was abundant at the conference not only from tectonicists but also from sedimentological (Nölke), paleontological (Nölke), mechanical (Lehmann) and oceanographic (Troll, Wüst) perspectives. Hans Cloos, the organizer of the conference, was also a fixist who together with Troll held the view that excepting the Pacific Ocean continents were not radically different from oceans in their behaviour. The mobilist theory of Émile Argand for the Alpine orogeny was criticized by Kurt Leuchs. The few drifters and mobilists at the conference appealed to biogeography (Kirsch, Wittmann), paleoclimatology (Wegener, K), paleontology (Gerth) and geodetic measurements (Wegener, K). F. Bernauer correctly equated Reykjanes in south-west Iceland with the Mid-Atlantic Ridge, arguing with this that the floor of the Atlantic Ocean was undergoing extension just like Reykjanes. Bernauer thought this extension had drifted the continents only 100–200 km apart, the approximate width of the volcanic zone in Iceland.

David Attenborough, who attended university in the second half of the 1940s, recounted an incident illustrating its lack of acceptance then: "I once asked one of my lecturers why he was not talking to us about continental drift and I was told, sneeringly, that if I could prove there was a force that could move continents, then he might think about it. The idea was moonshine, I was informed."

As late as 1953 – just five years before Carey introduced the theory of plate tectonics – the theory of continental drift was rejected by the physicist Scheidegger on the following grounds.

From the 1930s to the late 1950s, works by Vening-Meinesz, Holmes, Umbgrove, and numerous others outlined concepts that were close or nearly identical to modern plate tectonics theory. In particular, the English geologist Arthur Holmes proposed in 1920 that plate junctions might lie beneath the sea, and in 1928 that convection currents within the mantle might be the driving force. Holmes' views were particularly influential: in his bestselling textbook, "Principles of Physical Geology," he included a chapter on continental drift, proposing that Earth's mantle contained convection cells which dissipated radioactive heat and moved the crust at the surface.  Holmes' proposal resolved the phase disequilibrium objection (the underlying fluid was kept from solidifying by radioactive heating from the core).  However, scientific communication in the '30 and '40s was inhibited by the war, and the theory still required work to avoid foundering on the orogeny and isostasy objections.  Worse, the most viable forms of the theory predicted the existence of convection cell boundaries reaching deep into the earth that had yet to be observed.

In 1947, a team of scientists led by Maurice Ewing confirmed the existence of a rise in the central Atlantic Ocean, and found that the floor of the seabed beneath the sediments was chemically and physically different from continental crust.  As oceanographers continued to bathymeter the ocean basins, a system of mid-oceanic ridges was detected.  An important conclusion was that along this system, new ocean floor was being created, which led to the concept of the "Great Global Rift".

Meanwhile, scientists began recognizing odd magnetic variations across the ocean floor using devices developed during World War II to detect submarines.  Over the next decade, it became increasingly clear that the magnetization patterns were not anomalies, as had been originally supposed. In a series of papers in 1959-1963, Heezen, Dietz, Hess, Mason, Vine, Matthews, and Morley collectively realized that the magnetization of the ocean floor formed extensive, zebra-like patterns: one stripe would exhibit normal polarity and the adjoining stripes reversed polarity.  The best explanation was the "conveyor belt" or Vine–Matthews–Morley hypothesis.  New magma from deep within the Earth rises easily through these weak zones and eventually erupts along the crest of the ridges to create new oceanic crust.  The new crust is magnetized by the earth's magnetic field, which undergoes occasional reversals.  Formation of new crust then displaces the magnetized crust apart, akin to a conveyor belt — hence the name.

Without workable alternatives to explain the stripes, geophysicists were forced to conclude that Holmes had been right: ocean rifts were sites of perpetual orogeny at the boundaries of convection cells. By 1967, barely two decades after discovery of the mid-oceanic rifts, and a decade after discovery of the striping, plate tectonics had become axiomatic to modern geophysics.

In addition, Marie Tharp, in collaboration with Bruce Heezen, who initially ridiculed Tharp's observations that her maps confirmed continental drift theory, provided essential corroboration, using her skills in cartography and seismographic data, to confirm the theory. 

Evidence for the movement of continents on tectonic plates is now extensive. Similar plant and animal fossils are found around the shores of different continents, suggesting that they were once joined. The fossils of "Mesosaurus", a freshwater reptile rather like a small crocodile, found both in Brazil and South Africa, are one example; another is the discovery of fossils of the land reptile "Lystrosaurus" in rocks of the same age at locations in Africa, India, and Antarctica. There is also living evidence, with the same animals being found on two continents. Some earthworm families (such as Ocnerodrilidae, Acanthodrilidae, Octochaetidae) are found in South America and Africa.

The complementary arrangement of the facing sides of South America and Africa is obvious but a temporary coincidence. In millions of years, slab pull, ridge-push, and other forces of tectonophysics will further separate and rotate those two continents. It was that temporary feature that inspired Wegener to study what he defined as continental drift although he did not live to see his hypothesis generally accepted.

The widespread distribution of Permo-Carboniferous glacial sediments in South America, Africa, Madagascar, Arabia, India, Antarctica and Australia was one of the major pieces of evidence for the theory of continental drift. The continuity of glaciers, inferred from oriented glacial striations and deposits called tillites, suggested the existence of the supercontinent of Gondwana, which became a central element of the concept of continental drift. Striations indicated glacial flow away from the equator and toward the poles, based on continents' current positions and orientations, and supported the idea that the southern continents had previously been in dramatically different locations that were contiguous with one another.




</doc>
<doc id="6057" url="https://en.wikipedia.org/wiki?curid=6057" title="Commodores">
Commodores

Commodores is an American funk/soul band, which was at its peak in the late 1970s through the mid 1980s. The members of the group met as mostly freshmen at Tuskegee Institute (now Tuskegee University) in 1968, and signed with Motown in November 1972, having first caught the public eye opening for the Jackson 5 while on tour.

The group's most successful period was in the late 1970s and early 1980s when Lionel Richie was the co-lead singer. The band's biggest hit singles are ballads such as "Easy", "Three Times a Lady", and "Nightshift"; and funky dance hits which include "Brick House", "Fancy Dancer", "Lady (You Bring Me Up)", and "Too Hot ta Trot". In 1986, the Commodores won their first Grammy for the song "Nightshift".

The Commodores originally came together from two former student groups, the Mystics and the Jays. Richie described some members of the Mystics as "jazz buffs".
Together, a six-man band was created from which the notable individuals were Lionel Richie, Thomas McClary, and William King from the Mystics; Andre Callahan, Michael Gilbert, and Milan Williams were from the Jays. They wanted to change the name. To choose a new name, William King opened a dictionary and randomly picked a word. "We lucked out," he remarked with a laugh when telling this story to "People" magazine. "We almost became 'The Commodes.'"

The band originated while its members attended Tuskegee University in Alabama. After winning the university's annual freshman talent contest, they played at fraternity parties as well as a weekend gig at the Black Forest Inn, one of a few clubs in Tuskegee that catered to college students. They performed mostly cover tunes and some original songs with their first singer, James Ingram (not the famous solo artist). Ingram, older than the rest of the band, left to serve active duty in Vietnam, and was later replaced by Walter "Clyde" Orange, who wrote, or co-wrote, many of their hit tunes. Lionel Richie and Orange alternated as lead singers. (Orange was the lead singer on the Top 10 hits "Brick House" and "Nightshift".)

The early band was managed by Benny Ashburn, who brought them to his family's vacation lodge on Martha's Vineyard in 1971 and 1972. There, Ashburn test marketed the group by having them play at outdoor spaces such as parking lots and summer festivals. 

The Commodores made a brief appearance in the 1978 film "Thank God It's Friday". They performed the song "Too Hot ta Trot" during the dance contest; the songs "Brick House" and "Easy" were also played during the movie.

"Machine Gun", the instrumental title track from the band's debut album, became a staple at American sporting events, and is similarly featured in many films, including "Boogie Nights" and "Looking for Mr. Goodbar". It reached No. 22 on the "Billboard" Hot 100 in 1975. Another instrumental, "Cebu" (named after an island in the Philippines), later became a staple in the Quiet storm format. Three albums released in 1975 and 1976 ("Caught in the Act", "Movin' On", and "Hot on the Tracks") are considered the peak of their harder funk period. After those recordings the group started to move towards a softer sound. That move was hinted at in their 1976 Top Ten hits "Sweet Love" and "Just to Be Close to You". In 1977 the Commodores released "Easy", which became the group's biggest hit yet, reaching No. 4 in the U.S., followed by "Brick House", also top 5, both from their album "The Commodores", as was "Zoom". The group reached No. 1 in 1978 with "Three Times a Lady". In 1979 the Commodores scored another top-five ballad, "Sail On", before reaching the top of the charts once again with another ballad, "Still". In 1981 they released two top-ten hits with "Oh No" (No. 4) and their first upbeat single in almost five years, "Lady (You Bring Me Up)" (No. 8).

In 1982, Lionel Richie left to pursue a solo career. Skyler Jett replaced Richie as co-lead singer. Also in 1982, their manager Benjamin Ashburn who also managed another band Platinum Hook died of a heart attack aged 54.

Over time, several founding members left - McClary left in 1983 (shortly after Richie) to pursue a solo career and to develop a gospel music company. McClary was replaced by guitarist-vocalist Sheldon Reynolds, while LaPread left in 1986 and moved to Auckland, New Zealand. Reynolds departed for Earth, Wind & Fire in 1987, which prompted trumpeter William "WAK" King to take over primary guitar duties for live performances. Keyboardist Milan Williams exited the band in 1989 after allegedly refusing to tour South Africa.

The group also gradually abandoned its funk roots and moved into the more commercial pop arena. In 1984 former Heatwave singer James Dean "J.D." Nicholas assumed co-lead vocal duties with drummer Walter Orange. The band remained hitless until 1985 when their final Motown album, "Nightshift", produced by Dennis Lambert—all prior albums were produced by James Anthony Carmichael—delivered the title track "Nightshift" (No. 3 in the U.S.), a loving tribute to Marvin Gaye and Jackie Wilson, both of whom had died the previous year. "Nightshift" won the Commodores their first Grammy for Best R&B Performance by a Duo or Group With Vocals in 1985.

In 2010 a new version was recorded, dedicated to Michael Jackson. The Commodores were on a European tour performing at Wembley Arena, London, on June 25, 2009, when they walked off the stage after they were told that Michael Jackson had died. Initially the band thought it was a hoax. However, back in their dressing rooms they received confirmation and broke down in tears. The next night at Birmingham's NIA Arena, J.D. Nicholas added Jackson's name into the lyrics of the song, and thenceforth the Commodores have mentioned Jackson and other deceased R&B singers. Thus came the inspiration upon the one-year anniversary of Jackson's death, to re-record, with new lyrics, the hit song "Nightshift" as a tribute. 

In 1990 the Commodores formed Commodores Records and re-recorded their 20 greatest hits as "Commodores Hits Vol. I & II". They have recorded a live album "Commodores Live!" along with a DVD of the same name, and a Christmas album titled "Commodores Christmas". In 2012, the band was working on new material, some contributions written by current and former members.

The Commodores now consist of Walter "Clyde" Orange, James Dean "J.D." Nicholas, and William "WAK" King, along with their five-piece band, known as the "Mean Machine". The group continues to perform, playing at arenas, theaters, and festivals around the world.




Among multiple Grammy Award nominations, they won a Grammy for "Nightshift" in 1986. In 2003, they were inducted into the Vocal Group Hall of Fame.



</doc>
<doc id="6058" url="https://en.wikipedia.org/wiki?curid=6058" title="Collagen">
Collagen

Collagen is the main structural protein in the extracellular space in the various connective tissues in the body. As the main component of connective tissue, it is the most abundant protein in mammals, making 25% to 35% of the whole-body protein content. Collagen consists of amino acids wound together to form triple-helices of elongated fibrils. It is mostly found in fibrous tissues such as tendons, ligaments, and skin.

Depending upon the degree of mineralization, collagen tissues may be rigid (bone), compliant (tendon), or have a gradient from rigid to compliant (cartilage). It is also abundant in corneas, blood vessels, the gut, intervertebral discs, and the dentin in teeth. In muscle tissue, it serves as a major component of the endomysium. Collagen constitutes one to two percent of muscle tissue and accounts for 6% of the weight of strong, tendinous, muscles. The fibroblast is the most common cell that creates collagen. Gelatin, which is used in food and industry, is collagen that has been irreversibly hydrolyzed. Collagen has many medical uses in treating complications of the bones and skin.

The name "collagen" comes from the Greek κόλλα ("kólla"), meaning "glue", and suffix -γέν, "-gen", denoting "producing". This refers to the compound's early use in the process of boiling the skin and tendons of horses and other animals to obtain glue.

Over 90% of the collagen in the human body is type I. However, as of 2011, 28 types of collagen have been identified, described, and divided into several groups according to the structure they form: 


The five most common types are:


The collagenous cardiac skeleton which includes the four heart valve rings, is histologically, elastically and uniquely bound to cardiac muscle. The cardiac skeleton also includes the separating septa of the heart chambers – the interventricular septum and the atrioventricular septum. Collagen contribution to the measure of cardiac performance summarily represents a continuous torsional force opposed to the fluid mechanics of blood pressure emitted from the heart. The collagenous structure that divides the upper chambers of the heart from the lower chambers is an impermeable membrane that excludes both blood and electrical impulses through typical physiological means. With support from collagen, atrial fibrillation never deteriorates to ventricular fibrillation. Collagen is layered in variable densities with cardiac muscle mass. The mass, distribution, age and density of collagen all contribute to the compliance required to move blood back and forth. Individual cardiac valvular leaflets are folded into shape by specialized collagen under variable pressure. Gradual calcium deposition within collagen occurs as a natural function of aging. Calcified points within collagen matrices show contrast in a moving display of blood and muscle, enabling methods of cardiac imaging technology to arrive at ratios essentially stating blood in (cardiac input) and blood out (cardiac output). Pathology of the collagen underpinning of the heart is understood within the category of connective tissue disease.

Collagen has been widely used in cosmetic surgery, as a healing aid for burn patients for reconstruction of bone and a wide variety of dental, orthopedic, and surgical purposes. Both human and bovine collagen is widely used as dermal fillers for treatment of wrinkles and skin aging. Some points of interest are:

As the skeleton forms the structure of the body, it is vital that it maintains its strength, even after breaks and injuries. Collagen is used in bone grafting as it has a triple helical structure, making it a very strong molecule. It is ideal for use in bones, as it does not compromise the structural integrity of the skeleton. The triple helical structure of collagen prevents it from being broken down by enzymes, it enables adhesiveness of cells and it is important for the proper assembly of the extracellular matrix.

Collagen scaffolds are used in tissue regeneration, whether in sponges, thin sheets, or gels. Collagen has the correct properties for tissue regeneration such as pore structure, permeability, hydrophilicity, and being stable in vivo. Collagen scaffolds are also ideal for the deposition of cells such as osteoblasts and fibroblasts, and once inserted, growth is able to continue as normal in the tissue.

Collagens are widely employed in the construction of the artificial skin substitutes used in the management of severe burns and wounds. These collagens may be derived from bovine, equine, porcine, or even human sources; and are sometimes used in combination with silicones, glycosaminoglycans, fibroblasts, growth factors and other substances.

Collagen is one of the body’s key natural resources and a component of skin tissue that can benefit all stages of the wound healing process. When collagen is made available to the wound bed, closure can occur. Wound deterioration, followed sometimes by procedures such as amputation, can thus be avoided.

Collagen is a natural product and is thus used as a natural wound dressing and has properties that artificial wound dressings do not have. It is resistant against bacteria, which is of vital importance in a wound dressing. It helps to keep the wound sterile, because of its natural ability to fight infection. When collagen is used as a burn dressing, healthy granulation tissue is able to form very quickly over the burn, helping it to heal rapidly.

Throughout the 4 phases of wound healing, collagen performs the following functions in wound healing:

When hydrolyzed, collagen is reduced to small peptides, which can be ingested in the form of a dietary supplement or functional foods and beverages with the intent to aid joint and bone health and enhance skin health. Hydrolyzed collagen has a much smaller molecular weight in comparison to native collagen or gelatin. Studies suggests that more than 90% of hydrolyzed collagen is digested and available as small peptides in the blood stream within one hour. From the blood, the peptides (containing hydroxyproline) are transported into the target tissues (e.g., skin, bones, and cartilage), where the peptides act as building blocks for local cells and help boost the production of new collagen fibers.

Collagen is used in laboratory studies for cell culture, studying cell behavior and cellular interactions with the extracellular environment.

Some studies have shown efficacy of collagen supplementation for dogs with osteoarthritis pain, alone or in combination with other nutraceuticals like glucosamine and chondroitin.

The collagen protein is composed of a triple helix, which generally consists of two identical chains (α1) and an additional chain that differs slightly in its chemical composition (α2). The amino acid composition of collagen is atypical for proteins, particularly with respect to its high hydroxyproline content. The most common motifs in the amino acid sequence of collagen are glycine-proline-X and glycine-X-hydroxyproline, where X is any amino acid other than glycine, proline or hydroxyproline. The average amino acid composition for fish and mammal skin is given.

First, a three-dimensional stranded structure is assembled, with the amino acids glycine and proline as its principal components. This is not yet collagen but its precursor, procollagen. Procollagen is then modified by the addition of hydroxyl groups to the amino acids proline and lysine. This step is important for later glycosylation and the formation of the triple helix structure of collagen. Because the hydroxylase enzymes that perform these reactions require vitamin C as a cofactor, a long-term deficiency in this vitamin results in impaired collagen synthesis and scurvy. These hydroxylation reactions are catalyzed by two different enzymes: prolyl-4-hydroxylase and lysyl-hydroxylase. Vitamin C also serves with them in inducing these reactions. In this service, one molecule of vitamin C is destroyed for each H replaced by OH.

The synthesis of collagen occurs inside and outside of the cell. The formation of collagen which results in fibrillary collagen (most common form) is discussed here. Meshwork collagen, which is often involved in the formation of filtration systems, is the other form of collagen. All types of collagens are triple helices, and the differences lie in the make-up of the alpha peptides created in step 2.

Collagen has an unusual amino acid composition and sequence:

Cortisol stimulates degradation of (skin) collagen into amino acids.

Most collagen forms in a similar manner, but the following process is typical for type I:


Vitamin C deficiency causes scurvy, a serious and painful disease in which defective collagen prevents the formation of strong connective tissue. Gums deteriorate and bleed, with loss of teeth; skin discolors, and wounds do not heal. Prior to the 18th century, this condition was notorious among long-duration military, particularly naval, expeditions during which participants were deprived of foods containing vitamin C.

An autoimmune disease such as lupus erythematosus or rheumatoid arthritis may attack healthy collagen fibers.

Many bacteria and viruses secrete virulence factors, such as the enzyme collagenase, which destroys collagen or interferes with its production.

A single collagen molecule, tropocollagen, is used to make up larger collagen aggregates, such as fibrils. It is approximately 300 nm long and 1.5 nm in diameter, and it is made up of three polypeptide strands (called alpha peptides, see step 2), each of which has the conformation of a left-handed helix – this should not be confused with the right-handed alpha helix. These three left-handed helices are twisted together into a right-handed triple helix or "super helix", a cooperative quaternary structure stabilized by many hydrogen bonds. With type I collagen and possibly all fibrillar collagens, if not all collagens, each triple-helix associates into a right-handed super-super-coil referred to as the collagen microfibril. Each microfibril is interdigitated with its neighboring microfibrils to a degree that might suggest they are individually unstable, although within collagen fibrils, they are so well ordered as to be crystalline.

A distinctive feature of collagen is the regular arrangement of amino acids in each of the three chains of these collagen subunits. The sequence often follows the pattern Gly-Pro-X or Gly-X-Hyp, where X may be any of various other amino acid residues. Proline or hydroxyproline constitute about 1/6 of the total sequence. With glycine accounting for the 1/3 of the sequence, this means approximately half of the collagen sequence is not glycine, proline or hydroxyproline, a fact often missed due to the distraction of the unusual GXX character of collagen alpha-peptides. The high glycine content of collagen is important with respect to stabilization of the collagen helix as this allows the very close association of the collagen fibers within the molecule, facilitating hydrogen bonding and the formation of intermolecular cross-links. This kind of regular repetition and high glycine content is found in only a few other fibrous proteins, such as silk fibroin.

Collagen is not only a structural protein. Due to its key role in the determination of cell phenotype, cell adhesion, tissue regulation, and infrastructure, many sections of its non-proline-rich regions have cell or matrix association/regulation roles. The relatively high content of proline and hydroxyproline rings, with their geometrically constrained carboxyl and (secondary) amino groups, along with the rich abundance of glycine, accounts for the tendency of the individual polypeptide strands to form left-handed helices spontaneously, without any intrachain hydrogen bonding.

Because glycine is the smallest amino acid with no side chain, it plays a unique role in fibrous structural proteins. In collagen, Gly is required at every third position because the assembly of the triple helix puts this residue at the interior (axis) of the helix, where there is no space for a larger side group than glycine’s single hydrogen atom. For the same reason, the rings of the Pro and Hyp must point outward. These two amino acids help stabilize the triple helix—Hyp even more so than Pro; a lower concentration of them is required in animals such as fish, whose body temperatures are lower than most warm-blooded animals. Lower proline and hydroxyproline contents are characteristic of cold-water, but not warm-water fish; the latter tend to have similar proline and hydroxyproline contents to mammals. The lower proline and hydroxproline contents of cold-water fish and other poikilotherm animals leads to their collagen having a lower thermal stability than mammalian collagen. This lower thermal stability means that gelatin derived from fish collagen is not suitable for many food and industrial applications.

The tropocollagen subunits spontaneously self-assemble, with regularly staggered ends, into even larger arrays in the extracellular spaces of tissues. Additional assembly of fibrils is guided by fibroblasts, which deposit fully formed fibrils from fibripositors. In the fibrillar collagens, molecules are staggered to adjacent molecules by about 67 nm (a unit that is referred to as ‘D’ and changes depending upon the hydration state of the aggregate). In each D-period repeat of the microfibril, there is a part containing five molecules in cross-section, called the “overlap”, and a part containing only four molecules, called the "gap". These overlap and gap regions are retained as microfibrils assemble into fibrils, and are thus viewable using electron microscopy. The triple helical tropocollagens in the microfibrils are arranged in a quasihexagonal packing pattern.

There is some covalent crosslinking within the triple helices, and a variable amount of covalent crosslinking between tropocollagen helices forming well organized aggregates (such as fibrils). Larger fibrillar bundles are formed with the aid of several different classes of proteins (including different collagen types), glycoproteins, and proteoglycans to form the different types of mature tissues from alternate combinations of the same key players. Collagen's insolubility was a barrier to the study of monomeric collagen until it was found that tropocollagen from young animals can be extracted because it is not yet fully crosslinked. However, advances in microscopy techniques (i.e. electron microscopy (EM) and atomic force microscopy (AFM)) and X-ray diffraction have enabled researchers to obtain increasingly detailed images of collagen structure "in situ". These later advances are particularly important to better understanding the way in which collagen structure affects cell–cell and cell–matrix communication and how tissues are constructed in growth and repair and changed in development and disease. For example, using AFM–based nanoindentation it has been shown that a single collagen fibril is a heterogeneous material along its axial direction with significantly different mechanical properties in its gap and overlap regions, correlating with its different molecular organizations in these two regions.

Collagen fibrils/aggregates are arranged in different combinations and concentrations in various tissues to provide varying tissue properties. In bone, entire collagen triple helices lie in a parallel, staggered array. 40 nm gaps between the ends of the tropocollagen subunits (approximately equal to the gap region) probably serve as nucleation sites for the deposition of long, hard, fine crystals of the mineral component, which is hydroxylapatite (approximately) Ca(OH)(PO). Type I collagen gives bone its tensile strength.

Collagen-related diseases most commonly arise from genetic defects or nutritional deficiencies that affect the biosynthesis, assembly, postranslational modification, secretion, or other processes involved in normal collagen production.

In addition to the above-mentioned disorders, excessive deposition of collagen occurs in scleroderma.

One thousand mutations have been identified in 12 out of more than 20 types of collagen. These mutations can lead to various diseases at the tissue level.

Osteogenesis imperfecta – Caused by a mutation in type 1 collagen, dominant autosomal disorder, results in weak bones and irregular connective tissue, some cases can be mild while others can be lethal. Mild cases have lowered levels of collagen type 1 while severe cases have structural defects in collagen.

Chondrodysplasias – Skeletal disorder believed to be caused by a mutation in type 2 collagen, further research is being conducted to confirm this.

Ehlers-Danlos syndrome – Thirteen different types of this disorder, which lead to deformities in connective tissue, are known. Some of the rarer types can be lethal, leading to the rupture of arteries. Each syndrome is caused by a different mutation. For example, the vascular type (vEDS) of this disorder is caused by a mutation in collagen type 3.

Alport syndrome – Can be passed on genetically, usually as X-linked dominant, but also as both an autosomal dominant and autosomal recessive disorder, sufferers have problems with their kidneys and eyes, loss of hearing can also develop in during the childhood or adolescent years.

Knobloch syndrome – Caused by a mutation in the COL18A1 gene that codes for the production of collagen XVIII. Patients present with protrusion of the brain tissue and degeneration of the retina; an individual who has family members suffering from the disorder is at an increased risk of developing it themselves since there is a hereditary link.

Collagen is one of the long, fibrous structural proteins whose functions are quite different from those of globular proteins, such as enzymes. Tough bundles of collagen called "collagen fibers" are a major component of the extracellular matrix that supports most tissues and gives cells structure from the outside, but collagen is also found inside certain cells. Collagen has great tensile strength, and is the main component of fascia, cartilage, ligaments, tendons, bone and skin. Along with elastin and soft keratin, it is responsible for skin strength and elasticity, and its degradation leads to wrinkles that accompany aging. It strengthens blood vessels and plays a role in tissue development. It is present in the cornea and lens of the eye in crystalline form. It may be one of the most abundant proteins in the fossil record, given that it appears to fossilize frequently, even in bones from the Mesozoic and Paleozoic.

Collagen has a wide variety of applications, from food to medical. For instance, it is used in cosmetic surgery and burn surgery. It is widely used in the form of collagen casings for sausages, which are also used in the manufacture of musical strings.

If collagen is subject to sufficient denaturation, e.g. by heating, the three tropocollagen strands separate partially or completely into globular domains, containing a different secondary structure to the normal collagen polyproline II (PPII), e.g. random coils. This process describes the formation of gelatin, which is used in many foods, including flavored gelatin desserts. Besides food, gelatin has been used in pharmaceutical, cosmetic, and photography industries.

From the Greek for glue, "kolla", the word collagen means "glue producer" and refers to the early process of boiling the skin and sinews of horses and other animals to obtain glue. Collagen adhesive was used by Egyptians about 4,000 years ago, and Native Americans used it in bows about 1,500 years ago. The oldest glue in the world, carbon-dated as more than 8,000 years old, was found to be collagen—used as a protective lining on rope baskets and embroidered fabrics, and to hold utensils together; also in crisscross decorations on human skulls. Collagen normally converts to gelatin, but survived due to dry conditions. Animal glues are thermoplastic, softening again upon reheating, so they are still used in making musical instruments such as fine violins and guitars, which may have to be reopened for repairs—an application incompatible with tough, synthetic plastic adhesives, which are permanent. Animal sinews and skins, including leather, have been used to make useful articles for millennia.

Gelatin-resorcinol-formaldehyde glue (and with formaldehyde replaced by less-toxic pentanedial and ethanedial) has been used to repair experimental incisions in rabbit lungs.

The molecular and packing structures of collagen have eluded scientists over decades of research. The first evidence that it possesses a regular structure at the molecular level was presented in the mid-1930s. Since that time, many prominent scholars, including Nobel laureates Crick, Pauling, Rich and Yonath, and others, including Brodsky, Berman, and Ramachandran, concentrated on the conformation of the collagen monomer. Several competing models, although correctly dealing with the conformation of each individual peptide chain, gave way to the triple-helical "Madras" model of Ramachandran, which provided an essentially correct model of the molecule's quaternary structure although this model still required some refinement. The packing structure of collagen has not been defined to the same degree outside of the fibrillar collagen types, although it has been long known to be hexagonal or quasi-hexagonal. As with its monomeric structure, several conflicting models alleged that either the packing arrangement of collagen molecules is 'sheet-like' or microfibrillar. The microfibrillar structure of collagen fibrils in tendon, cornea and cartilage has been directly imaged by electron microscopy. The microfibrillar structure of tail tendon, as described by Fraser, Miller, and Wess (amongst others), was modeled as being closest to the observed structure, although it oversimplified the topological progression of neighboring collagen molecules, and hence did not predict the correct conformation of the discontinuous D-periodic pentameric arrangement termed simply: the microfibril. Various cross linking agents like L-Dopaquinone, embeline, potassium embelate and 5-O-methyl embelin could be developed as potential
cross-linking/stabilization agents of collagen preparation and its application as wound dressing sheet in clinical applications is enhanced.

The evolution of collagens was a fundamental step in the early evolution of life, supporting the coalescence of multicellular life forms.

Collagen D-banding is viable as periodic formation of ridging on all fibrils forming collagen. D-bands are created due to the semi-crystalline formation of the collagen within the fibrils. The pattern exhibited by D-banding is consistently independent of fibril diameter. When undergoing deformation, collagen fibrils may lose their D-banding, making the disappearance of the d-bands an indicator of the type of damage undergone by then tendon fibrils.


</doc>
<doc id="6059" url="https://en.wikipedia.org/wiki?curid=6059" title="Calvin and Hobbes">
Calvin and Hobbes

Calvin and Hobbes is a daily comic strip by American cartoonist Bill Watterson that was syndicated from November 18, 1985 to December 31, 1995. Commonly cited as "the last great newspaper comic", "Calvin and Hobbes" has enjoyed broad and enduring popularity, influence, and academic interest.

"Calvin and Hobbes" follows the humorous antics of Calvin, a precocious, mischievous, and adventurous six-year-old boy, and Hobbes, his sardonic stuffed tiger. Set in the contemporary suburban United States, the strip depicts Calvin's frequent flights of fancy and his friendship with Hobbes. It also examines Calvin's relationships with family and classmates, especially the love/hate relationship between him and his classmate Susie Derkins. Hobbes' dual nature is a defining motif for the strip: to Calvin, Hobbes is a living anthropomorphic tiger, while all the other characters see Hobbes as an inanimate stuffed toy. Though the series does not mention specific political figures or current events, it does explore broad issues like environmentalism, public education, philosophical quandaries, and the flaws of opinion polls.

At the height of its popularity, "Calvin and Hobbes" was featured in over 2,400 newspapers worldwide. In 2010, reruns of the strip appeared in more than 50 countries, and nearly 45 million copies of the "Calvin and Hobbes" books had been sold worldwide.

"Calvin and Hobbes" was conceived when Bill Watterson, who was working in an advertising job he detested, began devoting his spare time to developing a newspaper comic for potential syndication. He explored various strip ideas but all were rejected by the syndicates. United Feature Syndicate finally responded positively to one strip called "The Doghouse", which featured a side character (the main character's little brother) who had a stuffed tiger. United identified these characters as the strongest, and encouraged Watterson to develop them as the centre of their own strip. Though United Feature ultimately rejected the new strip as lacking in marketing potential, Universal Press Syndicate took it up.

The first strip was published on November 18, 1985 in 35 newspapers. Watterson was warned by the syndicate not to give up the day job yet, but it was not long before the series had become a hit. Within a year of syndication, the strip was published in roughly 250 newspapers and was proving to have international appeal with translation and wide circulation outside the United States.

Although "Calvin and Hobbes" would undergo continual artistic development and creative innovation over the period of syndication, the earliest strips demonstrate a remarkable consistency with the latest. Watterson introduced all the major characters within the first three weeks, and made no changes to the central cast over the 10 years of the strip's history.

By April 5, 1987, Watterson was featured in an article in "The Los Angeles Times". "Calvin and Hobbes" earned Watterson the Reuben Award from the National Cartoonists Society in the Outstanding Cartoonist of the Year category, first in 1986 and again in 1988. He was nominated another time in 1992. The Society awarded him the Humor Comic Strip Award for 1988. "Calvin and Hobbes" has also won several more awards.

As his creation grew in popularity, Watterson underwent a long and emotionally draining battle with his syndicate editors over his refusal to license his characters for merchandising. By 1991, Watterson had achieved his goal of securing a new contract that granted him legal control over his creation and all future licensing arrangements.

Having achieved his objective of creative control, Watterson's desire for privacy subsequently reasserted itself and he ceased all media interviews, relocated to New Mexico, and largely disappeared from public engagements, refusing to attend the ceremonies of any of the cartooning awards he won. The pressures of the battle over merchandising led to Watterson taking an extended break from May 5, 1991, to February 1, 1992, a move that was unprecedented in the world of syndicated cartoonists.

During Watterson's first sabbatical from the strip, Universal Press Syndicate continued to charge newspapers full price to re-run old "Calvin and Hobbes" strips. Few editors approved of the move, but the strip was so popular that they had no choice but to continue to run it for fear that competing newspapers might pick it up and draw its fans away. Watterson returned to the strip in 1992 with plans to produce his Sunday strip as an unbreakable half of a newspaper or tabloid page. This made him only the second cartoonist since Garry Trudeau to have sufficient popularity to demand more space and control over the presentation of their work.

Watterson took a second sabbatical from April 3 through December 31, 1994. When he returned, he had made the decision to end the strip. In 1995, Watterson sent a letter via his syndicate to all editors whose newspapers carried his strip announcing his plans to end the strip by the end of the year. Stating his belief that he had achieved everything that he wanted to within the medium, he announced his intention to work on future projects at a slower pace with fewer artistic compromises.

The final strip ran on Sunday, December 31, 1995. It depicted Calvin and Hobbes outside in freshly fallen snow, reveling in the wonder and excitement of the winter scene. "It's a magical world, Hobbes, ol' buddy... Let's go exploring!" Calvin exclaims as they zoom off over the snowy hills on their sled, leaving, according to one critic ten years later, "a hole in the comics page that no strip has been able to fill."

Syndicated comics were typically published five times a week in black and white, with a Sunday supplement version in a larger, full colour format. This larger format version of the strip was constrained by mandatory layout requirements that made it possible for newspaper editors to format the strip for different page sizes and layouts. 

Watterson grew increasingly frustrated by the shrinking of the available space for comics in the newspapers and the mandatory panel divisions that restricted his ability to produce better artwork and more creative storytelling. He lamented that without space for anything more than simple dialogue or sparse artwork, comics as an art form were becoming dilute, bland, and unoriginal. 

Watterson longed for the artistic freedom allotted to classic strips such as "Little Nemo" and "Krazy Kat", and in 1989 he gave a sample of what could be accomplished with such liberty in the opening pages of the Sunday strip compilation, "The Calvin and Hobbes Lazy Sunday Book—"a 8-page previously unpublished Calvin story fully illustrated in watercolour. The same book contained an afterword from the artist himself, reflecting on a time when comic strips were allocated a whole page of the newspaper and every comic was like a "colour poster".

Within two years, Watterson was ultimately successful in negotiating a deal that provided him more space and creative freedom. Following his 1991 sabbatical, Universal Press announced that Watterson had decided to sell his Sunday strip as an unbreakable half of a newspaper or tabloid page. Many editors and even a few cartoonists including Bil Keane ("The Family Circus") and Bruce Beattie ("Snafu") criticized him for what they perceived as arrogance and an unwillingness to abide by the normal practices of the cartoon business. Others, including Bill Amend ("Foxtrot"), Johnny Hart ("BC", "Wizard of Id") and Barbara Brandon ("Where I'm Coming From") supported him. The American Association of Sunday and Feature Editors even formally requested that Universal reconsider the changes. Watterson's own comments on the matter was that "editors will have to judge for themselves whether or not Calvin and Hobbes deserves the extra space. If they don't think the strip carries its own weight, they don't have to run it." Ultimately only 15 newspapers cancelled the strip in response to the layout changes.

Bill Watterson took two sabbaticals from the daily requirements of producing the Calvin and Hobbes comic strip. The first took place from May 5, 1991 to February 1, 1992, and second from April 3 through December 31, 1994. These sabbaticals were included in the new contract Watterson managed to negotiate with Universal Features in 1990. The sabbaticals were proposed by the syndicate themselves who, fearing Watterson's complete burnout, endeavoured to get another five years of work from their star artist.

Watterson remains only the third cartoonist with sufficient popularity and stature to receive a sabbatical from their syndicate, the first being Garry Trudeau ("Doonesbury") in 1983 and Gary Larson ("The Far Side") in 1989. Typically cartoonists are expected to produce sufficient strips to cover any period they may wish to take off. Watterson's lengthy sabbaticals received some mild criticism from his fellow cartoonists including Greg Evans ("Luann"), and Charles Schulz ("Peanuts"), one of Watterson's major artistic influences, even called it a "puzzle". Some cartoonists resented the idea that Watterson worked harder than others, whilst others supported it. At least one newspaper editor noted that the strip was the most popular in the country, and that he had "earned it". Following his second sabbatical, Watterson made the decision that he was going to retire from the comic strip entirely.

In spite of "Calvin and Hobbes"' popularity, the strip remains notable for the relative lack of official product merchandising. Bill Watterson held that comic strips should stand on their own as an art form and although he did not start out completely opposed to merchandising in all forms (or even for all comic strips), he did reject an early syndication deal that involved incorporating a more marketable, licensed character into his strip. In spite of being an unproven cartoonist, and having been flown all the way to New York to discuss the proposal, Watterson reflexively resented the idea of "cartooning by committee" and turned it down.

Later, when "Calvin" was accepted by Universal Syndicate, and began to grow in popularity, Watterson found himself at odds with the syndicate, which urged him to begin merchandising the characters and touring the country to promote the first collections of comic strips. Watterson refused. To him, the integrity of the strip and its artist would be undermined by commercialization, which he saw as a major negative influence in the world of cartoon art and he came to believe that licensing his character would only violate the spirit of his work. He gave an example of this in discussing his opposition to a Hobbes plush toy: that if the essence of Hobbes' nature in the strip is that it remain unresolved whether he is a real tiger or a stuffed toy, then creating a real stuffed toy would only destroy the magic. However, having initially signed away control over merchandising in his initial contract with the syndicate, Watterson would commence a lengthy and emotionally draining battle with Universal to gain control over his work. Ultimately Universal did not approve any products against Watterson's wishes, understanding that unlike other comic strips, it would be near impossible to separate the creator from the strip if Watterson chose to walk away. 

One estimate places the value of licensing revenue forgone by Watterson at $300–$400 million. Almost no legitimate "Calvin and Hobbes" merchandise exists. Exceptions produced during the strip's original run include two 16-month calendars (1988–89 and 1989–90), a t-shirt for the Smithsonian Exhibit, "Great American Comics: 100 Years of Cartoon Art" (1990) and the textbook "Teaching with" Calvin and Hobbes, which has been described as "perhaps the most difficult piece of official "Calvin and Hobbes" memorabilia to find." In 2010, Watterson did allow his characters to be included in a series of United States Postal Service stamps honoring five classic American comics. Licensed prints of "Calvin and Hobbes" can also be purchased through Uclick, the digital division of Andrews McMeel Universal, and have also been included in various academic works.

The strip's immense popularity has led to the appearance of various counterfeit items such as window decals and T-shirts that often feature crude humor, binge drinking and other themes that are not found in Watterson's work. Images from one strip in which Calvin and Hobbes dance to loud music at night were commonly used for copyright violations. After threat of a lawsuit alleging infringement of copyright and trademark, some sticker makers replaced Calvin with a different boy, while other makers made no changes. Watterson wryly commented, "I clearly miscalculated how popular it would be to show Calvin urinating on a Ford logo," but that they would be his "ticket to immortality".

Watterson has expressed admiration for animation as an artform. In a 1989 interview in "The Comics Journal" he described the appeal of being able to do things with a moving image that can't be done by a simple drawing: the distortion, the exaggeration and the control over the length of time an event is viewed. However, although the visual possibilities of animation appealed to Watterson, the idea of finding a voice for Calvin made him uncomfortable, as did the idea of working with a team of animators.. Ultimately, "Calvin and Hobbes" was never made into an animated series. Watterson later stated in "The Calvin and Hobbes Tenth Anniversary Book" that he liked the fact that his strip was a "low-tech, one-man operation," and that he took great pride in the fact that he drew every line and wrote every word on his own. Calls from major Hollywood figures interested in an adaptation of his work, including Jim Henson, George Lucas and Steven Spielberg were never returned and in a 2013 interview Watterson stated that he had "zero interest" in an animated adaptation as there was really no upside for him in doing so.

The strip borrows several elements and themes from three major influences: Walt Kelly's "Pogo", George Herriman's "Krazy Kat", and Charles M. Schulz's "Peanuts". Schulz and Kelly particularly influenced Watterson's outlook on comics during his formative years. 

Notable elements of Watterson's artistic style are his characters' diverse and often exaggerated expressions (particularly those of Calvin), elaborate and bizarre backgrounds for Calvin's flights of imagination, expressions of motion, and frequent visual jokes and metaphors. In the later years of the strip, with more panel space available for his use, Watterson experimented more freely with different panel layouts, art styles, stories without dialogue, and greater use of whitespace. He also makes a point of not showing certain things explicitly: the "Noodle Incident" and the children's book "Hamster Huey and the Gooey Kablooie" are left to the reader's imagination, where Watterson was sure they would be "more outrageous" than he could portray.

Watterson's technique started with minimalist pencil sketches drawn with a light pencil (though the larger Sunday strips often required more elaborate work) on a piece of Bristol board, with his brand of choice being Strathmore because he felt it held the drawings better on the page as opposed to the cheaper brands (Watterson said he would use any cheap pad of Bristol board his local supply store had, but switched to Strathmore after he found himself growing more and more displeased with the results). He would then use a small sable brush and India ink to fill in the rest of the drawing, saying that he did not want to simply trace over his penciling and thus make the inking more spontaneous. He lettered dialogue with a Rapidograph fountain pen, and he used a crowquill pen for odds and ends. Mistakes were covered with various forms of correction fluid, including the type used on typewriters. Watterson was careful in his use of color, often spending a great deal of time in choosing the right colors to employ for the weekly Sunday strip; his technique was to cut the color tabs the syndicate sent him into individual squares, lay out the colors, and then paint a watercolor approximation of the strip on tracing paper over the Bristol board and then mark the strip accordingly before sending it on. When "Calvin and Hobbes" began there were 64 colors available for the Sunday strips. For the later Sunday strips Watterson had 125 colors as well as the ability to fade the colors into each other.

Calvin, named after the 16th-century theologian John Calvin, is a six-year-old boy with blond, spiky hair and a distinctive red-and-black striped shirt, black pants, and sneakers. Despite his poor grades in school, Calvin demonstrates his intelligence through a sophisticated vocabulary and philosophical mind. Watterson described Calvin as having "not much of a filter between his brain and his mouth", a "little too intelligent for his age", lacking in restraint and not yet having the experience to "know the things that you shouldn't do." The comic strip largely revolves around Calvin's inner world, and his largely antagonistic experiences with those outside of it (fellow students, authority figures and his parents).

From Calvin's point of view, Hobbes is an anthropomorphic tiger, much larger than Calvin and full of independent attitudes and ideas. When the perspective shifts to any other character, readers see merely a stuffed animal, usually seated at an off-kilter angle and blankly staring into space. The true nature of the character is never resolved, instead as Watterson describes, a 'grown-up' version of reality is juxtaposed against Calvin's, with the reader left to "decide which is truer".

Hobbes is named after the 17th-century philosopher Thomas Hobbes, who held what Watterson describes as "a dim view of human nature." He typically exhibits a greater understanding of consequences than Calvin, although rarely intervenes in Calvin's activities beyond a few oblique warnings. The friendship between the two characters provides the core dynamic of the strip.

Calvin's unnamed mother and father are typical middle-class parents. Calvin's father is a patent attorney (like Watterson's own father) and his mother is a stay-at-home mom. Watterson says, "As far as the strip is concerned, they are important only as Calvin's mom and dad." Like many other characters in the strip, they are relatively down to earth and their sensible attitudes serve as a foil for Calvin's outlandish behavior.

Watterson says some fans are angered by the sometimes sardonic way that Calvin's parents respond to him. Watterson defends what Calvin's parents do, remarking that in the case of parenting a kid like Calvin, "I think they do a better job than I would." Calvin's father is overly concerned with "character building" activities in a number of strips, either in the things he makes Calvin do or in the austere eccentricities of his own lifestyle.

Susie Derkins, who first appeared early in the strip and is the only important character with both a first and last name, lives on Calvin's street and is one of his classmates. Her last name apparently derives from the pet beagle owned by Watterson's wife's family.

Susie is polite and studious, and she likes to play house or host tea parties with her stuffed animals. However, she is also depicted playing imaginary games with Calvin in which she is a high-powered lawyer or politician and he is her househusband. Though both of them hate to admit it, Calvin and Susie have quite a bit in common. For example, Susie is shown on occasion with a stuffed bunny rabbit named "Mr. Bun." Susie also has a mischievous (and sometimes aggressive) streak, which can be seen when she subverts Calvin's attempts to cheat on school tests by feeding him incorrect answers, or when she clobbers Calvin after he attacks her with snowballs. Susie also regularly bests Calvin in confrontations such as their water balloon and snowball fights, employing guile or force.

Hobbes often openly expresses romantic feelings for Susie, much to Calvin's disgust. Calvin starts a "club" (of which he and Hobbes are the only members) that he calls G.R.O.S.S. (Get Rid Of Slimy GirlS), and while holding "meetings" in Calvin's treehouse or in the "box of secrecy" in Calvin's room, they usually come up with some way to annoy or discomfit Susie, most of which backfire on them completely. In one instance, Calvin steals one of Susie's dolls for ransom, only to have Susie retaliate by nabbing Hobbes. Watterson admits that Calvin and Susie have a nascent crush on each other, and that Susie is inspired by the type of woman whom Watterson himself found attractive and eventually married.

Calvin also interacts with a handful of secondary characters. Several of these, including Rosalyn, his babysitter, Mrs Wormwood, his teacher, and Moe, the school bully, recur regularly through the duration of the strip.

Watterson used the strip to poke fun at the art world, principally through Calvin's unconventional creations of snowmen but also through other expressions of childhood art. When Miss Wormwood complains that he is wasting class time drawing impossible things (a "Stegosaurus" in a rocket ship, for example), Calvin proclaims himself "on the cutting edge of the "avant-garde"." He begins exploring the medium of snow when a warm day melts his snowman. His next sculpture "speaks to the horror of our own mortality, inviting the viewer to contemplate the evanescence of life." In later strips, Calvin's creative instincts diversify to include sidewalk drawings (or, as he terms them, examples of "suburban postmodernism").

Watterson also lampooned the academic world. In one example, Calvin carefully crafts an "artist's statement", claiming that such essays convey more messages than artworks themselves ever do (Hobbes blandly notes, "You misspelled "Weltanschauung""). He indulges in what Watterson calls "pop psychobabble" to justify his destructive rampages and shift blame to his parents, citing "toxic codependency." In one instance, he pens a book report based on the theory that the purpose of academic writing is to "inflate weak ideas, obscure poor reasoning, and inhibit clarity," entitled "The Dynamics of Interbeing and Monological Imperatives in "Dick and Jane:" A Study in Psychic Transrelational Gender Modes". Displaying his creation to Hobbes, he remarks, "Academia, here I come!" Watterson explains that he adapted this jargon (and similar examples from several other strips) from an actual book of art criticism.

Overall, Watterson's satirical essays serve to attack both sides, criticizing both the commercial mainstream and the artists who are supposed to be "outside" it. The strip for Sunday, June 21, 1992, criticized the naming of The Big Bang theory as not evocative of the wonders behind it, and coined the term "Horrendous Space Kablooie", an alternative that achieved some informal popularity among scientists and was often shortened to "the HSK." The term has also been referred to in newspapers, books, and university courses.

There are many recurring gags in the strip, some in "reality" and others in Calvin's imagination. These are as follows:

Calvin imagines himself as a great many things including dinosaurs, elephants, jungle-farers and superheroes. Three of his alter egos are well defined and recurrent:



Calvin has had several adventures involving corrugated cardboard boxes, which he adapts for many imaginative uses. In one strip, when Calvin shows off his Transmogrifier, a device that transforms its user into any desired creature or item, Hobbes remarks, "It's amazing what they do with corrugated cardboard these days." Calvin is able to change the function of the boxes by rewriting the label and flipping the box onto another side. In this way, a box can be used not only for its conventional purposes (a storage container for water balloons, for example), but also as a flying time machine, a duplicator or, with the attachment of a few wires and a colander, a "Cerebral Enhance-o-tron."

In the real world, Calvin's antics with his box have had varying effects. When he transmogrified into a tiger, he still appeared as a regular human child to his parents. However, in a story where he made several duplicates of himself, his parents are seen interacting with what does seem like multiple Calvins, including in a strip where two of him are seen in the same panel as his father. It is ultimately unknown what his parents do or do not see, as Calvin tries to hide most of his creations (or conceal their effects) so as not to traumatize them.

In addition, Calvin uses a cardboard box as a sidewalk kiosk to sell things. Often, Calvin offers merchandise no one would want, such as "suicide drink", "a swift kick in the butt" for one dollar, or a "frank appraisal of your looks" for fifty cents. In one strip, he sells "happiness" for ten cents: Calvin hit the customer in the face with a water balloon, explaining that he meant his own happiness. In another strip, he sold "insurance", firing a slingshot at those who refused to buy it. In some strips, he tried to sell "great ideas", and in one earlier strip, he attempted to sell the family car to obtain money for a grenade launcher. In yet another strip, he sells "life" for five cents, where the customer receives nothing in return, which, in Calvin's opinion, is life.

The box has also functioned as a secret meeting place for G.R.O.S.S., as the "Box of Secrecy".

Calvinball is an improvisational game introduced in a 1990 storyline that involved Calvin's negative experience of joining the school baseball team. Calvinball is a nomic or self-modifying game, a contest of wits, skill and creativity rather than stamina or athletic skill. The game is portrayed as a rebellion against conventional team sports and became a staple of the final 5 years of the comic. The only consistent rules of the game are that Calvinball may never be played with the same rules twice and that each participant must wear a mask.

When asked how to play, Watterson states: "It's pretty simple: you make up the rules as you go." In most appearances of the game, a comical array of conventional and non-conventional sporting equipment is involved, including a croquet set, a badminton set, assorted flags, bags, signs, a hobby horse, water buckets and balloons, with humorous allusions to unseen elements such as "time-fracture wickets". Scoring is portrayed as arbitrary and nonsensical ("Q to 12" and "oogy to boogy") and the lack of fixed rules leads to lengthy argument between the participants as to who scored, where the boundaries are, and when the game is finished. The game has been described in one academic work not as a new game based on fragments of an older one, but as the "constant connecting and disconnecting of parts, the constant evasion of rules or guidelines based on collective creativity."

Calvin often creates horrendous/dark humor scenes with his snowmen. He uses the snowman for social commentary, revenge, or pure enjoyment. Examples include Snowman Calvin being yelled at by Snowman Dad to shovel the snow; one snowman eating snow cones scooped out of a second snowman, who is lying on the ground with an ice-cream scoop in his back; a "snowman house of horror"; and snowmen representing the people he hates. "The ones I "really" hate are small, so they'll melt faster," he says. There was even an occasion on which Calvin accidentally brought a snowman to life and it made itself and a small army into "deranged mutant killer monster snow goons."

Calvin's snow art is often used as a commentary on art in general. For example, Calvin has complained more than once about the lack of originality in other people's snow art and compared it with his own grotesque snow sculptures. In one of these instances, Calvin and Hobbes claim to be the sole guardians of high culture; in another, Hobbes admires Calvin's willingness to put artistic integrity above marketability, causing Calvin to reconsider and make an ordinary snowman.

Calvin and Hobbes frequently ride downhill in a wagon, sled, or toboggan, depending on the season, as a device to add some physical comedy to the strip and because, according to Watterson, "it's a lot more interesting ... than talking heads." While the ride is sometimes the focus of the strip, it also frequently serves as a counterpoint or visual metaphor while Calvin ponders the meaning of life, death, God, philosophy or a variety of other weighty subjects. Many of their rides end in spectacular crashes which leave them battered and broken, a fact which convinces Hobbes to sometimes hop off before a ride even begins. In the final strip, Calvin and Hobbes depart on their toboggan to go exploring. This theme is similar (perhaps even homage) to scenarios in Walt Kelly's "Pogo".

G.R.O.S.S., which stands for Get Rid Of Slimy GirlS ("otherwise it doesn't spell anything"), is a club which consists of only two members: Calvin and Hobbes. The club was founded in the garage of their house. To clear space for its activities, Calvin and (purportedly) Hobbes push Calvin's parents' car, causing it to roll into a ditch (but not suffer damage); the incident necessitates changing the club's location to Calvin's treehouse. They hold meetings to attempt to annoy Susie Derkins. Notable actions include planting a fake secret tape near her in attempt to draw her in to a trap, trapping her in a closet at their house, and creating elaborate water balloon traps. Calvin gave himself and Hobbes important positions in the club, Calvin being "Dictator-for-Life" and Hobbes being "President-and-First-Tiger". They go into Calvin's treehouse for their club meetings and often get into fights during them. The password to get into the treehouse is intentionally long and difficult, which has on at least one occasion ruined Calvin's plans. As Hobbes is able to climb the tree without the rope, he is usually the one who comes up with the password, which often involves heaping praise upon tigers. An example of this can be seen in the comic strip where Calvin, rushing to get into the treehouse to throw things at a passing Susie Derkins, insults Hobbes, who is in the treehouse and thus has to let down the rope. Hobbes forces Calvin to say the password for insulting him. By the time Susie arrives, in time to hear Calvin saying some of the password, causing him to stumble, Calvin is on ""Verse Seven:" Tigers are perfect!/The E-pit-o-me/of good looks and grace/and quiet..uh..um..dignity". The opportunity to pelt Susie with something having passed, Calvin threatens to turn Hobbes into a rug. G.R.O.S.S. is one of the most common adventures that Calvin has. The club anthem begins: "Ohhhh Gross, best club in the cosmos..."

There are 18 "Calvin and Hobbes" books, published from 1987 to 1997. These include 11 collections, which form a complete archive of the newspaper strips, except for a single daily strip from November 28, 1985. (The collections "do" contain a strip for this date, but it is not the same strip that appeared in some newspapers. Treasuries usually combine the two preceding collections with bonus material and include color reprints of Sunday comics.)

Watterson included some new material in the treasuries. In "The Essential Calvin and Hobbes", which includes cartoons from the collections "Calvin and Hobbes" and "Something Under the Bed Is Drooling", the back cover features a scene of a giant Calvin rampaging through a town. The scene is based on Watterson's home town of Chagrin Falls, Ohio, and Calvin is holding the Chagrin Falls Popcorn Shop, an iconic candy and ice cream shop overlooking the town's namesake falls. Several of the treasuries incorporate additional poetry; "The Indispensable Calvin and Hobbes" book features a set of poems, ranging from just a few lines to an entire page, that cover topics such as Calvin's mother's "hindsight" and exploring the woods. In "The Essential Calvin and Hobbes", Watterson presents a long poem explaining a night's battle against a monster from Calvin's perspective.

A complete collection of "Calvin and Hobbes" strips, in three hardcover volumes totaling 1440 pages, was released on October 4, 2005, by Andrews McMeel Publishing. It includes color prints of the art used on paperback covers, the treasuries' extra illustrated stories and poems, and a new introduction by Bill Watterson in which he talks about his inspirations and his story leading up to the publication of the strip. The alternate 1985 strip is still omitted, and two other strips (January 7, 1987, and November 25, 1988) have altered dialogue. A four-volume paperback version was released November 13, 2012.

To celebrate the release (which coincided with the strip's 20th anniversary and the tenth anniversary of its absence from newspapers), Bill Watterson answered 15 questions submitted by readers.

Early books were printed in smaller format in black and white. These were later reproduced in twos in color in the "Treasuries" ("Essential", "Authoritative", and "Indispensable"), except for the contents of "Attack of the Deranged Mutant Killer Monster Snow Goons". Those Sunday strips were not reprinted in color until the "Complete" collection was finally published in 2005.

Watterson claims he named the books the ""Essential", "Authoritative", and "Indispensable"" because, as he says in "The Calvin and Hobbes Tenth Anniversary Book", the books are "obviously none of these things."

In 1993, paleontologist and paleoartist Gregory S. Paul praised Bill Watterson for the scientific accuracy of the dinosaurs appearing in "Calvin and Hobbes".

In her 1994 book "When Toys Come Alive", Lois Rostow Kuznets says that Hobbes serves both as a figure of Calvin's childish fantasy life and as an outlet for the expression of libidinous desires more associated with adults. Kuznets also looks at Calvin's other fantasies, suggesting that they are a second tier of fantasies utilized in places like school where transitional objects such as Hobbes would not be socially acceptable.

Political scientist James Q. Wilson, in a paean to "Calvin and Hobbes" upon Watterson's decision to end the strip in 1995, characterized it as "our only popular explication of the moral philosophy of Aristotle."

Alisa White Coleman analyzed the strip's underlying messages concerning ethics and values in "'Calvin and Hobbes': A Critique of Society's Values," published in the "Journal of Mass Media Ethics" in 2000.

A collection of original Sunday strips was exhibited at Ohio State University's Billy Ireland Cartoon Library & Museum in 2001. Watterson himself selected the strips and provided his own commentary for the exhibition catalog, which was later published by Andrews McMeel as "Calvin and Hobbes: Sunday Pages 1985–1995".

Since the discontinuation of "Calvin and Hobbes", individual strips have been licensed for reprint in schoolbooks, including the Christian homeschooling book "The Fallacy Detective" in 2002, and the university-level philosophy reader "Open Questions: Readings for Critical Thinking and Writing" in 2005; in the latter, the ethical views of Watterson and his characters Calvin and Hobbes are discussed in relation to the views of professional philosophers. Since 2009, Twitter users have indicated that "Calvin and Hobbes" strips have appeared in textbooks for subjects in the sciences, social sciences, mathematics, philosophy, and foreign language.

In a 2009 evaluation of the entire body of "Calvin and Hobbes" strips using grounded theory methodology, Christijan D. Draper found that: "Overall, "Calvin and Hobbes" suggests that meaningful time use is a key attribute of a life well lived," and that "the strip suggests one way to assess the meaning associated with time use is through preemptive retrospection by which a person looks at current experiences through the lens of an anticipated future..."

Jamey Heit's "Imagination and Meaning in Calvin and Hobbes", a critical, academic analysis of the strip, was published in 2012.

"Calvin and Hobbes" strips were again exhibited at the Billy Ireland Cartoon Library & Museum at Ohio State University in 2014, in an exhibition entitled "Exploring Calvin and Hobbes". An exhibition catalog by the same title, which also contained an interview with Watterson conducted by Jenny Robb, the curator of the museum, was published by Andrews McMeel in 2015.

Years after its original newspaper run, "Calvin and Hobbes" has continued to exert influence in entertainment, art, and fandom.

In television, Calvin and Hobbes are depicted in stop motion animation in the 2006 "Robot Chicken" episode "Lust for Puppets," and in traditional animation in the 2009 "Family Guy" episode "Not All Dogs Go to Heaven." In the 2013 "Community" episode "Paranormal Parentage," the characters Abed Nadir (Danny Pudi) and Troy Barnes (Donald Glover) dress as Calvin and Hobbes, respectively, for Halloween.

British artists, merchandisers, booksellers, and philosophers were interviewed for a 2009 BBC Radio 4 half-hour programme about the abiding popularity of the comic strip, narrated by Phill Jupitus.

The first book-length study of the strip, "Looking for Calvin and Hobbes: The Unconventional Story of Bill Watterson and His Revolutionary Comic Strip" by Nevin Martell, was first published in 2009; an expanded edition was published in 2010. The book chronicles Martell's quest to tell the story of "Calvin and Hobbes" and Watterson through research and interviews with people connected to the cartoonist and his work. The director of the later documentary "Dear Mr. Watterson" referenced "Looking for Calvin and Hobbes" in discussing the production of the movie, and Martell appears in the film.

The American documentary film "Dear Mr. Watterson", released in 2013, explores the impact and legacy of "Calvin and Hobbes" through interviews with authors, curators, historians, and numerous professional cartoonists.

The enduring significance of "Calvin and Hobbes" to international cartooning was recognized by the jury of the Angoulême International Comics Festival in 2014 by the awarding of its Grand Prix to Watterson, only the fourth American to ever receive the honor (after Will Eisner, Robert Crumb, and Art Spiegelman).

In 2016, 2017 and 2019, author Berkeley Breathed included "Calvin and Hobbes" in various "Bloom County" cartoons. He launched the first cartoon on April Fool's Day 2016 and jokingly issued a statement suggesting that he had acquired "Calvin and Hobbes" from Bill Watterson, who was "out of the Arizona facility, continent and looking forward to some well-earned financial security." While bearing Watterson's signature and drawing style, as well as featuring characters from both "Calvin and Hobbes" and Breathed's "Bloom County", it is unclear whether Watterson had any input into these cartoons or not.

"Calvin and Hobbes" remains the most viewed comic on GoComics, where they cycle through the old strips with an approximately 30-year delay. 

A number of artists and cartoonists have created works portraying Calvin as a teenager or an adult; the concept has also inspired writers.

In 2011, a comic strip appeared by cartoonists Dan and Tom Heyerman called "Hobbes and Bacon". The strip depicts Calvin as an adult, married to Susie Derkins, with a young daughter named after philosopher Francis Bacon, to whom Calvin gives Hobbes. Though consisting of only four strips originally, "Hobbes and Bacon" received considerable attention when it appeared and was continued by other cartoonists and artists.

A novel entitled "Calvin" by CLA Young Adult Book Award–winning author Martine Leavitt was published in 2015. The story tells of seventeen-year-old Calvin—who was born on the day that "Calvin and Hobbes" ended, and who has now been diagnosed with schizophrenia—and his hallucination of Hobbes, his childhood stuffed tiger. With his friend Susie, who might also be a hallucination, Calvin sets off to find Bill Watterson, in the hope that the cartoonist can provide aid for Calvin's condition.




</doc>
<doc id="6060" url="https://en.wikipedia.org/wiki?curid=6060" title="Campaign for Real Ale">
Campaign for Real Ale

The Campaign for Real Ale (CAMRA) is an independent voluntary consumer organisation headquartered in St Albans, England, which promotes real ale, real cider and the traditional British pub. With over 191,000 members, it is now the largest single-issue consumer group in the UK, and is a founding member of the European Beer Consumers Union (EBCU).

The organisation was founded in 1971 in Kruger's bar in Dunquin, Kerry, Ireland by Michael Hardman, Graham Lees, Jim Makin, and Bill Mellor, who were opposed to the growing mass production of beer and the homogenisation of the British brewing industry. The original name was the Campaign for the Revitalisation of Ale. Following the formation of the Campaign, the first annual general meeting took place in 1972, at the Rose Inn in Coton Road, Nuneaton. Early membership consisted of the four founders and their friends. Yet interest in CAMRA and its objectives spread rapidly, with 5,000 members signed up by 1973. Other early influential members included Christopher Hutt, author of "Death of the English Pub", who succeeded Hardman as chairman, Frank Baillie, author of "The Beer Drinker's Companion", and later the many times "Good Beer Guide" editor, Roger Protz.

On 31 March 2016, founder Michael Hardman returned to chair a Revitalisation Project Steering Group. The aim of the Revitalisation Project was to review the organisation's purpose and future direction. Consultation meetings took place in the spring and summer of 2016, and further discussion took place at the Bournemouth AGM and Conference in spring 2017 leading to possible refinement of proposals then subject to a final vote of all members at spring 2018's AGM and Conference. At that 2018 meeting all but one of the proposed special resolutions were passed by voting, which all members could participate in. The failed special resolution related to a proposal for the Campaign to act as the voice and represent the interests of all pub-goers and beer, cider and perry drinkers.

CAMRA's stated aims are:

CAMRA's campaigns include promoting small brewing and pub businesses, reforming licensing laws, reducing tax on beer, and stopping continued consolidation among local British brewers. It also makes an effort to promote less common varieties of beer, including stout, porter, and mild, as well as traditional cider and perry.

CAMRA does not support the promotion and sale of keg based craft beer in the UK. CAMRA's Internal Policy document states that real ale can only be served without the use of additional carbonation. This policy means that "any beer brand which is produced in both cask and keg versions" is not admitted to CAMRA festivals if the brewery's marketing is deemed to imply an equivalence of quality or character between the two versions.

In 2009, CAMRA announced that it had reached the 100,000 members mark and subsequently went on to pass the 150,000 members mark in 2013. Member benefits include a monthly newspaper, "What's Brewing" and a quarterly "BEER" magazine, and free or reduced price admission to CAMRA-organised beer festivals. In recent times CAMRA has obtained benefits for its members from some commercial organisations and increasingly some licensed premises offer members price reductions on real ale (and sometimes cider and perry).

CAMRA is organised on a federal basis, with numerous independent local branches, each covering a particular geographical area of the UK, that contribute to the central body of the organisation based in St Albans. It is governed by a voluntary unpaid national executive, elected by the membership. The local branches are grouped into 16 regions across the UK, such as the West Midlands or Wessex.

CAMRA publishes the "Good Beer Guide", an annually compiled directory of its recommended pubs and brewers; the "Good Cider Guide", an occasionally compiled directory of pubs that sell cider; the "Good Bottled Beer Guide", an occasionally compiled review of real ale in a bottle.
CAMRA members receive a monthly newspaper called "What's Brewing" and a quarterly colour magazine called "Beer".
It also runs the Great British Beer Festival, a yearly event held in London at which a large selection of cask ales and ciders are tasted. It also maintains a National Inventory of Historic Pub Interiors to help bring greater recognition and protection to Britain's most historic pubs. In 2013 CAMRA launched public access to a national pub database website called Whatpub.com.

CAMRA supports and promotes beer and cider festivals around the country, which are organised by local CAMRA branches. Generally, each festival charges an entry fee which either covers entry only or also includes a commemorative glass showing the details of the festival. A festival programme is usually also provided, with a list and description of the drinks available. Members often get discounted or free entrance to CAMRA festivals.

The Campaign also organises the annual Great British Beer Festival in August. It is now held in the Great, National & West Halls at the Olympia Exhibition Centre, in Kensington, London, having been held for a few years at Earl's Court as well as regionally in the past at venues such as Brighton and Leeds.

CAMRA presents awards for beers and pubs, such as the National Pub of the Year, in which approximately 4,000 active CAMRA members from 200 local branches vote for their favourite pub of the year. The branch winners are entered into 16 regional competitions which are then visited by several individuals who select the ones they like best. There are also the Pub Design Awards, which are held in association with English Heritage and the Victorian Society. These comprise several categories, including new build, refurbished and converted pubs. The best known CAMRA award is the Champion Beer of Britain, which is selected at the Great British Beer Festival, other awards include the Champion Beer of Scotland and the Champion Beer of Wales.

CAMRA developed the National Beer Scoring Scheme (NBSS) as an easy to use scheme for judging beer quality in pubs, to assist CAMRA branches in selecting pubs for the "Good Beer Guide". The person filling in the form records their name, date, the pub, the beer and the score. CAMRA members may also input their beer scores on line via the CAMRA website Whatpub.com.

The CAMRA Pub Heritage Group identifies, records and helps to protect pub interiors of historic and/or architectural importance, and seeks to get them listed.

The group maintains two inventories of Heritage pubs, the National Inventory (NI), which contains only those pubs that have been maintained in their original condition (or have been modified very little) for at least thirty years, but usually since at least World War II. The second, larger, inventory is the Regional Inventory (RI), which is broken down by county and contains both those pubs listed in the NI and other pubs that are not eligible for the NI, for reasons such as having been overly modified, but are still considered historically important, or have particular architectural value.

The NI contains 289 pubs .

The LocAle scheme was launched in 2007 to promote locally brewed beers. The scheme functions slightly differently in each area, and is managed by each branch, but each is similar: if the beer is to be promoted as a LocAle it must come from a brewery within a predetermined number of miles set by each CAMRA branch, generally around 20, although the North London branch has set it at 30 miles from brewery to pub, even if it comes from a distribution centre further away; in addition, each participating pub must keep at least one LocAle for sale at all times.

CAMRA members may join the CAMRA Members' Investment Club which, since 1989, has invested in real ale breweries and pub chains, although all investors must be CAMRA members.




</doc>
<doc id="6061" url="https://en.wikipedia.org/wiki?curid=6061" title="CNO cycle">
CNO cycle

The CNO cycle (for carbon–nitrogen–oxygen) is one of the two known sets of fusion reactions by which stars convert hydrogen to helium, the other being the proton–proton chain reaction (pp-chain reaction). Unlike the latter, the CNO cycle is a catalytic cycle. It is dominant in stars that are more than 1.3 times as massive as the Sun.

In the CNO cycle, four protons fuse, using carbon, nitrogen, and oxygen isotopes as catalysts, to produce one alpha particle, two positrons and two electron neutrinos. Although there are various paths and catalysts involved in the CNO cycles, all these cycles have the same net result:

The positrons will almost instantly annihilate with electrons, releasing energy in the form of gamma rays. The neutrinos escape from the star carrying away some energy. One nucleus goes on to become carbon, nitrogen, and oxygen isotopes through a number of transformations in an endless loop.

The proton–proton chain is more prominent in stars the mass of the Sun or less. This difference stems from temperature dependency differences between the two reactions; pp-chain reaction starts at temperatures around (4 megakelvin), making it the dominant energy source in smaller stars. A self-maintaining CNO chain starts at approximately , but its energy output rises much more rapidly with increasing temperatures so that it becomes the dominant source of energy at approximately .
The Sun has a core temperature of around , and only of nuclei produced in the Sun are
born in the CNO cycle. The CNO-I process was independently proposed by Carl von Weizsäcker and Hans Bethe in the late 1930s.

Under typical conditions found in stars, catalytic hydrogen burning by the CNO cycles is limited by proton captures. Specifically, the timescale for beta decay of the radioactive nuclei produced is faster than the timescale for fusion. Because of the long timescales involved, the cold CNO cycles convert hydrogen to helium slowly, allowing them to power stars in quiescent equilibrium for many years.

The first proposed catalytic cycle for the conversion of hydrogen into helium was initially called the carbon–nitrogen cycle (CN-cycle), also referred to as the Bethe–Weizsäcker cycle in honor of the independent work of Carl von Weizsäcker in 1937-38 and Hans Bethe. Bethe's 1939 papers on the CN-cycle drew on three earlier papers written in collaboration with Robert Bacher and Milton Stanley Livingston and which came to be known informally as "Bethe's Bible." It was considered the standard work on nuclear physics for many years and was a significant factor in his being awarded the 1967 Nobel Prize in Physics. Bethe's original calculations suggested the CN-cycle was the Sun's primary source of energy. This conclusion arose from what is now-known as a mistaken belief: that the abundance of nitrogen in the sun is approximately 10%, when it is actually less than half a percent. The CN-cycle, named as it contains no stable isotope of oxygen involves the following cycle of transformations:  →  →  →  →  →  → . This cycle is now understood as being the first part of a larger process, the CNO-cycle, and the main reactions in this part of the cycle (CNO-I) are: 
where the carbon-12 nucleus used in the first reaction is regenerated in the last reaction. After the two positrons emitted annihilate with two ambient electrons producing an additional 2.04 MeV, the total energy released in one cycle is 26.73 MeV; in some texts, authors are erroneously including the positron annihilation energy in with the beta-decay Q-value and then neglecting the equal amount of energy released by annihilation, leading to possible confusion. All values are calculated with reference to the Atomic Mass Evaluation 2003.

The limiting (slowest) reaction in the CNO-I cycle is the proton capture on . In 2006 it was experimentally measured down to stellar energies, revising the calculated age of globular clusters by around 1 billion years.

The neutrinos emitted in beta decay will have a spectrum of energy ranges, because although momentum is conserved, the momentum can be shared in any way between the positron and neutrino, with either emitted at rest and the other taking away the full energy, or anything in between, so long as all the energy from the Q-value is used. The total momentum received by the electron and the neutrino is not great enough to cause a significant recoil of the much heavier daughter nucleus and hence, its contribution to kinetic energy of the products, for the precision of values given here, can be neglected. Thus the neutrino emitted during the decay of nitrogen-13 can have an energy from zero up to 1.20 MeV, and the neutrino emitted during the decay of oxygen-15 can have an energy from zero up to 1.73 MeV. On average, about 1.7 MeV of the total energy output is taken away by neutrinos for each loop of the cycle, leaving about 25 MeV available for producing luminosity.

In a minor branch of the above reaction, occurring in the Sun's core 0.04% of the time, the final reaction involving shown above does not produce carbon-12 and an alpha particle, but instead produces oxygen-16 and a photon and continues →→→→→→:

Like the carbon, nitrogen, and oxygen involved in the main branch, the fluorine produced in the minor branch is merely an intermediate product and at steady state, does not accumulate in the star.

This subdominant branch is significant only for massive stars. The reactions are started when one of the reactions in CNO-II results in fluorine-18 and gamma instead of nitrogen-14 and alpha, and continues →→→→→→:

Like the CNO-III, this branch is also only significant in massive stars. The reactions are started when one of the reactions in CNO-III results in fluorine-19 and gamma instead of nitrogen-15 and alpha, and continues →→→→→→:

Under conditions of higher temperature and pressure, such as those found in novae and x-ray bursts, the rate of proton captures exceeds the rate of beta-decay, pushing the burning to the proton drip line. The essential idea is that a radioactive species will capture a proton before it can beta decay, opening new nuclear burning pathways that are otherwise inaccessible. Because of the higher temperatures involved, these catalytic cycles are typically referred to as the hot CNO cycles; because the timescales are limited by beta decays instead of proton captures, they are also called the beta-limited CNO cycles.

The difference between the CNO-I cycle and the HCNO-I cycle is that captures a proton instead of decaying, leading to the total sequence →→→→→→:

The notable difference between the CNO-II cycle and the HCNO-II cycle is that captures a proton instead of decaying, and neon is produced in a subsequent reaction on , leading to the total sequence →→→→→→:

An alternative to the HCNO-II cycle is that captures a proton moving towards higher mass and using the same helium production mechanism as the CNO-IV cycle as →→→→→→:

While the total number of "catalytic" nuclei are conserved in the cycle, in stellar evolution the relative proportions of the nuclei are altered. When the cycle is run to equilibrium, the ratio of the carbon-12/carbon-13 nuclei is driven to 3.5, and nitrogen-14 becomes the most numerous nucleus, regardless of initial composition. During a star's evolution, convective mixing episodes moves material, within which the CNO cycle has operated, from the star's interior to the surface, altering the observed composition of the star. Red giant stars are observed to have lower carbon-12/carbon-13 and carbon-12/nitrogen-14 ratios than do main sequence stars, which is considered to be convincing evidence for the operation of the CNO cycle.




</doc>
<doc id="6062" url="https://en.wikipedia.org/wiki?curid=6062" title="Craps">
Craps

Craps is a dice game in which the players make wagers on the outcome of the roll, or a series of rolls, of a pair of dice. Players may wager money against each other (playing "street craps") or a bank (playing "casino craps", also known as "table craps", or often just "craps"). Because it requires little equipment, "street craps" can be played in informal settings. While shooting craps, players may use slang terminology to place bets and actions.

Craps developed in the United States from a simplification of the western European game of hazard. The origins of hazard are obscure and may date to the Crusades. Hazard was brought from London to New Orleans in approximately 1807 by the returning Bernard Xavier Philippe de Marigny de Mandeville, the young gambler and scion of a family of wealthy colonial Louisiana landowners. Although in hazard the dice shooter may choose any number from five to nine to be the main number, de Marigny simplified the game such that the main number is always seven, the optimal choice among knowledgeable hazard players. Both hazard and its new offshoot were unfamiliar and rejected by Americans of his social class, leading de Marigny to introduce his novelty to the local underclass. Fieldhands taught their friends, and deckhands carried the new game up the Mississippi River. Celebrating the popular success of his novelty, de Marigny gave the name craps to a street in his New Orleans real estate development.

The central game, called "pass", from the French word for "pace" or "step", has been gradually supplemented over the decades by many companion games which can be played simultaneously. The entire collection of over one hundred separate and independent possible games is called craps. The name "craps" was a Louisiana mispronunciation of the word "crabs", which in London had been the joint epithet for the numbers two and three, which in hazard are the only permanent instant losing numbers for wagers on Pass.

For a century after its invention, craps was abused by casinos using unfair dice. To remedy the problem, in approximately 1907, a Philadelphia dice maker named John H. Winn introduced a layout which featured bets on both Pass and Don't Pass. Most modern casinos use his innovation.

Craps exploded in popularity during World War II, which brought most young American men of every social class into the military. The street version of craps was popular among soldiers, who often played it using a blanket as a shooting surface. Their military memories led to craps becoming the dominant game in postwar Las Vegas.

Bank craps or casino craps is played by one or more players betting against the casino rather than each other. Both the players and the dealers stand around a large rectangular craps table. Sitting is discouraged by most casinos unless a player has medical reasons for requiring a seat.

Players use casino chips rather than cash to bet on the Craps "layout," a fabric surface which displays the various bets. The bets vary somewhat among casinos in availability, locations, and payouts. The tables roughly resemble bathtubs and come in various sizes. In some locations, chips may be called checks, tokens, or plaques.

Against one long side is the casino's table bank: as many as two thousand casino chips in stacks of 20. The opposite long side is usually a long mirror. The U-shaped ends of the table have duplicate layouts and standing room for approximately eight players. In the center of the layout is an additional group of bets which are used by players from both ends. The vertical walls at each end are usually covered with a rubberized target surface covered with small pyramid shapes to randomize the dice which strike them. The top edges of the table walls have one or two horizontal grooves in which players may store their reserve chips.

The table is run by up to four casino employees: a boxman seated (usually the only seated employee) behind the casino's bank, who manages the chips, supervises the dealers, and handles "coloring up" players (exchanging small chip denominations for larger denominations in order to preserve the chips at a table); two base dealers who stand to either side of the boxman and collect and pay bets to players around their half of the table; and a stickman who stands directly across the table from the boxman, takes and pays (or directs the base dealers to do so) the bets in the center of the table, announces the results of each roll (usually with a distinctive patter), and moves the dice across the layout with an elongated wooden stick.

Each employee also watches for mistakes by the others because of the sometimes large number of bets and frantic pace of the game. In smaller casinos or at quiet times of day, one or more of these employees may be missing, and have their job covered by another, or cause player capacity to be reduced.

Some smaller casinos have introduced "mini-craps" tables which are operated with only two dealers; rather than being two essentially identical sides and the center area, a single set of major bets is presented, split by the center bets. Responsibility of the dealers is adjusted: the stickman continuing to handle the center bets, and the base dealer handling the other bets as well as cash and chip exchanges.

By contrast, in "street craps", there is no marked table and often the game is played with no back-stop against which the dice are to hit. (Despite the name "street craps", this game is often played in houses, usually on an un-carpeted garage or kitchen floor.) The wagers are made in cash, never in chips, and are usually thrown down onto the ground or floor by the players. There are no attendants, and so the progress of the game, fairness of the throws, and the way that the payouts are made for winning bets are self-policed by the players.

Each casino may set which bets are offered and different payouts for them, though a core set of bets and payouts is typical. Players take turns rolling two dice and whoever is throwing the dice is called the "shooter". Players can bet on the various options by placing chips directly on the appropriately-marked sections of the layout, or asking the base dealer or stickman to do so, depending on which bet is being made.

While acting as the shooter, a player must have a bet on the "Pass" line and/or the "Don't Pass" line. "Pass" and "Don't Pass" are sometimes called "Win" and "Don't Win" or "Right" and "Wrong" bets. The game is played in rounds and these "Pass" and "Don't Pass" bets are betting on the outcome of a round. The shooter is presented with multiple dice (typically five) by the "stickman", and must choose two for the round. The remaining dice are returned to the stickman's bowl and are not used.
Each round has two phases: "come-out" and "point". Dice are passed to the left. To start a round, the shooter makes one or more "come-out" rolls. The shooter must shoot toward the farther back wall and is generally required to hit the farther back wall with both dice. Casinos may allow a few warnings before enforcing the dice to hit the back wall and are generally lenient if at least one die hits the back wall. Both dice must be tossed in one throw. If only one die is thrown the shot is invalid. A come-out roll of 2, 3 or 12 is called "craps" or "crapping out", and anyone betting the Pass line loses. On the other hand, anyone betting the Don't Pass line on come out wins with a roll of 2 or 3 and ties (pushes) if a 12 is rolled. Shooters may keep rolling after crapping out; the dice are only required to be passed if a shooter sevens out (rolls a seven after a point has been established). A come-out roll of 7 or 11 is a "natural"; the Pass line wins and Don't Pass loses. The other possible numbers are the point numbers: 4, 5, 6, 8, 9, and 10. If the shooter rolls one of these numbers on the come-out roll, this establishes the "point" – to "pass" or "win", the point number must be rolled again before a seven.

The dealer flips a button to the "On" side and moves it to the point number signifying the second phase of the round. If the shooter "hits" the point value again (any value of the dice that sum to the point will do; the shooter doesn't have to exactly repeat the exact combination of the come-out roll) before rolling a seven, the Pass line wins and a new round starts. If the shooter rolls any seven before repeating the point number (a "seven-out"), the Pass line loses, the Don't Pass line wins, and the dice pass clockwise to the next new shooter for the next round. Once a point has been established any multi-roll bet (including Pass and/or Don't Pass line bets and odds) are unaffected by the 2, 3, 11 or 12; the only numbers which affect the round are the established point, any specific bet on a number, or any 7. Any single roll bet is always affected (win or lose) by the outcome of any roll.

While the come-out roll may specifically refer to the first roll of a new shooter, any roll where no point is established may be referred to as a come-out. By this definition the start of any new round regardless if it is the shooter's first toss can be referred to as a come-out roll.

Any player can make a bet on pass or don't pass as long as a point has not been established, or come or don't come as long as a point is established. All other bets, including an increase in odds behind the pass and don't pass lines, may be made at any time. All bets other than pass line and come may be removed or reduced any time before the bet loses. This is known as "taking it down" in craps.

The maximum bet for Place, Buy, Lay, Pass and Come bets are generally equal to table maximum. Lay bet maximum are equal to the table maximum win, so if a player wishes to lay the 4 or 10, he or she may bet twice at amount of the table maximum for the win to be table maximum. Odds behind Pass, Come, Don't Pass and Don't Come may be however larger than the odds offered allows and can be greater than the table maximum in some casinos. Don't odds are capped on the maximum allowed win some casino allow the odds bet itself to be larger than the maximum bet allowed as long as the win is capped at maximum odds. Single rolls bets can be lower than the table minimum, but the maximum bet allowed is also lower than the table maximum. The maximum allowed single roll bet is based on the maximum allowed win from a single roll.

In all the above scenarios, whenever the Pass line wins, the Don't Pass line loses, and vice versa, with one exception: on the come-out roll, a roll of 12 will cause Pass Line bets to lose, but Don't Pass bets are pushed (or "barred"), neither winning nor losing. (The same applies to "Come" and "Don't Come" bets, discussed below.)

A player wishing to play craps without being the shooter should approach the craps table and first check to see if the dealer's "On" button is on any of the point numbers.


In either case, all single or multi-roll proposition bets may be placed in either of the two rounds.

Between dice rolls there is a period for dealers to make payouts and collect losing bets, after which players can place new bets. The stickman monitors the action at a table and decides when to give the shooter the dice, after which no more betting is allowed.

When joining the game, one should place money on the table rather than passing it directly to a dealer, the dealer's exaggerated movements during the process of "making change" or "change only" (converting currency to an equivalent in casino cheques) are required so that any disputes can be later reviewed against security camera footage.

The dealers will insist that the shooter roll with one hand and that the dice bounce off the far wall surrounding the table. These requirements are meant to keep the game fair (preventing switching the dice or making a "controlled shot"). If a die leaves the table, the shooter will usually be asked to select another die from the remaining three but can request permission to use the same die if it passes the boxman's inspection. This requirement exists to keep the game fair and reduce the chance of loaded dice.

There are many local variants of the calls made by the stickman for rolls during a craps game. These frequently incorporate a reminder to the dealers as to which bets to pay or collect.


Rolls of 4, 6, 8, and 10 are called "hard" or "easy" (e.g. "six the hard way", "easy eight", "hard ten") depending on whether they were rolled as a "double" or as any other combination of values, because of their significance in center table bets known as the "hard ways". Hard way rolls are so named because there is only one way to roll them (i.e., the value on each die is the same when the number is rolled). Consequently, it is more likely to roll the number in combinations (easy) rather than as a double (hard).

The shooter is required to make either a pass line bet or a Don't Pass bet if he wants to shoot. On the come out roll each player may only make one bet on the Pass or Don't Pass, but may bet both if desired. The Pass Line and Don't Pass bet is optional for any player not shooting. In rare cases, some casinos require all players to make a minimum Pass Line or Don't Pass bet (if they want to make any other bet), whether they are currently shooting or not.

The fundamental bet in craps is the pass line bet, which is a bet for the shooter to win. This bet must be at least the table minimum and at most the table maximum.
The pass line bet pays even money.

The pass line bet is a contract bet. Once a pass line bet is made, it is always working and cannot be turned "Off", taken down, or reduced until a decision is reached – the point is made, or the shooter sevens out. A player may increase any corresponding odds (up to the table limit) behind the Pass line at any time after a point is established. Players may only bet the pass line on the come out roll when no point has been established, unless the casino allows put betting where the player can bet Pass line or increase an existing Pass line bet whenever desired and may take odds immediately if the point is already on.

A don't pass bet is a bet for the shooter to lose ("seven out, line away") and is almost the opposite of the pass line bet. Like the Pass bet, this bet must be at least the table minimum and at most the table maximum.
The don't pass bet pays even money.

The don't pass bet is a no-contract bet. After a point is established, a player may take down or reduce a don't pass bet and any corresponding odds at any time because odds of rolling a 7 before the point is in the player's favor. Once taken down or reduced, however, the don't pass bet may not be restored or increased. Because the shooter must have a line bet the shooter generally may not reduce a don't pass bet below the table minimum. In Las Vegas, a majority of casinos will allow the shooter to move the bet to the pass line in lieu of taking it down, however in other areas such as Pennsylvania and Atlantic City, this is not allowed. Even though players are allowed to remove the don't pass line bet after a point has been established, the bet cannot be turned "Off" without being removed. If a player chooses to remove the don't pass line bet, he or she can no longer lay odds behind the don't pass line. The player can, however, still make standard lay bets on any of the point numbers (4, 5, 6, 8, 9, 10).

There are two different ways to calculate the odds and house edge of this bet. The table below gives the numbers considering that the game ends in a push when a 12 is rolled, rather than being undetermined. Betting on don't pass is often called "playing the dark side", and it is considered by some players to be in poor taste, or even taboo, because it goes directly against conventional play, winning when most of the players lose.

If a 4, 5, 6, 8, 9, or 10 is thrown on the come-out roll (i.e., if a point is established), most casinos allow pass line players to take odds by placing up to some predetermined multiple of the pass line bet, behind the pass line. This additional bet wins if the point is rolled again before a 7 is rolled (the point is made) and pays at the true odds of 2-to-1 if 4 or 10 is the point, 3-to-2 if 5 or 9 is the point, or 6-to-5 if 6 or 8 is the point. Unlike the pass line bet itself, the pass line odds bet can be turned "Off" (not working), removed or reduced anytime before it loses. In Las Vegas, generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine odds and pass bet must be table minimum so players can bet the minimum single unit on odds depending on the point. If the point is a 4 or 10 players can bet as little as $1 on odds if the table minimum is low such as is $5, $10 or $15. If the player requests the pass odds be not working ("Off") and the shooter sevens-out or hits the point, the pass line bet will be lost or doubled and the pass odds returned.

Individual casinos (and sometimes tables within a casino) vary greatly in the maximum odds they offer, from single or double odds (one or two times the pass line bet) up to 100x or even unlimited odds. A variation often seen is "3-4-5X Odds", where the maximum allowed odds bet depends on the point: three times if the point is 4 or 10; four times on points of 5 or 9; or five times on points of 6 or 8. This rule simplifies the calculation of winnings: a maximum pass odds bet on a 3–4–5× table will always be paid at six times the pass line bet regardless of the point.

As odds bets are paid at true odds, in contrast with the pass line which is always even money, taking odds on a minimum pass line bet lessens the house advantage compared with betting the same total amount on the pass line only. A maximum odds bet on a minimum pass line bet often gives the lowest house edge available in any game in the casino. However, the odds bet cannot be made independently, so the house retains an edge on the pass line bet itself.

If a player is playing don't pass instead of pass, they may also lay odds by placing chips behind the don't pass line. If a 7 comes before the point is rolled, the odds pay at true odds of 1-to-2 if 4 or 10 is the point, 2-to-3 if 5 or 9 is the point, or 5-to-6 if 6 or 8 is the point. Typically the maximum lay bet will be expressed such that a player may win up to an amount equal to the maximum odds multiple at the table. If a player lays maximum odds with a point of 4 or 10 on a table offering five-times odds, he would be able to lay a maximum of ten times the amount of his Don't Pass bet. At 5x odds table, the maximum amount the combined bet can win will always be 6x the amount of the Don't Pass bet. Players can bet table minimum odds if desired and win less than table minimum. Like the Don't Pass bet the odds can be removed or reduced. Unlike the don't pass bet itself, the don't pass odds can be turned "Off" (not working). In Las Vegas generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine lay odds and Don't Pass bet must be table minimum so players may bet as little as the minimum two units on odds depending on the point. If the point is a 4 or 10 players can bet as little as $2 if the table minimum is low such as $5, $10 or $15 tables. If the player requests the don't pass odds to be not working ("Off") and the shooter hits the point or sevens-out, the don't pass bet will be lost or doubled and the don't pass odds returned. Unlike a standard lay bet on a point, lay odds behind the Don't Pass line does not charge commission (vig).

A Come bet can be visualized as starting an entirely new pass line bet, unique to that player. Like the Pass Line each player may only make one Come bet per roll, this does not exclude a player from betting odds on an already established Come point. This bet must be at least the table minimum and at most the table maximum. Players may bet both the Come and Don't Come on the same roll if desired. Come bets can only be made after a point has been established since, on the come-out roll, a Come bet would be the same thing as a pass line bet. A player making a Come bet will bet on the first point number that "comes" from the shooter's next roll, regardless of the table's round. If a 7 or 11 is rolled on the first round, it wins. If a 2, 3, or 12 is rolled, it loses. If instead the roll is 4, 5, 6, 8, 9, or 10, the Come bet will be moved by the base dealer onto a box representing the number the shooter threw. This number becomes the "come-bet point" and the player is allowed to take odds, just like a pass line bet. Also like a pass line bet, the come bet is a contract bet and is always working, and cannot be turned "Off", removed or reduced until it wins or loses. However, the odds taken behind a Come bet can be turned "Off" (not working), removed or reduced anytime before the bet loses. In Las Vegas generally odds bets are required to be the table minimum. In Atlantic City and Pennsylvania, the combine odds and pass bet must be table minimum so players can bet the minimum single unit depending on the point. If the point is a 4 or 10 players can bet as little as $1 if the table minimum is low such as $5, $10 or $15 minimums. If the player requests the Come odds to be not working ("Off") and the shooter sevens-out or hits the Come bet point, the Come bet will be lost or doubled and the Come odds returned. If the casino allows put betting a player may increase a Come bet after a point has been established and bet larger odds behind if desired. Put betting also allows a player to bet on a Come and take odds immediately on a point number without a Come bet point being established.

The dealer will place the odds on top of the come bet, but slightly off center in order to differentiate between the original bet and the odds. The second round wins if the shooter rolls the come bet point again before a seven. Winning come bets are paid the same as winning pass line bets: even money for the original bet and true odds for the odds bet. If, instead, the seven is rolled before the come-bet point, the come bet (and any odds bet) loses.

Because of the come bet, if the shooter makes their point, a player can find themselves in the situation where they still have a come bet (possibly with odds on it) and the next roll is a come-out roll. In this situation, odds bets on the come wagers are usually presumed to be not working for the come-out roll. That means that if the shooter rolls a 7 on the come-out roll, any players with active come bets waiting for a come-bet point lose their initial wager but will have their odds bets returned to them.

If the come-bet point is rolled on the come-out roll, the odds do not win but the come bet does and the odds bet is returned (along with the come bet and its payoff). The player can tell the dealer that they want their odds working, such that if the shooter rolls a number that matches the come point, the odds bet will win along with the come bet, and if a seven is rolled, both lose.

Many players will use a come bet as "insurance" against sevening out: if the shooter rolls a seven, the come bet pays 1:1, offsetting the loss of the pass line bet. The risk in this strategy is the situation where the shooter does not hit a seven for several rolls, leading to multiple come bets that will be lost if the shooter eventually sevens out.

In the same way that a come bet is similar to a pass line bet, a don't come bet is similar to a don't pass bet. Like the come, the don't come can only be bet after a point has already been established as it is the same as a don't pass line bet when no point is established. This bet must be at least the table minimum and at most the table maximum. A don't come bet is played in two rounds. If a 2 or 3 is rolled in the first round, it wins. If a 7 or 11 is rolled, it loses. If a 12 is rolled, it is a push (subject to the same 2/12 switch described above for the don't pass bet). If, instead, the roll is 4, 5, 6, 8, 9, or 10, the don't come bet will be moved by the base dealer onto a box representing the number the shooter threw. The second round wins if the shooter rolls a seven before the don't come point. Like the Don't Pass each player may only make one Don't Come bet per roll, this does not exclude a player from laying odds on an already established Don't Come points. Players may bet both the Don't Come and Come on the same roll if desired.

The player may lay odds on a don't come bet, just like a don't pass bet; in this case, the dealer (not the player) places the odds bet on top of the bet in the box, because of limited space, slightly offset to signify that it is an odds bet and not part of the original don't come bet. Lay odds behind a Don't Come are subject to the same rules as Don't Pass lay odds. Unlike a standard lay bet on a point, lay odds behind a don't come point does not charge commission (vig) and gives the player true odds. Like the don't pass line bet, don't come bets are no-contract, and can be removed or reduced after a don't come point has been established, but cannot be turned off ("not working") without being removed. A player may also call, "No Action" when a point is established, and the bet will not be moved to its point. This play is not to the player's advantage. If the bet is removed, the player can no longer lay odds behind the don't come point and cannot restore or increase the same don't come bet. Players must wait until next roll as long as a pass line point has been established (players cannot bet don't come on come out rolls) before they can make a new don't come bet. Las Vegas casinos which allow put betting allows players to move the Don't Come directly to any Come point as a put, however this is not allowed in Atlantic City or Pennsylvania. Unlike the don't come bet itself, the don't come odds can be turned "Off" (not working), removed or reduced if desired. In Las Vegas, players generally must lay at least table minimum on odds if desired and win less than table minimum, in Atlantic City and Pennsylvania players combined bet must be at least table minimum, so depending on the point number players may lay as little as 2 minimum units (e.g. 4 and 10). If the player requests the don't come odds be not working ("Off") and the shooter hits the don't come point or sevens-out, the don't come bet will be lost or doubled and the don't come odds returned.

Winning don't come bets are paid the same as winning don't pass bets: even money for the original bet and true odds for the odds lay. Unlike come bets, the odds laid behind points established by don't come bets are always working including come out rolls unless the player specifies otherwise.

These are bets that may not be settled on the first roll and may need any number of subsequent rolls before an outcome is determined.
Most multi-roll bets may fall into the situation where a point is made by the shooter before the outcome of the multi-roll bet is decided. These bets are often considered "not working" on the new come-out roll until the next point is established, unless the player calls the bet as "working."

Casino rules vary on this; some of these bets may not be callable, while others may be considered "working" during the come-out. Dealers will usually announce if bets are working unless otherwise called off. If a non-working point number placed, bought or laid becomes the new point as the result of a come-out, the bet is usually refunded, or can be moved to another number for free.

Players can bet any point number (4, 5, 6, 8, 9, 10) by placing their wager in the come area and telling the dealer how much and on what number(s), "30 on the 6", "5 on the 5" or "25 on the 10". These are typically "Place Bets to Win". These are bets that the number bet on will be rolled before a 7 is rolled. These bets are considered working bets, and will continue to be paid out each time a shooter rolls the number bet. By rules, place bets are not working on the come out roll but can be "turned on" by the player. Players may remove or reduce (bet must be at least table minimum) this bet anytime before it loses (seven out).

Place bets to win payout at slightly worse than the true odds: 9-to-5 on points 4 or 10, 7-to-5 on points 5 or 9, and 7-to-6 on points 6 or 8. The place bets on the outside numbers (4,5,9,10) should be made in units of $5, (on a $5 minimum table), in order to receive the correct exact payout of $5 paying $7 or $5 paying $9. The place bets on the 6 & 8 should be made in units of $6, (on a $5 minimum table), in order to receive the correct exact payout of $6 paying $7. For the 4 and 10, it is to the player's advantage to 'buy' the bet (see below).

There are also "Place Bets to Lose". Rarely casinos offer these bets. This bet is the opposite of the place bet to win and pays off if a 7 is rolled before the specific point number. The place bet to lose typically carries a lower house edge than a place bet to win. Payouts are 4–5 on points 6 or 8, 5–8 on 5 or 9, and 5–11 on 4 or 10.

In most cases, there are other methods to bet on point numbers than placing them, depending how the casino charges commissions and what bets are allowed. See Buy, Lay, and Put bets

Players can also buy a bet which are paid at true odds, but a 5% commission is charged on the amount of the bet. Buy bets are placed with the shooter betting at a specific number will come out before a player sevens out. The buy bet must be at least table minimum excluding commission, however some casinos require the minimum buy bet amount to be at least $20 to match the $1 charged on the 5% commission. Traditionally, the buy bet commission is paid no matter what, but in recent years a number of casinos have changed their policy to charge the commission only when the buy bet wins. Some casinos charge the commission as a one-time fee to buy the number; payouts are then always at true odds. Most casinos usually charge only $1 for a $25 green-chip bet (4% commission), or $2 for $50 (two green chips), reducing the house advantage a bit more. Players may remove or reduce this bet (bet must be at least table minimum excluding vig) anytime before it loses. Buy bets like place bets are not working when no point has been established unless the player specifies otherwise.

Where commission is charged only on wins, the commission is often deducted from the winning payoff—a winning $25 buy bet on the 10 would pay $49, for instance. The house edges stated in the table assume the commission is charged on all bets. They are reduced by at least a factor of two if commission is charged on winning bets only.

A lay bet is the opposite of a buy bet, where a player bets on a 7 to roll before the number that is laid. Players may only lay the 4, 5, 6, 8, 9 or 10 and may lay multiple numbers if desired. Just like the buy bet lay bets pay true odds, but because the lay bet is the opposite of the buy bet, the payout is reversed. Therefore, players get 1 to 2 for the numbers 4 and 10, 2 to 3 for the numbers 5 and 9, and 5 to 6 for the numbers 6 and 8. A 5% commission (vigorish, vig, juice) is charged up front on the possible winning amount. For example: A $40 Lay Bet on the 4 would pay $20 on a win. The 5% vig would be $1 based on the $20 win. (not $2 based on the $40 bet as the way buy bet commissions are figured.) Like the buy bet the commission is adjusted to suit the betting unit such that fraction of a dollar payouts are not needed. Casinos may charge the vig up front thereby requiring the player to pay a vig win or lose, other casinos may only take the vig if the bet wins. Taking vig only on wins lowers house edge. Players may removed or reduce this bet (bet must be at least table minimum) anytime before it loses. Some casinos in Las Vegas allow players to lay table minimum plus vig if desired and win less than table minimum. Lay bet maximums are equal to the table maximum win, so if a player wishes to lay the 4 or 10, he or she may bet twice at amount of the table maximum for the win to be table maximum. Other casinos require the minimum bet to win at $20 even at the lowest minimum tables in order to match the $1 vig, this requires a $40 bet. Similar to buy betting, some casinos only take commission on win reducing house edge. Unlike place and buy bets, lay bets are always working even when no point has been established. The player must specify otherwise if he or she wishes to have the bet not working.

If a player is unsure of whether a bet is a single or multi-roll bet, it can be noted that all single-roll bets will be displayed on the playing surface in one color (usually red), while all multi-roll bets will be displayed in a different color (usually yellow).

A put bet is a bet which allows players to increase or make a Pass line bet after a point has been established (after come-out roll). Players may make a put bet on the Pass line and take odds immediately or increase odds behind if a player decides to add money to an already existing Pass line bet. Put betting also allows players to increase an existing come bet for additional odds after a come point has been established or make a new come bet and take odds immediately behind if desired without a come bet point being established. If increased or added put bets on the Pass line and Come cannot be turned "Off", removed or reduced, but odds bet behind can be turned "Off", removed or reduced. The odds bet is generally required to be the table minimum. Player cannot put bet the Don't Pass or Don't Come. Put betting may give a larger house edge over place betting unless the casino offers high odds.

Put bets are generally allowed in Las Vegas, but not allowed in Atlantic City and Pennsylvania.

Put bets are better than place bets (to win) when betting more than 5-times odds over the flat bet portion of the put bet. For example, a player wants a $30 bet on the six. Looking at two possible bets: 1) Place the six, or 2) Put the six with odds. A $30 place bet on the six pays $35 if it wins. A $30 put bet would be a $5 flat line bet plus $25 (5-times) in odds, and also would pay $35 if it wins. Now, with a $60 bet on the six, the place bet wins $70, where the put bet ($5 + $55 in odds) would pay $71. The player needs to be at a table which not only allows put bets, but also high-times odds, to take this advantage.

This bet can only be placed on the numbers 4, 6, 8, and 10. In order for this bet to win, the chosen number must be rolled the "hard way" (as doubles) before a 7 or any other non-double combination ("easy way") totaling that number is rolled. For example, a player who bets a hard 6 can only win by seeing a 3–3 roll come up before any 7 or any easy roll totaling 6 (4–2 or 5–1); otherwise, he/she loses.

In Las Vegas casinos, this bet is generally working, including when no point has been established, unless the player specifies otherwise. In other casinos such as those in Atlantic City, hard ways are not working when the point is off unless the player requests to have it working on the come out roll.

Like single-roll bets, hard way bets can be lower than the table minimum; however, the maximum bet allowed is also lower than the table maximum. The minimum hard way bet can be a minimum one unit. For example, lower stake table minimums of $5 or $10, generally allow minimum hard ways bets of $1. The maximum bet is based on the maximum allowed win from a single roll.

Easy way is not a specific bet offered in standard casinos, but a term used to define any number combination which has two ways to roll. For example, (6–4, 4–6) would be a "10 easy". The 4, 6, 8 or 10 can be made both hard and easy ways. Betting point numbers (which pays off on easy or hard rolls of that number) or single-roll ("hop") bets (e.g., "hop the 2–4" is a bet for the next roll to be an easy six rolled as a two and four) are methods of betting easy ways.

A player can choose either the 6 or 8 being rolled before the shooter throws a seven. These wagers are usually avoided by experienced craps players since they pay even money (1:1) while a player can make place bets on the 6 or the 8, which pay more (7:6). Some casinos (especially all those in Atlantic City) do not even offer the Big 6 & 8. The bets are located in the corners behind the pass line, and bets may be placed directly by players.

The only real advantage offered by the Big 6 & 8 is that they can be bet for the table minimum, whereas a place bet minimum may sometimes be greater than the table minimum (e.g. $6 place bet on a $3 minimum game.) In addition place bets are usually not working, except by agreement, when the shooter is "coming out" i.e. shooting for a point, and Big 6 and 8 bets always work. Some modern layouts no longer show the Big 6/Big 8 bet.

Single-roll (proposition) bets are resolved in one dice roll by the shooter. Most of these are called "Service Bets", and they are located at the center of most craps tables. Only the stickman or a dealer can place a service bet. Single rolls bets can be lower than the table minimum, but the maximum bet allowed is also lower than the table maximum. The maximum bet is based on the maximum allowed win from a single roll. The lowest single roll bet can be a minimum one unit bet. For example, tables with minimums of $5 or $10 generally allow minimum single roll bets of $1. Single bets are always working by default unless the player specifies otherwise. The bets include:

2 (snake eyes, or Aces): Wins if shooter rolls a 2.

3 (ace-deuce): Wins if the shooter rolls a 3.

Yo: Wins if the shooter rolls 11.

12 (boxcars, midnight, or cornrows): Wins if shooter rolls a 12.

2 or 12 (hi-lo): Wins if shooter rolls a 2 or 12. The stickman places this bet on the line dividing the 2 and 12 bets.

Any Craps (Three-Way): Wins if the shooter rolls 2, 3 or 12.

C & E: A combined bet, a player is betting half their bet on craps (2,3,12) and the other half on 11 (yo). The combine payout is 3:1 on craps and 7:1 on 11 (yo). Another method of calculating the payout is to divide the total bet in half. The player would receive 7:1 minus half the total bet payout on half the total bet for craps and 15:1 minus half the total bet payout on half the total bet for 11 (yo). For example, using this method if a player were to bet $2 on C & E, $1 would receive 7:1 payout on craps minus $1 for the bet on 11 so the total profit would be $6. If an 11 was rolled the player would receive 15:1 minus $1 for the bet on craps so the player's total profit is $14. Both methods of calculation yield the same result so either method can be used. If a player wishes to take the bet down after a win the player would receive the whole bet not half even though only one of the two bets can win per roll. The minimum bet on C & E is double the lowest unit bet allowed at the table. So if the minimum single roll bet is $1 the lowest C & E bet allowed would be $2. Players are, however, able to make odd number bets larger than $2 if desired. One of the two bets will always lose, the other may win.

Any seven: Wins if the shooter rolls a 7 with 4:1 payout. This bet is also nicknamed Big Red, since the 7 on its betting space on the layout is usually large and red, and it is considered bad luck and a breach of etiquette to speak the word "seven" at the table.

Horn: This is a bet that involves betting on 1 unit each for 2, 3, 11 and 12 at the same time for the next roll. The bet is actually four separate bets, and pays off depending on which number is actually rolled. The combine payout is 27:4 for 2, 12 and 3:1 for 3, 11. Each individual bet has the same payout as a single bet on the specific numbers, 30:1 for 2 and 12 minus the other three bets, 15:1 for 3 and 11 minus the other three bets. If a player wins the bet he can take down all four bets instead of a single bet even though only one bet can win per roll. Many players, in order to eliminate the confusion of tossing four chips to the center of the table or having change made while bets are being placed, will make a five-unit Horn High bet, which is a four-way bet with the extra unit going to one specific number. For example, if one tosses a $5 chip into the center and says "horn high yo", you are placing four $1 bets on each of the horn numbers and the extra dollar will go on the yo (11). Horn bets are generally required to be in multiples of 4 or 5 with the minimum bet being 4 times the minimum unit allowed. For example, if the single roll minimum at the table is $1 the Horn bet must be $4 or more.

Whirl or World: A five-unit bet that is a combination of a horn and any-seven bet, with the idea that if a seven is rolled the bet is a push, because the money won on the seven is lost on the horn portions of the bet. The combine odds are 26:5 on the 2, 12, 11:5 on the 3, 11, and a push on the 7. Like the C & E and Horn bet, if a player wishes to take down the bet after a win he or she would receive all five units back. The minimum bet is five of the minimum units. For example, if the minimum single roll bet is $1, the minimum World/Whirl bet is $5.

On the Hop, Hop or Hopping: A single roll bet on any particular combination of the two dice on the next roll including combinations whose sum is 7 (e.g. 4 and 3). For example, if you bet on "5 and 1" on the hop, you are betting that the next roll will have a 5 on one die and a 1 on the other die. The bet pays 15:1 on easy ways (same as a bet on 3 or 11). Hard ways hop pays 30:1 (e.g., 3 and 3 on the hop, same as a bet on 2 or 12). The true odds are 17:1 and 35:1, resulting in a house edge of 11.11% and 13.89% respectively. When presented, hop bets are located at the center of the craps layout with the other proposition bets. If hop bets are not on the craps layout, they still may be bet on by players but they become the responsibility of the boxman to book the bet. Sometimes players may request to hop a whole number. In this case the money on the bet different combinations. For example, if a player says "hop the tens" (6–4, 5–5, 4–6) the player must give the dealer an even number bet so it can be divided among the hard and easy ways. If the player gives $10, $5 would be placed on the easy ways 10 with 15:1 odds and $5 would be placed on the hard way with 30:1 odds. If a player wishes to "hop the sevens" there would be three different combinations and six possible ways to roll a 7 (6–1, 5–2, 4–3, 3–4, 2–5, 1–6) therefore the player should bet in multiples of 3 so the bet can be divided among each combination with a 15:1 payout minus the other two bets, otherwise if players does not bet in multiples of 3, they would specific which combination has additional units.

Field: This bet is a wager that one of the numbers 2, 3, 4, 9, 10, 11, or 12 will appear on the next roll of the dice. This bet typically pays more (2:1 or 3:1) if 2 or 12 is rolled, and 1:1 if 3, 4, 9, 10 or 11 is rolled.
The Field bet is a "Self-Service" Bet. Unlike the other proposition bets which are handled by the dealers or stickman, the field bet is placed directly by the player.

Players identify their Field bets by placing them in the Field area directly in front of them or as close to their position as possible. The initial bet and/or any payouts can "ride" through several rolls until they lose, and are assumed to be "riding" by dealers. It is thus the player's responsibility to collect their bet and/or winnings immediately upon payout, before the next dice roll, if they do not wish to let it ride.

Fire Bet: A registered trademark owned by SHFL entertainment. Before the shooter begins, some casinos will allow a bet known as a fire bet to be placed. A fire bet is a bet of as little as $1 and generally up to a maximum of $5 to $10 sometimes higher, depending on casino, made in the hope that the next shooter will have a hot streak of setting and getting many points of different values. As different individual points are made by the shooter, they will be marked on the craps layout with a fire symbol.

The first three points will not pay out on the fire bet, but the fourth, fifth and sixth will pay out at increasing odds. The fourth point pays at 24-to-1, the fifth point pays at 249-to-1 and the 6th point pays at 999-to-1. Note that the points must all be different numbers for them to count towards the fire bet. For example, a shooter who successfully hits a point of 10 twice will only garner credit for the first one on the fire bet. Players must hit the established point in order for it to count toward the fire bet. The payout is determine by the number of points which have been established and hit after the shooter sevens out.

Bonus Craps: Is a registered trademark owned by Galaxy Gaming. Prior to the initial "come out roll", players may place an optional wager (usually a $1 minimum to a maximum $25) on one or more of the three Bonus Craps wagers, "All Small", "All Tall", or "All or Nothing at All." For players to win the "All Small" wager, the shooter must hit all five small numbers (2, 3, 4, 5, 6) before a seven is rolled; similarly, "All Tall" wins if all five high numbers (8, 9, 10, 11, 12) are hit before a seven is rolled.

These bets pay 35-for-1, for a house advantage of 7.76%. “All or Nothing at All” wins if the shooter hits all 10 numbers before a seven is rolled. This pays 176-for-1, for a house edge of 7.46%. For all three wagers, the order in which the numbers are hit does not matter. Whenever a seven is hit, including on the come out roll, all bonus bets lose, the bonus board is reset, and new bonus bets may be placed.

A player may wish to make multiple different bets. For example, a player may be wish to bet $1 on all hard ways and the horn. If one of the bets win the dealer may automatically replenish the losing bet with profits from the winning bet. In this example, if the shooter rolls a hard 8 (pays 9:1), the horn loses. The dealer may return $5 to the player and place the other $4 on the horn bet which lost. If the player does not want the bet replenished, he or she should request any or all bets be taken down.

A working bet is a live bet. Bets may also be on the board, but not in play and therefore not working. Pass line and come bets are always working meaning the chips are in play and the player is therefore wagering live money. Other bets may be working or not working depending whether a point has been established or player's choice. Place and buy bets are working by default when a point is established and not working when the point is off unless the player specifies otherwise. Lay bets are always working even if a point has not been established unless the player requests otherwise. At any time, a player may wish to take any bet or bets out of play. The dealer will put an "Off" button on the player's specific bet or bets; this allows the player to keep his chips on the board without a live wager. For example, if a player decides not to wager a place bet mid-roll but wishes to keep the chips on the number, he or she may request the bet be "not working" or "Off". The chips remain on the table, but the player cannot win from or lose chips which are not working.

The opposite is also allowed. By default place and buy bets are not working without an established point; a player may wish to wager chips before a point has been established. In this case, the player would request the bet be working in which the dealer will place an "On" button on the specified chips.

The probability of dice combinations determine the odds of the payout. The following chart shows the dice combinations needed to roll each number. The two and twelve are the hardest to roll since only one combination of dice is possible. The game of craps is built around the dice roll of seven, since it is the most easily rolled dice combination.

Viewed another way:

The expected value of all bets is usually negative, such that the average player will always lose money. This is because the house always sets the paid odds to below the actual odds. The only exception is the "odds" bet that the player is allowed to make after a point is established on a pass/come don't pass/don't come bet (the odds portion of the bet has a long-term expected value of 0). However, this "free odds" bet cannot be made independently, so the expected value of the entire bet, including odds, is still negative. Since there is no correlation between die rolls, there is normally no possible long-term winning strategy in craps.

There are occasional promotional variants that provide either no house edge or even a player edge. One example is a field bet that pays 3:1 on 12 and 2:1 on either 3 or 11. Overall, given the 5:4 true odds of this bet, and the weighted average paid odds of approximately 7:5, the player has a 5% advantage on this bet. This is sometimes seen at casinos running limited-time incentives, in jurisdictions or gaming houses that require the game to be fair, or in layouts for use in informal settings using play money. No casino currently runs a craps table with a bet that yields a player edge full-time.

Maximizing the size of the odds bet in relation to the line bet will reduce, but never eliminate the house edge, and will increase variance. Most casinos have a limit on how large the odds bet can be in relation to the line bet, with single, double, and five times odds common. Some casinos offer 3–4–5 odds, referring to the maximum multiple of the line bet a player can place in odds for the points of 4 and 10, 5 and 9, and 6 and 8, respectively. During promotional periods, a casino may even offer 100x odds bets, which reduces the house edge to almost nothing, but dramatically increases variance, as the player will be betting in large betting units.

Since several of the multiple roll bets pay off in ratios of fractions on the dollar, it is important that the player bets in multiples that will allow a correct payoff in complete dollars. Normally, payoffs will be rounded down to the nearest dollar, resulting in a higher house advantage. These bets include all place bets, taking odds, and buying on numbers 6, 8, 5, and 9, as well as laying all numbers.

These variants depend on the casino and the table, and sometimes a casino will have different tables that use or omit these variants and others.


When craps is played in a casino, all bets have a house advantage. That is, it can be shown mathematically that a player will (with 100% probability) lose all his or her money to the casino in the long run, while in the short run the player is more likely to lose money than make money. There may be players who are lucky and get ahead for a period of time, but in the long run these winning streaks are eroded away. One can slow, but not eliminate, one's average losses by only placing bets with the smallest house advantage.

The pass/don't pass line, come/don't come line, place 6, place 8, buy 4 and buy 10 (only under the casino rules where commission is charged only on wins) have the lowest house edge in the casino, and all other bets will, on average, lose money between three and twelve times faster because of the difference in house edges.

The place bets and buy bets differ from the pass line and come line, in that place bets and buy bets can be removed at any time, since, while they are multi-roll bets, their odds of winning do not change from roll to roll, whereas pass line bets and come line bets are a combination of different odds on their first roll and subsequent rolls. The first roll of a pass line bet is 2:1 advantage for the player (8 wins, 4 losses), but it's "paid for" by subsequent rolls that are at the same disadvantage to the player as the don't pass bets were at an advantage. As such, they cannot profitably let you take down the bet after the first roll. Players can bet or lay odds behind an established point depending on whether it was a Pass/Come or Don't Pass/Don't Come to lower house edge by receiving true odds on the point. Casinos which allow put betting allows players to increase or make new pass/come bets after the come-out roll. This bet generally has a higher house edge than place betting, unless the casino offers high odds.

Conversely, you can take back (pick up) a don't pass or don't come bet after the first roll, but this cannot be recommended, because you already endured the disadvantaged part of the combination – the first roll. On that come-out roll, you win just 3 times (2 and 3), while losing 8 of them (7 and 11) and pushing one (12) out of the 36 possible rolls. On the other 24 rolls that become a point, your don't pass bet is now to your advantage by 6:3 (4 and 10), 6:4 (5 and 9) and 6:5 (6 and 8). If a player chooses to remove the initial don't come and/or don't pass line bet, he or she can no longer lay odds behind the bet and cannot re-bet the same don't pass and/or don't come number (players must make a new don't pass or come bets if desired). However, players can still make standard lay bets odds on any of the point numbers (4,5,6,8,9,10).

Among these, and the remaining numbers and possible bets, there are a myriad of systems and progressions that can be used with many combinations of numbers.

An important alternative metric is house advantage per roll (rather than per bet), which may be expressed in loss per hour. The typical pace of rolls varies depending on the number of players, but 102 rolls per hour is a cited rate for a nearly full table. This same reference states that only "29.6% of total rolls are come out rolls, on average", so for this alternative metric, needing extra rolls to resolve the pass line bet, for example, is factored. This number then permits calculation of rate of loss per hour, and per the 4 day/5 hour per day gambling trip:


Besides the rules of the game itself, a number of formal and informal rules are commonly applied in the table form of Craps, especially when played in a casino.

To reduce the potential opportunity for switching dice by sleight-of-hand, players are not supposed to handle the dice with more than one hand (such as shaking them in cupped hands before rolling) nor take the dice past the edge of the table. If a player wishes to change shooting hands, they may set the dice on the table, let go, then take them with the other hand.

When throwing the dice, the player is expected to hit the farthest wall at the opposite end of the table (these walls are typically augmented with pyramidal structures to ensure highly unpredictable bouncing after impact). Casinos will sometimes allow a roll that does not hit the opposite wall as long as the dice are thrown past the middle of the table; a very short roll will be nullified as a "no roll". The dice may not be slid across the table and must be tossed. These rules are intended to prevent dexterous players from physically influencing the outcome of the roll.

Players are generally asked not to throw the dice above a certain height (such as the eye level of the dealers). This is both for the safety of those around the table, and to eliminate the potential use of such a throw as a distraction device in order to cheat.

Dice are still considered "in play" if they land on players' bets on the table, the dealer's working stacks, on the marker puck, or with one die resting on top of the other. The roll is invalid if either or both dice land in the boxman's bank, the stickman's bowl (where the extra three dice are kept between rolls), or in the rails around the top of the table where players chips are kept. If one or both dice hits a player or dealer and rolls back onto the table, the roll counts as long as the person being hit did not intentionally interfere with either of the dice, though some casinos will rule "no roll" for this situation. If one or both leave the table, it is also a "no roll", and the dice may either be replaced or examined by the boxman and returned to play.

Shooters may wish to "set" the dice to a particular starting configuration before throwing (such as showing a particular number or combination, stacking the dice, or spacing them to be picked up between different fingers), but if they do, they are often asked to be quick about it so as not to delay the game. Some casinos disallow such rituals to speed up the pace of the game. Some may also discourage or disallow unsanitary practices such as kissing or spitting on the dice.

In most casinos, players are not allowed to hand anything directly to dealers, and vice versa. Items such as cash, checks, and chips are exchanged by laying them down on the table; for example, when "buying in" (paying cash for chips), players are expected to place the cash on the layout: the dealer will take it and then place the chips in front of the player. This rule is enforced in order to allow the casino to easily monitor and record all transfers via overhead surveillance cameras, and to reduce the opportunity for cheating via sleight-of-hand.

Most casinos prohibit "call bets", and may have a warning such as "No Call Bets" printed on the layout to make this clear. This means a player may not call out a bet without also placing the corresponding chips on the table. Such a rule reduces the potential for misunderstanding in loud environments, as well as disputes over the amount that the player intended to bet after the outcome has been decided. Some casinos choose to allow call bets once players have bought-in. When allowed, they are usually made when a player wishes to bet at the last second, immediately before the dice are thrown, to avoid the risk of obstructing the roll.

Craps is among the most social and most superstitious of all gambling games, which leads to an enormous variety of informal rules of etiquette that players may be expected to follow. An exhaustive list of these is beyond the scope of this article, but the guidelines below are most commonly given.

Tipping the dealers is universal and expected in Craps. As in most other casino games, a player may simply place (or toss) chips onto the table and say, "For the dealers", "For the crew", "etc." In craps, it is also common to place a bet for the dealers. This is usually done one of three ways: by placing an ordinary bet and simply declaring it for the dealers, as a "two-way", or "on top". A "Two-Way" is a bet for both parties: for example, a player may toss in two chips and say "Two Way Hard Eight", which will be understood to mean one chip for the player and one chip for the dealers. Players may also place a stack of chips for a bet as usual, but leave the top chip off-center and announce "on top for the dealers". The dealer's portion is often called a "toke" bet, which comes from the practice of using $1 slot machine tokens to place dealer bets in some casinos.

In some cases, players may also tip each other, for example as a show of gratitude to the thrower for a roll on which they win a substantial bet.

Craps players routinely practice a wide range of superstitious behaviors, and may expect or demand these from other players as well.

Most prominently, it is universally considered bad luck to say the word "seven" (after the "come-out", a roll of 7 is a loss for "pass" bets). Dealers themselves often make significant efforts to avoid calling out the number. When necessary, participants may refer to seven with a "nickname" such as "Big Red" (or just "Red"), "the S-word", etc.

Although no wagering system can consistently beat casino games based on independent trials such as craps, that does not stop gamblers from believing in them. One of the best known systems is the Martingale System. In this strategy, the gambler doubles his bet after every loss. After a win, the bet is reset to the original bet. The theory is that the first win would recover all previous losses plus win a profit equal to the original stake. The main article on martingale describes the flaws in this system.

Other systems depend on the gambler's fallacy, which in craps terms is the belief that past dice rolls influence the probabilities of future dice rolls. For example, the gambler's fallacy indicates that a craps player should bet on eleven if an eleven has not appeared or has appeared too often in the last 20 rolls. In practice this can be observed as players respond to a roll such as a Hard Six with an immediate wager on the Hard Six.

In reality, each roll of the dice is an independent event, so the probability of rolling eleven is exactly 1/18 on every roll, regardless of the number of times eleven has come up in the last x rolls. Even if the dice are actually biased toward particular results ("loaded"), each roll is still independent of all the previous ones. The common term to describe this is "dice have no memory".

Another approach is to "set" the dice in a particular orientation, and then throw them in such a manner that they do not tumble randomly. The theory is that given exactly the same throw from exactly the same starting configuration, the dice will tumble in the same way and therefore show the same or similar values every time.

Casinos take steps to prevent this. The dice are usually required to hit the back wall of the table, which is normally faced with an angular texture such as pyramids, making controlled spins more difficult. There has been no independent evidence that such methods can be successfully applied in a real casino.

Bank craps is a variation of the original craps game and is sometimes known as Las Vegas Craps. This variant is quite popular in Nevada gambling houses, and its availability online has now made it a globally played game. Bank craps uses a special table layout and all bets must be made against the house. In Bank Craps, the dice are thrown over a wire or a string that is normally stretched a few inches from the table's surface. The lowest house edge (for the pass/don't pass) in this variation is around 1.4%. Generally, if the word "craps" is used without any modifier, it can be inferred to mean this version of the game, to which most of this article refers.

Crapless craps, also known as Bastard Craps, is a simple version of the original craps game, and is normally played as an online private game. The biggest difference between crapless craps and original craps, is that the shooter (person throwing the dice) is at a far greater disadvantage and has a house edge of 5.38%. Another difference is that this is one of the craps games in which a player can bet on rolling a 2, 3, 11 or 12 before a 7 is thrown. In crapless craps, 2 and 12 have odds of 11:2 and have a house edge of 7.143% while 3 and 11 have odds of 11:4 with a house edge of 6.25%.

New York Craps is one of the variations of craps played mostly in the Eastern coast of the US, true to its name. History states that this game was actually found and played in casinos in Yugoslavia, the UK and the Bahamas. In this craps variant, the house edge is greater than Las Vegas Craps or Bank craps. The table layout is also different, and is called a double-end-dealer table. This variation is different from the original craps game in several ways, but the primary difference is that New York craps doesn't allow Come or Don't Come bets. New York Craps Players bet on box numbers like 4, 5, 6, 8, 9 or 10. The overall house edge in New York craps is 5%.

In order to get around California laws barring the payout of a game being directly related to the roll of dice, Indian reservations have adapted the game to substitute cards for dice.

In one variation, there are no dice at all. Two shoes are used, each containing some number of regular card decks that have been stripped down to just the Aces and deuces through sixes. The boxman simply deals one card from each shoe and that is the roll on which bets are settled. Since a card-counting scheme is easily devised to make use of the information of cards that have already been dealt, a relatively small portion (less than 50%) of each shoe is usually dealt in order to protect the house.

In a similar variation, cards representing dice are dealt directly from a continuous shuffling machine (CSM). Typically, the CSM will hold approximately 264 cards, or 44 sets of 1 through 6 spot cards. Two cards are dealt from the CSM for each roll. The game is played exactly as regular craps, but the roll distribution of the remaining cards in the CSM is slightly skewed from the normal symmetric distribution of dice.

Even if the dealer were to shuffle each roll back into the CSM, the effect of buffering a number of cards in the chute of the CSM provides information about the skew of the next roll. Analysis shows this type of game is biased towards the don't pass and don't come bets. A player betting don't pass and don't come every roll and laying 10x odds receives a 2% profit on the initial don't pass / don't come bet each roll. Using a counting system allows the player to attain a similar return at lower variance.

To replicate the original dice odds exactly without dice or possibility of card-counting, another scheme uses two shuffle machines with just one deck of Ace through 6 each. Each machine selects one of the 6 cards at random and this is the roll. The selected cards are replaced and the decks are reshuffled for the next roll.

In this game variation, one red deck and one blue deck of six cards each (A through 6), and a red die and a blue die are used. Each deck is shuffled separately, usually by machine. Each card is then dealt onto the layout, into the 6 red and 6 blue numbered boxes. The shooter then shoots the dice. The red card in the red-numbered box corresponding to the red die, and the blue card in the blue-numbered box corresponding to the blue die are then turned over to form the roll on which bets are settled.

Another variation uses a red and a blue deck of 36 custom playing cards each. Each card has a picture of a two-die roll on it – from 1–1 to 6–6. The shooter shoots what looks like a red and a blue die, called "cubes". They are numbered such that they can never throw a pair, and that the blue one will show a higher value than the red one exactly half the time. One such scheme could be 222555 on the red die and 333444 on the blue die.

One card is dealt from the red deck and one is dealt from the blue deck. The shooter throws the "cubes" and the color of the cube that is higher selects the color of the card to be used to settle bets. On one such table, an additional one-roll prop bet was offered: If the card that was turned over for the "roll" was either 1–1 or 6–6, the other card was also turned over. If the other card was the "opposite" (6–6 or 1–1, respectively) of the first card, the bet paid 500:1 for this 647:1 proposition.

And additional variation uses a single set of 6 cards, and regular dice. The roll of the dice maps to the card in that position, and if a pair is rolled, then the mapped card is used twice, as a pair.

Recreational or informal playing of craps outside of a casino is referred to as street craps or private craps. The most notable difference between playing street craps and bank craps is that there is no bank or house to cover bets in street craps. Players must bet against each other by covering or fading each other's bets for the game to be played. If money is used instead of chips and depending on the laws of where it is being played, street craps can be an illegal form of gambling.

There are many variations of street craps. The simplest way is to either agree on or roll a number as the point, then roll the point again before you roll a seven. Unlike more complex proposition bets offered by casinos, street craps has more simplified betting options. The shooter is required to make either a Pass or a Don't Pass bet if he wants to roll the dice. Another player must choose to cover the shooter to create a stake for the game to continue.

If there are several players, the rotation of the player who must cover the shooter may change with the shooter (comparable to a blind in poker). The person covering the shooter will always bet against the shooter. For example, if the shooter made a "Pass" bet, the person covering the shooter would make a "Don't Pass" bet to win. Once the shooter is covered, other players may make Pass/Don't Pass bets, or any other proposition bets, as long as there is another player willing to cover.

Due to the random nature of the game, in popular culture a "crapshoot" is often used to describe an action with an unpredictable outcome.

A Golden Arm is a craps player who rolls the dice for longer than one hour without losing. The first Golden Arm was Stanley Fujitake, who rolled 118 times without sevening out in 3 hours and 6 minutes at the California Hotel and Casino in 1989.

In the 1993 film "A Bronx Tale", Eddie Mush quotes the line "Baby needs new shoes" before rolling a hard 12. Calogero, while working is told by Sonny to roll the dice and makes the Italian mobsters money in an underground game.

The current record for length of a "hand" (successive rounds won by the same shooter) is 154 rolls including 25 passes by Patricia DeMauro of New Jersey, lasting 4 hours and 18 minutes, at The Borgata in Atlantic City, New Jersey, on May 23–24, 2009. She bested by over an hour the record held for almost 20 years – that of Fujitake.

The prayer or invocation "Baby needs a new pair of shoes!" is associated with shooting craps.

Floating craps is an illegal operation of craps. The term "floating" refers to the practice of the game's operators using portable tables and equipment to quickly move the game from location to location to stay ahead of the law enforcement authorities. The term may have originated in the 1930s when Benny Binion (later known for founding the downtown Las Vegas hotel Binions) set up an illegal craps game utilizing tables created from portable crates for the Texas Centennial Exposition.

The 1950 Broadway musical "Guys and Dolls" features a major plot point revolving around a floating craps game.

In the 1950s and 1960s The Sands Hotel in Las Vegas had a craps table that floated in the swimming pool, as a joke reference to the notoriety of the term.

Notes


</doc>
<doc id="6066" url="https://en.wikipedia.org/wiki?curid=6066" title="Carl von Clausewitz">
Carl von Clausewitz

Carl Philipp Gottfried (or Gottlieb) von Clausewitz (; 1 June 1780 – 16 November 1831) was a Prussian general and military theorist who stressed the "moral" (meaning, in modern terms, psychological) and political aspects of war. His most notable work, "Vom Kriege" ("On War"), was unfinished at his death.
Clausewitz was a realist in many different senses and, while in some respects a romantic, also drew heavily on the rationalist ideas of the European Enlightenment.

Clausewitz's thinking is often described as Hegelian because of his dialectical method; but, although he was probably personally acquainted with Hegel, there remains debate as to whether or not Clausewitz was in fact influenced by him. He stressed the dialectical interaction of diverse factors, noting how unexpected developments unfolding under the "fog of war" (i.e., in the face of incomplete, dubious, and often completely erroneous information and high levels of fear, doubt, and excitement) call for rapid decisions by alert commanders. He saw history as a vital check on erudite abstractions that did not accord with experience. In contrast to the early work of Antoine-Henri Jomini, he argued that war could not be quantified or reduced to mapwork, geometry, and graphs. Clausewitz had many aphorisms, of which the most famous is "War is the continuation of politics by other means."

Clausewitz's Christian names are sometimes given in non-German sources as "Karl", "Carl Philipp Gottlieb," or "Carl Maria." He spelled his own given name with a "C" in order to identify with the classical Western tradition; writers who use "Karl" are often seeking to emphasise his German (rather than European) identity. "Carl Philipp Gottfried" appears on Clausewitz's tombstone. Nonetheless, sources such as military historian Peter Paret and "Encyclopædia Britannica" use Gottlieb instead of Gottfried.

Clausewitz was born on 1 June 1780 in Burg bei Magdeburg in the Prussian Duchy of Magdeburg as the fourth and youngest son of a family that made claims to noble status which Carl accepted. Clausewitz's family claimed descent from the Barons of Clausewitz in Upper Silesia, though scholars question the connection. His grandfather, the son of a Lutheran pastor, had been a professor of theology. Clausewitz's father, once a lieutenant in the army of Frederick the Great, King of Prussia, held a minor post in the Prussian internal-revenue service. Clausewitz entered the Prussian military service at the age of twelve as a lance-corporal, eventually attaining the rank of major general.

Clausewitz served in the Rhine Campaigns (1793–1794) including the Siege of Mainz, when the Prussian army invaded France during the French Revolution, and fought in the Napoleonic Wars from 1806 to 1815. He entered the "Kriegsakademie" (also cited as "The German War School", the "Military Academy in Berlin", and the "Prussian Military Academy," later the "War College") in Berlin in 1801 (aged 21), probably studied the writings of the philosophers Immanuel Kant and/or Fichte and Schleiermacher and won the regard of General Gerhard von Scharnhorst, the future first chief-of-staff of the newly reformed Prussian Army (appointed 1809). Clausewitz, Hermann von Boyen (1771–1848) and Karl von Grolman (1777–1843) were among Scharnhorst's primary allies in his efforts to reform the Prussian army between 1807 and 1814.

Clausewitz served during the Jena Campaign as aide-de-camp to Prince August. At the Battle of Jena-Auerstedt on 14 October 1806 – when Napoleon invaded Prussia and defeated the massed Prussian-Saxon army commanded by Karl Wilhelm Ferdinand, Duke of Brunswick – he was captured, one of the 25,000 prisoners taken that day as the Prussian army disintegrated. He was 26. Clausewitz was held prisoner with his prince in France from 1807 to 1808. Returning to Prussia, he assisted in the reform of the Prussian army and state.
On 10 December 1810 he married the socially prominent Countess Marie von Brühl, whom he had first met in 1803. She was a member of the noble German von Brühl family originating in Thuringia. The couple moved in the highest circles, socialising with Berlin's political, literary and intellectual élite. Marie was well-educated and politically well-connected—she played an important role in her husband's career progress and intellectual evolution. She also edited, published, and introduced his collected works.

Opposed to Prussia's enforced alliance with Napoleon I, Clausewitz left the Prussian army and served in the Imperial Russian Army from 1812 to 1813 during the Russian Campaign, taking part in the Battle of Borodino (1812). Like many Prussian officers serving in Russia, he joined the Russian-German Legion in 1813. In the service of the Russian Empire, Clausewitz helped negotiate the Convention of Tauroggen (1812), which prepared the way for the coalition of Prussia, Russia, and the United Kingdom that ultimately defeated Napoleon and his allies.

In 1815 the Russian-German Legion became integrated into the Prussian Army and Clausewitz re-entered Prussian service as a colonel. He was soon appointed chief-of-staff of Johann von Thielmann's III Corps. In that capacity he served at the Battle of Ligny and the Battle of Wavre during the Waterloo Campaign in 1815. An army led personally by Napoleon defeated the Prussians at Ligny (south of Mont-Saint-Jean and the village of Waterloo) on 16 June 1815, but Napoleon's failure to destroy the Prussian forces led to his defeat a few days later at the Battle of Waterloo (18 June 1815), when the Prussian forces unexpectedly arrived on his right flank late in the afternoon to support the Anglo-Dutch-Belgian forces pressing his front. Clausewitz's unit fought at Wavre (18–19 June 1815), preventing large reinforcements from reaching Napoleon at Waterloo. After the war Clausewitz served as the director of the "Kriegsakademie", where he served until 1830. In that year he returned to duty with the army. Soon afterwards, the outbreak of several revolutions around Europe and a crisis in Poland appeared to presage another major European war. Clausewitz was appointed chief of staff of the only army Prussia was able to mobilise in this emergency, which was sent to the Polish border. Its commander, Gneisenau, died of cholera (August 1831), and Clausewitz took command of the Prussian army's efforts to construct a "cordon sanitaire" to contain the great cholera outbreak (the first time cholera had appeared in modern heartland Europe, causing a continent-wide panic). Clausewitz himself died of the same disease shortly afterwards, on 17 November 1831.

His widow edited, published, and wrote the introduction to his "magnum opus" on the philosophy of war in 1832. (He had started working on the text in 1816, but had not completed it.) She wrote the preface for "On War" and by 1835 had published most of his collected works. She died in January 1836.

Clausewitz was a professional combat soldier who was involved in numerous military campaigns, but he is famous primarily as a military theorist interested in the examination of war, utilising the campaigns of Frederick the Great and Napoleon as frames of reference for his work. He wrote a careful, systematic, philosophical examination of war in all its aspects. The result was his principal book, "On War," a major work on the philosophy of war. It was unfinished when Clausewitz died and contains material written at different stages in his intellectual evolution, producing some significant contradictions between different sections. The sequence and precise character of that evolution is a source of much debate, as are exact meaning behind his seemingly contradictory claims (discussions pertinent to the tactical, operational and strategic levels of war are one example). Clausewitz constantly sought to revise the text, particularly between 1827 and his departure on his last field assignments, to include more material on "people's war" and forms of war other than high-intensity warfare between states, but relatively little of this material was included in the book. Soldiers before this time had written treatises on various military subjects, but none had undertaken a great philosophical examination of war on the scale of those written by Clausewitz and Leo Tolstoy, both of which were inspired by the events of the Napoleonic Era.

Clausewitz's work is still studied today, demonstrating its continued relevance. More than sixteen major English-language books that focused specifically on his work were published between 2005 and 2014, whereas his 19th-century rival Jomini faded from influence. The historian Lynn Montross said the outcome, "may be explained by the fact that Jomini produced a system of war, Clausewitz a philosophy. The one has been outdated by new weapons, the other still influences the strategy behind those weapons." Jomini did not attempt to define war but Clausewitz did, providing (and dialectically comparing) a number of definitions. The first is his dialectical thesis: "War is thus an act of force to compel our enemy to do our will." The second, often treated as Clausewitz's 'bottom line,' is in fact merely his dialectical antithesis: "War is merely the continuation of policy by other means." The synthesis of his dialectical examination of the nature of war is his famous "trinity," saying that war is "a fascinating trinity—composed of primordial violence, hatred, and enmity, which are to be regarded as a blind natural force; the play of chance and probability, within which the creative spirit is free to roam; and its element of subordination, as an instrument of policy, which makes it subject to pure reason." Christopher Bassford says the best shorthand for Clausewitz's trinity should be something like "violent emotion/chance/rational calculation." However, it is frequently presented as "people/army/government," a misunderstanding based on a later paragraph in the same chapter. This misrepresentation was popularised by U.S. Army Colonel Harry Summers' Vietnam-era interpretation, facilitated by weaknesses in the 1976 Howard/Paret translation.

The degree to which Clausewitz managed to revise his manuscript to reflect that synthesis is the subject of much debate. His final reference to war and "Politik", however, goes beyond his widely quoted antithesis: "War is simply the continuation of political intercourse with the addition of other means. We deliberately use the phrase 'with the addition of other means' because we also want to make it clear that war in itself does not suspend political intercourse or change it into something entirely different. In essentials that intercourse continues, irrespective of the means it employs. The main lines along which military events progress, and to which they are restricted, are political lines that continue throughout the war into the subsequent peace."
Clausewitz introduced systematic philosophical contemplation into Western military thinking, with powerful implications not only for historical and analytical writing but also for practical policy, military instruction, and operational planning. He relied on his own experiences, contemporary writings about Napoleon, and on deep historical research. His historiographical approach is evident in his first extended study, written when he was 25, of the Thirty Years War. He rejects the Enlightenment's view of the war as a chaotic muddle and instead explains its drawn-out operations by the economy and technology of the age, the social characteristics of the troops, and the commanders' politics and psychology. In "On War", Clausewitz sees all wars as the sum of decisions, actions, and reactions in an uncertain and dangerous context, and also as a socio-political phenomenon. He also stressed the complex nature of war, which encompasses both the socio-political and the operational and stresses the primacy of state policy.

The word "strategy" had only recently come into usage in modern Europe, and Clausewitz's definition is quite narrow: "the use of engagements for the object of war." Clausewitz conceived of war as a political, social, and military phenomenon which might — depending on circumstances — involve the entire population of a nation at war. In any case, Clausewitz saw military force as an instrument that states and other political actors use to pursue the ends of policy, in a dialectic between opposing wills, each with the aim of imposing his policies and will upon his enemy.

Clausewitz's emphasis on the inherent superiority of the defence suggests that habitual aggressors are likely to end up as failures. The inherent superiority of the defence obviously does not mean that the defender will always win, however: there are other asymmetries to be considered. He was interested in co-operation between the regular army and militia or partisan forces, or citizen soldiers, as one possible — sometimes the only — method of defence. In the circumstances of the Wars of the French Revolution and with Napoleon, which were energised by a rising spirit of nationalism, he emphasised the need for states to involve their entire populations in the conduct of war. This point is especially important, as these wars demonstrated that such energies could be of decisive importance and for a time led to a democratisation of the armed forces much as universal suffrage democratised politics.

While Clausewitz was intensely aware of the value of intelligence at all levels, he was also very sceptical of the accuracy of much military intelligence: "Many intelligence reports in war are contradictory; even more are false, and most are uncertain... In short, most intelligence is false." This circumstance is generally described as part of the fog of war. Such sceptical comments apply only to intelligence at the tactical and operational levels; at the strategic and political levels he constantly stressed the requirement for the best possible understanding of what today would be called strategic and political intelligence. His conclusions were influenced by his experiences in the Prussian Army, which was often in an intelligence fog due partly to the superior abilities of Napoleon's system but even more to the nature of war. Clausewitz acknowledges that friction creates enormous difficulties for the realisation of any plan, and the "fog of war" hinders commanders from knowing what is happening. It is precisely in the context of this challenge that he develops the concept of military genius, whose capabilities are seen above all in the execution of operations. 'Military genius' is not simply a matter of intellect, but a combination of qualities of intellect, experience, personality, and temperament (and there are many possible such combinations) that create a very highly developed mental aptitude for the waging of war.

Key ideas discussed in "On War" include:

Clausewitz used a dialectical method to construct his argument, leading to frequent misinterpretation of his ideas. British military theorist B. H. Liddell Hart contends that the enthusiastic acceptance by the Prussian military establishment – especially Moltke the Elder, a former student of his – of what they believed to be Clausewitz's ideas, and the subsequent widespread adoption of the Prussian military system worldwide, had a deleterious effect on military theory and practice, due to their egregious misinterpretation of his ideas:

As so often happens, Clausewitz's disciples carried his teaching to an extreme which their master had not intended... [Clausewitz's] theory of war was expounded in a way too abstract and involved for ordinary soldier-minds, essentially concrete, to follow the course of his argument – which often turned back from the direction in which it was apparently leading. Impressed yet befogged, they grasped at his vivid leading phrases, seeing only their surface meaning, and missing the deeper current of his thought.

As described by Christopher Bassford, then-professor of strategy at the National War College of the United States:

One of the main sources of confusion about Clausewitz's approach lies in his dialectical method of presentation. For example, Clausewitz's famous line that "War is a mere continuation of politics by other means," ("Der Krieg ist eine bloße Fortsetzung der Politik mit anderen Mitteln") while accurate as far as it goes, was not intended as a statement of fact. It is the antithesis in a dialectical argument whose thesis is the point – made earlier in the analysis – that "war is nothing but a duel [or wrestling match, a better translation of the German "Zweikampf"] on a larger scale." His synthesis, which resolves the deficiencies of these two bold statements, says that war is neither "nothing but" an act of brute force nor "merely" a rational act of politics or policy. This synthesis lies in his "fascinating trinity" [wunderliche Dreifaltigkeit]: a dynamic, inherently unstable interaction of the forces of violent emotion, chance, and rational calculation.

Another example of this confusion is the idea that Clausewitz was a proponent of total war as used in the Third Reich's propaganda in the 1940s. In fact, he never used the term "total war": rather, he discussed "absolute war," a concept which evolved into the much more abstract notion of "ideal war" discussed at the very beginning of "Vom Kriege"--the purely "logical" result of the forces underlying a "pure," Platonic "ideal" of war. In what he called a "logical fantasy," war cannot be waged in a limited way: the rules of competition will force participants to use all means at their disposal to achieve victory. But in the "real world", he said, such rigid logic is unrealistic and dangerous. As a practical matter, the military objectives in "real" war that support political objectives generally fall into two broad types: "war to achieve limited aims"; and war to "disarm" the enemy, "to render [him] politically helpless or militarily impotent." Thus the complete defeat of the enemy may not be necessary, desirable, or even possible.

In modern times the reconstruction of Clausewitzian theory has been a matter of much dispute. One analysis was that of Panagiotis Kondylis, a Greek-German writer and philosopher, who opposed the interpretations of Raymond Aron in "Penser la Guerre, Clausewitz", and other liberal writers. According to Aron, Clausewitz was one of the first writers to condemn the militarism of the Prussian general staff and its war-proneness, based on Clausewitz's argument that "war is a continuation of politics by other means." In "Theory of War," Kondylis claims that this is inconsistent with Clausewitzian thought. He claims that Clausewitz was morally indifferent to war (though this probably reflects a lack of familiarity with personal letters from Clausewitz, which demonstrate an acute awareness of war's tragic aspects) and that his advice regarding politics' dominance over the conduct of war has nothing to do with pacifist ideas. For Clausewitz, war is simply one unique means that is sometimes applied to the eternal quest for power, of "raison d'État" in an anarchic and unsafe world.

Other notable writers who have studied Clausewitz's texts and translated them into English are historians Peter Paret of the Institute for Advanced Study and Sir Michael Howard, and the philosopher, musician, and game theorist Anatol Rapoport. Howard and Paret edited the most widely used edition of "On War" (Princeton University Press, 1976/1984) and have produced comparative studies of Clausewitz and other theorists, such as Tolstoy. Bernard Brodie's "A Guide to the Reading of "On War"", in the 1976 Princeton translation, expressed his interpretations of the Prussian's theories and provided students with an influential synopsis of this vital work.

The British military historian John Keegan attacked Clausewitz's theory in his book "A History of Warfare". Keegan argued that Clausewitz assumed the existence of states, yet 'war antedates the state, diplomacy and strategy by many millennia.'

Clausewitz died without completing "On War", but despite this his ideas have been widely influential in military theory and have had a strong influence on German military thought specifically. Later Prussian and German generals, such as Helmuth Graf von Moltke, were clearly influenced by Clausewitz: Moltke's widely quoted statement that "No campaign plan survives first contact with the enemy" is a classic reflection of Clausewitz's insistence on the roles of chance, friction, "fog," uncertainty, and interactivity in war.

Clausewitz's influence spread to British thinking as well, though at first more as a historian and analyst than as a theorist. See for example Wellington's extended essay discussing Clausewitz's study of the Campaign of 1815—Wellington's only serious written discussion of the battle, which was widely discussed in 19th-century Britain. Clausewitz's broader thinking came to the fore following Britain's military embarrassments in the Boer War (1899–1902). One example of a heavy Clausewitzian influence in that era is Spenser Wilkinson, journalist, the first Chichele Professor of Military History at Oxford University, and perhaps the most prominent military analyst in Britain from c. 1885 until well into the interwar period. Another is naval historian Julian Corbett (1854–1922), whose work reflected a deep if idiosyncratic adherence to Clausewitz's concepts and frequently an emphasis on Clausewitz's ideas about 'limited war' and the inherent strengths of the defensive form of war. Corbett's practical strategic views were often in prominent public conflict with Wilkinson's – see, for example, Wilkinson's article "Strategy at Sea," "The Morning Post", 12 February 1912. Following the First World War, however, the influential British military commentator B. H. Liddell Hart in the 1920s erroneously attributed to him the doctrine of "total war" that during the First World War had been embraced by many European general staffs and emulated by the British. More recent scholars typically see that war as so confused in terms of political rationale that it in fact contradicts much of "On War." One of the most influential British Clausewitzians today is Colin S. Gray; historian Hew Strachan (like Wilkinson also the Chichele Professor of Military History at Oxford University, since 2001) has been an energetic proponent of the "study" of Clausewitz, but his own views on Clausewitz's ideas are somewhat ambivalent.

With some interesting exceptions (e.g., John McAuley Palmer, Robert M. Johnston, Hoffman Nickerson), Clausewitz had little influence on American military thought before 1945 other than via British writers, though Generals Eisenhower and Patton were avid readers. He did influence Karl Marx, Friedrich Engels, Vladimir Lenin, Leon Trotsky and Mao Zedong, and thus the Communist Soviet and Chinese traditions, as Lenin emphasised the inevitability of wars among capitalist states in the age of imperialism and presented the armed struggle of the working class as the only path toward the eventual elimination of war. Because Lenin was an admirer of Clausewitz and called him "one of the great military writers", his influence on the Red Army was immense. The Russian historian A.N. Mertsalov commented that "It was an irony of fate that the view in the USSR was that it was Lenin who shaped the attitude towards Clausewitz, and that Lenin's dictum that war is a continuation of politics is taken from the work of this [allegedly] anti-humanist anti-revolutionary." The American mathematician Anatol Rapoport wrote in 1968 that Clausewitz as interpreted by Lenin formed the basis of all Soviet military thinking since 1917, and quoted the remarks by Marshal V.D. Sokolovsky:

In describing the essence of war, Marxism-Leninism takes as its point of departure the premise that war is not an aim in itself, but rather a tool of politics. In his remarks on Clausewitz's "On War", Lenin stressed that "Politics is the reason, and war is only the tool, not the other way around. Consequently, it remains only to subordinate the military point of view to the political".

Henry A. Kissinger, however, described Lenin's approach as being that politics is a continuation of war by other means, thus turning Clausewitz's argument "on its head."

Rapoport argued that:

As for Lenin's approval of Clausewitz, it probably stems from his obsession with the struggle for power. The whole Marxist conception of history is that of successive struggles for power, primarily between social classes. This was constantly applied by Lenin in a variety of contexts. Thus the entire history of philosophy appears in Lenin's writings as a vast struggle between "idealism" and "materialism". The fate of the socialist movement was to be decided by a struggle between the revolutionists and the reformers. Clausewitz's acceptance of the struggle for power as the essence of international politics must have impressed Lenin as starkly realistic.

Clausewitz directly influenced Mao Zedong, who read "On War" in 1938 and organised a seminar on Clausewitz for the Party leadership in Yan'an. Thus the "Clausewitzian" content in many of Mao's writings is not merely a regurgitation of Lenin but reflects Mao's study.
The idea that war involves inherent "friction" that distorts, to a greater or lesser degree, all prior arrangements, has become common currency in fields such as business strategy and sport. The phrase "fog of war" derives from Clausewitz's stress on how confused warfare can seem while immersed within it. The term center of gravity, used in a military context derives from Clausewitz's usage, which he took from Newtonian mechanics. In U.S. military doctrine, "center of gravity" refers to the basis of an opponent's power at the operational, strategic, or political level, though this is only one aspect of Clausewitz's use of the term.

The deterrence strategy of the United States in the 1950s was closely inspired by President Dwight Eisenhower’s reading of Clausewitz as a young officer in the 1920s. Eisenhower was greatly impressed by Clausewitz’s example of a theoretical, idealised “absolute war” in "Vom Krieg" as a way of demonstrating how absurd it would be to attempt such a strategy in practice. For Eisenhower, the age of nuclear weapons had made what was for Clausewitz in the early 19th century only a theoretical vision an all too real possibility in the mid-20th century. From Eisenhower's viewpoint, the best deterrent to war was to show the world just how appalling and horrific a nuclear “absolute war” would be if it should ever occur, hence a series of much publicised nuclear tests in the Pacific, giving first priority in the defence budget to nuclear weapons and delivery systems over conventional weapons, and making repeated statements in public that the United States was able and willing at all times to use nuclear weapons. In this way through the massive retaliation doctrine and the closely related foreign policy concept of brinkmanship, Eisenhower hoped to hold out a creditable vision of Clausewitzian nuclear “absolute war” in order to deter the Soviet Union and/or China from ever risking a war or even conditions that might lead to a war with the United States.
After 1970, some theorists claimed that nuclear proliferation made Clausewitzian concepts obsolete after the 20th-century period in which they dominated the world. John E. Sheppard, Jr., argues that by developing nuclear weapons, state-based conventional armies simultaneously both perfected their original purpose, to destroy a mirror image of themselves, and made themselves obsolete. No two powers have used nuclear weapons against each other, instead using conventional means or proxy wars to settle disputes. If such a conflict did occur, presumably both combatants would be annihilated. Heavily influenced by the war in Vietnam and by antipathy to American strategist Henry Kissinger, the American biologist, musician, and game-theorist Anatol Rapoport argued in 1968 that a Clausewitzian view of war was not only obsolete in the age of nuclear weapons, but also highly dangerous as it promoted a "zero-sum paradigm" to international relations and a "dissolution of rationality" amongst decision-makers.

The end of the 20th century and the beginning of the 21st century have seen many instances of state armies attempting to suppress insurgencies, terrorism, and other forms of asymmetrical warfare. Clausewitz did not focus solely on wars between countries with well-defined armies. The era of the French Revolution and Napoleon was full of revolutions, rebellions, and violence by "non-state actors", such as the wars in the French Vendée and in Spain. Clausewitz wrote a series of “Lectures on Small War” and studied the rebellion in the Vendée (1793–1796) and the Tyrolean uprising of 1809. In his famous “Bekenntnisdenkschrift” of 1812, he called for a “Spanish war in Germany” and laid out a comprehensive guerrilla strategy to be waged against Napoleon. In "On War" he included a famous chapter on “The People in Arms.”

One prominent critic of Clausewitz is the Israeli military historian Martin van Creveld. In his book "The Transformation of War", Creveld argued that Clausewitz's famous "Trinity" of people, army, and government was an obsolete socio-political construct based on the state, which was rapidly passing from the scene as the key player in war, and that he (Creveld) had constructed a new "non-trinitarian" model for modern warfare. Creveld's work has had great influence. Daniel Moran replied, 'The most egregious misrepresentation of Clausewitz's famous metaphor must be that of Martin van Creveld, who has declared Clausewitz to be an apostle of Trinitarian War, by which he means, incomprehensibly, a war of 'state against state and army against army,' from which the influence of the people is entirely excluded." Christopher Bassford went further, noting that one need only "read" the paragraph in which Clausewitz defined his Trinity to see "that the words 'people,' 'army,' and 'government' appear nowhere at all in the list of the Trinity’s components... Creveld's and Keegan's assault on Clausewitz's Trinity is not only a classic 'blow into the air,' i.e., an assault on a position Clausewitz doesn't occupy. It is also a pointless attack on a concept that is quite useful in its own right. In any case, their failure to read the actual wording of the theory they so vociferously attack, and to grasp its deep relevance to the phenomena they describe, is hard to credit."

Some have gone further and suggested that Clausewitz's best-known aphorism, that war is a continuation of politics by other means, is not only irrelevant today but also inapplicable historically. For an opposing view see the sixteen essays presented in "Clausewitz in the Twenty-First Century" edited by Hew Strachan and Andreas Herberg-Rothe.

In military academies, schools, and universities worldwide, Clausewitz's literature is often mandatory reading.

Literature

Film

Video games

August Otto Rühle von Lilienstern – Prussian officer from whom Clausewitz allegedly took, without acknowledgment, several important ideas (including that about war as pursuing political aims) made famous in "On War". However, such ideas as Clausewitz and Lilienstern shared in common derived from a common influence, i.e., Scharnhorst, who was Clausewitz's "second father" and professional mentor.

Informational notes
Citations
Further reading




</doc>
<doc id="6068" url="https://en.wikipedia.org/wiki?curid=6068" title="Common Lisp">
Common Lisp

Common Lisp (CL) is a dialect of the Lisp programming language, published in ANSI standard document "ANSI INCITS 226-1994 (R2004)" (formerly "X3.226-1994 (R1999)"). The Common Lisp HyperSpec, a hyperlinked HTML version, has been derived from the ANSI Common Lisp standard.

The Common Lisp language was developed as a standardized and improved successor of Maclisp. By the early 1980s several groups were already at work on diverse successors to MacLisp: Lisp Machine Lisp (aka ZetaLisp), Spice Lisp, NIL and S-1 Lisp. Common Lisp sought to unify, standardise, and extend the features of these MacLisp dialects. Common Lisp is not an implementation, but rather a language specification. Several implementations of the Common Lisp standard are available, including free and open-source software and proprietary products.
Common Lisp is a general-purpose, multi-paradigm programming language. It supports a combination of procedural, functional, and object-oriented programming paradigms. As a dynamic programming language, it facilitates evolutionary and incremental software development, with iterative compilation into efficient run-time programs. This incremental development is often done interactively without interrupting the running application.

It also supports optional type annotation and casting, which can be added as necessary at the later profiling and optimization stages, to permit the compiler to generate more efficient code. For instance, codice_1 can hold an unboxed integer in a range supported by the hardware and implementation, permitting more efficient arithmetic than on big integers or arbitrary precision types. Similarly, the compiler can be told on a per-module or per-function basis which type safety level is wanted, using "optimize" declarations.

Common Lisp includes CLOS, an object system that supports multimethods and method combinations. It is often implemented with a Metaobject Protocol.

Common Lisp is extensible through standard features such as "Lisp macros" (code transformations) and "reader macros" (input parsers for characters).

Common Lisp provides some backwards compatibility to Maclisp and to John McCarthy's original Lisp. This allows older Lisp software to be ported to Common Lisp.

Work on Common Lisp started in 1981 after an initiative by ARPA manager Bob Engelmore to develop a single community standard Lisp dialect. Much of the initial language design was done via electronic mail. In 1982, Guy L. Steele, Jr. gave the first overview of Common Lisp at the 1982 ACM Symposium on LISP and functional programming.

The first language documentation was published 1984 as Common Lisp the Language, first edition. A second edition, published in 1990, incorporated many changes to the language, made during the ANSI Common Lisp standardization process. The final ANSI Common Lisp standard then was published in 1994. Since then no update to the standard has been published. Various extensions and improvements to Common Lisp (examples are Unicode, Concurrency, CLOS-based IO) have been provided by implementations and libraries (many available via Quicklisp).

Common Lisp is a dialect of Lisp. It uses S-expressions to denote both code and data structure. Function calls, macro forms and special forms are written as lists, with the name of the operator first, as in these examples:

Common Lisp has many data types.

"Number" types include integers, ratios, floating-point numbers, and complex numbers. Common Lisp uses bignums to represent numerical values of arbitrary size and precision. The ratio type represents fractions exactly, a facility not available in many languages. Common Lisp automatically coerces numeric values among these types as appropriate.

The Common Lisp "character" type is not limited to ASCII characters. Most modern implementations allow Unicode characters.

The "symbol" type is common to Lisp languages, but largely unknown outside them. A symbol is a unique, named data object with several parts: name, value, function, property list and package. Of these, "value cell" and "function cell" are the most important. Symbols in Lisp are often used similarly to identifiers in other languages: to hold the value of a variable; however there are many other uses. Normally, when a symbol is evaluated, its value is returned. Some symbols evaluate to themselves, for example all symbols in the keyword package are self-evaluating. Boolean values in Common Lisp are represented by the self-evaluating symbols T and NIL. Common Lisp has namespaces for symbols, called 'packages'.

A number of functions are available for rounding scalar numeric values in various ways. The function codice_2 rounds the argument to the nearest integer, with halfway cases rounded to the even integer. The functions codice_3, codice_4, and codice_5 round towards zero, down, or up respectively. All these functions return the discarded fractional part as a secondary value. For example, codice_6 yields -3, 0.5; codice_7 yields -2, -0.5; codice_8 yields 2, 0.5; and codice_9 yields 4, -0.5.

"Sequence" types in Common Lisp include lists, vectors, bit-vectors, and strings. There are many operations that can work on any sequence type.

As in almost all other Lisp dialects, "lists" in Common Lisp are composed of "conses", sometimes called "cons cells" or "pairs". A cons is a data structure with two slots, called its "car" and "cdr". A list is a linked chain of conses or the empty list. Each cons's car refers to a member of the list (possibly another list). Each cons's cdr refers to the next cons—except for the last cons in a list, whose cdr refers to the codice_10 value. Conses can also easily be used to implement trees and other complex data structures; though it is usually advised to use structure or class instances instead. It is also possible to create circular data structures with conses.

Common Lisp supports multidimensional "arrays", and can dynamically resize "adjustable" arrays if required. Multidimensional arrays can be used for matrix mathematics. A "vector" is a one-dimensional array. Arrays can carry any type as members (even mixed types in the same array) or can be specialized to contain a specific type of members, as in a vector of bits. Usually only a few types are supported. Many implementations can optimize array functions when the array used is type-specialized. Two type-specialized array types are standard: a "string" is a vector of characters, while a "bit-vector" is a vector of bits.

"Hash tables" store associations between data objects. Any object may be used as key or value. Hash tables are automatically resized as needed.

"Packages" are collections of symbols, used chiefly to separate the parts of a program into namespaces. A package may "export" some symbols, marking them as part of a public interface. Packages can use other packages.

"Structures", similar in use to C structs and Pascal records, represent arbitrary complex data structures with any number and type of fields (called "slots"). Structures allow single-inheritance.

"Classes" are similar to structures, but offer more dynamic features and multiple-inheritance. (See CLOS). Classes have been added late to Common Lisp and there is some conceptual overlap with structures. Objects created of classes are called "Instances". A special case are Generic Functions. Generic Functions are both functions and instances.

Common Lisp supports first-class functions. For instance, it is possible to write functions that take other functions as arguments or return functions as well. This makes it possible to describe very general operations.

The Common Lisp library relies heavily on such higher-order functions. For example, the codice_11 function takes a relational operator as an argument and key function as an optional keyword argument. This can be used not only to sort any type of data, but also to sort data structures according to a key.

The evaluation model for functions is very simple. When the evaluator encounters a form codice_12 then it presumes that the symbol named f is one of the following:


If f is the name of a function, then the arguments a1, a2, ..., an are evaluated in left-to-right order, and the function is found and invoked with those values supplied as parameters.

The macro codice_14 defines functions where a function definition gives the name of the function, the names of any arguments, and a function body:

Function definitions may include compiler directives, known as "declarations", which provide hints to the compiler about optimization settings or the data types of arguments. They may also include "documentation strings" (docstrings), which the Lisp system may use to provide interactive documentation:

Anonymous functions (function literals) are defined using codice_13 expressions, e.g. codice_16 for a function that squares its argument. Lisp programming style frequently uses higher-order functions for which it is useful to provide anonymous functions as arguments.

Local functions can be defined with codice_17 and codice_18.

There are a number of other operators related to the definition and manipulation of functions. For instance, a function may be compiled with the codice_19 operator. (Some Lisp systems run functions using an interpreter by default unless instructed to compile; others compile every function).

The macro codice_20 defines generic functions. Generic functions are a collection of methods.
The macro codice_21 defines methods.

Methods can specialize their parameters over CLOS "standard classes", "system classes", "structure classes" or objects. For many types there are corresponding "system classes".

When a generic function is called, multiple-dispatch will determine the effective method to use.

Generic Functions are also a first class data type. There are many more features to Generic Functions and Methods than described above.

The namespace for function names is separate from the namespace for data variables. This is a key difference between Common Lisp and Scheme. For Common Lisp, operators that define names in the function namespace include codice_14, codice_17, codice_18, codice_21 and codice_20.

To pass a function by name as an argument to another function, one must use the codice_27 special operator, commonly abbreviated as codice_28. The first codice_11 example above refers to the function named by the symbol codice_30 in the function namespace, with the code codice_31. Conversely, to call a function passed in such a way, one would use the codice_32 operator on the argument.

Scheme's evaluation model is simpler: there is only one namespace, and all positions in the form are evaluated (in any order) -- not just the arguments. Code written in one dialect is therefore sometimes confusing to programmers more experienced in the other. For instance, many Common Lisp programmers like to use descriptive variable names such as "list" or "string" which could cause problems in Scheme, as they would locally shadow function names.

Whether a separate namespace for functions is an advantage is a source of contention in the Lisp community. It is usually referred to as the "Lisp-1 vs. Lisp-2 debate". Lisp-1 refers to Scheme's model and Lisp-2 refers to Common Lisp's model. These names were coined in a 1988 paper by Richard P. Gabriel and Kent Pitman, which extensively compares the two approaches.

Common Lisp supports the concept of "multiple values", where any expression always has a single "primary value", but it might also have any number of "secondary values", which might be received and inspected by interested callers. This concept is distinct from returning a list value, as the secondary values are fully optional, and passed via a dedicated side channel. This means that callers may remain entirely unaware of the secondary values being there if they have no need for them, and it makes it convenient to use the mechanism for communicating information that is sometimes useful, but not always necessary. For example,




Multiple values are supported by a handful of standard forms, most common of which are the codice_35 special form for accessing secondary values and codice_36 for returning multiple values:

Other data types in Common Lisp include:


Like programs in many other programming languages, Common Lisp programs make use of names to refer to variables, functions, and many other kinds of entities. Named references are subject to scope.

The association between a name and the entity which the name refers to is called a binding.

Scope refers to the set of circumstances in which a name is determined to have a particular binding.

The circumstances which determine scope in Common Lisp include:


To understand what a symbol refers to, the Common Lisp programmer must know what kind of reference is being expressed, what kind of scope it uses if it is a variable reference (dynamic versus lexical scope), and also the run-time situation: in what environment is the reference resolved, where was the binding introduced into the environment, et cetera.

Some environments in Lisp are globally pervasive. For instance, if a new type is defined, it is known everywhere thereafter. References to that type look it up in this global environment.

One type of environment in Common Lisp is the dynamic environment. Bindings established in this environment have dynamic extent, which means that a binding is established at the start of the execution of some construct, such as a codice_51 block, and disappears when that construct finishes executing: its lifetime is tied to the dynamic activation and deactivation of a block. However, a dynamic binding is not just visible within that block; it is also visible to all functions invoked from that block. This type of visibility is known as indefinite scope. Bindings which exhibit dynamic extent (lifetime tied to the activation and deactivation of a block) and indefinite scope (visible to all functions which are called from that block) are said to have dynamic scope.

Common Lisp has support for dynamically scoped variables, which are also called special variables. Certain other kinds of bindings are necessarily dynamically scoped also, such as restarts and catch tags. Function bindings cannot be dynamically scoped using codice_17 (which only provides lexically scoped function bindings), but function objects (a first-level object in Common Lisp) can be assigned to dynamically scoped variables, bound using codice_51 in dynamic scope, then called using codice_32 or codice_57.

Dynamic scope is extremely useful because it adds referential clarity and discipline to global variables. Global variables are frowned upon in computer science as potential sources of error, because they can give rise to ad-hoc, covert channels of communication among modules that lead to unwanted, surprising interactions.

In Common Lisp, a special variable which has only a top-level binding behaves just like a global variable in other programming languages. A new value can be stored into it, and that value simply replaces what is in the top-level binding. Careless replacement of the value of a global variable is at the heart of bugs caused by use of global variables. However, another way to work with a special variable is to give it a new, local binding within an expression. This is sometimes referred to as "rebinding" the variable. Binding a dynamically scoped variable temporarily creates a new memory location for that variable, and associates the name with that location. While that binding is in effect, all references to that variable refer to the new binding; the previous binding is hidden. When execution of the binding expression terminates, the temporary memory location is gone, and the old binding is revealed, with the original value intact. Of course, multiple dynamic bindings for the same variable can be nested.

In Common Lisp implementations which support multithreading, dynamic scopes are specific to each thread of execution. Thus special variables serve as an abstraction for thread local storage. If one thread rebinds a special variable, this rebinding has no effect on that variable in other threads. The value stored in a binding can only be retrieved by the thread which created that binding. If each thread binds some special variable codice_58, then codice_58 behaves like thread-local storage. Among threads which do not rebind codice_58, it behaves like an ordinary global: all of these threads refer to the same top-level binding of codice_58.

Dynamic variables can be used to extend the execution context with additional context information which is implicitly passed from function to function without having to appear as an extra function parameter. This is especially useful when the control transfer has to pass through layers of unrelated code, which simply cannot be extended with extra parameters to pass the additional data. A situation like this usually calls for a global variable. That global variable must be saved and restored, so that the scheme doesn't break under recursion: dynamic variable rebinding takes care of this. And that variable must be made thread-local (or else a big mutex must be used) so the scheme doesn't break under threads: dynamic scope implementations can take care of this also.

In the Common Lisp library, there are many standard special variables. For instance, all standard I/O streams are stored in the top-level bindings of well-known special variables. The standard output stream is stored in *standard-output*.

Suppose a function foo writes to standard output:

To capture its output in a character string, *standard-output* can be bound to a string stream and called:

Common Lisp supports lexical environments. Formally, the bindings in a lexical environment have lexical scope and may have either indefinite extent or dynamic extent, depending on the type of namespace. Lexical scope means that visibility is physically restricted to the block in which the binding is established. References which are not textually (i.e. lexically) embedded in that block simply do not see that binding.

The tags in a TAGBODY have lexical scope. The expression (GO X) is erroneous if it is not actually embedded in a TAGBODY which contains a label X. However, the label bindings disappear when the TAGBODY terminates its execution, because they have dynamic extent. If that block of code is re-entered by the invocation of a lexical closure, it is invalid for the body of that closure to try to transfer control to a tag via GO:

When the TAGBODY is executed, it first evaluates the setf form which stores a function in the special variable *stashed*. Then the (go end-label) transfers control to end-label, skipping the code (print "Hello"). Since end-label is at the end of the tagbody, the tagbody terminates, yielding NIL. Suppose that the previously remembered function is now called:

This situation is erroneous. One implementation's response is an error condition containing the message, "GO: tagbody for tag SOME-LABEL has already been left". The function tried to evaluate (go some-label), which is lexically embedded in the tagbody, and resolves to the label. However, the tagbody isn't executing (its extent has ended), and so the control transfer cannot take place.

Local function bindings in Lisp have lexical scope, and variable bindings also have lexical scope by default. By contrast with GO labels, both of these have indefinite extent. When a lexical function or variable binding is established, that binding continues to exist for as long as references to it are possible, even after the construct which established that binding has terminated. References to lexical variables and functions after the termination of their establishing construct are possible thanks to lexical closures.

Lexical binding is the default binding mode for Common Lisp variables. For an individual symbol, it can be switched to dynamic scope, either by a local declaration, by a global declaration. The latter may occur implicitly through the use of a construct like DEFVAR or DEFPARAMETER. It is an important convention in Common Lisp programming that special (i.e. dynamically scoped) variables have names which begin and end with an asterisk sigil codice_62 in what is called the “earmuff convention”. If adhered to, this convention effectively creates a separate namespace for special variables, so that variables intended to be lexical are not accidentally made special.

Lexical scope is useful for several reasons.

Firstly, references to variables and functions can be compiled to efficient machine code, because the run-time environment structure is relatively simple. In many cases it can be optimized to stack storage, so opening and closing lexical scopes has minimal overhead. Even in cases where full closures must be generated, access to the closure's environment is still efficient; typically each variable becomes an offset into a vector of bindings, and so a variable reference becomes a simple load or store instruction with a base-plus-offset addressing mode.

Secondly, lexical scope (combined with indefinite extent) gives rise to the lexical closure, which in turn creates a whole paradigm of programming centered around the use of functions being first-class objects, which is at the root of functional programming.

Thirdly, perhaps most importantly, even if lexical closures are not exploited, the use of lexical scope isolates program modules from unwanted interactions. Due to their restricted visibility, lexical variables are private. If one module A binds a lexical variable X, and calls another module B, references to X in B will not accidentally resolve to the X bound in A. B simply has no access to X. For situations in which disciplined interactions through a variable are desirable, Common Lisp provides special variables. Special variables allow for a module A to set up a binding for a variable X which is visible to another module B, called from A. Being able to do this is an advantage, and being able to prevent it from happening is also an advantage; consequently, Common Lisp supports both lexical and dynamic scope.

A "macro" in Lisp superficially resembles a function in usage. However, rather than representing an expression which is evaluated, it represents a transformation of the program source code. The macro gets the source it surrounds as arguments, binds them to its parameters and computes a new source form. This new form can also use a macro. The macro expansion is repeated until the new source form does not use a macro. The final computed form is the source code executed at runtime.

Typical uses of macros in Lisp:


Various standard Common Lisp features also need to be implemented as macros, such as:


Macros are defined by the "defmacro" macro. The special operator "macrolet" allows the definition of local (lexically scoped) macros. It is also possible to define macros for symbols using "define-symbol-macro" and "symbol-macrolet".

Paul Graham's book On Lisp describes the use of macros in Common Lisp in detail. Doug Hoyte's book Let Over Lambda extends the discussion on macros, claiming "Macros are the single greatest advantage that lisp has as a programming language and the single greatest advantage of any programming language." Hoyte provides several examples of iterative development of macros.

Macros allow Lisp programmers to create new syntactic forms in the language. One typical use is to create new control structures. The example macro provides an codice_73 looping construct. The syntax is:

The macro definition for "until":
"tagbody" is a primitive Common Lisp special operator which provides the ability to name tags and use the "go" form to jump to those tags. The backquote "`" provides a notation that provides code templates, where the value of forms preceded with a comma are filled in. Forms preceded with comma and at-sign are "spliced" in. The tagbody form tests the end condition. If the condition is true, it jumps to the end tag. Otherwise the provided body code is executed and then it jumps to the start tag.

An example form using above "until" macro:
The code can be expanded using the function "macroexpand-1". The expansion for above example looks like this:
During macro expansion the value of the variable "test" is "(= (random 10) 0)" and the value of the variable "body" is "((write-line "Hello"))". The body is a list of forms.

Symbols are usually automatically upcased. The expansion uses the TAGBODY with two labels. The symbols for these labels are computed by GENSYM and are not interned in any package. Two "go" forms use these tags to jump to. Since "tagbody" is a primitive operator in Common Lisp (and not a macro), it will not be expanded into something else. The expanded form uses the "when" macro, which also will be expanded. Fully expanding a source form is called "code walking".

In the fully expanded ("walked") form, the "when" form is replaced by the primitive "if":
All macros must be expanded before the source code containing them can be evaluated or compiled normally. Macros can be considered functions that accept and return S-expressions - similar to abstract syntax trees, but not limited to those. These functions are invoked before the evaluator or compiler to produce the final source code.
Macros are written in normal Common Lisp, and may use any Common Lisp (or third-party) operator available.

Common Lisp macros are capable of what is commonly called "variable capture", where symbols in the macro-expansion body coincide with those in the calling context, allowing the programmer to create macros wherein various symbols have special meaning. The term "variable capture" is somewhat misleading, because all namespaces are vulnerable to unwanted capture, including the operator and function namespace, the tagbody label namespace, catch tag, condition handler and restart namespaces.

"Variable capture" can introduce software defects. This happens in one of the following two ways:


The Scheme dialect of Lisp provides a macro-writing system which provides the referential transparency that eliminates both types of capture problem. This type of macro system is sometimes called "hygienic", in particular by its proponents (who regard macro systems which do not automatically solve this problem as unhygienic). 

In Common Lisp, macro hygiene is ensured one of two different ways.

One approach is to use gensyms: guaranteed-unique symbols which can be used in a macro-expansion without threat of capture. The use of gensyms in a macro definition is a manual chore, but macros can be written which simplify the instantiation and use of gensyms. Gensyms solve type 2 capture easily, but they are not applicable to type 1 capture in the same way, because the macro expansion cannot rename the interfering symbols in the surrounding code which capture its references. Gensyms could be used to provide stable aliases for the global symbols which the macro expansion needs. The macro expansion would use these secret aliases rather than the well-known names, so redefinition of the well-known names would have no ill effect on the macro.

Another approach is to use packages. A macro defined in its own package can simply use internal symbols in that package in its expansion. The use of packages deals with type 1 and type 2 capture.

However, packages don't solve the type 1 capture of references to standard Common Lisp functions and operators. The reason is that the use of packages to solve capture problems revolves around the use of private symbols (symbols in one package, which are not imported into, or otherwise made visible in other packages). Whereas the Common Lisp library symbols are external, and frequently imported into or made visible in user-defined packages.

The following is an example of unwanted capture in the operator namespace, occurring in the expansion of a macro:

The codice_73 macro will expand into a form which calls codice_75 which is intended to refer to the standard Common Lisp macro codice_75. However, in this context, codice_75 may have a completely different meaning, so codice_73 may not work properly.

Common Lisp solves the problem of the shadowing of standard operators and functions by forbidding their redefinition. Because it redefines the standard operator codice_75, the preceding is actually a fragment of non-conforming Common Lisp, which allows implementations to diagnose and reject it.

The "condition system" is responsible for exception handling in Common Lisp. It provides "conditions", "handler"s and "restart"s. "Condition"s are objects describing an exceptional situation (for example an error). If a "condition" is signaled, the Common Lisp system searches for a "handler" for this condition type and calls the handler. The "handler" can now search for restarts and use one of these restarts to automatically repair the current problem, using information such as the condition type and any relevant information provided as part of the condition object, and call the appropriate restart function.

These restarts, if unhandled by code, can be presented to users (as part of a user interface, that of a debugger for example), so that the user can select and invoke one of the available restarts. Since the condition handler is called in the context of the error (without unwinding the stack), full error recovery is possible in many cases, where other exception handling systems would have already terminated the current routine. The debugger itself can also be customized or replaced using the codice_80 dynamic variable. Code found within "unwind-protect" forms such as finalizers will also be executed as appropriate despite the exception.

In the following example (using Symbolics Genera) the user tries to open a file in a Lisp function "test" called from the Read-Eval-Print-LOOP (REPL), when the file does not exist. The Lisp system presents four restarts. The user selects the "Retry OPEN using a different pathname" restart and enters a different pathname (lispm-init.lisp instead of lispm-int.lisp). The user code does not contain any error handling code. The whole error handling and restart code is provided by the Lisp system, which can handle and repair the error without terminating the user code.

Common Lisp includes a toolkit for object-oriented programming, the Common Lisp Object System or CLOS, which is one of the most powerful object systems available in any language. For example, Peter Norvig explains how many Design Patterns are simpler to implement in a dynamic language with the features of CLOS (Multiple Inheritance, Mixins, Multimethods, Metaclasses, Method combinations, etc.).
Several extensions to Common Lisp for object-oriented programming have been proposed to be included into the ANSI Common Lisp standard, but eventually CLOS was adopted as the standard object-system for Common Lisp. CLOS is a dynamic object system with multiple dispatch and multiple inheritance, and differs radically from the OOP facilities found in static languages such as C++ or Java. As a dynamic object system, CLOS allows changes at runtime to generic functions and classes. Methods can be added and removed, classes can be added and redefined, objects can be updated for class changes and the class of objects can be changed.

CLOS has been integrated into ANSI Common Lisp. Generic functions can be used like normal functions and are a first-class data type. Every CLOS class is integrated into the Common Lisp type system. Many Common Lisp types have a corresponding class. There is more potential use of CLOS for Common Lisp. The specification does not say whether conditions are implemented with CLOS. Pathnames and streams could be implemented with CLOS. These further usage possibilities of CLOS for ANSI Common Lisp are not part of the standard. Actual Common Lisp implementations use CLOS for pathnames, streams, input–output, conditions, the implementation of CLOS itself and more.

Several implementations of earlier Lisp dialects provided both an interpreter and a compiler. Unfortunately often the semantics were different. These earlier Lisps implemented lexical scoping in the compiler and dynamic scoping in the interpreter. Common Lisp requires that both the interpreter and compiler use lexical scoping by default. The Common Lisp standard describes both the semantics of the interpreter and a compiler. The compiler can be called using the function "compile" for individual functions and using the function "compile-file" for files. Common Lisp allows type declarations and provides ways to influence the compiler code generation policy. For the latter various optimization qualities can be given values between 0 (not important) and 3 (most important): "speed", "space", "safety", "debug" and "compilation-speed".

There is also a function to evaluate Lisp code: codice_81. codice_81 takes code as pre-parsed s-expressions and not, like in some other languages, as text strings. This way code can be constructed with the usual Lisp functions for constructing lists and symbols and then this code can be evaluated with the function codice_81. Several Common Lisp implementations (like Clozure CL and SBCL) are implementing codice_81 using their compiler. This way code is compiled, even though it is evaluated using the function codice_81.

The file compiler is invoked using the function "compile-file". The generated file with compiled code is called a "fasl" (from "fast load") file. These "fasl" files and also source code files can be loaded with the function "load" into a running Common Lisp system. Depending on the implementation, the file compiler generates byte-code (for example for the Java Virtual Machine), C language code (which then is compiled with a C compiler) or, directly, native code.

Common Lisp implementations can be used interactively, even though the code gets fully compiled. The idea of an Interpreted language thus does not apply for interactive Common Lisp.

The language makes distinction between read-time, compile-time, load-time and run-time, and allows user code to also make this distinction to perform the wanted type of processing at the wanted step.

Some special operators are provided to especially suit interactive development; for instance, codice_86 will only assign a value to its provided variable if it wasn't already bound, while codice_87 will always perform the assignment. This distinction is useful when interactively evaluating, compiling and loading code in a live image.

Some features are also provided to help writing compilers and interpreters. Symbols consist of first-level objects and are directly manipulable by user code. The codice_88 special operator allows to create lexical bindings programmatically, while packages are also manipulable. The Lisp compiler is available at runtime to compile files or individual functions. These make it easy to use Lisp as an intermediate compiler or interpreter for another language.

The following program calculates the smallest number of people in a room for whom the probability of completely unique birthdays is less than 50% (the birthday paradox, where for 1 person the probability is obviously 100%, for 2 it is 364/365, etc.). The answer is 23.

By convention, constants in Common Lisp are enclosed with + characters.

Calling the example function using the REPL (Read Eval Print Loop):

We define a class codice_89 and a method for displaying the name and age of a person.
Next we define a group of persons as a list of codice_89 objects.
Then we iterate over the sorted list.

It prints the three names with descending age.

Use of the LOOP macro is demonstrated:

Example use:

Compare with the built in exponentiation:

WITH-OPEN-FILE is a macro that opens a file and provides a stream. When the form is returning, the file is automatically closed. FUNCALL calls a function object. The LOOP collects all lines that match the predicate.

The function AVAILABLE-SHELLS calls above function LIST-MATCHING-LINES with a pathname and an anonymous function as the predicate. The predicate returns the pathname of a shell or NIL (if the string is not the filename of a shell).

Example results (on Mac OS X 10.6):

Common Lisp is most frequently compared with, and contrasted to, Scheme—if only because they are the two most popular Lisp dialects. Scheme predates CL, and comes not only from the same Lisp tradition but from some of the same engineers—Guy L. Steele, with whom Gerald Jay Sussman designed Scheme, chaired the standards committee for Common Lisp.

Common Lisp is a general-purpose programming language, in contrast to Lisp variants such as Emacs Lisp and AutoLISP which are extension languages embedded in particular products (GNU Emacs and AutoCAD, respectively). Unlike many earlier Lisps, Common Lisp (like Scheme) uses lexical variable scope by default for both interpreted and compiled code.

Most of the Lisp systems whose designs contributed to Common Lisp—such as ZetaLisp and Franz Lisp—used dynamically scoped variables in their interpreters and lexically scoped variables in their compilers. Scheme introduced the sole use of lexically scoped variables to Lisp; an inspiration from ALGOL 68 which was widely recognized as a good idea. CL supports dynamically scoped variables as well, but they must be explicitly declared as "special". There are no differences in scoping between ANSI CL interpreters and compilers.

Common Lisp is sometimes termed a "Lisp-2" and Scheme a "Lisp-1", referring to CL's use of separate namespaces for functions and variables. (In fact, CL has "many" namespaces, such as those for go tags, block names, and codice_72 keywords). There is a long-standing controversy between CL and Scheme advocates over the tradeoffs involved in multiple namespaces. In Scheme, it is (broadly) necessary to avoid giving variables names which clash with functions; Scheme functions frequently have arguments named codice_92, codice_93, or codice_94 so as not to conflict with the system function codice_95. However, in CL it is necessary to explicitly refer to the function namespace when passing a function as an argument—which is also a common occurrence, as in the codice_11 example above.

CL also differs from Scheme in its handling of boolean values. Scheme uses the special values #t and #f to represent truth and falsity. CL follows the older Lisp convention of using the symbols T and NIL, with NIL standing also for the empty list. In CL, "any" non-NIL value is treated as true by conditionals, such as codice_68, whereas in Scheme all non-#f values are treated as true. These conventions allow some operators in both languages to serve both as predicates (answering a boolean-valued question) and as returning a useful value for further computation, but in Scheme the value '() which is equivalent to NIL in Common Lisp evaluates to true in a boolean expression.

Lastly, the Scheme standards documents require tail-call optimization, which the CL standard does not. Most CL implementations do offer tail-call optimization, although often only when the programmer uses an optimization directive. Nonetheless, common CL coding style does not favor the ubiquitous use of recursion that Scheme style prefers—what a Scheme programmer would express with tail recursion, a CL user would usually express with an iterative expression in codice_75, codice_99, codice_72, or (more recently) with the codice_101 package.

See the Category .

Common Lisp is defined by a specification (like Ada and C) rather than by one implementation (like Perl before version 6). There are many implementations, and the standard details areas in which they may validly differ.

In addition, implementations tend to come with extensions, which provide functionality not covered in the standard:


Free and open-source software libraries have been created to support extensions to Common Lisp in a portable way, and are most notably found in the repositories of the Common-Lisp.net and CLOCC (Common Lisp Open Code Collection) projects.

Common Lisp implementations may use any mix of native code compilation, byte code compilation or interpretation. Common Lisp has been designed to support incremental compilers, file compilers and block compilers. Standard declarations to optimize compilation (such as function inlining or type specialization) are proposed in the language specification. Most Common Lisp implementations compile source code to native machine code. Some implementations can create (optimized) stand-alone applications. Others compile to interpreted bytecode, which is less efficient than native code, but eases binary-code portability. There are also compilers that compile Common Lisp code to C code. The misconception that Lisp is a purely interpreted language is most likely because Lisp environments provide an interactive prompt and that code is compiled one-by-one, in an incremental way. With Common Lisp incremental compilation is widely used.

Some Unix-based implementations (CLISP, SBCL) can be used as a scripting language; that is, invoked by the system transparently in the way that a Perl or Unix shell interpreter is.




See the Category .

Common Lisp is used to develop research applications (often in Artificial Intelligence), for rapid development of prototypes or for deployed applications

Common Lisp is used in many commercial applications, including the Yahoo! Store web-commerce site, which originally involved Paul Graham and was later rewritten in C++ and Perl. Other notable examples include:


There also exist open-source applications written in Common Lisp, such as:


Since 2011, Zach Beane, with support of the Common Lisp Foundation, has maintained the Quicklisp library manager. It allows automatic download, installing, and loading of over 3600 libraries, all of which are required to work on more than just one implementation of Common Lisp and to have a license that allows their redistribution.

A chronological list of books published (or about to be published) about Common Lisp (the language) or about programming with Common Lisp (especially AI programming).



</doc>
<doc id="6069" url="https://en.wikipedia.org/wiki?curid=6069" title="Color code">
Color code

A color code or colour code is a system for displaying information by using different colors.

The earliest examples of color codes in use are for long distance communication by use of flags, as in semaphore communication. The United Kingdom adopted a color code scheme for such communication wherein red signified danger and white signified safety, with other colors having similar assignments of meaning.

As chemistry and other technologies advanced, it became expedient to use coloration as a signal for telling apart things that would otherwise be confusingly similar, such as wiring in electrical and electronic devices, and pharmaceutical pills.

The use of color codes has been extended to abstractions, such as the Homeland Security Advisory System color code in the United States. Similarly, hospital emergency codes often incorporate colors (such as the widely used "Code Blue" indicating a cardiac arrest), although they may also include numbers, and may not conform to a uniform standard.

Color codes do present some potential problems. On forms and signage, the use of color can distract from black and white text. They are often difficult for color blind and blind people to interpret, and even for those with normal color vision, use of a large number of colors to code a large number of variables can lead to use of confusingly similar colors.

Systems incorporating color-coding include:




</doc>
<doc id="6080" url="https://en.wikipedia.org/wiki?curid=6080" title="CGI">
CGI

CGI may refer to:






</doc>
<doc id="6082" url="https://en.wikipedia.org/wiki?curid=6082" title="Cortex">
Cortex

Cortex or cortical may refer to:






</doc>
<doc id="6084" url="https://en.wikipedia.org/wiki?curid=6084" title="Collection">
Collection

Collection or Collections may refer to:

Collection may also refer to:




</doc>
<doc id="6085" url="https://en.wikipedia.org/wiki?curid=6085" title="Cauchy sequence">
Cauchy sequence

In mathematics, a Cauchy sequence (; ), named after Augustin-Louis Cauchy, is a sequence whose elements become arbitrarily close to each other as the sequence progresses. More precisely, given any small positive distance, all but a finite number of elements of the sequence are less than that given distance from each other.

It is not sufficient for each term to become arbitrarily close to the term. For instance, in the sequence of square roots of natural numbers:
the consecutive terms become arbitrarily close to each other:
However, with growing values of the index , the terms become arbitrarily large, so for any index and distance , there exists an index big enough such that . (Actually, any suffices.) As a result, despite how far one goes, the remaining terms of the sequence never get close to , hence the sequence is not Cauchy.

The utility of Cauchy sequences lies in the fact that in a complete metric space (one where all such sequences are known to converge to a limit), the criterion for convergence depends only on the terms of the sequence itself, as opposed to the definition of convergence, which uses the limit value as well as the terms. This is often exploited in algorithms, both theoretical and applied, where an iterative process can be shown relatively easily to produce a Cauchy sequence, consisting of the iterates, thus fulfilling a logical condition, such as termination.

The notions above are not as unfamiliar as they might at first appear. The customary acceptance of the fact that any real number "x" has a decimal expansion is an implicit acknowledgment that a particular Cauchy sequence of rational numbers (whose terms are the successive truncations of the decimal expansion of "x") has the real limit "x". In some cases it may be difficult to describe "x" independently of such a limiting process involving rational numbers.

Generalizations of Cauchy sequences in more abstract uniform spaces exist in the form of Cauchy filters and Cauchy nets.

A sequence

of real numbers is called a Cauchy sequence if for every positive real number "ε", there is a positive integer "N" such that for all natural numbers "m", "n" > "N"

where the vertical bars denote the absolute value. In a similar way one can define Cauchy sequences of rational or complex numbers. Cauchy formulated such a condition by requiring formula_5 to be infinitesimal for every pair of infinite "m", "n".

Since the definition of a Cauchy sequence only involves metric concepts, it is straightforward to generalize it to any metric space "X". To do so, the absolute value |"x" - "x"| is replaced by the distance "d"("x", "x") (where "d" denotes a metric) between "x" and "x".

Formally, given a metric space ("X", "d"), a sequence

is Cauchy, if for every positive real number "ε" > 0 there is a positive integer "N" such that for all positive integers "m", "n" > "N", the distance

Roughly speaking, the terms of the sequence are getting closer and closer together in a way that suggests that the sequence ought to have a limit in "X". Nonetheless, such a limit does not always exist within "X": the property of a space that every Cauchy sequence converges in the space is called "completeness", and is detailed below.

A metric space ("X", "d") in which every Cauchy sequence converges to an element of "X" is called complete.

The real numbers are complete under the metric induced by the usual absolute value, and one of the standard constructions of the real numbers involves Cauchy sequences of rational numbers. In this construction, each equivalence class of Cauchy sequences of rational numbers with a certain tail behavior—that is, each class of sequences that get arbitrarily close to one another— is a real number.

A rather different type of example is afforded by a metric space "X" which has the discrete metric (where any two distinct points are at distance 1 from each other). Any Cauchy sequence of elements of "X" must be constant beyond some fixed point, and converges to the eventually repeating term.

The rational numbers Q are not complete (for the usual distance):
There are sequences of rationals that converge (in R) to irrational numbers; these are Cauchy sequences having no limit in Q. In fact, if a real number "x" is irrational, then the sequence ("x"), whose "n"-th term is the truncation to "n" decimal places of the decimal expansion of "x", gives a Cauchy sequence of rational numbers with irrational limit "x". Irrational numbers certainly exist in R, for example:


The open interval formula_11 in the set of real numbers with an ordinary distance in R is not a complete space: there is a sequence formula_12 in it, which is Cauchy (for arbitrarily small distance bound formula_13 all terms formula_14 of formula_15 fit in the formula_16 interval), however does not converge in formula_17 — its 'limit', number formula_18, does not belong to the space formula_17.


These last two properties, together with the Bolzano–Weierstrass theorem, yield one standard proof of the completeness of the real numbers, closely related to both the Bolzano–Weierstrass theorem and the Heine–Borel theorem. Every Cauchy sequence of real numbers is bounded, hence by Bolzano-Weierstrass has a convergent subsequence, hence is itself convergent. This proof of the completeness of the real numbers implicitly makes use of the least upper bound axiom. The alternative approach, mentioned above, of the real numbers as the completion of the rational numbers, makes the completeness of the real numbers tautological.

One of the standard illustrations of the advantage of being able to work with Cauchy sequences and make use of completeness is provided by consideration of the summation of an infinite series of real numbers
(or, more generally, of elements of any complete normed linear space, or Banach space). Such a series 
formula_20 is considered to be convergent if and only if the sequence of partial sums formula_21 is convergent, where 
formula_22. It is a routine matter 
to determine whether the sequence of partial sums is Cauchy or not,
since for positive integers "p" > "q", 

If formula_24 is a uniformly continuous map between the metric spaces "M" and "N" and ("x") is a Cauchy sequence in "M", then formula_25 is a Cauchy sequence in "N". If formula_26 and formula_27 are two Cauchy sequences in the rational, real or complex numbers, then the sum formula_28 and the product formula_29 are also Cauchy sequences.

There is also a concept of Cauchy sequence for a topological vector space formula_30: Pick a local base formula_31 for formula_30 about 0; then (formula_33) is a Cauchy sequence if for each member formula_34, there is some number formula_35 such that whenever 
formula_36 is an element of formula_37. If the topology of formula_30 is compatible with a translation-invariant metric formula_39, the two definitions agree.

Since the topological vector space definition of Cauchy sequence requires only that there be a continuous "subtraction" operation, it can just as well be stated in the context of a topological group: A sequence formula_40 in a topological group formula_41 is a Cauchy sequence if for every open neighbourhood formula_42 of the identity in formula_41 there exists some number formula_35 such that whenever formula_45 it follows that formula_46. As above, it is sufficient to check this for the neighbourhoods in any local base of the identity in formula_41.

As in the construction of the completion of a metric space, one can furthermore define the binary relation on Cauchy sequences in formula_41 that formula_40 and formula_50 are equivalent if for every open neighbourhood formula_42 of the identity in formula_41 there exists some number formula_35 such that whenever formula_45 it follows that formula_55. This relation is an equivalence relation: It is reflexive since the sequences are Cauchy sequences. It is symmetric since formula_56 which by continuity of the inverse is another open neighbourhood of the identity. It is transitive since formula_57 where formula_58 and formula_59 are open neighbourhoods of the identity such that formula_60; such pairs exist by the continuity of the group operation.

There is also a concept of Cauchy sequence in a group formula_41:
Let formula_62 be a decreasing sequence of normal subgroups of formula_41 of finite index.
Then a sequence formula_26 in formula_41 is said to be Cauchy (w.r.t. formula_66) if and only if for any formula_67 there is formula_35 such that formula_69.

Technically, this is the same thing as a topological group Cauchy sequence for a particular choice of topology on formula_41, namely that for which formula_66 is a local base.

The set formula_72 of such Cauchy sequences forms a group (for the componentwise product), and the set formula_73 of null sequences (s.th. formula_74) is a normal subgroup of formula_72. The factor group formula_76 is called the completion of formula_41 with respect to formula_66.

One can then show that this completion is isomorphic to the inverse limit of the sequence formula_79.

An example of this construction, familiar in number theory and algebraic geometry is the construction of the "p"-adic completion of the integers with respect to a prime "p". In this case, "G" is the integers under addition, and "H" is the additive subgroup consisting of integer multiples of "p".

If formula_66 is a cofinal sequence (i.e., any normal subgroup of finite index contains some formula_81), then this completion is canonical in the sense that it is isomorphic to the inverse limit of formula_82, where formula_66 varies over normal subgroups of finite index. For further details, see ch. I.10 in Lang's "Algebra".

In constructive mathematics, Cauchy sequences often must be given with a "modulus of Cauchy convergence" to be useful. If formula_84 is a Cauchy sequence in the set formula_30, then a modulus of Cauchy convergence for the sequence is a function formula_86 from the set of natural numbers to itself, such that formula_87.

Clearly, any sequence with a modulus of Cauchy convergence is a Cauchy sequence. The converse (that every Cauchy sequence has a modulus) follows from the well-ordering property of the natural numbers (let formula_88 be the smallest possible formula_35 in the definition of Cauchy sequence, taking formula_67 to be formula_91). However, this well-ordering property does not hold in constructive mathematics (it is equivalent to the principle of excluded middle). On the other hand, this converse also follows (directly) from the principle of dependent choice (in fact, it will follow from the weaker AC), which is generally accepted by constructive mathematicians. Thus, moduli of Cauchy convergence are needed directly only by constructive mathematicians who (like Fred Richman) do not wish to use any form of choice.

That said, using a modulus of Cauchy convergence can simplify both definitions and theorems in constructive analysis. Perhaps even more useful are "regular Cauchy sequences", sequences with a given modulus of Cauchy convergence (usually formula_92 or formula_93). Any Cauchy sequence with a modulus of Cauchy convergence is equivalent (in the sense used to form the completion of a metric space) to a regular Cauchy sequence; this can be proved without using any form of the axiom of choice. Regular Cauchy sequences were used by Errett Bishop in his Foundations of Constructive Analysis, but they have also been used by Douglas Bridges in a non-constructive textbook (). However, Bridges also works on mathematical constructivism; the concept has not spread far outside of that milieu.

A real sequence formula_94 has a natural hyperreal extension, defined for hypernatural values "H" of the index "n" in addition to the usual natural "n". The sequence is Cauchy if and only if for every infinite "H" and "K", the values formula_95 and formula_96 are infinitely close, or adequal, i.e. 
where "st" is the standard part function.

 introduced a notion of Cauchy completion of a category. Applied to Q (the category whose objects are rational numbers, and there is a morphism from "x" to "y" if and only if "x" ≤ "y"), this Cauchy completion yields R (again interpreted as a category using its natural ordering).




</doc>
<doc id="6088" url="https://en.wikipedia.org/wiki?curid=6088" title="Common Era">
Common Era

Common Era or Current Era (CE) is one of the notation systems for the world's most widely used calendar era. BCE (Before the Common Era or Before the Current Era) is the era before CE. BCE and CE are alternatives to the Dionysian BC and AD system respectively. The Dionysian era distinguishes eras using AD (", "[the] year of [the] Lord") and BC ("before Christ"). Since the two notation systems are numerically equivalent, " CE" corresponds to "AD " and "400 BCE" corresponds to "400 BC". Both notations refer to the Gregorian calendar (and its predecessor, the Julian calendar). The year-numbering system used by the Gregorian calendar is used throughout the world today, and is an international standard for civil calendars.

The expression has been traced back to 1615, when it first appeared in a book by Johannes Kepler as the Latin usage ", and to 1635 in English as "Vulgar Era". The term "Common Era" can be found in English as early as 1708, and became more widely used in the mid-19th century by Jewish religious scholars. In the later 20th century, the use of CE and BCE was popularized in academic and scientific publications as a culturally neutral term. It is also used by some authors and publishers who wish to emphasize sensitivity to non-Christians, by not explicitly referencing Jesus as "Christ" and "Dominus" ("Lord") through use of the abbreviation "AD".

The year numbering system used with Common Era notation was devised by the Christian monk Dionysius Exiguus in the year 525 to replace the Era of Martyrs system, because he did not wish to continue the memory of a tyrant who persecuted Christians. He attempted to number years from an initial reference date ("epoch"), an event he referred to as the Incarnation of Jesus. Dionysius labeled the column of the table in which he introduced the new era as ""Anni Domini Nostri Jesu Christi"".

Numbering years in this manner became more widespread in Europe with its usage by Bede in England in 731. Bede also introduced the practice of dating years before what he supposed was the year of birth of Jesus, and the practice of not using a year zero. In 1422, Portugal became the last Western European country to switch to the system begun by Dionysius.

The term "Common Era" is traced back in English to its appearance as "Vulgar Era" to distinguish dates on the Ecclesiastic calendar in popular use from dates of the regnal year, the year of reign of a sovereign, typically used in national law. (The word 'vulgar' originally meant 'of the ordinary people', with no derogatory associations).

The first use of the Latin term "anno aerae nostrae vulgaris" discovered so far was in a 1615 book by Johannes Kepler. Kepler uses it again, as "ab Anno vulgaris aerae", in a 1616 table of ephemerides, and again, as "ab anno vulgaris aerae", in 1617. A 1635 English edition of that book has the title page in English – so far, the earliest-found usage of "Vulgar Era" in English. A 1701 book edited by John LeClerc includes "Before Christ according to the Vulgar Æra, 6". A 1716 book in English by Dean Humphrey Prideaux says, "before the beginning of the vulgar æra, by which we now compute the years from his incarnation." A 1796 book uses the term "vulgar era of the nativity".

The first so-far-discovered usage of "Christian Era" is as the Latin phrase "annus aerae christianae" on the title page of a 1584 theology book. In 1649, the Latin phrase "annus æræ Christianæ" appeared in the title of an English almanac. A 1652 ephemeris is the first instance so-far-found for English usage of "Christian Era".

The English phrase "common Era" appears at least as early as 1708, and in a 1715 book on astronomy is used interchangeably with "Christian Era" and "Vulgar Era". A 1759 history book uses "common æra" in a generic sense, to refer to the common era of the Jews. The first-so-far found usage of the phrase "before the common era" is in a 1770 work that also uses "common era" and "vulgar era" as synonyms, in a translation of a book originally written in German. The 1797 edition of the Encyclopædia Britannica uses the terms "vulgar era" and "common era" synonymously. In 1835, in his book "Living Oracles", Alexander Campbell, wrote: "The vulgar Era, or Anno Domini; the fourth year of Jesus Christ, the first of which was but eight days", and also refers to the "common era" as a synonym for "vulgar era" with "the fact that our Lord was born on the 4th year before the vulgar era, called Anno Domini, thus making (for example) the 42d year from his birth to correspond with the 38th of the common era..." The "Catholic Encyclopedia" (1909) in at least one article reports all three terms (Christian, Vulgar, Common Era) being commonly understood by the early 20th century.

The phrase "common era", in lower case, also appeared in the 19th century in a "generic" sense, not necessarily to refer to the Christian Era, but to any system of dates in common use throughout a civilization. Thus, "the common era of the Jews", "the common era of the Mahometans", "common era of the world", "the common era of the foundation of Rome". When it did refer to the Christian Era, it was sometimes qualified, e.g., "common era of the Incarnation", "common era of the Nativity", or "common era of the birth of Christ".

An adapted translation of "Common Era" into pseudo-Latin as "Era Vulgaris" was adopted in the 20th century by some followers of Aleister Crowley, and thus the abbreviation "e.v." or "EV" may sometimes be seen as a replacement for AD.

Although Jews have their own Hebrew calendar, they often use the Gregorian calendar, without the AD prefix. As early as 1825, the abbreviation VE (for Vulgar Era) was in use among Jews to denote years in the Western calendar. Common Era notation has also been in use for Hebrew lessons for "more than a century". Some Jewish academics were already using the "CE" and "BCE" abbreviations by the mid-19th century, such as in 1856, when Rabbi and historian Morris Jacob Raphall used the abbreviation in his book "Post-Biblical History of The Jews".

In general publications, in the 200 years between 1808 and 2008 the ratio of usage of BCE to BC has increased by about 20% and CE to AD by about 50%, primarily since 1980.

Some academics in the fields of theology, education and history have adopted CE and BCE notation, although there is some disagreement.

More visible uses of Common Era notation have recently surfaced at major museums in the English-speaking world. Furthermore, several style guides now prefer or mandate its usage.
Even some style guides for Christian churches prefer its use: for example, the Episcopal Diocese "Maryland Church News".

In the United States, the usage of the BCE/CE notation in textbooks is growing. Some publications have moved over to using it exclusively. For example, the 2007 World Almanac was the first edition to switch over to the BCE/CE usage, ending a 138-year usage of the traditional BC/AD dating notation. It is used by the College Board in its history tests, and by the Norton Anthology of English Literature. Others have taken a different approach. The US-based History Channel uses BCE/CE notation in articles on non-Christian religious topics such as Jerusalem and Judaism.

In 2002, England and Wales introduced the BCE/CE notation system into the official school curriculum.

In June 2006, in the United States, the Kentucky State School Board reversed its decision to use BCE and CE in the state's new Program of Studies, leaving education of students about these concepts a matter of discretion at the local level.

Also in 2011, media reports suggested that the BC/AD notation in Australian school textbooks would be replaced by BCE/CE notation. The story became national news and drew opposition from some politicians and church leaders. Weeks after the story broke, the Australian Curriculum, Assessment and Reporting Authority denied the rumour and stated that the BC/AD notation would remain, with CE and BCE as an optional suggested learning activity.

The use of CE in Jewish scholarship was historically motivated by the desire to avoid the implicit "Our Lord" in the abbreviation "AD". Although other aspects of dating systems are based in Christian origins, AD is a direct reference to Jesus as Lord.

Proponents of the Common Era notation assert that the use of BCE/CE shows sensitivity to those who use the same year numbering system as the one that originated with and is currently used by Christians, but who are not themselves Christian.

Former United Nations Secretary-General Kofi Annan has argued:

[T]he Christian calendar no longer belongs exclusively to Christians. People of all faiths have taken to using it simply as a matter of convenience. There is so much interaction between people of different faiths and cultures – different civilizations, if you like – that some shared way of reckoning time is a necessity. And so the Christian Era has become the Common Era.

Adena K. Berkowitz, when arguing at the Supreme Court opted to use BCE and CE because "Given the multicultural society that we live in, the traditional Jewish designations – B.C.E. and C.E. – cast a wider net of inclusion".

Some oppose the Common Era notation for explicitly religious reasons. Because the BC/AD notation is based on the traditional year of the conception or birth of Jesus, some Christians are offended by the removal of the reference to him in era notation. The Southern Baptist Convention supports retaining the BC/AD abbreviations. Roman Catholic priest and writer on interfaith issues Raimon Panikkar argued that the BCE/CE usage is the less inclusive option as, in his view, using the designation BCE/CE is a "return... to the most bigoted Christian colonialism" towards non-Christians, who do not necessarily consider the time period following the beginning of the calendar to be a "common era".

There are also secular concerns. English language expert Kenneth G. Wilson speculated in his style guide that "if we do end by casting aside the AD/BC convention, almost certainly some will argue that we ought to cast aside as well the conventional numbering system [that is, the method of numbering years] itself, given its Christian basis." The short lived French Republican Calendar, for example, began with the first year of the French First Republic and rejected the seven-day week (with its connections to the Book of Genesis) for a ten-day week.

According to a "Los Angeles Times" report, it was a student's use of BCE/CE notation, inspired by its use within Wikipedia, which prompted the teacher and politician Andrew Schlafly to found Conservapedia, a cultural conservative wiki. One of its "Conservapedia Commandments" is that users must always apply BC/AD notation, since its sponsors perceive BCE/CE notation to "deny the historical basis" of the dating system.

The abbreviation BCE, just as with BC, always follows the year number. Unlike AD, which traditionally precedes the year number, CE always follows the year number (if context requires that it be written at all). Thus, the current year is written as in both notations (or, if further clarity is needed, as CE, or as AD ), and the year that Socrates died is represented as 399 BCE (the same year that is represented by 399 BC in the BC/AD notation). The abbreviations are sometimes written with small capital letters, or with periods (e.g., "B.C.E." or "C.E."). Style guides for academic texts on religion generally prefer BCE/CE to BC/AD.





</doc>
<doc id="6091" url="https://en.wikipedia.org/wiki?curid=6091" title="Charles Robert Malden">
Charles Robert Malden

Charles Robert Malden (9 August 1797 – 23 May 1855), was a nineteenth-century British naval officer, surveyor and educator. He is the discoverer of Malden Island in the central Pacific, which is named in his honour. He also founded Windlesham House School at Brighton, England.

Malden was born in Putney, Surrey, son of Jonas Malden, a surgeon. He entered British naval service at the age of 11 on 22 June 1809. He served nine years as a volunteer 1st class, midshipman, and shipmate, including one year in the English Channel and Bay of Biscay (1809), four years at the Cape of Good Hope and in the East Indies (1809–14), two and a half years on the North American and West Indian stations (1814–16), and a year and a half in the Mediterranean (1817–18). He was present at the capture of Mauritius and Java, and at the battles of Baltimore and New Orleans.

He passed the examination in the elements of mathematics and the theory of navigation at the Royal Naval Academy on 2–4 September 1816, and became a 1st Lieutenant on 1 September 1818. In eight years of active service as an officer, he served two and a half years in a surveying ship in the Mediterranean (1818–21), one and a half years in a surveying sloop in the English Channel and off the coast of Ireland (1823–24), and one and a half years as Surveyor of the frigate during a voyage (1824–26) to and from the Hawaiian Islands (then known as the "Sandwich islands").
In Hawaii he surveyed harbours which, he noted, were "said not to exist by Captains Cook and Vancouver." On the return voyage he discovered and explored uninhabited Malden Island in the central Pacific on 30 July 1825. After his return he left active service but remained at half pay. He served for several years as hydrographer to King William IV.

He married Frances Cole, daughter of Rev. William Hodgson Cole, rector of West Clandon and Vicar of Wonersh, near Guildford, Surrey, on 8 April 1828. Malden became the father of seven sons and a daughter.

From 1830-36 he took pupils for the Royal Navy at Ryde, Isle of Wight. He purchased the school of Henry Worsley at Newport, Isle of Wight, in December 1836, reopened it as a preparatory school on 20 February 1837, and moved it to Montpelier Road in Brighton in December 1837. He built the Windlesham House School at Brighton in 1844, and conducted the school until his death there in 1855.


</doc>
<doc id="6094" url="https://en.wikipedia.org/wiki?curid=6094" title="CPD">
CPD

CPD may refer to:







</doc>
<doc id="6095" url="https://en.wikipedia.org/wiki?curid=6095" title="Chechnya">
Chechnya

Chechnya (; ; , ), officially the Chechen Republic (; ; , ), is a federal subject (a republic) of Russia. 

It is a Federal Subject of Russia located in the North Caucasus, and within of the Caspian Sea. The capital of the republic is the city of Grozny. , the republic was reported to have a population of 1,268,989 people; however, that number has been questioned by multiple demographers, who think such population growth after two deadly wars is highly implausible.

After the dissolution of the Soviet Union in 1991, the Chechen-Ingush ASSR was split into two parts: the Republic of Ingushetia and the Chechen Republic. The latter proclaimed the Chechen Republic of Ichkeria, which sought independence. Following the First Chechen War with Russia, Chechnya gained "de facto" independence as the Chechen Republic of Ichkeria. Russian federal control was restored during the Second Chechen War. Since then there has been a systematic reconstruction and rebuilding process, though sporadic fighting continues to take place in the mountains and southern regions into 2019.

According to Leonti Mroveli, the 11th-century Georgian chronicler, the word Caucasian is derived from the Vainakh ancestor Kavkas.
According to George Anchabadze of Ilia State University American linguist Johanna Nichols "has used language to connect the modern people of the Caucasus region to the ancient farmers of the Fertile Crescent" and her research suggests that "farmers of the region were proto-Nakh-Daghestanians." Nichols stated: "The Nakh–Dagestanian languages are the closest thing we have to a direct continuation of the cultural and linguistic community that gave rise to Western civilization." Henry Harpending, University of Utah, supports her claims.

People living in prehistoric mountain cave settlements used tools, mastered fire, and used animal skins for warmth and other purposes. Traces of human settlement that date back to 40,000 BC were found near Lake Kezanoi. Cave paintings, artifacts, and other archaeological evidence indicates continuous habitation for some 8,000 years.

10,000–6000 BC

6000–4000 BC

4000–3000 BC

900–1200 AD

1239 AD

1300–1400 AD

1500 AD

Peter I first sought to increase Russia's political influence in the Caucasus and the Caspian Sea at the expense of Safavid Persia when he launched the Russo-Persian War (1722–1723). Notable in Chechen history, this particular Russo-Persian War marked the first military encounter between Imperial Russia and the Vainakh. Russian forces succeeded in taking much of the Caucasian territories from Iran for several years.

As the Russians took control of the Caspian corridor and moved into Persian-ruled Dagestan, Peter's forces ran into mountain tribes. Peter sent a cavalry force to subdue them, but the Chechens routed them. In 1732, after Russia already ceded back most of the Caucasus to Persia, now led by Nader Shah, following the Treaty of Resht, Russian troops clashed again with Chechens in a village called Chechen-aul along the Argun River. The Russians were defeated again and withdrew, but this battle is responsible for the apocryphal story about how the Nokchi came to be known as "Chechens"-the people ostensibly named for the place the battle had taken place. The name Chechen was however already used since as early as 1692.

Under intermittent Persian rule since 1555, in 1783 the eastern Georgians of Kartl-Kakheti led by Erekle II and Russia signed the Treaty of Georgievsk. According to this treaty, Kartl-Kakheti received protection from Russia, and Georgia abjured any dependence on Iran. In order to increase its influence in the Caucasus and to secure communications with Kartli and other minority Christian regions of the Transcaucasia which it considered useful in its wars against Persia and Turkey, the Russian Empire began conquering the Northern Caucasus mountains. The Russian Empire used Christianity to justify its conquests, allowing Islam to spread widely because it positioned itself as the religion of liberation from tsardom, which viewed Nakh tribes as "bandits". The rebellion was led by Mansur Ushurma, a Chechen Naqshbandi (Sufi) sheikh—with wavering military support from other North Caucasian tribes. Mansur hoped to establish a Transcaucasus Islamic state under shari'a law. He was unable to fully achieve this because in the course of the war he was betrayed by the Ottomans, handed over to Russians, and executed in 1794.

Following the forced ceding of the current territories of Dagestan, most of Azerbaijan, and Georgia by Persia to Russia, following the Russo-Persian War (1804–1813) and its outcoming Treaty of Gulistan, Russia significantly widened its foothold in the Caucasus at the expense of Persia. Another successful Caucasus war against Persia several years later, starting in 1826 and ending in 1828 with the Treaty of Turkmenchay, and a successful war against Ottoman Turkey in 1828, enabled Russia to use a much larger portion of its army in subduing the natives of the North Caucasus.

The resistance of the Nakh tribes never ended and was a fertile ground for a new Muslim-Avar commander, Imam Shamil, who fought against the Russians from 1834 to 1859 (see Murid War). In 1859, Shamil was captured by Russians at aul Gunib. Shamil left Boysangur Benoiski, a Chechen with one arm, one eye, and one leg, in charge of command at Gunib. Benoiski broke through the siege and continued to fight Russia for another two years until he was captured and killed by Russians. The Russian tsar hoped that by sparing the life of Shamil, the resistance in the North Caucasus would stop, but it did not. Russia began to use a colonization tactic by destroying Nakh settlements and building Cossack defense lines in the lowlands. The Cossacks suffered defeat after defeat and were constantly attacked by mountaineers, who were robbing them of food and weaponry.

The tsarists' regime used a different approach at the end of the 1860s. They offered Chechens and Ingush to leave the Caucasus for the Ottoman Empire (see Muhajir (Caucasus)). It is estimated that about 80% of Chechens and Ingush left the Caucasus during the deportation. It weakened the resistance which went from open warfare to insurgent warfare. One of the notable Chechen resistance fighters at the end of the 19th century was a Chechen abrek Zelimkhan Gushmazukaev and his comrade-in-arms Ingush abrek Sulom-Beck Sagopshinski. Together they built up small units which constantly harassed Russian military convoys, government mints, and government post-service, mainly in Ingushetia and Chechnya. Ingush aul Kek was completely burned when the Ingush refused to hand over Zelimkhan. Zelimkhan was killed at the beginning of the 20th century. The war between Nakh tribes and Russia resurfaced during the times of the Russian Revolution, which saw the Nakh struggle against Anton Denikin and later against the Soviet Union.

On December 21, 1917, Ingushetia, Chechnya, and Dagestan declared independence from Russia and formed a single state: "United Mountain Dwellers of the North Caucasus" (also known as the Mountainous Republic of the Northern Caucasus) which was recognized by major world powers. The capital of the new state was moved to Temir-Khan-Shura (Dagestan). Tapa Tchermoeff, a prominent Chechen statesman, was elected the first prime minister of the state. The second prime minister elected was Vassan-Girey Dzhabagiev, an Ingush statesman, who also was the author of the constitution of the republic in 1917, and in 1920 he was re-elected for the third term. In 1921 the Russians attacked and occupied the country and forcefully absorbed it into the Soviet state. The Caucasian war for independence restarted, and the government went into exile.

During Soviet rule, Chechnya and Ingushetia were combined to form Chechen-Ingush Autonomous Soviet Socialist Republic. In the 1930s Chechnya was flooded with many Ukrainians fleeing the Holodomor. As a result, many of the Ukrainians settled in Chechen-Ingush ASSR permanently and survived the famine. 
Although over 50,000 Chechens and over 12,000 Ingush were fighting against Nazi Germany on the front line (including heroes of the USSR: Abukhadzhi Idrisov, Khanpasha Nuradilov, Movlid Visaitov), and although Nazi German troops were fought to a complete stop at two Chechen-Ingush ASSR cities Malgobek and Ordzhonikidze (renamed to Vladikavkaz) after capturing half of the Caucasus in less than a month; Chechens and Ingush were falsely accused as Nazi supporters and entire nations were deported during Operation Lentil to the Kazakh SSR (later Kazakhstan) in 1944 near the end of World War II where over 60% of Chechen and Ingush populations perished. American historian Norman Naimark writes: The deportation was supposedly justified by the materials prepared by notorious NKVD officer Bogdan Kobulov accusing Chechens and Ingush in a mass conspiracy preparing rebellion and providing assistance to the German forces. Many of the materials were later proved to be fabricated. Even distinguished Red Army officers who fought bravely against Germans (e.g. the commander of 255th Separate Chechen-Ingush regiment Movlid Visaitov, the first to contact American forces at Elbe river) were deported. There is a theory that the real reason why Chechens and Ingush were deported is the desire of Russia to attack Turkey, a non-communist country, as Chechens and Ingush could impede such plans. In 2004, the European Parliament recognized the deportation of Chechens and Ingush as an act of genocide.

The territory of the Chechen-Ingush Autonomous Soviet Socialist Republic was divided between Stavropol Krai (where Grozny Okrug was formed), the Dagestan ASSR, the North Ossetian ASSR, and the Georgian SSR.

The Chechens and Ingush were allowed to return to their land after 1956 during de-Stalinization under Nikita Khrushchev when Chechen-Ingush Autonomous Soviet Socialist Republic was restored but both boundaries and ethnic composition of the territory significantly changed. There were many (predominantly Russian) migrants from other parts of the Soviet Union, who often settled in the abandoned family homes of Chechens and Ingushes. The republic lost its Prigorodny District which transferred to North Ossetian ASSR but gained predominantly Russian Naursky District and Shelkovskoy District that is considered the homeland for Terek Cossacks.

The Russification policies towards Chechens continued after 1956, with Russian language proficiency required in many aspects of life to provide Chechens better opportunities for advancement in the Soviet system.

On November 26, 1990, the Supreme Council of Chechen-Ingush ASSR adopted the "Declaration of State Sovereignty of the Chechen-Ingush Republic". This declaration was part of the reorganization of the Soviet Union. This new treaty would have been signed August 22, 1991, which would have transformed 15 republic states into more than 80. The August 19–21, 1991 Soviet coup d'état attempt led to the abandonment of this reorganization.

With the impending dissolution of the Soviet Union in 1991, an independence movement, the Chechen National Congress, was formed, led by ex-Soviet Air Force general and new Chechen President Dzhokhar Dudayev. It campaigned for the recognition of Chechnya as a separate nation. This movement was opposed by Boris Yeltsin's Russian Federation, which argued that Chechnya had not been an independent entity within the Soviet Union—as the Baltic, Central Asian, and other Caucasian States had—but was part of the Russian Soviet Federative Socialist Republic and hence did not have a right under the Soviet constitution to secede. It also argued that other republics of Russia, such as Tatarstan, would consider seceding from the Russian Federation if Chechnya were granted that right. Finally, it argued that Chechnya was a major hub in the oil infrastructure of Russia and hence its secession would hurt the country's economy and energy access.

In the ensuing decade, the territory was locked in an ongoing struggle between various factions, usually fighting unconventionally and forgoing the position held by the several successive Russian governments through the current administration.

The First Chechen War took place from 1994 to 1996, when Russian forces attempted to regain control over Chechnya, which had declared independence in November 1991. Despite overwhelming numerical superiority in men, weaponry, and air support, the Russian forces were unable to establish effective permanent control over the mountainous area due to numerous successful full-scale battles and insurgency raids. In three months, Russia lost more tanks (over 1,997 tanks) in Grozny than during the Battle of Berlin in 1945.
The Budyonnovsk hospital hostage crisis in 1995 shocked the Russian public and led to international condemnation of the Chechen rebels.

In April 1996 the first democratically elected president of Chechnya, Dzhokhar Dudayev, was killed by Russian forces using a booby trap bomb and a missile fired from a warplane after he was located by triangulating the position of a satellite phone he was using.

The widespread demoralization of the Russian forces in the area and a successful offensive to re-take Grozny by Chechen rebel’s forces led by Aslan Maskhadov prompted Russian President Boris Yeltsin to declare a ceasefire in 1996, and sign a peace treaty a year later that saw a withdrawal of Russian forces.

After the war, parliamentary and presidential elections took place in January 1997 in Chechnya and brought to power new President Aslan Maskhadov, chief of staff and prime minister in the Chechen coalition government, for a five-year term. Maskhadov sought to maintain Chechen sovereignty while pressing the Russian government to help rebuild the republic, whose formal economy and infrastructure were virtually destroyed. Russia continued to send money for the rehabilitation of the republic; it also provided pensions and funds for schools and hospitals. Most of these funds were taken by Chechen authorities and divided between favored warlords. Nearly half a million people (40% of Chechnya's prewar population) had been internally displaced and lived in refugee camps or overcrowded villages. There was an economic downturn. Two Russian brigades were permanently stationed in Chechnya.

In lieu of the devastated economic structure, kidnapping emerged as the principal source of income countrywide, procuring over US$200 million during the three-year independence of the chaotic fledgling state, although victims were rarely killed. In 1998, 176 people were kidnapped, 90 of whom were released, according to official accounts. President Maskhadov started a major campaign against hostage-takers, and on October 25, 1998, Shadid Bargishev, Chechnya's top anti-kidnapping official, was killed in a remote-controlled car bombing. Bargishev's colleagues then insisted they would not be intimidated by the attack and would go ahead with their offensive. Political violence and religious extremism, blamed on "Wahhabism", was rife. In 1998, Grozny authorities declared a state of emergency. Tensions led to open clashes between the Chechen National Guard and Islamist militants, such as the July 1998 confrontation in Gudermes.

The War of Dagestan began on August 7, 1999, during which the Islamic International Brigade (IIPB) began an unsuccessful incursion into the neighboring Russian republic of Dagestan in favor of the Shura of Dagestan which sought independence from Russia. In September, a series of apartment bombs that killed around 300 people in several Russian cities, including Moscow, were blamed on the Chechen separatists. Some journalists contested the official explanation, instead blaming the Russian Secret Service for blowing up the buildings to initiate a new military campaign against Chechnya. In response to the bombings, a prolonged air campaign of retaliatory strikes against the Ichkerian regime and a ground offensive that began in October 1999 marked the beginning of the Second Chechen War. Much better organized and planned than the first Chechen War, the Russian military took control over most regions. The Russian forces used brutal force, killing 60 Chechen civilians during a mop-up operation in Aldy, Chechnya on February 5, 2000. After the re-capture of Grozny in February 2000, the Ichkerian regime fell apart.

Chechen rebels continued to fight Russian troops and conduct terrorist attacks. In October 2002, 40–50 Chechen rebels seized a Moscow theater and took about 900 civilians hostage. The crisis ended with 117 hostages and up to 50 rebels dead, mostly due to an unknown aerosol pumped throughout the building by Russian special forces to incapacitate the people inside.

In September 2004, separatist rebels occupied a school in the town of Beslan, North Ossetia, demanding recognition of the independence of Chechnya and a Russian withdrawal. 1,100 people (including 777 children) were taken hostage. The attack lasted three days, resulting in the deaths of over 331 people, including 186 children.

In response to the increasing terrorism, Russia tightened its grip on Chechnya and expanded its anti-terrorist operations throughout the region. Russia installed a pro-Russian Chechen regime. In 2003, a referendum was held on a constitution that reintegrated Chechnya within Russia but provided limited autonomy. According to the Chechen government, the referendum passed with 95.5% of the votes and almost 80% turnout. The Economist was skeptical of the results, arguing that "few outside the Kremlin regard the referendum as fair". After the 2004 school siege, Russian president Vladimir Putin announced sweeping security and political reforms, sealing borders in the Caucasus region and revealing plans to give the central government more power. He also vowed to take tougher action against domestic terrorism, including preemptive strikes against Chechen separatists. In 2005 and 2006, prominent separatist leaders Aslan Maskhadov and Shamil Basayev were killed.

Since 2007, Chechnya has been run by Ramzan Kadyrov. Kadyrov's rule has been characterized by high-level corruption, a poor human rights record, and a growing cult of personality. However, his rule has also seen Chechnya rebuild, with much of Grozny already reconstructed.

In April 2009, Russia ended its counter-terrorism operation and pulled out the bulk of its army. The insurgency in the North Caucasus continued even after this date. The Caucasus Emirate has fully adopted the tenets of being a Salafist-takfiri jihadist group through its strict adherence to upholding tawhid, its obedience to the literal interpretation of the Quran and the Sunnah, and its complete rejection of bid‘ah, taqlid, and ijtihad.

Chechnya under Kadyrov has also made it to the headlines with the ruthless persecution of gay people.

Situated in the eastern part of the North Caucasus, partially in Eastern Europe, Chechnya is surrounded on nearly all sides by Russian Federal territory. In the west, it borders North Ossetia and Ingushetia, in the north, Stavropol Krai, in the east, Dagestan, and to the south, Georgia. Its capital is Grozny.

Rivers:


There are no true districts of Chechnya, but many believe that the different dialects of the Chechen language define different districts. The main dialects are:
Grozny, also known as the Dzhokhar dialect, is the dialect of people who live in and in some towns around Grozny.
Naskhish, a dialect spoken to the northeast of Chechnya. The most notable difference in this dialect is the addition of the letters "ȯ", "ј" and "є"
Day, pronounced like the word 'die' is spoken in a small section of the south, around and in the town of Day.

There are other dialects which are believed to define districts, but because these areas are so isolated, not much research has been done on these areas.

According to the 2010 Census, the population of the republic is 1,268,989, up from 1,103,686 recorded in the 2002 Census. As of the 2010 Census, Chechens at 1,206,551 make up 95.3% of the republic's population. Other groups include Russians (24,382, or 1.9%), Kumyks (12,221, or 1%), Ingush (1,296 or 0.1%) and a host of smaller groups, each accounting for less than 0.5% of the total population. The Armenian community, which used to number around 15,000 in Grozny alone, has dwindled to a few families. The Armenian church of Grozny was demolished in 1930. The birth rate was 25.41 in 2004. (25.7 in Achkhoi Martan, 19.8 in Groznyy, 17.5 in Kurchaloi, 28.3 in Urus Martan and 11.1 in Vedeno). According to the Chechen State Statistical Committee, Chechnya's population had grown to 1.205 million in January 2006.

At the end of the Soviet era, ethnic Russians (including Cossacks) comprised about 23% of the population (269,000 in 1989).

According to some Russian sources, from 1991 to 1994 tens of thousands of people of non-Chechen ethnicity (mostly Russians, Ukrainians, and Armenians) left the republic amidst reports of violence and discrimination against the non-Chechen population, as well as widespread lawlessness and ethnic cleansing under the government of Dzhokhar Dudayev.

However, regarding this exodus, there is an alternative view. According to the Russian economists Boris Lvin and Andrei Iliaronov,
The Chechen authorities are regularly accused of crimes against the population, especially the Russian-speaking people. However, before the current war the emigration of the Russian-speaking population from Chechnya was no more intense than that from Kalmykia, Tuva and Sakha-Yakutia. In Grozny itself there remained a 200,000 strong Russian-speaking population which did not hasten to leave it.
The languages used in the Republic are Chechen and Russian. Chechen belongs to the Vaynakh or North-central Caucasian language family, which also includes Ingush and Batsb. Some scholars place it in a wider North Caucasian languages.

Chechnya has one of the youngest populations in the generally aging Russian Federation; in the early 1990s, it was among the few regions experiencing natural population growth. Since 2002, Chechnya has experienced a classic post-conflict baby-boom. Chechen demographers in 2008 termed highly implausible the reported overall population growth as infant mortality in Chechnya was said to be 60 percent higher than the Russian average in 2007 and to have risen by 3.9 percent compared with 2006. Many experts have expressed doubts about the increase from 1.1 million in the 1990s to an estimated nearly 1.3 million in 2010 following two devastating wars that displaced hundreds of thousands of people and virtually eliminated the large ethnic Russian minority in the republic. According to Russian demographer Dmitry Bogoyavlensky, the 2002 census results were clearly manipulated in the North Caucasus: an estimated 800,000 to 1 million non-existent people were added to the actual population of the region. Another Russian demographer, Anatoly Vishnevsky, pointed out that according to the 2002 census, some age groups, like those born in 1950, appeared to be larger in 2002 than in 1989. With the 2002 census, Moscow wanted to show there were not too many casualties and that the refugees had returned to Chechnya, while the local authorities wanted to receive more funds and thus needed a higher population to justify their demands. Also, in the multiethnic republics of North Caucasus normally unlike in other parts of Russia, government positions are distributed among the ethnicities according to their ratio in the general population. So ethnicities are zealously guarding their numbers in order not to be outnumbered by others and thereby left with less representation in the government and the local economy. Some 40 percent of newborns had some kind of genetic defect.

According to Russian media, Chechnya has the lowest alcohol consumption in Russia.

Note: TFR 2009–12 source.

Islam is the predominant religion in Chechnya, practiced by 95% of those polled in Grozny in 2010. Chechens are overwhelmingly adherents to the Shafi'i Madhhab of Sunni Islam, the republic having converted to Islam between the 16th and the 19th centuries. Due to historical importance, many Chechens are Sufis, of either the Qadiri or Naqshbandi orders. Most of the population follows either the Shafi'i or the Hanafi, schools of jurisprudence, fiqh. The Shafi'i school of jurisprudence has a long tradition among the Chechens, and thus it remains the most practiced.

The once-strong Russian minority in Chechnya, mostly Terek Cossacks and estimated as numbering approximately 25,000 in 2012, are predominantly Russian Orthodox, although presently only one church exists in Grozny. In August 2011, Archbishop Zosima of Vladikavkaz and Makhachkala performed the first mass baptism ceremony in the history of the Chechen Republic in the Terek River of Naursky District in which 35 citizens of Naursky and Shelkovsky districts were converted to Orthodoxy.

On 19 January 2015, 12 days after the Charlie Hebdo shooting, a march took place in Grozny against the publication of caricatures of the prophet Mohammed. The Chechen Ministry of Interior reported that more than a million people participated, while according to the sources of Caucasian Knot the number was between 350,000 and 500,000.

Since 1990, the Chechen Republic has had many legal, military, and civil conflicts involving separatist movements and pro-Russian authorities. Today, Chechnya is a relatively stable federal republic, although there is still some separatist movement activity. Its regional constitution entered into effect on April 2, 2003, after an all-Chechen referendum was held on March 23, 2003. Some Chechens were controlled by regional teips, or clans, despite the existence of pro- and anti-Russian political structures.

The former separatist religious leader (mufti) Akhmad Kadyrov, looked upon as a traitor by many separatists, was elected president with 83% of the vote in an internationally monitored election on October 5, 2003. Incidents of ballot stuffing and voter intimidation by Russian soldiers and the exclusion of separatist parties from the polls were subsequently reported by the Organization for Security and Co-operation in Europe (OSCE) monitors. On May 9, 2004, Kadyrov was assassinated in Grozny football stadium by a landmine explosion that was planted beneath a VIP stage and detonated during a parade, and Sergey Abramov was appointed to the position of acting prime minister after the incident. However, since 2005 Ramzan Kadyrov (son of Akhmad Kadyrov) has been the caretaker prime minister, and in 2007 was appointed as the new president. Many allege he is the wealthiest and most powerful man in the republic, with control over a large private militia referred to as the "Kadyrovtsy". The militia, which began as his father's security force, has been accused of killings and kidnappings by human rights organizations such as Human Rights Watch.

In 2009, the US government financed American organization Freedom House included Chechnya in the "Worst of the Worst" list of most repressive societies in the world, together with Burma, North Korea, Tibet, and others.

In addition to the Russian regional government, there was a separatist Ichkeria government that was not recognized by any state (although members have been given political asylum in European and Arab countries, as well as the United States).

Ichkeria was a member of the Unrepresented Nations and Peoples Organization between 1991 and 2010. Former president of Georgia Zviad Gamsakhurdia deposed in a military coup of 1991 and a participant of the Georgian Civil War, recognized the independence of the Chechen Republic of Ichkeria in 1993. Diplomatic relations with Ichkeria were also established by the partially recognized Islamic Emirate of Afghanistan under the Taliban government on January 16, 2000. This recognition ceased with the fall of the Taliban in 2001. However, despite Taliban recognition, there were no friendly relations between the Taliban and Ichkeria—Maskhadov rejected their recognition, stating that the Taliban were illegitimate. Ichkeria also received vocal support from the Baltic countries, a group of Ukrainian nationalists and Poland; Estonia once voted to recognize, but the act never was followed through due to pressure applied by both Russia and the EU.

The president of this government was Aslan Maskhadov, the Foreign Minister was Ilyas Akhmadov, who was the spokesman for Maskhadov. Aslan Maskhadov had been elected in an internationally monitored election in 1997 for 4 years, which took place after signing a peace agreement with Russia. In 2001 he issued a decree prolonging his office for one additional year; he was unable to participate in the 2003 presidential election since separatist parties were barred by the Russian government, and Maskhadov faced accusations of terrorist offenses in Russia. Maskhadov left Grozny and moved to the separatist-controlled areas of the south at the onset of the Second Chechen War. Maskhadov was unable to influence a number of warlords who retain effective control over Chechen territory, and his power was diminished as a result. Russian forces killed Maskhadov on March 8, 2005, and the assassination of Maskhadov was widely criticized since it left no legitimate Chechen separatist leader with whom to conduct peace talks. Akhmed Zakayev, Deputy Prime Minister and a Foreign Minister under Maskhadov, was appointed shortly after the 1997 election and is currently living under asylum in England. He and others chose Abdul Khalim Saidullayev, a relatively unknown Islamic judge who was previously the host of an Islamic program on Chechen television, to replace Maskhadov following his death. On June 17, 2006, it was reported that Russian special forces killed Abdul Khalim Saidullayev in a raid in a Chechen town Argun.

The successor of Saidullayev became Doku Umarov. On October 31, 2007, Umarov abolished the Chechen Republic of Ichkeria and its presidency and in its place proclaimed the Caucasian Emirate with himself as its Emir. This change of status has been rejected by many Chechen politicians and military leaders who continue to support the existence of the republic.

The Internal Displacement Monitoring Centre reports that after hundreds of thousands of ethnic Russians and Chechens fled their homes following inter-ethnic and separatist conflicts in Chechnya in 1994 and 1999, more than 150,000 people still remain displaced in Russia today.

On September 1, 1997, Criminal Code reportedly being implemented in the Chechen Republic-Ichkeriya, Article 148 punishes "anal sexual intercourse between a man and a woman or a man and a man". For first- and second-time offenders, the punishment is caning. A third conviction leads to the death penalty, which can be carried out in a number of ways including stoning or beheading.

Human rights groups criticized the conduct of the 2005 parliamentary elections as unfairly influenced by the central Russian government and military.

In 2006 Human Rights Watch reported that pro-Russian Chechen forces under the command of Ramzan Kadyrov, as well as federal police personnel, used torture to get information about separatist forces. "If you are detained in Chechnya, you face a real and immediate risk of torture. And there is little chance that your torturer will be held accountable", said Holly Cartner, Director of the Europe and Central Asia division of the Human Rights Watch.

On February 1, 2009, "The New York Times" released extensive evidence to support allegations of consistent torture and executions under the Kadyrov government. The accusations were sparked by the assassination in Austria of a former Chechen rebel who had gained access to Kadyrov's inner circle, 27-year-old Umar Israilov.

On July 1, 2009, Amnesty International released a detailed report covering the human rights violations committed by the Russian Federation against Chechen citizens. Among the most prominent features was that those abused had no method of redress against assaults, ranging from kidnapping to torture, while those responsible were never held accountable. This led to the conclusion that Chechnya was being ruled without law, being run into further devastating destabilization.
On March 10, 2011, Human Rights Watch reported that since Chechenization, the government has pushed for enforced Islamic dress code and other traditions which violently repress women. The president Ramzan Kadyrov is quoted as saying "I have the right to criticize my wife. She doesn't. With us [in Chechen society], a wife is a housewife. A woman should know her place. A woman should give her love to us [men]... She would be [man's] property. And the man is the owner. Here, if a woman does not behave properly, her husband, father, and brother are responsible. According to our tradition, if a woman fools around, her family members kill her... That's how it happens, a brother kills his sister or a husband kills his wife... As a president, I cannot allow for them to kill. So, let women not wear shorts...". He has also openly defended honor killings on several occasions.

On July 9, 2017, Russian newspaper Novaya Gazeta reported that a number of people were subject to an extrajudicial execution on the night of January 26, 2017. It published 27 names of the people known to be dead, but stressed that the list is "not all [of those killed]"; the newspaper asserted that 50 people may have been killed in the execution. Some of the dead were gay, but not all; the deaths appeared to have been triggered by the death of a policeman, and according to the author of the report, Elena Milashina, were executed for terrorism.

In 2017, it was reported by Novaya Gazeta and human rights groups that Chechen authorities had allegedly set up concentration camps, one of which is in Argun, where gay men are interrogated and subjected to physical violence.

On 11 January 2019, it was reported that another 'gay purge' had begun in the country in December 2018, with several gay men and women being detained. The Russian LGBT Network believes that around 40 persons were detained and two killed.

During the war, the Chechen economy fell apart. In 1994, the separatists planned to introduce a new currency, but the change did not occur due to the re-taking of Chechnya by Russian troops in the Second Chechen War.

The economic situation in Chechnya has improved considerably since 2000. According to the "New York Times", major efforts to rebuild Grozny have been made, and improvements in the political situation have led some officials to consider setting up a tourism industry, though there are claims that construction workers are being irregularly paid and that poor people have been displaced.

Chechnya's unemployment was 67% in 2006 and fell to 21.5% in 2014

Total revenues of the budget of Chechnya for 2017 are 59.2 billion rubles. Of these, 48.5 billion rubles are so-called "gratuitous receipts" from the federal budget of the Russian Federation.






</doc>
<doc id="6097" url="https://en.wikipedia.org/wiki?curid=6097" title="Canonization">
Canonization

Canonization is the act by which a Christian church declares that a person who has died was a saint, upon which declaration the person is included in the "canon", or list, of recognized saints. Originally, a person was recognized as a saint without any formal process. Later, different processes were developed, such as those used today in the Roman Catholic Church, the Eastern Orthodox Church, Oriental Orthodox Church and the Anglican Communion.

The first persons honored as saints were the martyrs. Pious legends of their deaths were considered affirmations of the truth of their faith in Christ.

The Roman Rite's Canon of the Mass contains only the names of martyrs, along with that of the Blessed Virgin Mary and, since 1962, that of St. Joseph her spouse.

By the fourth century, however, "confessors"—people who had confessed their faith not by dying but by word and life—began to be venerated publicly. Examples of such people are Saint Hilarion and Saint Ephrem the Syrian in the East, and Saint Martin of Tours and Saint Hilary of Poitiers in the West. Their names were inserted in the diptychs, the lists of saints explicitly venerated in the liturgy, and their tombs were honoured in like manner as those of the martyrs. Since the witness of their lives was not as unequivocal as that of the martyrs, they were venerated publicly only with the approval by the local bishop. This process is often referred to as "local canonization".

This approval was required even for veneration of a reputed martyr. In his history of the Donatist heresy, Saint Optatus recounts that at Carthage a Catholic matron, named Lucilla, incurred the censures of the Church for having kissed the relics of a reputed martyr whose claims to martyrdom had not been juridically proved. And Saint Cyprian (died 258) recommended that the utmost diligence be observed in investigating the claims of those who were said to have died for the faith. All the circumstances accompanying the martyrdom were to be inquired into; the faith of those who suffered, and the motives that animated them were to be rigorously examined, in order to prevent the recognition of undeserving persons. Evidence was sought from the court records of the trials or from people who had been present at the trials.
Augustine of Hippo (died 430) tells of the procedure which was followed in his day for the recognition of a martyr. The bishop of the diocese in which the martyrdom took place set up a canonical process for conducting the inquiry with the utmost severity. The acts of the process were sent either to the metropolitan or primate, who carefully examined the cause, and, after consultation with the suffragan bishops, declared whether the deceased was worthy of the name of 'martyr' and public veneration.

Acts of formal recognition, such as the erection of an altar over the saint's tomb or transferring the saint's relics to a church, were preceded by formal inquiries into the sanctity of the person's life and the miracles attributed to that person's intercession.

Such acts of recognition of a saint were authoritative, in the strict sense, only for the diocese or ecclesiastical province for which they were issued, but with the spread of the fame of a saint, were often accepted elsewhere also.

The Church of England, the Mother Church of the Anglican Communion, canonized Charles I as a saint, in the Convocations of Canterbury and York of 1660.

In the Catholic Church, both Latin and constituent Eastern churches, the act of canonization is reserved to the Apostolic See and occurs at the conclusion of a long process requiring extensive proof that the candidate for canonization lived and died in such an exemplary and holy way that they are worthy to be recognized as a saint. The Church's official recognition of sanctity implies that the person is now in Heaven and that they may be publicly invoked and mentioned officially in the liturgy of the Church, including in the "Litany of the Saints". 

In the Catholic Church, canonization is a decree that allows universal veneration of the saint in the liturgy of the Roman Rite. For permission to venerate merely locally, only beatification is needed.

For several centuries the Bishops, or in some places only the Primates and Patriarchs, could grant martyrs and confessors public ecclesiastical honor; such honor, however, was always decreed only for the local territory of which the grantors had jurisdiction. Only acceptance of the "cultus" by the Pope made the "cultus" universal, because he alone can rule the universal Catholic Church. Abuses, however, crept into this discipline, due as well to indiscretions of popular fervor as to the negligence of some bishops in inquiring into the lives of those whom they permitted to be honoured as saints.

In the Medieval West, the Apostolic See was asked to intervene in the question of canonizations so as to ensure more authoritative decisions. The canonization of Saint Udalric, Bishop of Augsburg by Pope John XV in 993 was the first undoubted example of Papal canonization of a saint from outside of Rome; some historians maintain further that the first Papal canonization was of St. Swibert by Pope Leo III in 804.

Thereafter, recourse to the judgment of the Pope was had more frequently. Toward the end of the eleventh century the Popes judged it necessary to restrict episcopal authority regarding canonization, and therefore decreed that the virtues and miracles of persons proposed for public veneration should be examined in councils, more specifically in general councils. Pope Urban II, Pope Calixtus II, and Pope Eugene III conformed to this discipline.

Hugh de Boves, Archbishop of Rouen, canonized Walter of Pontoise, or St. Gaultier, in 1153, the final saint in Western Europe to be canonized by an authority other than the Pope: "The last case of canonization by a metropolitan is said to have been that of St. Gaultier, or Gaucher, [A]bbot of Pontoise, by the Archbishop of Rouen. A decree of Pope Alexander III [in] 1170 gave the prerogative to the [P]ope thenceforth, so far as the Western Church was concerned." In a decretal of 1173, Pope Alexander III reprimanded some bishops for permitting veneration of a man who was merely killed while intoxicated, prohibited veneration of the man, and most significantly decreed that "you shall not therefore presume to honor him in the future; for, even if miracles were worked through him, it is not lawful for you to venerate him as a saint without the authority of the Catholic Church." Theologians disagree as to the full import of the decretal of Pope Alexander III: either a new law was instituted, in which case the Pope then for the first time reserved the right of beatification to himself, or an existing law was confirmed.

However, the procedure initiated by the decretal of Pope Alexander III was confirmed by a bull of Pope Innocent III issued on the occasion of the canonization of Cunigunde of Luxembourg in 1200. The bull of Pope Innocent III resulted in increasingly elaborate inquiries to the Apostolic See concerning canonizations. Because the decretal of Pope Alexander III did not end all controversy and some bishops did not obey it in so far as it regarded beatification, the right of which they had certainly possessed hitherto, Pope Urban VIII issued the Apostolic letter "Caelestis Hierusalem cives" of 5 July 1634 that exclusively reserved to the Apostolic See both its immemorial right of canonization and that of beatification. He further regulated both of these acts by issuing his "Decreta servanda in beatificatione et canonizatione Sanctorum" on 12 March 1642.

In his "De Servorum Dei beatificatione et de Beatorum canonizatione" of five volumes the eminent canonist Prospero Lambertini (1675–1758), who later became Pope Benedict XIV, elaborated on the procedural norms of Pope Urban VIII's Apostolic letter "Caelestis Hierusalem cives" of 1634 and "Decreta servanda in beatificatione et canonizatione Sanctorum" of 1642, and on the conventional practice of the time. His work published from 1734 to 1738 governed the proceedings until 1917. The article "Beatification and canonization process in 1914" describes the procedures followed until the promulgation of the "Codex" of 1917. The substance of "De Servorum Dei beatifιcatione et de Beatorum canonizatione" was incorporated into the "Codex Iuris Canonici" ("Code of Canon Law") of 1917, which governed until the promulgation of the revised "Codex Iuris Canonici" in 1983 by Pope John Paul II. Prior to promulgation of the revised "Codex" in 1983, Bl. Pope Paul VI initiated a simplification of the procedures.

The Apostolic constitution "Divinus Perfectionis Magister" of Pope John Paul II of 25 January 1983 and the norms issued by the Congregation for the Causes of Saints on 7 February 1983 to implement the constitution in dioceses, continued the simplification of the process initiated by Pope Paul VI. Contrary to popular belief, the reforms did not eliminate the office of the Promoter of the Faith (Latin: "Promotor Fidei"), popularly known as the "Devil's advocate", whose office is to question the material presented in favor of canonization. The reforms were intended to reduce the adversarial nature of the process. In November 2012 Pope Benedict XVI appointed Monsignor Carmello Pellegrino as Promoter of the Faith.

Candidates for canonization undergo the following process:


The satisfaction of the applicable conditions permits beatification, which then bestows on the Venerable the title of "Blessed" (Latin: "Beatus" or "Beata"). A feast day will be designated, but its observance is ordinarily only permitted for the Blessed's home diocese, to specific locations associated with them, or to the churches or houses of the Blessed's religious order if they belonged to one. Parishes may not normally be named in honor of beati.


Canonization is a statement of the Church that the person certainly enjoys the Beatific Vision of Heaven. The title of "Saint" (Latin: "Sanctus" or "Sancta") is then proper, reflecting that the Saint is a refulgence of the holiness ("sanctitas") of God Himself, which alone comes from God's gift. The Saint is assigned a feast day which "may" be celebrated anywhere in the universal Church, although it is not necessarily added to the General Roman Calendar or local calendars as an "obligatory" feast; parish churches may be erected in his honor; and the faithful may freely celebrate and honor the Saint.

Although recognition of sainthood by the Pope does not directly concern a fact of Divine revelation, nonetheless it must be "definitively held" by the faithful as "infallible" pursuant to, at the least, the Universal Magisterium of the Church, because it is a truth related to revelation by historical necessity.

Regarding the Eastern Catholic Churches, individual "sui juris" churches have the right to "glorify" saints for their own jurisdictions, though this has rarely happened.

Popes have several times permitted to the universal Church, without executing the ordinary judicial process of canonization described above, the veneration as a saint, the ""cultus"" of one long venerated as such locally. This act of a pope is denominated "equipollent" or "equivalent canonization" and "confirmation of "cultus"". According to the rules Pope Benedict XIV ("regnat" 17 August 1740 – 3 May 1758) instituted, there are three conditions for an equipollent canonization: (1) existence of an ancient "cultus" of the person, (2) a general and constant attestation to the virtues or martyrdom of the person by credible historians, and (3) uninterrupted fame of the person as a worker of miracles.

As examples, prior to his pontificate, of this mode of canonization, Pope Benedict XIV himself enumerated the equipollent canonizations of Saints:

Later equipollent canonizations include those of Saints:

Pope Francis added Saints:

The following terms are used for canonization by the autocephalous national Orthodox Churches: канонизация or прославление "glorification" (Russian Orthodox Church), კანონიზაცია "kanonizats’ia" (Georgian Orthodox Church), канонизација (Serbian Orthodox Church), "canonizare" (Romanian Orthodox Church), and Канонизация (Bulgarian Orthodox Church). The following terms are used for canonization by other autocephalous Orthodox Churches: αγιοκατάταξη (Katharevousa: ἁγιοκατάταξις) "agiokatataxi/agiokatataxis", "ranking among saints" (Ecumenical Patriarchate of Constantinople, Church of Cyprus, Church of Greece), "kanonizim" (Albanian Orthodox Church), "kanonizacja" (Polish Orthodox Church), and "kanonizace/kanonizácia" (Czech and Slovak Orthodox Church).

The Orthodox Church in America, an Eastern Orthodox Church partly recognized as autocephalous, uses the term "glorification" for granting official recognition to someone as a saint—see glorification.

Within the Armenian Apostolic Church, part of Oriental Orthodoxy, there had been discussions since the 1980s about canonizing the victims of the Armenian Genocide. On April 23, 2015, all of the victims of the genocide were canonized.

The General Conference of the United Methodist Church has formally declared individuals "martyrs", including Dietrich Bonhoeffer (in 2008) and Martin Luther King Jr. (in 2012).




</doc>
<doc id="6099" url="https://en.wikipedia.org/wiki?curid=6099" title="Carboxylic acid">
Carboxylic acid

A carboxylic acid is an organic compound that contains a carboxyl group (C(=O)OH). The general formula of a carboxylic acid is R–COOH, with R referring to the rest of the (possibly quite large) molecule. Carboxylic acids occur widely. Important examples include the amino acids (which make up proteins) and acetic acid (which is part of vinegar). Deprotonation of a carboxyl group gives a carboxylate anion. Important carboxylate salts are soaps.

Carboxylic acids are commonly identified by their trivial names. They often have the suffix "-ic acid". IUPAC-recommended names also exist; in this system, carboxylic acids have an "-oic acid" suffix. For example, butyric acid (CHCOH) is butanoic acid by IUPAC guidelines. For nomenclature of complex molecules containing a carboxylic acid, the carboxyl can be considered position one of the parent chain even if there are other substituents, for example, 3-chloropropanoic acid. Alternately, it can be named as a "carboxy" or "carboxylic acid" substituent on another parent structure, for example, 2-carboxyfuran.

The carboxylate anion (R–COO or RCO) of a carboxylic acid is usually named with the suffix "-ate", in keeping with the general pattern of "-ic acid" and "-ate" for a conjugate acid and its conjugate base, respectively. For example, the conjugate base of acetic acid is acetate.

Carboxylic acids are polar. Because they are both hydrogen-bond acceptors (the carbonyl –C=O) and hydrogen-bond donors (the hydroxyl –OH), they also participate in hydrogen bonding. Together the hydroxyl and carbonyl group forms the functional group carboxyl. Carboxylic acids usually exist as dimers in nonpolar media due to their tendency to "self-associate". Smaller carboxylic acids (1 to 5 carbons) are soluble in water, whereas higher carboxylic acids have limited solubility due to the increasing hydrophobic nature of the alkyl chain. These longer chain acids tend to be rather soluble in less-polar solvents such as ethers and alcohols. Even hydrophobic carboxylic acids react aqueous sodium hydroxide to give water soluble sodium salts. For example, enathic acid has a small solubility in water (0.2 g/L), but its sodium salt is very soluble in water:
Carboxylic acids tend to have higher boiling points than water, not only because of their increased surface area, but also because of their tendency to form stabilised dimers through hydrogen bonds. For boiling to occur, either the dimer bonds must be broken or the entire dimer arrangement must be vaporised, both of which increase the enthalpy of vaporization requirements significantly.

Carboxylic acids are Brønsted–Lowry acids because they are proton (H) donors. They are the most common type of organic acid.

Carboxylic acids are typically weak acids, meaning that they only partially dissociate into HO cations and RCOO anions in neutral aqueous solution. For example, at room temperature, in a 1-molar solution of acetic acid, only 0.4% of the acid are dissociated. Electron-withdrawing substituents, such as -CF group, give stronger acids (the pKa of formic acid is 3.75 whereas trifluoroacetic acid, with a trifluoromethyl substituent, has a pK of 0.23). Electron-donating substituents give weaker acids (the pK of formic acid is 3.75 whereas acetic acid, with a methyl substituent, has a pK of 4.76)

Deprotonation of carboxylic acids gives carboxylate anions; these are resonance stabilized, because the negative charge is delocalized over the two oxygen atoms, increasing the stability of the anion. Each of the carbon–oxygen bonds in the carboxylate anion has a partial double-bond character. The carbonyl carbon's partial positive charge is also weakened by the -/ negative charges on the 2 oxygen atoms.

Carboxylic acids often have strong sour odors. Esters of carboxylic acids tend to have pleasant odors, and many are used in perfume.

Carboxylic acids are readily identified as such by infrared spectroscopy. They exhibit a sharp band associated with vibration of the C–O vibration bond ("ν") between 1680 and 1725 cm. A characteristic "ν" band appears as a broad peak in the 2500 to 3000 cm region. By H NMR spectrometry, the hydroxyl hydrogen appears in the 10–13 ppm region, although it is often either broadened or not observed owing to exchange with traces of water.

Many carboxylic acids are produced industrially on a large scale. They are also pervasive in nature. Esters of fatty acids are the main components of lipids and polyamides of aminocarboxylic acids are the main components of proteins.

Carboxylic acids are used in the production of polymers, pharmaceuticals, solvents, and food additives. Industrially important carboxylic acids include acetic acid (component of vinegar, precursor to solvents and coatings), acrylic and methacrylic acids (precursors to polymers, adhesives), adipic acid (polymers), citric acid (beverages), ethylenediaminetetraacetic acid (chelating agent), fatty acids (coatings), maleic acid (polymers), propionic acid (food preservative), terephthalic acid (polymers).

In general, industrial routes to carboxylic acids differ from those used on smaller scale because they require specialized equipment.


Preparative methods for small scale reactions for research or for production of fine chemicals often employ expensive consumable reagents.

Many reactions afford carboxylic acids but are used only in specific cases or are mainly of academic interest:

The most widely practiced reactions convert carboxylic acids into esters, amides, carboxylate salts, acid chlorides, and alcohols. Carboxylic acids react with bases to form carboxylate salts, in which the hydrogen of the hydroxyl (–OH) group is replaced with a metal cation. Thus, acetic acid found in vinegar reacts with sodium bicarbonate (baking soda) to form sodium acetate, carbon dioxide, and water:

Carboxylic acids also react with alcohols to give esters. This process is widely used, e.g. in the production of polyesters. Likewise, carboxylic acids are converted into amides, but this conversion typically does not occur by direct reaction of the carboxylic acid and the amine. Instead esters are typical precursors to amides. The conversion of amino acids into peptides is a major biochemical process that requires ATP.

The hydroxyl group on carboxylic acids may be replaced with a chlorine atom using thionyl chloride to give acyl chlorides. In nature, carboxylic acids are converted to thioesters.

Like esters, most of carboxylic acid can be reduced to alcohols by hydrogenation or using hydride or alkyl transferring agents (since they will deprotonate the acids instead without trasfer) such as lithium aluminium hydride or Grignard reagents (organolithium compounds).

"N","N"-Dimethyl(chloromethylene)ammonium chloride (ClHC=N(CH)Cl) is a highly chemoselective agent for carboxylic acid reduction. It selectively activates the carboxylic acid to give the carboxymethyleneammonium salt, which can be reduced by a mild reductant like lithium tris("t"-butoxy)aluminum hydride to afford an aldehyde in a one pot procedure. This procedure is known to tolerate reactive carbonyl functionalities such as ketone as well as moderately reactive ester, olefin, nitrile, and halide moieties.


The radical COOH (CAS# 2564-86-5) has only a fleeting isolated existence. The acid dissociation constant of COOH has been measured using electron paramagnetic resonance spectroscopy. The carboxyl group tends to dimerise to form oxalic acid.




</doc>
<doc id="6100" url="https://en.wikipedia.org/wiki?curid=6100" title="Chernobyl">
Chernobyl

Chernobyl () or Chornobyl () is a ghost city in the Chernobyl Exclusion Zone, situated in the Ivankiv Raion of northern Kiev Oblast, Ukraine, near Ukraine's border with Belarus. Chernobyl is about north of Kiev, and approximately southwest of the Belarusian city of Gomel. Chernobyl is from Ukraine’s border with Belarus. The city was the administrative center of Chernobyl Raion (district) from 1923, until it was disestablished in 1988. Before its evacuation, the city had about 14,000 residents.

The city was evacuated on 27 April 1986, 30 hours after the nuclear accident at the nearby Chernobyl Nuclear Power Plant, which was the most disastrous nuclear accident in history. The plant was within the Chernobyl Raion district. After the accident, administration of the Chernobyl Raion district was transferred to the neighboring Ivankiv Raion. Pripyat, a city of 50,000 people built in the 1970s as a home for the power-plant's workers, was closer to the power plant than Chernobyl, and was also evacuated. The city of Slavutych, built for those evacuated from Pripyat, also received the population evacuated from Chernobyl.

Although Chernobyl is mostly a ghost town today, a small number of people still live there, in houses marked with signs that read "Owner of this house lives here". Workers on watch and administrative personnel of the Chernobyl Exclusion Zone are also stationed in the city. There are two general stores, and a tourist hotel.

The city's name is the same as a Ukrainian name for "Artemisia vulgaris", mugwort or common wormwood, which is (or more commonly ). The name is inherited from or , a compound of + , the parts related to and , so named in distinction to the lighter-stemmed wormwood "A. absinthium".

The name in languages used nearby is:

The name in languages formerly used in the area is:
Chernobyl was originally part of the land of Kievan Rus′. The first known mention of Chernobyl is from an 1193 charter, which describes it as a hunting-lodge of Knyaz Rurik Rostislavich. In the 13th century, it was a crown village of the Grand Duchy of Lithuania. The village was granted to Filon Kmita, a captain of the royal cavalry, as a fiefdom in 1566. The province where Chernobyl is located was transferred to the Kingdom of Poland in 1569, and later annexed by the Russian Empire in 1793. Prior to the 20th century, Chernobyl was inhabited by Ukrainian, some Polish peasants and a relatively large number of Jews.

Jews were brought to Chernobyl by Filon Kmita, during the Polish campaign of colonization. After 1596, the traditionally Eastern Orthodox Ukrainian peasantry of the district were forcibly converted, by Poland, to the Greek Catholic Uniate religion. Many of these converts returned to Eastern Orthodoxy after the Partitions of Poland.

In 1626, during the Counter-reformation, the Dominican church and monastery were founded by Lukasz Sapieha. A group of Old Catholics opposed the decrees of the Council of Trent. In 1832, following the failed Polish November Uprising, the Dominican monastery was sequestrated. The church of the Old Catholics was disbanded in 1852.

In the second half of the 18th century, Chernobyl became a major center of Hasidic Judaism. The Chernobyl Hasidic dynasty had been founded by Rabbi Menachem Nachum Twersky. The Jewish population suffered greatly from pogroms in October 1905 and in March–April 1919; many Jews were killed or robbed at the instigation of the Russian nationalist Black Hundreds. When the Twersky Dynasty left Chernobyl in 1920, it ceased to exist as a centre of Hasidism.

Chernobyl had a population of 10,800 in 1898, including 7,200 Jews. Chernobyl was occupied in World War I; Ukrainians and Bolsheviks fought over the city in the ensuing Civil War. In the Polish–Soviet War of 1919–20, Chernobyl was taken first by the Polish Army and then by cavalry of the Red Army. From 1921 onwards, it was incorporated into the Ukrainian SSR.

Between 1929 and 1933, Chernobyl suffered from killings during Stalin's collectivization campaign. It was also affected by the famine that resulted from Stalin's policies. The Polish and German community of Chernobyl was deported to Kazakhstan in 1936, during the Frontier Clearances.

During World War II, Chernobyl was occupied by the German Army from 25 August 1941 to 17 November 1943. The Jewish community was murdered during the Nazi occupation of 1941–44.

Twenty years later, the area was chosen as the site of the first nuclear power station to be built on Ukrainian soil. The Duga over-the-horizon radar array, several miles outside of Chernobyl, was the origin of the Russian Woodpecker; it was designed as part of an anti-ballistic missile early warning radar network. With the dissolution of the Soviet Union in 1991, Chernobyl remained part of Ukraine.

On 26 April 1986, Reactor No. 4 at the Chernobyl Nuclear Power Plant exploded after unsanctioned experiments on the reactor by plant operators were done improperly and the operators lost control. The loss of control was due to design flaws of the RBMK reactor, which made it unstable when operated at low power, and prone to thermal runaway where increases in temperature increase reactor power output.

Chernobyl city was evacuated soon after the disaster. The base of operations for the administration and monitoring of the Chernobyl Exclusion Zone was moved from Pripyat to Chernobyl. Chernobyl currently contains offices for the State Agency of Ukraine on the Exclusion Zone Management and accommodations for visitors. Apartment blocks have been repurposed as accommodations for employees of the State Agency. The length of time that workers may spend within the Chernobyl Exclusion Zone is restricted by regulations that have been implemented to limit radiation exposure.

The city has become overgrown and many types of animals live there. According to census information collected over an extended period of time, it is estimated that more mammals live there now than before the disaster.

Although Chernobyl will remain uninhabitable for thousands of years due to elevated levels of background radiation, guided tours of the exclusion zone are possible.

In 2003, the United Nations Development Programme launched a project, called the Chernobyl Recovery and Development Programme (CRDP), for the recovery of the affected areas. The program, initiated in February 2002, based its activities on the Human Consequences of the Chernobyl Nuclear Accident report recommendations. The main goal of the CRDP's activities is supporting the efforts of the Government of Ukraine to mitigate the long-term social, economic, and ecological consequences of the Chernobyl disaster. CRDP works in the four areas of Ukraine that have been most affected by the Chernobyl nuclear accident: Kiev Oblast, Zhytomyrska Oblast, partially Kiev, Chernihivska Oblast, and Rivne Oblast. In 2016, the "New Safe Containment Shield" was placed on the damaged reactor to stop the flow of radiation. Today, visits are allowed to Chernobyl, but limited by strict rules. Visitors will pass areas of high radiation and cannot enter buildings.

"Chernobylite" is the name cited by two media sources for highly radioactive, unusual and potentially novel crystalline formations found at the Chernobyl power plant after the meltdown. These formations were found in the basement below Reactor No. 4 during an investigation into missing reactor fuel.




</doc>
<doc id="6102" url="https://en.wikipedia.org/wiki?curid=6102" title="Cyan">
Cyan

Cyan (, ) is a greenish-blue color. It is evoked by light with a predominant wavelength of between 490520 nm, between the wavelengths of green and blue.

In the subtractive color system, or CMYK (subtractive), which can be overlaid to produce all colors in paint and color printing, cyan is one of the primary colors, along with magenta, yellow, and black. In the additive color system, or RGB (additive) color model, used to create all the colors on a computer or television display, cyan is made by mixing equal amounts of green and blue light. Cyan is the complement of red; it can be made by the removal of red from white light. Mixing red light and cyan light at the right intensity will make white light.

The web color cyan is synonymous with aqua. Other colors in the cyan color range are teal, turquoise, electric blue, aquamarine, and others described as blue-green.

Its name is derived from the Ancient Greek κύανος, transliterated "kyanos", meaning "dark blue, dark blue enamel, Lapis lazuli". It was formerly known as "cyan blue" or cyan-blue, and its first recorded use as a color name in English was in 1879. Further origins of the color name can be traced back to a dye produced from the cornflower ("Centaurea cyanus").

In most languages, 'cyan' is not a basic color term and it phenomenologically appears as a greenish vibrant hue of blue to most English speakers. Reasons for why cyan is not linguistically acknowledged as a basic color term can be found in the frequent lack of distinction between blue and green in many languages.

The web color cyan shown at right is a secondary color in the RGB color model, which uses combinations of red, green and blue light to create all the colors on computer and television displays. In X11 colors, this color is called both cyan and aqua. In the HTML color list, this same color is called aqua.

The web colors are more vivid than the cyan used in the CMYK color system, and the web colors cannot be accurately reproduced on a printed page. To reproduce the web color cyan in inks, it is necessary to add some white ink to the printer's cyan below, so when it is reproduced in printing, it is not a primary subtractive color. It is called "aqua" (a name in use since 1598) because it is a color commonly associated with water, such as the appearance of the water at a tropical beach.
Cyan is also one of the common inks used in four-color printing, along with magenta, yellow, and black; this set of colors is referred to as CMYK as in spectrum(s).

While both the additive secondary and the subtractive primary are called "cyan", they can be substantially different from one another. Cyan printing ink can be more saturated or less saturated than the RGB secondary cyan, depending on what RGB color space and ink are considered.

Process cyan is not an RGB color, and there is no fixed conversion from CMYK primaries to RGB. Different formulations are used for printer's ink, so there can be variations in the printed color that is pure cyan ink. This is because real-world subtractive (unlike additive) color mixing does not consistently produce the same result when mixing apparently identical colors, since the specific frequencies filtered out to produce that color affect how it interacts with other colors. Phthalocyanine blue is one such commonly used pigment. A typical formulation of "process cyan" is shown in the color box at right.










</doc>
<doc id="6105" url="https://en.wikipedia.org/wiki?curid=6105" title="Conventional insulin therapy">
Conventional insulin therapy

Conventional insulin therapy is a therapeutic regimen for treatment of diabetes mellitus which contrasts with the newer intensive insulin therapy.

This older method (prior to the development home blood glucose monitoring) is still in use in a proportion of cases.

Conventional insulin therapy is characterized by:

The down side of this method is that it is difficult to achieve as good results of glycemic control as with intensive insulin therapy. The advantage is that, for diabetics with a regular lifestyle, the regime is less intrusive than the intensive therapy.


</doc>
<doc id="6109" url="https://en.wikipedia.org/wiki?curid=6109" title="Cream">
Cream

Cream is a dairy product composed of the higher-butterfat layer skimmed from the top of milk before homogenization. In un-homogenized milk, the fat, which is less dense, eventually rises to the top. In the industrial production of cream, this process is accelerated by using centrifuges called "separators". In many countries, it is sold in several grades depending on the total butterfat content. It can be dried to a powder for shipment to distant markets, and contains high levels of saturated fat.

Cream skimmed from milk may be called "sweet cream" to distinguish it from cream skimmed from whey, a by-product of cheese-making. Whey cream has a lower fat content and tastes more salty, tangy and "cheesy". In many countries, cream is usually sold partially fermented: sour cream, crème fraîche, and so on. Both forms have many culinary uses in sweet, bitter, salty and tangy dishes.

Cream produced by cattle (particularly Jersey cattle) grazing on natural pasture often contains some natural carotenoid pigments derived from the plants they eat; this gives it a slightly yellow tone, hence the name of the yellowish-white color: cream. This is also the origin of butter's yellow color. Cream from goat's milk, or from cows fed indoors on grain or grain-based pellets, is white.

Cream is used as an ingredient in many foods, including ice cream, many sauces, soups, stews, puddings, and some custard bases, and is also used for cakes. Whipped cream is served as a topping on ice cream sundaes, milkshakes, lassi, eggnog, sweet pies, strawberries, blueberries or peaches. Irish cream is an alcoholic liqueur which blends cream with whiskey, and often honey, wine, or coffee. Cream is also used in Indian curries such as masala dishes.

Cream (usually light/single cream or half and half) is often added to coffee in the US and Canada.

Both single and double cream can be used in cooking. Double cream or full-fat crème fraîche are often used when cream is added to a hot sauce, to prevent any problem with it separating or "splitting". Double cream can be thinned with milk to make an approximation of single cream.

The French word "crème" denotes not only dairy cream, but also other thick liquids such as sweet and savory custards, which are normally made with milk, not cream.

Different grades of cream are distinguished by their fat content, whether they have been heat-treated, whipped, and so on. In many jurisdictions, there are regulations for each type.

The Australia New Zealand Food Standards Code – Standard 2.5.2 – Defines cream as a milk product comparatively rich in fat, in the form of an emulsion of fat-in-skim milk, which can be obtained by separation from milk. Cream must contain no less than 350 g/kg (35%) milk fat.

Manufacturers labels may distinguish between different fat contents, a general guideline is as follows:

Canadian cream definitions are similar to those used in the United States, except for "light cream", which is very low-fat cream, usually with 5% or 6% butterfat. Specific product characteristics are generally uniform throughout Canada, but names vary by both geographic and linguistic area and by manufacturer: "coffee cream" may be 10% or 18% and "half-and-half" ("crème légère") may be 3%, 5%, 6% or 10%, all depending on location and brand.
Cream in Canada is defined to be the liquid obtained from milk after separating the various components to increase the milk fat content. Canadian Food and Drug Regulations allow stabilizers and acidity regulators. For heat-treated whipping cream, regulations disallow more than 0.25% skim milk powder, 0.1% glucose solids, 0.005% calcium sulphate, 0.2% microcrystalline cellulose, and 0.02% xanthan gum. The content of milk fat present in canned cream must be displayed as a percentage followed by "milk fat", "B.F", or "M.F". Fat content may also be displayed on canned cream in Canada.

In France, the use of the term "cream" for food products is defined by the decree 80-313 of April 23, 1980. It specifies the minimum rate of milk fat (12%) as well as the rules for pasteurisation or UHT sterilisation. The mention "crème fraîche" (fresh cream) can only be used for pasteurised creams conditioned on production site within 24h after pasteurisation. Even if food additives complying with French and European laws are allowed, usually, none will be found in plain "crèmes" and "crèmes fraîches" apart from lactic ferments (some low cost creams (or close to creams) can contain thickening agents, but rarely). Fat is commonly displayed "XX% M.G." for "matière grasse" (fat matter) on packagings.
In Japan, cream sold in supermarkets is usually between 35% and 48% butterfat.

Russia, as well as other EAC countries, legally separates cream into two classes: normal (10–34% butterfat) and heavy (35–58%), but the industry has pretty much standardized around the following types:
In Sweden, cream is usually sold as:

Mellangrädde (27%) is, nowadays, a less common variant. 
Gräddfil (usually 12 %) and Creme Fraiche (usually around 35 %) are two common sour cream products.

In Switzerland, the types of cream are legally defined as follows:

Sour cream and crème fraîche (German: Sauerrahm, Crème fraîche; French: crème acidulée, crème fraîche; Italian: panna acidula, crème fraîche) are defined as cream soured by bacterial cultures.

Thick cream (German: verdickter Rahm; French: crème épaissie; Italian: panna addensata) is defined as cream thickened using thickening agents.

In the United Kingdom, the types of cream are legally defined as follows:

In the United States, cream is usually sold as:

Most cream products sold in the United States at retail contain the minimum permissible fat content for their product type, e.g., "Half and half" almost always contains only 10.5% butterfat.<br> Not all grades are defined by all jurisdictions, and the exact fat content ranges vary. The above figures, except for "manufacturer's cream", are based on the Code of Federal Regulations, Title 21, Part 131

Cream may have thickening agents and stabilizers added. Thickeners include sodium alginate, carrageenan, gelatine, sodium bicarbonate, tetrasodium pyrophosphate, and alginic acid.

Other processing may be carried out. For example, cream has a tendency to produce oily globules (called "feathering") when added to coffee. The stability of the cream may be increased by increasing the non-fat solids content, which can be done by partial demineralisation and addition of sodium caseinate, although this is expensive.

 by churning cream to separate the butterfat and buttermilk. This can be done by hand or by machine.

Whipped cream is made by whisking or mixing air into cream with more than 30% fat, to turn the liquid cream into a soft solid. Nitrous oxide, from whipped-cream chargers may also be used to make whipped cream.

Sour cream, common in many countries including the U.S., Canada and Australia, is cream (12 to 16% or more milk fat) that has been subjected to a bacterial culture that produces lactic acid (0.5%+), which sours and thickens it.

Crème fraîche (28% milk fat) is slightly soured with bacterial culture, but not as sour or as thick as sour cream. Mexican crema (or cream espesa) is similar to crème fraîche.

Smetana is a heavy cream derived (15–40% milk fat) Central and Eastern European sweet or sour cream.

Rjome or rømme is Norwegian sour cream containing 35% milk fat, similar to Icelandic sýrður rjómi.

Clotted cream, common in the United Kingdom, is made through a process that starts by slowly heating whole milk to produce a very high-fat (55%) product. This is similar to Indian malai.

Reduced cream is a cream product used in New Zealand to make Kiwi dip.

Some non-edible substances are called creams due to their consistency: shoe cream is runny, unlike regular waxy shoe polish; hand/body 'creme' or "skin cream" is meant for moisturizing the skin.

Regulations in many jurisdictions restrict the use of the word "cream" for foods. Words such as "creme", "kreme", "creame", or "whipped topping" (e.g., Cool Whip) are often used for products which cannot legally be called cream, though in some jurisdictions even these spellings may be disallowed, for example under the doctrine of "idem sonans". Oreo and Hydrox cookies are a type of sandwich cookie in which two biscuits have a soft, sweet filling between them that is called "crème filling." In some cases, foods can be described as cream although they do not contain predominantly milk fats; for example in Britain "ice cream" does not have to be a dairy product (although it must be labelled "contains non-milk fat"), and salad cream is the customary name for a condiment that has been produced since the 1920s.

In other languages, cognates of "cream" are also sometimes used for non-food products, such as fogkrém (Hungarian for toothpaste), or Sonnencreme (German for suntan lotion).




</doc>
<doc id="6111" url="https://en.wikipedia.org/wiki?curid=6111" title="Chemical vapor deposition">
Chemical vapor deposition

Chemical vapor deposition (CVD) is a vacuum deposition method used to produce high quality, high-performance, solid materials. The process is often used in the semiconductor industry to produce thin films.

In typical CVD, the wafer (substrate) is exposed to one or more volatile precursors, which react and/or decompose on the substrate surface to produce the desired deposit. Frequently, volatile by-products are also produced, which are removed by gas flow through the reaction chamber.

Microfabrication processes widely use CVD to deposit materials in various forms, including: monocrystalline, polycrystalline, amorphous, and epitaxial. These materials include: silicon (dioxide, carbide, nitride, oxynitride), carbon (fiber, nanofibers, nanotubes, diamond and graphene), fluorocarbons, filaments, tungsten, titanium nitride and various high-k dielectrics.

CVD is practiced in a variety of formats. These processes generally differ in the means by which chemical reactions are initiated.

Most modern CVD is either LPCVD or UHVCVD.

CVD is commonly used to deposit conformal films and augment substrate surfaces in ways that more traditional surface modification techniques are not capable of. CVD is extremely useful in the process of atomic layer deposition at depositing extremely thin layers of material. A variety of applications for such films exist. Gallium arsenide is used in some integrated circuits (ICs) and photovoltaic devices. Amorphous polysilicon is used in photovoltaic devices. Certain carbides and nitrides confer wear-resistance. Polymerization by CVD, perhaps the most versatile of all applications, allows for super-thin coatings which possess some very desirable qualities, such as lubricity, hydrophobicity and weather-resistance to name a few. CVD of metal-organic frameworks, a class of crystalline nanoporous materials, has recently been demonstrated. Applications for these films are anticipated in gas sensing and low-k dielectrics
CVD techniques are adventageous for membrane coatings as well, such as those in desalination or water treatment, as these coatings can be sufficiently uniform (conformal) and thin that they do not clog membrane pores.

Polycrystalline silicon is deposited from trichlorosilane (SiHCl) or silane (SiH), using the following reactions:

This reaction is usually performed in LPCVD systems, with either pure silane feedstock, or a solution of silane with 70–80% nitrogen. Temperatures between 600 and 650 °C and pressures between 25 and 150 Pa yield a growth rate between 10 and 20 nm per minute. An alternative process uses a hydrogen-based solution. The hydrogen reduces the growth rate, but the temperature is raised to 850 or even 1050 °C to compensate. Polysilicon may be grown directly with doping, if gases such as phosphine, arsine or diborane are added to the CVD chamber. Diborane increases the growth rate, but arsine and phosphine decrease it.

Silicon dioxide (usually called simply "oxide" in the semiconductor industry) may be deposited by several different processes. Common source gases include silane and oxygen, dichlorosilane (SiClH) and nitrous oxide (NO), or tetraethylorthosilicate (TEOS; Si(OCH)). The reactions are as follows:

The choice of source gas depends on the thermal stability of the substrate; for instance, aluminium is sensitive to high temperature. Silane deposits between 300 and 500 °C, dichlorosilane at around 900 °C, and TEOS between 650 and 750 °C, resulting in a layer of "low- temperature oxide" (LTO). However, silane produces a lower-quality oxide than the other methods (lower dielectric strength, for instance), and it deposits nonconformally. Any of these reactions may be used in LPCVD, but the silane reaction is also done in APCVD. CVD oxide invariably has lower quality than thermal oxide, but thermal oxidation can only be used in the earliest stages of IC manufacturing.

Oxide may also be grown with impurities (alloying or "doping"). This may have two purposes. During further process steps that occur at high temperature, the impurities may diffuse from the oxide into adjacent layers (most notably silicon) and dope them. Oxides containing 5–15% impurities by mass are often used for this purpose. In addition, silicon dioxide alloyed with phosphorus pentoxide ("P-glass") can be used to smooth out uneven surfaces. P-glass softens and reflows at temperatures above 1000 °C. This process requires a phosphorus concentration of at least 6%, but concentrations above 8% can corrode aluminium. Phosphorus is deposited from phosphine gas and oxygen:

Glasses containing both boron and phosphorus (borophosphosilicate glass, BPSG) undergo viscous flow at lower temperatures; around 850 °C is achievable with glasses containing around 5 weight % of both constituents, but stability in air can be difficult to achieve. Phosphorus oxide in high concentrations interacts with ambient moisture to produce phosphoric acid. Crystals of BPO can also precipitate from the flowing glass on cooling; these crystals are not readily etched in the standard reactive plasmas used to pattern oxides, and will result in circuit defects in integrated circuit manufacturing.

Besides these intentional impurities, CVD oxide may contain byproducts of the deposition. TEOS produces a relatively pure oxide, whereas silane introduces hydrogen impurities, and dichlorosilane introduces chlorine.

Lower temperature deposition of silicon dioxide and doped glasses from TEOS using ozone rather than oxygen has also been explored (350 to 500 °C). Ozone glasses have excellent conformality but tend to be hygroscopic – that is, they absorb water from the air due to the incorporation of silanol (Si-OH) in the glass. Infrared spectroscopy and mechanical strain as a function of temperature are valuable diagnostic tools for diagnosing such problems.

Silicon nitride is often used as an insulator and chemical barrier in manufacturing ICs. The following two reactions deposit silicon nitride from the gas phase:

Silicon nitride deposited by LPCVD contains up to 8% hydrogen. It also experiences strong tensile stress, which may crack films thicker than 200 nm. However, it has higher resistivity and dielectric strength than most insulators commonly available in microfabrication (10 Ω·cm and 10 MV/cm, respectively).

Another two reactions may be used in plasma to deposit SiNH:

These films have much less tensile stress, but worse electrical properties (resistivity 10 to 10 Ω·cm, and dielectric strength 1 to 5 MV/cm).

CVD for tungsten is achieved from tungsten hexafluoride (WF), which may be deposited in two ways:

Other metals, notably aluminium and copper, can be deposited by CVD. , commercially cost-effective CVD for copper did not exist, although volatile sources exist, such as Cu(hfac). Copper is typically deposited by electroplating. Aluminum can be deposited from triisobutylaluminium (TIBAL) and related organoaluminium compounds.

CVD for molybdenum, tantalum, titanium, nickel is widely used. These metals can form useful silicides when deposited onto silicon. Mo, Ta and Ti are deposited by LPCVD, from their pentachlorides. Nickel, molybdenum, and tungsten can be deposited at low temperatures from their carbonyl precursors. In general, for an arbitrary metal "M", the chloride deposition reaction is as follows:

whereas the carbonyl decomposition reaction can happen spontaneously under thermal treatment or acoustic cavitation and is as follows:

the decomposition of metal carbonyls is often violently precipitated by moisture or air, where oxygen reacts with the metal precursor to form metal or metal oxide along with carbon dioxide.

Niobium(V) oxide layers can be produced by the thermal decomposition of niobium(V) ethoxide with the loss of diethyl ether according to the equation:

Many variations of CVD can be utilized to synthesize graphene. Although many advancements have been made, the processes listed below are not commercially viable yet.

The most popular carbon source that is used to produce graphene is methane gas. One of the less popular choices is petroleum asphalt, notable for being inexpensive but more difficult to work with.

Although methane is the most popular carbon source, hydrogen is required during the preparation process to promote carbon deposition on the substrate. If the flow ratio of methane and hydrogen are not appropriate, it will cause undesirable results. During the growth of graphene, the role of methane is to provide a carbon source, the role of hydrogen is to provide H atoms to corrode amorphous C, and improve the quality of graphene. But excessive H atoms can also corrode graphene. As a result, the integrity of the crystal lattice is destroyed, and the quality of graphene is deteriorated. Therefore, by optimizing the flow rate of methane and hydrogen gases in the growth process, the quality of graphene can be improved.

The use of catalyst is viable in changing the physical process of graphene production. Notable examples include iron nanoparticles, nickel foam, and gallium vapor. These catalysts can either be used in situ during graphene buildup, or situated at some distance away at the deposition area. Some catalysts require another step to remove them from the sample material.

The direct growth of high-quality, large single-crystalline domains of graphene on a dielectric substrate is of vital importance for applications in electronics and optoelectronics. Combining the advantages of both catalytic CVD and the ultra-flat dielectric substrate, gaseous catalyst-assisted CVD paves the way for synthesizing high-quality graphene for device applications while avoiding the transfer process.
Physical conditions such as surrounding pressure, temperature, carrier gas, and chamber material play a big role in production of graphene.

Most systems use LPCVD with pressures ranging from 1 to 1500 Pa. However, some still use APCVD. Low pressures are used more commonly as they help prevent unwanted reactions and produce more uniform thickness of deposition on the substrate.

On the other hand, temperatures used range from 800–1050 °C. High temperatures translate to an increase of the rate of reaction. Caution has to be exercised as high temperatures do pose higher danger levels in addition to greater energy costs.
Hydrogen gas and inert gases such as argon are flowed into the system. These gases act as a carrier, enhancing surface reaction and improving reaction rate, thereby increasing deposition of graphene onto the substrate.
Standard quartz tubing and chambers are used in CVD of graphene. Quartz is chosen because it has a very high melting point and is chemically inert. In other words, quartz does not interfere with any physical or chemical reactions regardless of the conditions.
Raman spectroscopy, X-ray spectroscopy, transmission electron microscopy (TEM), and scanning electron microscopy (SEM) are used to examine and characterize the graphene samples.

Raman spectroscopy is used to characterize and identify the graphene particles; X-ray spectroscopy is used to characterize chemical states; TEM is used to provide fine details regarding the internal composition of graphene; SEM is used to examine the surface and topography.

Sometimes, atomic force microscopy (AFM) is used to measure local properties such as friction and magnetism.

Cold wall CVD technique can be used to study the underlying surface science involved in graphene nucleation and growth as it allows unprecedented control of process parameters like gas flow rates, temperature and pressure as demonstrated in a recent study. The study was carried out in a home-built vertical cold wall system utilizing resistive heating by passing direct current through the substrate. It provided conclusive insight into a typical surface-mediated nucleation and growth mechanism involved in two-dimensional materials grown using catalytic CVD under conditions sought out in the semiconductor industry.

In spite of graphene's exciting electronic and thermal properties, it is unsuitable as a transistor for future digital devices, due to the absence of a bandgap between the conduction and valence bands. This makes it impossible to switch between on and off states with respect to electron flow. Scaling things down, graphene nanoribbons of less than 10 nm in width do exhibit electronic bandgaps and are therefore potential candidates for digital devices. Precise control over their dimensions, and hence electronic properties, however, represents a challenging goal, and the ribbons typically possess rough edges that are detrimental to their performance.

CVD can be used to produce a synthetic diamond by creating the circumstances necessary for carbon atoms in a gas to settle on a substrate in crystalline form. CVD of diamonds has received much attention in the materials sciences because it allows many new applications that had previously been considered too expensive. CVD diamond growth typically occurs under low pressure (1–27 kPa; 0.145–3.926 psi; 7.5–203 Torr) and involves feeding varying amounts of gases into a chamber, energizing them and providing conditions for diamond growth on the substrate. The gases always include a carbon source, and typically include hydrogen as well, though the amounts used vary greatly depending on the type of diamond being grown. Energy sources include hot filament, microwave power, and arc discharges, among others. The energy source is intended to generate a plasma in which the gases are broken down and more complex chemistries occur. The actual chemical process for diamond growth is still under study and is complicated by the very wide variety of diamond growth processes used.

Using CVD, films of diamond can be grown over large areas of substrate with control over the properties of the diamond produced. In the past, when high pressure high temperature (HPHT) techniques were used to produce a diamond, the result was typically very small free standing diamonds of varying sizes. With CVD diamond growth areas of greater than fifteen centimeters (six inches) diameter have been achieved and much larger areas are likely to be successfully coated with diamond in the future. Improving this process is key to enabling several important applications.

The growth of diamond directly on a substrate allows the addition of many of diamond's important qualities to other materials. Since diamond has the highest thermal conductivity of any bulk material, layering diamond onto high heat producing electronics (such as optics and transistors) allows the diamond to be used as a heat sink. Diamond films are being grown on valve rings, cutting tools, and other objects that benefit from diamond's hardness and exceedingly low wear rate. In each case the diamond growth must be carefully done to achieve the necessary adhesion onto the substrate. Diamond's very high scratch resistance and thermal conductivity, combined with a lower coefficient of thermal expansion than Pyrex glass, a coefficient of friction close to that of Teflon (polytetrafluoroethylene) and strong lipophilicity would make it a nearly ideal non-stick coating for cookware if large substrate areas could be coated economically.

CVD growth allows one to control the properties of the diamond produced. In the area of diamond growth, the word "diamond" is used as a description of any material primarily made up of sp3-bonded carbon, and there are many different types of diamond included in this. By regulating the processing parameters—especially the gases introduced, but also including the pressure the system is operated under, the temperature of the diamond, and the method of generating plasma—many different materials that can be considered diamond can be made. Single crystal diamond can be made containing various dopants. Polycrystalline diamond consisting of grain sizes from several nanometers to several micrometers can be grown. Some polycrystalline diamond grains are surrounded by thin, non-diamond carbon, while others are not. These different factors affect the diamond's hardness, smoothness, conductivity, optical properties and more.

Commercially, mercury cadmium telluride is of continuing interest for detection of infrared radiation. Consisting of an alloy of CdTe and HgTe, this material can be prepared from the dimethyl derivatives of the respective elements.



</doc>
<doc id="6112" url="https://en.wikipedia.org/wiki?curid=6112" title="CN Tower">
CN Tower

The CN Tower () is a concrete communications and observation tower located in Downtown Toronto, Ontario, Canada. Built on the former Railway Lands, it was completed in 1976. Its name "CN" originally referred to Canadian National, the railway company that built the tower. Following the railway's decision to divest non-core freight railway assets prior to the company's privatization in 1995, it transferred the tower to the Canada Lands Company, a federal Crown corporation responsible for real estate development.

The CN Tower held the record for the world's tallest free-standing structure for 32 years until 2007 when it was surpassed by the Burj Khalifa and was the world's tallest tower until 2009 when it was surpassed by the Canton Tower. It is now the ninth tallest free-standing structure in the world and remains the tallest free-standing structure in the Western Hemisphere. In 1995, the CN Tower was declared one of the modern Seven Wonders of the World by the American Society of Civil Engineers. It also belongs to the World Federation of Great Towers.

It is a signature icon of Toronto's skyline and attracts more than two million international visitors annually.

The original concept of the CN Tower originated in 1968 when the Canadian National Railway wanted to build a large TV and radio communication platform to serve the Toronto area, as well as demonstrate the strength of Canadian industry and CN in particular. These plans evolved over the next few years, and the project became official in 1972.

The tower would have been part of Metro Centre (see CityPlace), a large development south of Front Street on the Railway Lands, a large railway switching yard that was being made redundant by newer yards outside the city. Key project team members were NCK Engineering as structural engineer; John Andrews Architects; Webb, Zerafa, Menkes, Housden Architects; Foundation Building Construction; and Canron (Eastern Structural Division).

As Toronto grew rapidly during the late 1960s and early 1970s, multiple skyscrapers were constructed in the downtown core, most notably First Canadian Place. The reflective nature of the new buildings compromised the quality of broadcast signals necessitating new, higher antennas that were at least tall.

At the time, most data communications took place over point-to-point microwave links, whose dish antennae covered the roofs of large buildings. As each new skyscraper was added to the downtown, former line-of-sight links were no longer possible. CN intended to rent "hub" space for microwave links, visible from almost any building in the Toronto area.

The CN Tower can be seen from at least as far away as Kennedy Street in Aurora, Ontario, approximately to the north. It is also viewable to the naked eye from east of Toronto in Oshawa, several points along the Niagara Escarpment west of Toronto in Hamilton, Ontario, and to the south from Fort Niagara State Park in the U.S. state of New York.

The original plan for the tower envisioned a tripod consisting of three independent cylindrical "pillars" linked at various heights by structural bridges. Had it been built, this design would have been considerably shorter, with the metal antenna located roughly where the concrete section between the main level and the SkyPod lies today. As the design effort continued, it evolved into the current design with a single continuous hexagonal core to the SkyPod, with three support legs blended into the hexagon below the main level, forming a large Y-shape structure at the ground level.

The idea for the main level in its current form evolved around this time, but the Space Deck (later renamed SkyPod) was not part of the plans until some time later. One engineer in particular felt that visitors would feel the higher observation deck would be worth paying extra for, and the costs in terms of construction were not prohibitive. It was also some time around this point that it was realized that the tower could become the world's tallest structure, and plans were changed to incorporate subtle modifications throughout the structure to this end.

Construction on the CN Tower began on February 6, 1973, with massive excavations at the tower base for the foundation. By the time the foundation was complete, of earth and shale were removed to a depth of in the centre, and a base incorporating of concrete with of rebar and of steel cable had been built to a thickness of . This portion of the construction was fairly rapid, with only four months needed between the start and the foundation being ready for construction on top.

To create the main support pillar, workers constructed a hydraulically raised slipform at the base. This was a fairly unprecedented engineering feat on its own, consisting of a large metal platform that raised itself on jacks at about per day as the concrete below set. Concrete was poured Monday to Friday (and not continuously) by a small team of people until February 22, 1974, at which time it had already become the tallest structure in Canada, surpassing the recently built Inco Superstack in Sudbury, which was built using similar methods.

In total, the tower contains of concrete, all of which was mixed on-site in order to ensure batch consistency. Through the pour, the vertical accuracy of the tower was maintained by comparing the slip form's location to massive plumb bobs hanging from it, observed by small telescopes from the ground. Over the height of the tower, it varies from true vertical accuracy by only .
In August 1974, construction of the main level commenced. Using 45 hydraulic jacks attached to cables strung from a temporary steel crown anchored to the top of the tower, twelve giant steel and wooden bracket forms were slowly raised, ultimately taking about a week to crawl up to their final position. These forms were used to create the brackets that support the main level, as well as a base for the construction of the main level itself. The Space Deck (currently named SkyPod) was built of concrete poured into a wooden frame attached to rebar at the lower level deck, and then reinforced with a large steel compression band around the outside.

The antenna was originally to be raised by crane as well, but during construction the Sikorsky S-64 Skycrane helicopter became available when the United States Army sold one to civilian operators. The helicopter, named "Olga", was first used to remove the crane, and then flew the antenna up in 36 sections.

The flights of the antenna pieces were a minor tourist attraction of their own, and the schedule was printed in the local newspapers. Use of the helicopter saved months of construction time, with this phase taking only three and a half weeks instead of the planned six months. The tower was topped-off on April 2, 1975, after 26 months of construction, officially capturing the height record from Moscow's Ostankino Tower, and bringing the total mass to .

Two years into the construction, plans for Metro Centre were scrapped, leaving the tower isolated on the Railway Lands in what was then a largely abandoned light-industrial space. This caused serious problems for tourists to access the tower. Ned Baldwin, project architect with John Andrews, wrote at the time that "All of the logic which dictated the design of the lower accommodation has been upset," and that "Under such ludicrous circumstances Canadian National would hardly have chosen this location to build."

The CN Tower opened to the public on June 26, 1976. The construction costs of approximately ($ in dollars) were repaid in fifteen years. Canadian National Railway sold the tower prior to taking the company private in 1995, when it decided to divest all operations not directly related to its core freight shipping businesses.

From the mid-1970s to the mid-1980s, the CN Tower was practically the only development along Front Street West; it was still possible to see Lake Ontario from the foot of the CN Tower due to the expansive parking lots and lack of development in the area at the time. As the area around the tower was developed, particularly with the completion of the Metro Toronto Convention Centre in 1984 and the SkyDome in 1989 (renamed Rogers Centre in 2005), the former Railway Lands were redeveloped and the tower became the centre of a newly developing entertainment area. Access was greatly improved with the construction of the SkyWalk in 1989, which connected the tower and SkyDome to the nearby Union Station railway and subway station, and, in turn, to the city's PATH underground pedestrian system. By the mid-1990s, it was the centre of a thriving tourist district. The entire area continues to be an area of intense building, notably a boom in condominium construction in the first two decades of the 21st century, as well as the 2013 opening of the Ripley's Aquarium by the base of the tower.


The CN Tower consists of several substructures. The main portion of the tower is a hollow concrete hexagonal pillar containing the stairwells and power and plumbing connections. The Tower's six elevators are located in the three inverted angles created by the Tower's hexagonal shape (two elevators per angle). Each of the three elevator shafts is lined with glass, allowing for views of the city as the glass-windowed elevators make their way through the Tower. The stairwell was originally located in one of these angles (the one facing north), but was moved into the central hollow of the Tower; the Tower's new fifth and sixth elevators were placed in the hexagonal angle that once contained the stairwell. On top of the main concrete portion of the Tower is a tall metal broadcast antenna, carrying TV and radio signals. There are three visitor areas: the Glass Floor and Outdoor Observation Terrace, which are both located at an elevation of , the Indoor Lookout Level (formerly known as "Indoor Observation Level") located at , and the higher SkyPod (formerly known as "Space Deck") at , just below the metal antenna. The hexagonal shape can be seen between the two areas; however, below the main deck, three large supporting legs give the tower the appearance of a large tripod.

The main deck level is seven storeys, some of which are open to the public. Below the public areas — at — is a large white donut-shaped radome containing the structure's microwave receivers. The glass floor and outdoor observation deck are at . The glass floor has an area of and can withstand a pressure of . The floor's thermal glass units are thick, consisting of a pane of laminated glass, airspace and a pane of laminated glass. Some people experience acrophobia when standing on the glass floor and looking down at the ground below. In 2008, one elevator was upgraded to add a glass floor panel, believed to have the highest vertical rise of any elevator equipped with this feature. The Horizons Cafe and the lookout level are at . The 360 Restaurant, a revolving restaurant that completes a full rotation once every 72 minutes, is at . When the tower first opened, it also featured a disco named Sparkles (at the Indoor Observation Level), billed as the highest disco and dance floor in the world.

The SkyPod was once the highest public observation deck in the world until it was surpassed by the Shanghai World Financial Center in 2008.

A metal staircase reaches the main deck level after 1,776 steps, and the SkyPod above after 2,579 steps; it is the tallest metal staircase on Earth. These stairs are intended for emergency use only and are not open to the public, except for twice per year for charity stair-climb events. The average climber takes approximately 30 minutes to climb to the base of the radome, but the fastest climb on record is 7 minutes and 52 seconds in 1989 by Brendan Keenoy, an Ontario Provincial Police officer. In 2002, Canadian Olympian and Paralympic champion Jeff Adams climbed the stairs of the tower in a specially designed wheelchair. The stairs were originally on one of the three sides of the tower (facing north), with a glass view, but these were later replaced with the third elevator pair and the stairs were moved to the inside of the core. Top climbs on the new, windowless stairwell used since around 2003 have generally been over ten minutes.

On August 1, 2011, the CN Tower opened the EdgeWalk, an amusement in which thrill-seekers can walk on and around the roof of the main pod of the tower at , which is directly above the 360 Restaurant. It is the world's highest full-circle, hands-free walk. Visitors are tethered to an overhead rail system and walk around the edge of the CN Tower's main pod above the 360 Restaurant on a metal floor. The attraction is closed throughout the winter and during periods of electrical storms and high winds.

One of the notable guests who visited EdgeWalk includes Canadian comedian Rick Mercer, as featured on his CBC Television news satire show, "Rick Mercer Report". There, he was accompanied by Canadian pop singer Jann Arden. The episode aired on April 2013.

A freezing rain storm on March 2, 2007, resulted in a layer of ice several centimetres thick forming on the side of the tower and other downtown buildings. The sun thawed the ice, and winds of up to blew some of it away from the structure. There were fears that cars and windows of nearby buildings would be smashed by large chunks of ice. In response, police closed some streets surrounding the tower. During morning rush hour on March 5 of the same year, police expanded the area of closed streets to include the Gardiner Expressway away from the tower as increased winds blew the ice farther away, as far north as King Street West, away, where a taxicab window was shattered. Subsequently, on March 6, 2007, the Gardiner Expressway reopened after winds abated.

On April 16, 2018, falling ice from the CN Tower punctured the roof of the nearby Rogers Centre, causing the Toronto Blue Jays to postpone the game that day to the following day as a doubleheader; this was the third doubleheader held at the Rogers Centre. On April 20, the CN Tower reopened.

In August 2000, a fire broke out at the Ostankino Tower in Moscow killing three people and causing extensive damage. The fire was blamed on poor maintenance and outdated equipment. The failure of the fire-suppression systems and the lack of proper equipment for firefighters allowed the fire to destroy most of the interior and spark fears the tower might even collapse.

The Ostankino Tower was completed nine years before the CN Tower, and is only shorter. The parallels between the towers led to some concern that the CN Tower could be at risk of a similar tragedy. However, Canadian officials subsequently stated that it is "highly unlikely" that a similar disaster could occur at the CN Tower, as it has important safeguards that were not present in the Ostankino Tower. Specifically, officials cited:


Officials also noted that the CN Tower has an excellent safety record, although there was an electrical fire in the antennae on August 16, 2017 — the tower's first fire. Moreover, other supertall structures built between 1967 and 1976 — such as the Willis Tower (formerly the Sears Tower), the World Trade Center (until its destruction on September 11, 2001), the Fernsehturm Berlin, the Aon Center, 875 North Michigan Avenue (formerly the John Hancock Center), and First Canadian Place — also have excellent safety records, which suggests that the Ostankino Tower accident was a rare safety failure, and that the likelihood of similar events occurring at other supertall structures is extremely low.

The CN Tower was originally lit at night with incandescent lights, which were removed in 1997, because they were inefficient and expensive to repair. In June 2007, the tower was outfitted with 1,330 super-bright LED lights inside the elevator shafts, shooting over the main pod and upward to the top of the tower's mast to light the tower from dusk until 2 a.m. The official opening ceremony took place on June 28 before the Canada Day holiday weekend.

The tower changes its lighting scheme on holidays and to commemorate major events. After the 95th Grey Cup in Toronto, the tower was lit in green and white to represent the colours of the Grey Cup champion Saskatchewan Roughriders. From sundown on August 27, 2011, to sunrise the following day, the tower was lit in orange, the official colour of the New Democratic Party (NDP), to commemorate the death of federal NDP leader and leader of the official opposition Jack Layton. When former South African president Nelson Mandela died, the tower was lit in the colours of the South African flag. When former federal finance minister under Stephen Harper's Conservatives Jim Flaherty died, the tower was lit in green to reflect his Irish Canadian heritage. On the night of the attacks on Paris on November 13, 2015, the tower displayed the colours of the French flag.

Programmed from a desktop computer with a wireless network interface controller in Burlington, Ontario, the LEDs use less energy to light than the previous incandescent lights (10% less energy than the dimly lit version and 60% less than the brightly lit version). The estimated cost to use the LEDs is $1,000 per month.

During the spring and autumn bird migration seasons, the lights would be turned off to comply with the voluntary Fatal Light Awareness Program, which "encourages buildings to dim unnecessary exterior lighting to mitigate bird mortality during spring and summer migration."

The CN Tower is the tallest freestanding structure in the Western Hemisphere. As of 2013, there are only two other freestanding structures in the Western Hemisphere which exceed in height; the Willis Tower in Chicago, which stands at when measured to its pinnacle; and the topped-out One World Trade Center in New York City, which has a pinnacle height of , or approximately shorter than the CN Tower. Due to the symbolism of the number 1776 (the year of the signing of the United States Declaration of Independence), the height of One World Trade Center is unlikely to be increased. The proposed Chicago Spire was expected to exceed the height of the CN Tower, but its construction was halted early due to financial difficulties amid the Great Recession, and was eventually cancelled in 2010.

"Guinness World Records" has called the CN Tower "the world's tallest self-supporting tower" and "the world's tallest free-standing tower". Although Guinness did list this description of the CN Tower under the heading "tallest building" at least once, it has also listed it under "tallest tower", omitting it from its list of "tallest buildings." In 1996, Guinness changed the tower's classification to "World's Tallest Building and Freestanding Structure". Emporis and the Council on Tall Buildings and Urban Habitat both listed the CN Tower as the world's tallest free-standing structure on land, and specifically state that the CN Tower is not a true building, thereby awarding the title of world's tallest building to Taipei 101, which is shorter than the CN Tower. The issue of what was tallest became moot when Burj Khalifa, then under construction, exceeded the height of the CN Tower in 2007 (see below).

Although the CN Tower contains a restaurant, a gift shop and multiple observation levels, it does not have floors continuously from the ground, and therefore it is not considered a building by the Council on Tall Buildings and Urban Habitat (CTBUH) or Emporis. CTBUH defines a building as "a structure that is designed for residential, business, or manufacturing purposes. An essential characteristic of a building is that it has floors." The CN Tower and other similar structures—such as the Ostankino Tower in Moscow, Russia; the Oriental Pearl Tower in Shanghai, China; the Stratosphere Tower in Las Vegas, Nevada, United States; and the Eiffel Tower in Paris, France—are categorized as "towers", which are free-standing structures that may have observation decks and a few other habitable levels, but do not have floors from the ground up. The CN Tower was the tallest tower by this definition until 2010 (see below).

Taller than the CN Tower are numerous radio masts and towers, which are held in place by guy-wires, the tallest being the KVLY-TV mast in Blanchard, North Dakota, USA at tall, leading to a distinction between these and "free-standing" structures. Additionally, the Petronius Platform stands above its base on the bottom of the Gulf of Mexico, but only the top of this oil and natural gas platform are above water, and the structure is thus partially supported by its buoyancy. Like the CN Tower, none of these taller structures are commonly considered buildings.

On September 12, 2007, Burj Khalifa, which is a hotel, residential and commercial building in Dubai, United Arab Emirates, and was formerly known as Burj Dubai before opening, passed the CN Tower's 553.33-metre height. The CN Tower held the record of tallest free-standing structure on land for over 30 years.

After Burj Khalifa had been formally recognized by the Guinness World Records as the world's tallest freestanding structure, Guinness re-certified CN Tower as the world's tallest freestanding tower. The tower definition used by Guinness was defined by the Council on Tall Buildings and Urban Habitat as 'a building in which less than 50% of the construction is usable floor space'. "Guinness World Records" editor-in-chief Craig Glenday announced that Burj Khalifa was not classified as a tower because it has too much usable floor space to be considered to be a tower. CN Tower still held world records for highest above ground wine cellar (in 360 Restaurant) at 351 metres, highest above ground restaurant at 346 metres (Horizons Restaurant), and tallest free-standing concrete tower during Guinness's recertification. The CN Tower was surpassed in 2009 by the Canton Tower in Guangzhou, China, which stands at tall, as the world's tallest tower; which in turn was surpassed by the Tokyo Skytree in 2011, which currently is the tallest tower at in height. The CN Tower, as of 2018, stands as the ninth-tallest free-standing structure on land, remains the tallest free-standing structure in the Western Hemisphere, and is the third-tallest tower.

Since the construction of the tower had been completed, it has gained the following world height records:

The CN Tower has been and continues to be used as a communications tower for a number of different media and by numerous companies.

There is no AM broadcasting on the CN Tower. The FM antennas are situated above ground.



The CN Tower has been featured in numerous films, television shows, music recording covers, and video games. The tower also has its own official mascot, which resembles the tower itself.




</doc>
<doc id="6113" url="https://en.wikipedia.org/wiki?curid=6113" title="Chain rule">
Chain rule

In calculus, the chain rule is a formula for computing the derivative of the composition of two or more functions. That is, if "f" and "g" are functions, then the chain rule expresses the derivative of their composition (the function which maps "x" to "f"("g"("x")) ) in terms of the derivatives of "f" and "g" and the product of functions as follows:

This may equivalently be expressed in terms of the variable. Let , or equivalently, for all "x". Then one can also write

The chain rule may be written in Leibniz's notation in the following way. If a variable "z" depends on the variable "y", which itself depends on the variable "x", so that "y" and "z" are therefore dependent variables, then "z", via the intermediate variable of "y", depends on "x" as well. The chain rule then states,

The two versions of the chain rule are related; if formula_4 and formula_5, then

In integration, the counterpart to the chain rule is the substitution rule.

The chain rule seems to have first been used by Gottfried Wilhelm Leibniz. He used it to calculate the derivative of formula_7 as the composite of the square root function and the function formula_8. He first mentioned it in a 1676 memoir (with a sign error in the calculation). The common notation of chain rule is due to Leibniz. Guillaume de l'Hôpital used the chain rule implicitly in his "Analyse des infiniment petits". The chain rule does not appear in any of Leonhard Euler's analysis books, even though they were written over a hundred years after Leibniz's discovery.

Suppose that a skydiver jumps from an aircraft. Assume that "t" seconds after his jump, his height above sea level in meters is given by . One model for the atmospheric pressure at a height "h" is . These two equations can be differentiated and combined in various ways to produce the following data:
The chain rule gives a method for computing in terms of and . While it is always possible to directly apply the definition of the derivative to compute the derivative of a composite function, this is usually very difficult. The utility of the chain rule is that it turns a complicated derivative into several easy derivatives.

The chain rule states that, under appropriate conditions,
In this example, this equals

In the statement of the chain rule, "f" and "g" play slightly different roles because "f"′ is evaluated at "g"("t") whereas "g"′ is evaluated at "t". This is necessary to make the units work out correctly. For example, suppose that we want to compute the rate of change in atmospheric pressure ten seconds after the skydiver jumps. This is and has units of pascals per second. The factor "g"′(10) in the chain rule is the velocity of the skydiver ten seconds after his jump, and it is expressed in meters per second. "f"′("g"(10)) is the change in pressure with respect to height at the height "g"(10) and is expressed in pascals per meter. The product of "f"′("g"(10)) and "g"′(10) therefore has the correct units of pascals per second. It is not possible to evaluate "f" anywhere else. For instance, because the 10 in the problem represents ten seconds, the expression "f"′(10) represents the change in pressure at a height of ten seconds, which is nonsense. Similarly, because meters per second, the expression "f"′("g"′(10)) represents the change in pressure at a height of −98 meters per second, which is also nonsense. However, "g"(10) is 3020 meters above sea level, the height of the skydiver ten seconds after his jump. This has the correct units for an input to "f".

The simplest form of the chain rule is for real-valued functions of one real variable. It says that if "g" is a function that is differentiable at a point "c" (i.e. the derivative "g"′("c") exists) and "f" is a function that is differentiable at "g"("c"), then the composite function is differentiable at "c", and the derivative is

The rule is sometimes abbreviated as

If and , then this abbreviated form is written in Leibniz notation as:

The points where the derivatives are evaluated may also be stated explicitly:

It may be possible to apply the chain rule even when there are no formulas for the functions which are being differentiated. This can happen when the derivatives are measured directly. Suppose that a car is driving up a tall mountain. The car's speedometer measures its speed directly. If the grade is known, then the rate of ascent can be calculated using trigonometry. Suppose that the car is ascending at . Standard models for the Earth's atmosphere imply that the temperature drops about per kilometer ascended (called the lapse rate). To find the temperature drop per hour, we apply the chain rule. Let the function be the altitude of the car at time , and let the function be the temperature kilometers above sea level. and are not known exactly: For example, the altitude where the car starts is not known and the temperature on the mountain is not known. However, their derivatives are known: is , and is . The chain rule says that the derivative of the composite function is the product of the derivative of and the derivative of . This is .

One of the reasons why this computation is possible is because is a constant function. This is because the above model is very simple. A more accurate description of how the temperature near the car varies over time would require an accurate model of how the temperature varies at different altitudes. This model may not have a constant derivative. To compute the temperature change in such a model, it would be necessary to know and not just , because without knowing it is not possible to know where to evaluate .

The chain rule can be applied to composites of more than two functions. To take the derivative of a composite of more than two functions, notice that the composite of "f", "g", and "h" (in that order) is the composite of "f" with . The chain rule says that to compute the derivative of , it is sufficient to compute the derivative of "f" and the derivative of . The derivative of "f" can be calculated directly, and the derivative of can be calculated by applying the chain rule again.

For concreteness, consider the function
This can be decomposed as the composite of three functions:
Their derivatives are:
The chain rule says that the derivative of their composite at the point is:

In Leibniz notation, this is:
or for short,
The derivative function is therefore:

Another way of computing this derivative is to view the composite function as the composite of and "h". Applying the chain rule to this situation gives:
This is the same as what was computed above. This should be expected because .

Sometimes it is necessary to differentiate an arbitrarily long composition of the form formula_23. In this case, define
where formula_25 and formula_26 when formula_27. Then the chain rule takes the form
or, in the Lagrange notation,

The chain rule can be used to derive some well-known differentiation rules. For example, the quotient rule is a consequence of the chain rule and the product rule. To see this, write the function "f"("x")/"g"("x") as the product . First apply the product rule:
To compute the derivative of 1/"g"("x"), notice that it is the composite of "g" with the reciprocal function, that is, the function that sends "x" to 1/"x". The derivative of the reciprocal function is −1/"x". By applying the chain rule, the last expression becomes:
which is the usual formula for the quotient rule.

Suppose that has an inverse function. Call its inverse function "f" so that we have . There is a formula for the derivative of "f" in terms of the derivative of "g". To see this, note that "f" and "g" satisfy the formula
Because the functions "f"("g"("x")) and "x" are equal, their derivatives must be equal. The derivative of "x" is the constant function with value 1, and the derivative of "f"("g"("x")) is determined by the chain rule. Therefore, we have:
To express "f"′ as a function of an independent variable "y", we substitute "f"("y") for "x" wherever it appears. Then we can solve for "f"′.

For example, consider the function . It has an inverse . Because , the above formula says that

This formula is true whenever "g" is differentiable and its inverse "f" is also differentiable. This formula can fail when one of these conditions is not true. For example, consider . Its inverse is , which is not differentiable at zero. If we attempt to use the above formula to compute the derivative of "f" at zero, then we must evaluate 1/"g"′("f"(0)). Since and , we must evaluate 1/0, which is undefined. Therefore, the formula fails in this case. This is not surprising because "f" is not differentiable at zero.

Faà di Bruno's formula generalizes the chain rule to higher derivatives. Assuming that and , then the first few derivatives are:

One proof of the chain rule begins with the definition of the derivative:
Assume for the moment that "g"("x") does not equal "g"("a") for any "x" near "a". Then the previous expression is equal to the product of two factors:
If "g" oscillates near "a", then it might happen that no matter how close one gets to "a", there is always an even closer "x" such that "g"("x") equals "g"("a"). For example, this happens for near the point . Whenever this happens, the above expression is undefined because it involves division by zero. To work around this, introduce a function "Q" as follows:
We will show that the difference quotient for is always equal to:
Whenever "g"("x") is not equal to "g"("a"), this is clear because the factors of cancel. When "g"("x") equals "g"("a"), then the difference quotient for is zero because "f"("g"("x")) equals "f"("g"("a")), and the above product is zero because it equals "f"′("g"("a")) times zero. So the above product is always equal to the difference quotient, and to show that the derivative of at "a" exists and to determine its value, we need only show that the limit as "x" goes to "a" of the above product exists and determine its value.

To do this, recall that the limit of a product exists if the limits of its factors exist. When this happens, the limit of the product of these two factors will equal the product of the limits of the factors. The two factors are "Q"("g"("x")) and . The latter is the difference quotient for "g" at "a", and because "g" is differentiable at "a" by assumption, its limit as "x" tends to "a" exists and equals "g"′("a").

It remains to study "Q"("g"("x")). "Q" is defined wherever "f" is. Furthermore, "f" is differentiable at "g"("a") by assumption, so "Q" is continuous at "g"("a"), by definition of the derivative. The function "g" is continuous at "a" because it is differentiable at "a", and therefore is continuous at "a". So its limit as "x" goes to "a" exists and equals "Q"("g"("a")), which is "f"′("g"("a")).

This shows that the limits of both factors exist and that they equal "f"′("g"("a")) and "g"′("a"), respectively. Therefore, the derivative of at "a" exists and equals "f"′("g"("a"))"g"′("a").

Another way of proving the chain rule is to measure the error in the linear approximation determined by the derivative. This proof has the advantage that it generalizes to several variables. It relies on the following equivalent definition of differentiability at a point: A function "g" is differentiable at "a" if there exists a real number "g"′("a") and a function "ε"("h") that tends to zero as "h" tends to zero, and furthermore
Here the left-hand side represents the true difference between the value of "g" at "a" and at , whereas the right-hand side represents the approximation determined by the derivative plus an error term.

In the situation of the chain rule, such a function "ε" exists because "g" is assumed to be differentiable at "a". Again by assumption, a similar function also exists for "f" at "g"("a"). Calling this function "η", we have
The above definition imposes no constraints on "η"(0), even though it is assumed that "η"("k") tends to zero as "k" tends to zero. If we set , then "η" is continuous at 0.

Proving the theorem requires studying the difference as "h" tends to zero. The first step is to substitute for using the definition of differentiability of "g" at "a":
The next step is to use the definition of differentiability of "f" at "g"("a"). This requires a term of the form for some "k". In the above equation, the correct "k" varies with "h". Set and the right hand side becomes . Applying the definition of the derivative gives:
To study the behavior of this expression as "h" tends to zero, expand "k". After regrouping the terms, the right-hand side becomes:
Because "ε"("h") and "η"("k") tend to zero as "h" tends to zero, the first two bracketed terms tend to zero as "h" tends to zero. Applying the same theorem on products of limits as in the first proof, the third bracketed term also tends zero. Because the above expression is equal to the difference , by the definition of the derivative is differentiable at "a" and its derivative is 

The role of "Q" in the first proof is played by "η" in this proof. They are related by the equation:
The need to define "Q" at "g"("a") is analogous to the need to define "η" at zero.

Constantin Carathéodory's alternative definition of the differentiability of a function can be used to give an elegant proof of the chain rule.

Under this definition, a function is differentiable at a point if and only if there is a function , continuous at and such that . There is at most one such function, and if is differentiable at then .

Given the assumptions of the chain rule and the fact that differentiable functions and compositions of continuous functions are continuous, we have that there exist functions , continuous at and , continuous at and such that,
and
Therefore,
but the function given by is continuous at , and we get, for this 
A similar approach works for continuously differentiable (vector-)functions of many variables. This method of factoring also allows a unified approach to stronger forms of differentiability, when the derivative is required to be Lipschitz continuous, Hölder continuous, etc. Differentiation itself can be viewed as the polynomial remainder theorem (the little Bézout theorem, or factor theorem), generalized to an appropriate class of functions. 

If formula_51 and formula_52 then choosing infinitesimal formula_53 we compute the corresponding formula_54 and then the corresponding formula_55, so that
and applying the standard part we obtain
which is the chain rule.

The generalization of the chain rule to multi-variable functions is rather technical. However, it is simpler to write in the case of functions of the form 

As this case occurs often in the study of functions of a single variable, it is worth describing it separately.

For writing the chain rule for a function of the form 
one needs the partial derivatives of with respect to its arguments. The usual notations for partial derivatives involve names for the arguments of the function. As these arguments are not named in the above formula, it is simpler and clearer to denote by 
the derivative of with respect to its th argument, and by 
the value of this derivative at .

With this notation, the chain rule is

If the function is addition, that is, if 
then formula_63 (the constant function 1). 
Thus, the chain rule gives 

For multiplication
the partials are formula_66 and formula_67 Thus, 

The case of exponentiation
is slightly more complicated, as 
and, as formula_71
It follows that 

The simplest way for writing the chain rule in the general case is to use the total derivative, which is a linear transformation that captures all directional derivatives in a single formula. Consider differentiable functions and , and a point in . Let denote by the total derivative of at and denote the total derivative of at . These two derivatives are linear transformations and , respectively, so they can be composed. The chain rule for total derivatives is that their composite is the total derivative of at :
or for short,
The higher-dimensional chain rule can be proved using a technique similar to the second proof given above.

Because the total derivative is a linear transformation, the functions appearing in the formula can be rewritten as matrices. The matrix corresponding to a total derivative is called a Jacobian matrix, and the composite of two derivatives corresponds to the product of their Jacobian matrices. From this perspective the chain rule therefore says:
or for short,

That is, the Jacobian of a composite function is the product of the Jacobians of the composed functions (evaluated at the appropriate points).

The higher-dimensional chain rule is a generalization of the one-dimensional chain rule. If "k", "m", and "n" are 1, so that and , then the Jacobian matrices of "f" and "g" are . Specifically, they are:
The Jacobian of "f" ∘ "g" is the product of these matrices, so it is , as expected from the one-dimensional chain rule. In the language of linear transformations, "D"("g") is the function which scales a vector by a factor of "g"′("a") and "D"("f") is the function which scales a vector by a factor of "f"′("g"("a")). The chain rule says that the composite of these two linear transformations is the linear transformation , and therefore it is the function that scales a vector by "f"′("g"("a"))⋅"g"′("a").

Another way of writing the chain rule is used when "f" and "g" are expressed in terms of their components as and . In this case, the above rule for Jacobian matrices is usually written as:

The chain rule for total derivatives implies a chain rule for partial derivatives. Recall that when the total derivative exists, the partial derivative in the "i"th coordinate direction is found by multiplying the Jacobian matrix by the "i"th basis vector. By doing this to the formula above, we find:
Since the entries of the Jacobian matrix are partial derivatives, we may simplify the above formula to get:
More conceptually, this rule expresses the fact that a change in the "x" direction may change all of "g" through "g", and any of these changes may affect "f".

In the special case where , so that "f" is a real-valued function, then this formula simplifies even further:
This can be rewritten as a dot product. Recalling that , the partial derivative is also a vector, and the chain rule says that:

Given where and , determine the value of and using the chain rule.
and

Faà di Bruno's formula for higher-order derivatives of single-variable functions generalizes to the multivariable case. If is a function of as above, then the second derivative of is:

All extensions of calculus have a chain rule. In most of these, the formula remains the same, though the meaning of that formula may be vastly different.

One generalization is to manifolds. In this situation, the chain rule represents the fact that the derivative of is the composite of the derivative of "f" and the derivative of "g". This theorem is an immediate consequence of the higher dimensional chain rule given above, and it has exactly the same formula.

The chain rule is also valid for Fréchet derivatives in Banach spaces. The same formula holds as before. This case and the previous one admit a simultaneous generalization to Banach manifolds.

In abstract algebra, the derivative is interpreted as a morphism of modules of Kähler differentials. A ring homomorphism of commutative rings determines a morphism of Kähler differentials which sends an element "dr" to "d"("f"("r")), the exterior differential of "f"("r"). The formula holds in this context as well.

The common feature of these examples is that they are expressions of the idea that the derivative is part of a functor. A functor is an operation on spaces and functions between them. It associates to each space a new space and to each function between two spaces a new function between the corresponding new spaces. In each of the above cases, the functor sends each space to its tangent bundle and it sends each function to its derivative. For example, in the manifold case, the derivative sends a "C"-manifold to a "C"-manifold (its tangent bundle) and a "C"-function to its total derivative. There is one requirement for this to be a functor, namely that the derivative of a composite must be the composite of the derivatives. This is exactly the formula .

There are also chain rules in stochastic calculus. One of these, Itō's lemma, expresses the composite of an Itō process (or more generally a semimartingale) "dX" with a twice-differentiable function "f". In Itō's lemma, the derivative of the composite function depends not only on "dX" and the derivative of "f" but also on the second derivative of "f". The dependence on the second derivative is a consequence of the non-zero quadratic variation of the stochastic process, which broadly speaking means that the process can move up and down in a very rough way. This variant of the chain rule is not an example of a functor because the two functions being composed are of different types.




</doc>
<doc id="6115" url="https://en.wikipedia.org/wiki?curid=6115" title="P versus NP problem">
P versus NP problem

The P versus NP problem is a major unsolved problem in computer science. It asks whether every problem whose solution can be quickly verified (technically, verified in polynomial time) can also be solved quickly (again, in polynomial time).

The underlying issues were first discussed in the 1950s, in letters from John Forbes Nash Jr. to the National Security Agency, and from Kurt Gödel to John von Neumann. The precise statement of the P versus NP problem was introduced in 1971 by Stephen Cook in his seminal paper "The complexity of theorem proving procedures" (and independently by Leonid Levin in 1973) and is considered by many to be the most important open problem in computer science. It is one of the seven Millennium Prize Problems selected by the Clay Mathematics Institute, each of which carries a US$1,000,000 prize for the first correct solution.

The informal term "quickly", used above, means the existence of an algorithm solving the task that runs in polynomial time, such that the time to complete the task varies as a polynomial function on the size of the input to the algorithm (as opposed to, say, exponential time). The general class of questions for which some algorithm can provide an answer in polynomial time is called "class P" or just "P". For some questions, there is no known way to find an answer quickly, but if one is provided with information showing what the answer is, it is possible to verify the answer quickly. The class of questions for which an answer can be "verified" in polynomial time is called NP, which stands for "nondeterministic polynomial time".

Consider Sudoku, a game where the player is given a partially filled-in grid of numbers and attempts to complete the grid following certain rules. Given an incomplete Sudoku grid, of any size, is there at least one legal solution? Any proposed solution is easily verified, and the time to check a solution grows slowly (polynomially) as the grid gets bigger. However, all known algorithms for finding solutions take, for difficult examples, time that grows exponentially as the grid gets bigger. So, Sudoku is in NP (quickly checkable) but does not seem to be in P (quickly solvable). Thousands of other problems seem similar, in that they are fast to check but slow to solve. Researchers have shown that many of the problems in NP have the extra property that a fast solution to any one of them could be used to build a quick solution to any other problem in NP, a property called NP-completeness. Decades of searching have not yielded a fast solution to any of these problems, so most scientists suspect that none of these problems can be solved quickly. This, however, has never been proven.

An answer to the P = NP question would determine whether problems that can be verified in polynomial time, like Sudoku, can also be solved in polynomial time. If it turned out that P ≠ NP, it would mean that there are problems in NP that are harder to compute than to verify: they could not be solved in polynomial time, but the answer could be verified in polynomial time.

Aside from being an important problem in computational theory, a proof either way would have profound implications for mathematics, cryptography, algorithm research, artificial intelligence, game theory, multimedia processing, philosophy, economics and many other fields.

Although the P versus NP problem was formally defined in 1971, there were previous inklings of the problems involved, the difficulty of proof, and the potential consequences. In 1955, mathematician John Nash wrote a letter to the NSA, where he speculated that cracking a sufficiently complex code would require time exponential in the length of the key. If proved (and Nash was suitably skeptical) this would imply what is now called P ≠ NP, since a proposed key can easily be verified in polynomial time. Another mention of the underlying problem occurred in a 1956 letter written by Kurt Gödel to John von Neumann. Gödel asked whether theorem-proving (now known to be co-NP-complete) could be solved in quadratic or linear time, and pointed out one of the most important consequences—that if so, then the discovery of mathematical proofs could be automated.

The relation between the complexity classes P and NP is studied in computational complexity theory, the part of the theory of computation dealing with the resources required during computation to solve a given problem. The most common resources are time (how many steps it takes to solve a problem) and space (how much memory it takes to solve a problem).

In such analysis, a model of the computer for which time must be analyzed is required. Typically such models assume that the computer is "deterministic" (given the computer's present state and any inputs, there is only one possible action that the computer might take) and "sequential" (it performs actions one after the other).

In this theory, the class P consists of all those "decision problems" (defined below) that can be solved on a deterministic sequential machine in an amount of time that is polynomial in the size of the input; the class NP consists of all those decision problems whose positive solutions can be verified in polynomial time given the right information, or equivalently, whose solution can be found in polynomial time on a non-deterministic machine. Clearly, P ⊆ NP. Arguably the biggest open question in theoretical computer science concerns the relationship between those two classes:
Since 2002, William Gasarch has conducted three polls of researchers concerning this and related questions. Confidence that P ≠ NP has been increasing - in 2019, 88% believed P ≠ NP, as opposed to 83% in 2012 and 61% in 2002. When restricted to experts, the 2019 answers became 99% P ≠ NP.

To attack the P = NP question, the concept of NP-completeness is very useful. NP-complete problems are a set of problems to each of which any other NP-problem can be reduced in polynomial time and whose solution may still be verified in polynomial time. That is, any NP problem can be transformed into any of the NP-complete problems. Informally, an NP-complete problem is an NP problem that is at least as "tough" as any other problem in NP.

NP-hard problems are those at least as hard as NP problems, i.e., all NP problems can be reduced (in polynomial time) to them. NP-hard problems need not be in NP, i.e., they need not have solutions verifiable in polynomial time.

For instance, the Boolean satisfiability problem is NP-complete by the Cook–Levin theorem, so "any" instance of "any" problem in NP can be transformed mechanically into an instance of the Boolean satisfiability problem in polynomial time. The Boolean satisfiability problem is one of many such NP-complete problems. If any NP-complete problem is in P, then it would follow that P = NP. However, many important problems have been shown to be NP-complete, and no fast algorithm for any of them is known.

Based on the definition alone it is not obvious that NP-complete problems exist; however, a trivial and contrived NP-complete problem can be formulated as follows: given a description of a Turing machine "M" guaranteed to halt in polynomial time, does there exist a polynomial-size input that "M" will accept? It is in NP because (given an input) it is simple to check whether "M" accepts the input by simulating "M"; it is NP-complete because the verifier for any particular instance of a problem in NP can be encoded as a polynomial-time machine "M" that takes the solution to be verified as input. Then the question of whether the instance is a yes or no instance is determined by whether a valid input exists.

The first natural problem proven to be NP-complete was the Boolean satisfiability problem, also known as SAT. As noted above, this is the Cook–Levin theorem; its proof that satisfiability is NP-complete contains technical details about Turing machines as they relate to the definition of NP. However, after this problem was proved to be NP-complete, proof by reduction provided a simpler way to show that many other problems are also NP-complete, including the game Sudoku discussed earlier. In this case, the proof shows that a solution of Sudoku in polynomial time could also be used to complete Latin squares in polynomial time. This in turn gives a solution to the problem of partitioning tri-partite graphs into triangles, which could then be used to find solutions for the special case of SAT known as 3-SAT, which then provides a solution for general Boolean satisfiability. So a polynomial time solution to Sudoku leads, by a series of mechanical transformations, to a polynomial time solution of satisfiability, which in turn can be used to solve any other NP-problem in polynomial time. Using transformations like this, a vast class of seemingly unrelated problems are all reducible to one another, and are in a sense "the same problem".

Although it is unknown whether P = NP, problems outside of P are known. Just as the class P is defined in terms of polynomial running time, the class EXPTIME is the set of all decision problems that have "exponential" running time. In other words, any problem in EXPTIME is solvable by a deterministic Turing machine in O(2) time, where "p"("n") is a polynomial function of "n". A decision problem is EXPTIME-complete if it is in EXPTIME, and every problem in EXPTIME has a polynomial-time many-one reduction to it. A number of problems are known to be EXPTIME-complete. Because it can be shown that P ≠ EXPTIME, these problems are outside P, and so require more than polynomial time. In fact, by the time hierarchy theorem, they cannot be solved in significantly less than exponential time. Examples include finding a perfect strategy for chess positions on an "N" × "N" board and similar problems for other board games.

The problem of deciding the truth of a statement in Presburger arithmetic requires even more time. Fischer and Rabin proved in 1974 that every algorithm that decides the truth of Presburger statements of length "n" has a runtime of at least formula_1 for some constant "c". Hence, the problem is known to need more than exponential run time. Even more difficult are the undecidable problems, such as the halting problem. They cannot be completely solved by any algorithm, in the sense that for any particular algorithm there is at least one input for which that algorithm will not produce the right answer; it will either produce the wrong answer, finish without giving a conclusive answer, or otherwise run forever without producing any answer at all.

It is also possible to consider questions other than decision problems. One such class, consisting of counting problems, is called #P: whereas an NP problem asks "Are there any solutions?", the corresponding #P problem asks "How many solutions are there?" Clearly, a #P problem must be at least as hard as the corresponding NP problem, since a count of solutions immediately tells if at least one solution exists, if the count is greater than zero. Surprisingly, some #P problems that are believed to be difficult correspond to easy (for example linear-time) P problems. For these problems, it is very easy to tell whether solutions exist, but thought to be very hard to tell how many. Many of these problems are #P-complete, and hence among the hardest problems in #P, since a polynomial time solution to any of them would allow a polynomial time solution to all other #P problems.

In 1975, Richard E. Ladner showed that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.

The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to László Babai and Eugene Luks, has run time 2 for graphs with "n" vertices.

The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a factor less than "k". No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP = co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes expected time

to factor an "n"-bit integer. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time, although this does not indicate where the problem lies with respect to non-quantum complexity classes.

All of the above discussion has assumed that P means "easy" and "not in P" means "hard", an assumption known as "Cobham's thesis". It is a common and reasonably accurate assumption in complexity theory; however, it has some caveats.

First, it is not always true in practice. A theoretical polynomial algorithm may have extremely large constant factors or exponents thus rendering it impractical. On the other hand, even if a problem is shown to be NP-complete, and even if P ≠ NP, there may still be effective approaches to tackling the problem in practice. There are algorithms for many NP-complete problems, such as the knapsack problem, the traveling salesman problem and the Boolean satisfiability problem, that can solve to optimality many real-world instances in reasonable time. The empirical average-case complexity (time vs. problem size) of such algorithms can be surprisingly low. An example is the simplex algorithm in linear programming, which works surprisingly well in practice; despite having exponential worst-case time complexity it runs on par with the best known polynomial-time algorithms.

Second, there are types of computations which do not conform to the Turing machine model on which P and NP are defined, such as quantum computation and randomized algorithms.

According to polls, most computer scientists believe that P ≠ NP. A key reason for this belief is that after decades of studying these problems no one has been able to find a polynomial-time algorithm for any of more than 3000 important known NP-complete problems (see List of NP-complete problems). These algorithms were sought long before the concept of NP-completeness was even defined (Karp's 21 NP-complete problems, among the first found, were all well-known existing problems at the time they were shown to be NP-complete). Furthermore, the result P = NP would imply many other startling results that are currently believed to be false, such as NP = co-NP and P = PH.

It is also intuitively argued that the existence of problems that are hard to solve but for which the solutions are easy to verify matches real-world experience.
On the other hand, some researchers believe that there is overconfidence in believing P ≠ NP and that researchers should explore proofs of P = NP as well. For example, in 2002 these statements were made:

One of the reasons the problem attracts so much attention is the consequences of the answer. Either direction of resolution would advance theory enormously, and perhaps have huge practical consequences as well.

A proof that P = NP could have stunning practical consequences if the proof leads to efficient methods for solving some of the important problems in NP. It is also possible that a proof would not lead directly to efficient methods, perhaps if the proof is non-constructive, or the size of the bounding polynomial is too big to be efficient in practice. The consequences, both positive and negative, arise since various NP-complete problems are fundamental in many fields.

Cryptography, for example, relies on certain problems being difficult. A constructive and efficient solution to an NP-complete problem such as 3-SAT would break most existing cryptosystems including:
These would need to be modified or replaced by information-theoretically secure solutions not inherently based on P-NP equivalence.

On the other hand, there are enormous positive consequences that would follow from rendering tractable many currently mathematically intractable problems. For instance, many problems in operations research are NP-complete, such as some types of integer programming and the travelling salesman problem. Efficient solutions to these problems would have enormous implications for logistics. Many other important problems, such as some problems in protein structure prediction, are also NP-complete; if these problems were efficiently solvable it could spur considerable advances in life sciences and biotechnology.

But such changes may pale in significance compared to the revolution an efficient method for solving NP-complete problems would cause in mathematics itself. Gödel, in his early thoughts on computational complexity, noted that a mechanical method that could solve any problem would revolutionize mathematics:

Similarly, Stephen Cook says

Research mathematicians spend their careers trying to prove theorems, and some proofs have taken decades or even centuries to find after problems have been stated—for instance, Fermat's Last Theorem took over three centuries to prove. A method that is guaranteed to find proofs to theorems, should one exist of a "reasonable" size, would essentially end this struggle.

Donald Knuth has stated that he has come to believe that P = NP, but is reserved about the impact of a possible proof:
A proof that showed that P ≠ NP would lack the practical computational benefits of a proof that P = NP, but would nevertheless represent a very significant advance in computational complexity theory and provide guidance for future research. It would allow one to show in a formal way that many common problems cannot be solved efficiently, so that the attention of researchers can be focused on partial solutions or solutions to other problems. Due to widespread belief in P ≠ NP, much of this focusing of research has already taken place.

Also P ≠ NP still leaves open the average-case complexity of hard problems in NP. For example, it is possible that SAT requires exponential time in the worst case, but that almost all randomly selected instances of it are efficiently solvable. Russell Impagliazzo has described five hypothetical "worlds" that could result from different possible resolutions to the average-case complexity question. These range from "Algorithmica", where P = NP and problems like SAT can be solved efficiently in all instances, to "Cryptomania", where P ≠ NP and generating hard instances of problems outside P is easy, with three intermediate possibilities reflecting different possible distributions of difficulty over instances of NP-hard problems. The "world" where P ≠ NP but all problems in NP are tractable in the average case is called "Heuristica" in the paper. A Princeton University workshop in 2009 studied the status of the five worlds.

Although the P = NP problem itself remains open despite a million-dollar prize and a huge amount of dedicated research, efforts to solve the problem have led to several new techniques. In particular, some of the most fruitful research related to the P = NP problem has been in showing that existing proof techniques are not powerful enough to answer the question, thus suggesting that novel technical approaches are required.

As additional evidence for the difficulty of the problem, essentially all known proof techniques in computational complexity theory fall into one of the following classifications, each of which is known to be insufficient to prove that P ≠ NP:
These barriers are another reason why NP-complete problems are useful: if a polynomial-time algorithm can be demonstrated for an NP-complete problem, this would solve the P = NP problem in a way not excluded by the above results.

These barriers have also led some computer scientists to suggest that the P versus NP problem may be independent of standard axiom systems like ZFC (cannot be proved or disproved within them). The interpretation of an independence result could be that either no polynomial-time algorithm exists for any NP-complete problem, and such a proof cannot be constructed in (e.g.) ZFC, or that polynomial-time algorithms for NP-complete problems may exist, but it is impossible to prove in ZFC that such algorithms are correct. However, if it can be shown, using techniques of the sort that are currently known to be applicable, that the problem cannot be decided even with much weaker assumptions extending the Peano axioms (PA) for integer arithmetic, then there would necessarily exist nearly-polynomial-time algorithms for every problem in NP. Therefore, if one believes (as most complexity theorists do) that not all problems in NP have efficient algorithms, it would follow that proofs of independence using those techniques cannot be possible. Additionally, this result implies that proving independence from PA or ZFC using currently known techniques is no easier than proving the existence of efficient algorithms for all problems in NP.

While the P versus NP problem is generally considered unsolved, many amateur and some professional researchers have claimed solutions. Gerhard J. Woeginger maintains a list that, as of 2018, contains 62 purported proofs of P = NP, 50 of P ≠ NP, 2 proofs the problem is unprovable, and one proof that it is undecidable. An August 2010 claim of proof that P ≠ NP, by Vinay Deolalikar, a researcher at HP Labs, received heavy Internet and press attention after two leading specialists described it as " to be a relatively serious attempt". The proof has been reviewed publicly by academics, and Neil Immerman, an expert in the field, has pointed out two possibly fatal errors in the proof.
In September 2010, Deolalikar was reported to be working on a detailed expansion of his attempted proof. However, opinions expressed by several notable theoretical computer scientists indicate that the attempted proof is neither correct nor a significant advancement in the understanding of the problem. This assessment prompted a May 2013 article in "The New Yorker" to call the proof attempt "thoroughly discredited".

The P = NP problem can be restated in terms of expressible certain classes of logical statements, as a result of work in descriptive complexity.

Consider all languages of finite structures with a fixed signature including a linear order relation. Then, all such languages in P can be expressed in first-order logic with the addition of a suitable least fixed-point combinator. Effectively, this, in combination with the order, allows the definition of recursive functions. As long as the signature contains at least one predicate or function in addition to the distinguished order relation, so that the amount of space taken to store such finite structures is actually polynomial in the number of elements in the structure, this precisely characterizes P.

Similarly, NP is the set of languages expressible in existential second-order logic—that is, second-order logic restricted to exclude universal quantification over relations, functions, and subsets. The languages in the polynomial hierarchy, PH, correspond to all of second-order logic. Thus, the question "is P a proper subset of NP" can be reformulated as "is existential second-order logic able to describe languages (of finite linearly ordered structures with nontrivial signature) that first-order logic with least fixed point cannot?". The word "existential" can even be dropped from the previous characterization, since P = NP if and only if P = PH (as the former would establish that NP = co-NP, which in turn implies that NP = PH).

No algorithm for any NP-complete problem is known to run in polynomial time. However, there are algorithms known for NP-complete problems with the property that if P = NP, then the algorithm runs in polynomial time on accepting instances (although with enormous constants, making the algorithm impractical). However, these algorithms do not qualify as polynomial time because their running time on rejecting instances are not polynomial. The following algorithm, due to Levin (without any citation), is such an example below. It correctly accepts the NP-complete language SUBSET-SUM. It runs in polynomial time on inputs that are in SUBSET-SUM if and only if P = NP:

If, and only if, P = NP, then this is a polynomial-time algorithm accepting an NP-complete language. "Accepting" means it gives "yes" answers in polynomial time, but is allowed to run forever when the answer is "no" (also known as a "semi-algorithm").

This algorithm is enormously impractical, even if P = NP. If the shortest program that can solve SUBSET-SUM in polynomial time is "b" bits long, the above algorithm will try at least other programs first.

Conceptually speaking, a "decision problem" is a problem that takes as input some string "w" over an alphabet Σ, and outputs "yes" or "no". If there is an algorithm (say a Turing machine, or a computer program with unbounded memory) that can produce the correct answer for any input string of length "n" in at most "cn" steps, where "k" and "c" are constants independent of the input string, then we say that the problem can be solved in "polynomial time" and we place it in the class P. Formally, P is defined as the set of all languages that can be decided by a deterministic polynomial-time Turing machine. That is,
where
and a deterministic polynomial-time Turing machine is a deterministic Turing machine "M" that satisfies the following two conditions:


NP can be defined similarly using nondeterministic Turing machines (the traditional way). However, a modern approach to define NP is to use the concept of "certificate" and "verifier". Formally, NP is defined as the set of languages over a finite alphabet that have a verifier that runs in polynomial time, where the notion of "verifier" is defined as follows.

Let "L" be a language over a finite alphabet, Σ.

"L" ∈ NP if, and only if, there exists a binary relation formula_9 and a positive integer "k" such that the following two conditions are satisfied:


A Turing machine that decides "L" is called a "verifier" for "L" and a "y" such that ("x", "y") ∈ "R" is called a "certificate of membership" of "x" in "L".

In general, a verifier does not have to be polynomial-time. However, for "L" to be in NP, there must be a verifier that runs in polynomial time.

Let
Clearly, the question of whether a given "x" is a composite is equivalent to the question of whether "x" is a member of COMPOSITE. It can be shown that COMPOSITE ∈ NP by verifying that it satisfies the above definition (if we identify natural numbers with their binary representations).

COMPOSITE also happens to be in P, a fact demonstrated by the invention of the AKS primality test.

There are many equivalent ways of describing NP-completeness.

Let "L" be a language over a finite alphabet Σ.

"L" is NP-complete if, and only if, the following two conditions are satisfied:


Alternatively, if "L" ∈ NP, and there is another NP-complete problem that can be polynomial-time reduced to "L", then "L" is NP-complete. This is a common way of proving some new problem is NP-complete.

The film "Travelling Salesman", by director Timothy Lanzone, is the story of four mathematicians hired by the US government to solve the P versus NP problem.

In the sixth episode of "The Simpsons" seventh season "Treehouse of Horror VI", the equation P=NP is seen shortly after Homer accidentally stumbles into the "third dimension".

In the second episode of season 2 of "Elementary", "Solve for X" revolves around Sherlock and Watson investigating the murders of mathematicians who were attempting to solve P versus NP.





</doc>
<doc id="6117" url="https://en.wikipedia.org/wiki?curid=6117" title="Charles Sanders Peirce">
Charles Sanders Peirce

Charles Sanders Peirce (, ; September 10, 1839 – April 19, 1914) was an American philosopher, logician, mathematician, and scientist who is sometimes known as "the father of pragmatism". He was educated as a chemist and employed as a scientist for thirty years. Today he is appreciated largely for his contributions to logic, mathematics, philosophy, scientific methodology, semiotics, and for his founding of pragmatism.

An innovator in mathematics, statistics, philosophy, research methodology, and various sciences, Peirce considered himself, first and foremost, a logician. He made major contributions to logic, but logic for him encompassed much of that which is now called epistemology and philosophy of science. He saw logic as the formal branch of semiotics, of which he is a founder, which foreshadowed the debate among logical positivists and proponents of philosophy of language that dominated 20th century Western philosophy. Additionally, he defined the concept of abductive reasoning, as well as rigorously formulated mathematical induction and deductive reasoning. As early as 1886 he saw that logical operations could be carried out by electrical switching circuits. The same idea was used decades later to produce digital computers.

In 1934, the philosopher Paul Weiss called Peirce "the most original and versatile of American philosophers and America's greatest logician". "Webster's Biographical Dictionary" said in 1943 that Peirce was "now regarded as the most original thinker and greatest logician of his time". Keith Devlin similarly referred to Peirce as one of the greatest philosophers ever.
Peirce was born at 3 Phillips Place in Cambridge, Massachusetts. He was the son of Sarah Hunt Mills and Benjamin Peirce, himself a professor of astronomy and mathematics at Harvard University and perhaps the first serious research mathematician in America. At age 12, Charles read his older brother's copy of Richard Whately's "Elements of Logic", then the leading English-language text on the subject. So began his lifelong fascination with logic and reasoning. He went on to earn a Bachelor of Arts degree and a Master of Arts degree (1862) from Harvard. In 1863 the Lawrence Scientific School awarded him a Bachelor of Science degree, Harvard's first "summa cum laude" chemistry degree. His academic record was otherwise undistinguished. At Harvard, he began lifelong friendships with Francis Ellingwood Abbot, Chauncey Wright, and William James. One of his Harvard instructors, Charles William Eliot, formed an unfavorable opinion of Peirce. This proved fateful, because Eliot, while President of Harvard (1869–1909—a period encompassing nearly all of Peirce's working life—repeatedly vetoed Peirce'e employment at the university.

Peirce suffered from his late-teens onward from a nervous condition then known as "facial neuralgia", which would today be diagnosed as trigeminal neuralgia. His biographer, Joseph Brent, says that when in the throes of its pain "he was, at first, almost stupefied, and then aloof, cold, depressed, extremely suspicious, impatient of the slightest crossing, and subject to violent outbursts of temper". Its consequences may have led to the social isolation which made his life's later years so tragic.

Between 1859 and 1891, Peirce was intermittently employed in various scientific capacities by the United States Coast Survey and its successor, the United States Coast and Geodetic Survey, where he enjoyed his highly influential father's protection until the latter's death in 1880. That employment exempted Peirce from having to take part in the American Civil War; it would have been very awkward for him to do so, as the Boston Brahmin Peirces sympathized with the Confederacy. At the Survey, he worked mainly in geodesy and gravimetry, refining the use of pendulums to determine small local variations in the Earth's gravity. He was elected a resident fellow of the American Academy of Arts and Sciences in January 1867. The Survey sent him to Europe five times, first in 1871 as part of a group sent to observe a solar eclipse. There, he sought out Augustus De Morgan, William Stanley Jevons, and William Kingdon Clifford, British mathematicians and logicians whose turn of mind resembled his own. From 1869 to 1872, he was employed as an Assistant in Harvard's astronomical observatory, doing important work on determining the brightness of stars and the shape of the Milky Way. On April 20, 1877 he was elected a member of the National Academy of Sciences. Also in 1877, he proposed measuring the meter as so many wavelengths of light of a certain frequency, the kind of definition employed from 1960 to 1983.

During the 1880s, Peirce's indifference to bureaucratic detail waxed while his Survey work's quality and timeliness waned. Peirce took years to write reports that he should have completed in months. Meanwhile, he wrote entries, ultimately thousands during 1883–1909, on philosophy, logic, science, and other subjects for the encyclopedic "Century Dictionary". In 1885, an investigation by the Allison Commission exonerated Peirce, but led to the dismissal of Superintendent Julius Hilgard and several other Coast Survey employees for misuse of public funds. In 1891, Peirce resigned from the Coast Survey at Superintendent Thomas Corwin Mendenhall's request. He never again held regular employment.

In 1879, Peirce was appointed Lecturer in logic at Johns Hopkins University, which had strong departments in a number of areas that interested him, such as philosophy (Royce and Dewey completed their PhDs at Hopkins), psychology (taught by G. Stanley Hall and studied by Joseph Jastrow, who coauthored a landmark empirical study with Peirce), and mathematics (taught by J. J. Sylvester, who came to admire Peirce's work on mathematics and logic). His "Studies in Logic by Members of the Johns Hopkins University" (1883) contained works by himself and Allan Marquand, Christine Ladd, Benjamin Ives Gilman, and Oscar Howard Mitchell, several of whom were his graduate students. Peirce's nontenured position at Hopkins was the only academic appointment he ever held.

Brent documents something Peirce never suspected, namely that his efforts to obtain academic employment, grants, and scientific respectability were repeatedly frustrated by the covert opposition of a major Canadian-American scientist of the day, Simon Newcomb. Peirce's efforts may also have been hampered by what Brent characterizes as "his difficult personality". Conversely, Keith Devlin believes that Peirce's work was too far ahead of his time to be appreciated by the academic establishment of the day and that this played a large role in his inability to obtain a tenured position.

Peirce's personal life undoubtedly worked against his professional success. After his first wife, Harriet Melusina Fay ("Zina"), left him in 1875, Peirce, while still legally married, became involved with Juliette, whose last name, given variously as Froissy and Pourtalai, and nationality (she spoke French) remains uncertain. When his divorce from Zina became final in 1883, he married Juliette. That year, Newcomb pointed out to a Johns Hopkins trustee that Peirce, while a Hopkins employee, had lived and traveled with a woman to whom he was not married; the ensuing scandal led to his dismissal in January 1884. Over the years Peirce sought academic employment at various universities without success. He had no children by either marriage.

In 1887 Peirce spent part of his inheritance from his parents to buy of rural land near Milford, Pennsylvania, which never yielded an economic return. There he had an 1854 farmhouse remodeled to his design. The Peirces named the property "Arisbe". There they lived with few interruptions for the rest of their lives, Charles writing prolifically, much of it unpublished to this day (see Works). Living beyond their means soon led to grave financial and legal difficulties. He spent much of his last two decades unable to afford heat in winter and subsisting on old bread donated by the local baker. Unable to afford new stationery, he wrote on the verso side of old manuscripts. An outstanding warrant for assault and unpaid debts led to his being a fugitive in New York City for a while. Several people, including his brother James Mills Peirce and his neighbors, relatives of Gifford Pinchot, settled his debts and paid his property taxes and mortgage.

Peirce did some scientific and engineering consulting and wrote much for meager pay, mainly encyclopedic dictionary entries, and reviews for "The Nation" (with whose editor, Wendell Phillips Garrison, he became friendly). He did translations for the Smithsonian Institution, at its director Samuel Langley's instigation. Peirce also did substantial mathematical calculations for Langley's research on powered flight. Hoping to make money, Peirce tried inventing. He began but did not complete a number of books. In 1888, President Grover Cleveland appointed him to the Assay Commission.

From 1890 on, he had a friend and admirer in Judge Francis C. Russell of Chicago, who introduced Peirce to editor Paul Carus and owner Edward C. Hegeler of the pioneering American philosophy journal "The Monist", which eventually published at least 14 articles by Peirce. He wrote many texts in James Mark Baldwin's "Dictionary of Philosophy and Psychology" (1901–1905); half of those credited to him appear to have been written actually by Christine Ladd-Franklin under his supervision. He applied in 1902 to the newly formed Carnegie Institution for a grant to write a systematic book of his life's work. The application was doomed; his nemesis, Newcomb, served on the Institution's executive committee, and its President had been the President of Johns Hopkins at the time of Peirce's dismissal.

The one who did the most to help Peirce in these desperate times was his old friend William James, dedicating his "Will to Believe" (1897) to Peirce, and arranging for Peirce to be paid to give two series of lectures at or near Harvard (1898 and 1903). Most important, each year from 1907 until James's death in 1910, James wrote to his friends in the Boston intelligentsia to request financial aid for Peirce; the fund continued even after James died. Peirce reciprocated by designating James's eldest son as his heir should Juliette predecease him. It has been believed that this was also why Peirce used "Santiago" ("St. James" in English) as a middle name, but he appeared in print as early as 1890 as Charles Santiago Peirce. (See Charles Santiago Sanders Peirce for discussion and references).

Peirce died destitute in Milford, Pennsylvania, twenty years before his widow. Juliette Peirce kept the urn with Peirce's ashes at Arisbe. In 1934, Pennsylvania Governor Gifford Pinchot arranged for Juliette's burial on Milford Cemetery. The urn with Peirce's ashes was interred with Juliette.

Peirce grew up in a home where the supremacy of the white Anglo-Saxon male was taken for granted and Negro slavery was considered natural.

Until the outbreak of the Civil War his father described himself as a secessionist, but after the outbreak of the war, this stopped and he became a Union partisan, providing donations to the Sanitary Commission, the leading Northern war charity. No members of the Peirce family volunteered or enlisted. Peirce shared his father's views and liked to use the following syllogism to illustrate the unreliability of traditional forms of logic (see also: ):

<poem>
All Men are equal in their political rights.
Negroes are Men.
Therefore, negroes are equal in political rights to whites.
</poem>

Bertrand Russell (1959) wrote "Beyond doubt [...] he was one of the most original minds of the later nineteenth century and certainly the greatest American thinker ever". Russell and Whitehead's "Principia Mathematica", published from 1910 to 1913, do not mention Peirce (Peirce's work was not widely known until later). A. N. Whitehead, while reading some of Peirce's unpublished manuscripts soon after arriving at Harvard in 1924, was struck by how Peirce had anticipated his own "process" thinking. (On Peirce and process metaphysics, see Lowe 1964). Karl Popper viewed Peirce as "one of the greatest philosophers of all times". Yet Peirce's achievements were not immediately recognized. His imposing contemporaries William James and Josiah Royce admired him and Cassius Jackson Keyser, at Columbia and C. K. Ogden, wrote about Peirce with respect but to no immediate effect.

The first scholar to give Peirce his considered professional attention was Royce's student Morris Raphael Cohen, the editor of an anthology of Peirce's writings titled "Chance, Love, and Logic" (1923), and the author of the first bibliography of Peirce's scattered writings. John Dewey, studied under Peirce at Johns Hopkins. From 1916 onward, Dewey's writings repeatedly mention Peirce with deference. His 1938 "Logic: The Theory of Inquiry" is much influenced by Peirce. The publication of the first six volumes of "Collected Papers" (1931–1935), the most important event to date in Peirce studies and one that Cohen made possible by raising the needed funds, did not prompt an outpouring of secondary studies. The editors of those volumes, Charles Hartshorne and Paul Weiss, did not become Peirce specialists. Early landmarks of the secondary literature include the monographs by Buchler (1939), Feibleman (1946), and Goudge (1950), the 1941 PhD thesis by Arthur W. Burks (who went on to edit volumes 7 and 8), and the studies edited by Wiener and Young (1952). The Charles S. Peirce Society was founded in 1946. Its "Transactions", an academic quarterly specializing in Peirce's pragmatism and American philosophy has appeared since 1965. (See Phillips 2014, 62 for discussion of Peirce and Dewey relative to transactionalism).

In 1949, while doing unrelated archival work, the historian of mathematics Carolyn Eisele (1902–2000) chanced on an autograph letter by Peirce. So began her forty years of research on Peirce, 'the mathematician and scientist', culminating in Eisele (1976, 1979, 1985). Beginning around 1960, the philosopher and historian of ideas Max Fisch (1900–1995) emerged as an authority on Peirce (Fisch, 1986). He includes many of his relevant articles in a survey (Fisch 1986: 422–48) of the impact of Peirce's thought through 1983.

Peirce has gained a significant international following, marked by university research centers devoted to Peirce studies and pragmatism in Brazil (CeneP/CIEP), Finland (HPRC and ), Germany (Wirth's group, Hoffman's and Otte's group, and Deuser's and Härle's group), France (L'I.R.S.C.E.), Spain (GEP), and Italy (CSP). His writings have been translated into several languages, including German, French, Finnish, Spanish, and Swedish. Since 1950, there have been French, Italian, Spanish, British, and Brazilian Peirce scholars of note. For many years, the North American philosophy department most devoted to Peirce was the University of Toronto, thanks in part to the leadership of Thomas Goudge and David Savan. In recent years, U.S. Peirce scholars have clustered at Indiana University – Purdue University Indianapolis, home of the Peirce Edition Project (PEP) –, and Pennsylvania State University.

In recent years, Peirce's trichotomy of signs is exploited by a growing number of practitioners for marketing and design tasks.

Peirce's reputation rests largely on a number of academic papers published in American scientific and scholarly journals such as "Proceedings of the American Academy of Arts and Sciences", the "Journal of Speculative Philosophy", "The Monist", "Popular Science Monthly", the "American Journal of Mathematics", "Memoirs of the National Academy of Sciences", "The Nation", and others. See Articles by Peirce, published in his lifetime for an extensive list with links to them online. The only full-length book (neither extract nor pamphlet) that Peirce authored and saw published in his lifetime was "Photometric Researches" (1878), a 181-page monograph on the applications of spectrographic methods to astronomy. While at Johns Hopkins, he edited "Studies in Logic" (1883), containing chapters by himself and his graduate students. Besides lectures during his years (1879–1884) as lecturer in Logic at Johns Hopkins, he gave at least nine series of lectures, many now published; see Lectures by Peirce.

After Peirce's death, Harvard University obtained from Peirce's widow the papers found in his study, but did not microfilm them until 1964. Only after Richard Robin (1967) catalogued this "Nachlass" did it become clear that Peirce had left approximately 1650 unpublished manuscripts, totaling over 100,000 pages, mostly still unpublished except on microfilm. On the vicissitudes of Peirce's papers, see Houser (1989). Reportedly the papers remain in unsatisfactory condition.

The first published anthology of Peirce's articles was the one-volume "Chance, Love and Logic: Philosophical Essays", edited by Morris Raphael Cohen, 1923, still in print. Other one-volume anthologies were published in 1940, 1957, 1958, 1972, 1994, and 2009, most still in print. The main posthumous editions of Peirce's works in their long trek to light, often multi-volume, and some still in print, have included:

1931–1958: "Collected Papers of Charles Sanders Peirce" (CP), 8 volumes, includes many published works, along with a selection of previously unpublished work and a smattering of his correspondence. This long-time standard edition drawn from Peirce's work from the 1860s to 1913 remains the most comprehensive survey of his prolific output from 1893 to 1913. It is organized thematically, but texts (including lecture series) are often split up across volumes, while texts from various stages in Peirce's development are often combined, requiring frequent visits to editors' notes. Edited (1–6) by Charles Hartshorne and Paul Weiss and (7–8) by Arthur Burks, in print and online.

1975–1987: "Charles Sanders Peirce: Contributions to" The Nation, 4 volumes, includes Peirce's more than 300 reviews and articles published 1869–1908 in "The Nation". Edited by Kenneth Laine Ketner and James Edward Cook, online.

1976: "The New Elements of Mathematics by Charles S. Peirce", 4 volumes in 5, included many previously unpublished Peirce manuscripts on mathematical subjects, along with Peirce's important published mathematical articles. Edited by Carolyn Eisele, back in print.

1977: "Semiotic and Significs: The Correspondence between C. S. Peirce and Victoria Lady Welby" (2nd edition 2001), included Peirce's entire correspondence (1903–1912) with Victoria, Lady Welby. Peirce's other published correspondence is largely limited to the 14 letters included in volume 8 of the "Collected Papers", and the 20-odd pre-1890 items included so far in the "Writings". Edited by Charles S. Hardwick with James Cook, out of print.

1982–now: "Writings of Charles S. Peirce, A Chronological Edition" (W), Volumes 1–6 & 8, of a projected 30. The limited coverage, and defective editing and organization, of the "Collected Papers" led Max Fisch and others in the 1970s to found the Peirce Edition Project (PEP), whose mission is to prepare a more complete critical chronological edition. Only seven volumes have appeared to date, but they cover the period from 1859 to 1892, when Peirce carried out much of his best-known work. "Writings of Charles S. Peirce", 8 was published in November 2010; and work continues on "Writings of Charles S. Peirce", 7, 9, and 11. In print and online.

1985: "Historical Perspectives on Peirce's Logic of Science: A History of Science", 2 volumes. Auspitz has said, "The extent of Peirce's immersion in the science of his day is evident in his reviews in the "Nation" [...] and in his papers, grant applications, and publishers' prospectuses in the history and practice of science", referring latterly to "Historical Perspectives". Edited by Carolyn Eisele, back in print.

1992: "Reasoning and the Logic of Things" collects in one place Peirce's 1898 series of lectures invited by William James. Edited by Kenneth Laine Ketner, with commentary by Hilary Putnam, in print.

1992–1998: "The Essential Peirce" (EP), 2 volumes, is an important recent sampler of Peirce's philosophical writings. Edited (1) by Nathan Hauser and Christian Kloesel and (2) by "Peirce Edition Project" editors, in print.

1997: "Pragmatism as a Principle and Method of Right Thinking" collects Peirce's 1903 Harvard "Lectures on Pragmatism" in a study edition, including drafts, of Peirce's lecture manuscripts, which had been previously published in abridged form; the lectures now also appear in "The Essential Peirce", 2. Edited by Patricia Ann Turisi, in print.

2010: "Philosophy of Mathematics: Selected Writings" collects important writings by Peirce on the subject, many not previously in print. Edited by Matthew E. Moore, in print.

Peirce's most important work in pure mathematics was in logical and foundational areas. He also worked on linear algebra, matrices, various geometries, topology and Listing numbers, Bell numbers, graphs, the four-color problem, and the nature of continuity.

He worked on applied mathematics in economics, engineering, and map projections (such as the Peirce quincuncial projection), and was especially active in probability and statistics.


Peirce made a number of striking discoveries in formal logic and foundational mathematics, nearly all of which came to be appreciated only long after he died:

In 1860 he suggested a cardinal arithmetic for infinite numbers, years before any work by Georg Cantor (who completed his dissertation in 1867) and without access to Bernard Bolzano's 1851 (posthumous) "Paradoxien des Unendlichen".

In 1880–1881 he showed how Boolean algebra could be done via a repeated sufficient single binary operation (logical NOR), anticipating Henry M. Sheffer by 33 years. (See also De Morgan's Laws.)

In 1881 he set out the axiomatization of natural number arithmetic, a few years before Richard Dedekind and Giuseppe Peano. In the same paper Peirce gave, years before Dedekind, the first purely cardinal definition of a finite set in the sense now known as "Dedekind-finite", and implied by the same stroke an important formal definition of an infinite set (Dedekind-infinite), as a set that can be put into a one-to-one correspondence with one of its proper subsets.

In 1885 he distinguished between first-order and second-order quantification. In the same paper he set out what can be read as the first (primitive) axiomatic set theory, anticipating Zermelo by about two decades (Brady 2000, pp. 132–33).

In 1886, he saw that Boolean calculations could be carried out via electrical switches, anticipating Claude Shannon by more than 50 years. 
By the later 1890s he was devising existential graphs, a diagrammatic notation for the predicate calculus. Based on them are John F. Sowa's conceptual graphs and Sun-Joo Shin's diagrammatic reasoning.


Peirce wrote drafts for an introductory textbook, with the working title "The New Elements of Mathematics", that presented mathematics from an original standpoint. Those drafts and many other of his previously unpublished mathematical manuscripts finally appeared in "The New Elements of Mathematics by Charles S. Peirce" (1976), edited by mathematician Carolyn Eisele.


Peirce agreed with Auguste Comte in regarding mathematics as more basic than philosophy and the special sciences (of nature and mind). Peirce classified mathematics into three subareas: (1) mathematics of logic, (2) discrete series, and (3) pseudo-continua (as he called them, including the real numbers) and continua. Influenced by his father Benjamin, Peirce argued that mathematics studies purely hypothetical objects and is not just the science of quantity but is more broadly the science which draws necessary conclusions; that mathematics aids logic, not vice versa; and that logic itself is part of philosophy and is the science "about" drawing conclusions necessary and otherwise.

 Beginning with his first paper on the "Logic of Relatives" (1870), Peirce extended the theory of relations that Augustus De Morgan had just recently awakened from its Cinderella slumbers. Much of the mathematics of relations now taken for granted was "borrowed" from Peirce, not always with all due credit; on that and on how the young Bertrand Russell, especially his "Principles of Mathematics" and "Principia Mathematica", did not do Peirce justice, see Anellis (1995). In 1918 the logician C. I. Lewis wrote, "The contributions of C.S. Peirce to symbolic logic are more numerous and varied than those of any other writer—at least in the nineteenth century." Beginning in 1940, Alfred Tarski and his students rediscovered aspects of Peirce's larger vision of relational logic, developing the perspective of relation algebra.

Relational logic gained applications. In mathematics, it influenced the abstract analysis of E. H. Moore and the lattice theory of Garrett Birkhoff. In computer science, the relational model for databases was developed with Peircean ideas in work of Edgar F. Codd, who was a doctoral student of Arthur W. Burks, a Peirce scholar. In economics, relational logic was used by Frank P. Ramsey, John von Neumann, and Paul Samuelson to study preferences and utility and by Kenneth J. Arrow in "Social Choice and Individual Values", following Arrow's association with Tarski at City College of New York.

On Peirce and his contemporaries Ernst Schröder and Gottlob Frege, Hilary Putnam (1982) documented that Frege's work on the logic of quantifiers had little influence on his contemporaries, although it was published four years before the work of Peirce and his student Oscar Howard Mitchell. Putnam found that mathematicians and logicians learned about the logic of quantifiers through the independent work of Peirce and Mitchell, particularly through Peirce's "On the Algebra of Logic: A Contribution to the Philosophy of Notation" (1885), published in the premier American mathematical journal of the day, and cited by Peano and Schröder, among others, who ignored Frege. They also adopted and modified Peirce's notations, typographical variants of those now used. Peirce apparently was ignorant of Frege's work, despite their overlapping achievements in logic, philosophy of language, and the foundations of mathematics.

Peirce's work on formal logic had admirers besides Ernst Schröder:

A philosophy of logic, grounded in his categories and semiotic, can be extracted from Peirce's writings and, along with Peirce's logical work more generally, is exposited and defended in Hilary Putnam (1982); the Introduction in Nathan Houser "et al." (1997); and Randall Dipert's chapter in Cheryl Misak (2004).

Continuity and synechism are central in Peirce's philosophy: "I did not at first suppose that it was, as I gradually came to find it, the master-Key of philosophy".

From a mathematical point of view, he embraced infinitesimals and worked long on the mathematics of continua. He long held that the real numbers constitute a pseudo-continuum; that a true continuum is the real subject matter of "analysis situs" (topology); and that a true continuum of instants exceeds—and within any lapse of time has room for—any Aleph number (any infinite "multitude" as he called it) of instants.

In 1908 Peirce wrote that he found that a true continuum might have or lack such room. Jérôme Havenel (2008): "It is on 26 May 1908, that Peirce finally gave up his idea that in every continuum there is room for whatever collection of any multitude. From now on, there are different kinds of continua, which have different properties."

Peirce held that science achieves statistical probabilities, not certainties, and that spontaneity (absolute chance) is real (see Tychism on his view). Most of his statistical writings promote the frequency interpretation of probability (objective ratios of cases), and many of his writings express skepticism about (and criticize the use of) probability when such models are not based on objective randomization. Though Peirce was largely a frequentist, his possible world semantics introduced the "propensity" theory of probability before Karl Popper. Peirce (sometimes with Joseph Jastrow) investigated the probability judgments of experimental subjects, "perhaps the very first" elicitation and estimation of subjective probabilities in experimental psychology and (what came to be called) Bayesian statistics.

Peirce was one of the founders of statistics. He formulated modern statistics in "Illustrations of the Logic of Science" (1877–1878) and "A Theory of Probable Inference" (1883). With a repeated measures design, Charles Sanders Peirce and Joseph Jastrow introduced blinded, controlled randomized experiments in 1884 (Hacking 1990:205) (before Ronald A. Fisher). He invented optimal design for experiments on gravity, in which he "corrected the means". He used correlation and smoothing. Peirce extended the work on outliers by Benjamin Peirce, his father. He introduced terms "confidence" and "likelihood" (before Jerzy Neyman and Fisher). (See Stephen Stigler's historical books and Ian Hacking 1990).

Peirce was a working scientist for 30 years, and arguably was a professional philosopher only during the five years he lectured at Johns Hopkins. He learned philosophy mainly by reading, each day, a few pages of Immanuel Kant's "Critique of Pure Reason", in the original German, while a Harvard undergraduate. His writings bear on a wide array of disciplines, including mathematics, logic, philosophy, statistics, astronomy, metrology, geodesy, experimental psychology, economics, linguistics, and the history and philosophy of science. This work has enjoyed renewed interest and approval, a revival inspired not only by his anticipations of recent scientific developments but also by his demonstration of how philosophy can be applied effectively to human problems.

Peirce's philosophy includes (see below in related sections) a pervasive three-category system, belief that truth is immutable and is both independent from actual opinion (fallibilism) and discoverable (no radical skepticism), logic as formal semiotic on signs, on arguments, and on inquiry's ways—including philosophical pragmatism (which he founded), critical common-sensism, and scientific method—and, in metaphysics: Scholastic realism, e.g. John Duns Scotus, belief in God, freedom, and at least an attenuated immortality, objective idealism, and belief in the reality of continuity and of absolute chance, mechanical necessity, and creative love. In his work, fallibilism and pragmatism may seem to work somewhat like skepticism and positivism, respectively, in others' work. However, for Peirce, fallibilism is balanced by an anti-skepticism and is a basis for belief in the reality of absolute chance and of continuity, and pragmatism commits one to anti-nominalist belief in the reality of the general (CP 5.453–57).

For Peirce, First Philosophy, which he also called cenoscopy, is less basic than mathematics and more basic than the special sciences (of nature and mind). It studies positive phenomena in general, phenomena available to any person at any waking moment, and does not settle questions by resorting to special experiences. He divided such philosophy into (1) phenomenology (which he also called phaneroscopy or categorics), (2) normative sciences (esthetics, ethics, and logic), and (3) metaphysics; his views on them are discussed in order below.

On May 14, 1867, the 27-year-old Peirce presented a paper entitled "On a New List of Categories" to the American Academy of Arts and Sciences, which published it the following year. The paper outlined a theory of predication, involving three universal categories that Peirce developed in response to reading Aristotle, Immanuel Kant, and G. W. F. Hegel, categories that Peirce applied throughout his work for the rest of his life. Peirce scholars generally regard the "New List" as foundational or breaking the ground for Peirce's "architectonic", his blueprint for a pragmatic philosophy. In the categories one will discern, concentrated, the pattern that one finds formed by the three grades of clearness in "" (1878 paper foundational to pragmatism), and in numerous other trichotomies in his work.

"On a New List of Categories" is cast as a Kantian deduction; it is short but dense and difficult to summarize. The following table is compiled from that and later works. In 1893, Peirce restated most of it for a less advanced audience. 

Peirce did not write extensively in aesthetics and ethics, but came by 1902 to hold that aesthetics, ethics, and logic, in that order, comprise the normative sciences. He characterized aesthetics as the study of the good (grasped as the admirable), and thus of the ends governing all conduct and thought.

Peirce regarded logic "per se" as a division of philosophy, as a normative science based on esthetics and ethics, as more basic than metaphysics, and as "the art of devising methods of research". More generally, as inference, "logic is rooted in the social principle", since inference depends on a standpoint that, in a sense, is unlimited. Peirce called (with no sense of deprecation) "mathematics of logic" much of the kind of thing which, in current research and applications, is called simply "logic". He was productive in both (philosophical) logic and logic's mathematics, which were connected deeply in his work and thought.

Peirce argued that logic is formal semiotic, the formal study of signs in the broadest sense, not only signs that are artificial, linguistic, or symbolic, but also signs that are semblances or are indexical such as reactions. Peirce held that "all this universe is perfused with signs, if it is not composed exclusively of signs", along with their representational and inferential relations. He argued that, since all thought takes time, all thought is in signs and sign processes ("semiosis") such as the inquiry process. He divided logic into: (1) speculative grammar, or stechiology, on how signs can be meaningful and, in relation to that, what kinds of signs there are, how they combine, and how some embody or incorporate others; (2) logical critic, or logic proper, on the modes of inference; and (3) speculative or universal rhetoric, or methodeutic, the philosophical theory of inquiry, including pragmatism.

In his "F.R.L." [First Rule of Logic] (1899), Peirce states that the first, and "in one sense, the sole", rule of reason is that, "to learn, one needs to desire to learn" and desire it without resting satisfied with that which one is inclined to think. So, the first rule is, "to wonder". Peirce proceeds to a critical theme in research practices and the shaping of theories:
Do not block the way of inquiry.
Peirce adds, that method and economy are best in research but no outright sin inheres in trying any theory in the sense that the investigation via its trial adoption can proceed unimpeded and undiscouraged, and that "the one unpardonable offence" is a philosophical barricade against truth's advance, an offense to which "metaphysicians in all ages have shown themselves the most addicted". Peirce in many writings holds that logic precedes metaphysics (ontological, religious, and physical).

Peirce goes on to list four common barriers to inquiry: (1) Assertion of absolute certainty; (2) maintaining that something is absolutely unknowable; (3) maintaining that something is absolutely inexplicable because absolutely basic or ultimate; (4) holding that perfect exactitude is possible, especially such as to quite preclude unusual and anomalous phenomena. To refuse absolute theoretical certainty is the heart of "fallibilism", which Peirce unfolds into refusals to set up any of the listed barriers. Peirce elsewhere argues (1897) that logic's presupposition of fallibilism leads at length to the view that chance and continuity are very real (tychism and synechism).

The First Rule of Logic pertains to the mind's presuppositions in undertaking reason and logic, presuppositions, for instance, that truth and the real do not depend on yours or my opinion of them but do depend on representational relation and consist in the destined end in investigation taken far enough (see below). He describes such ideas as, collectively, hopes which, in particular cases, one is unable seriously to doubt.

 In three articles in 1868–1869, Peirce rejected mere verbal or hyperbolic doubt and first or ultimate principles, and argued that we have (as he numbered them):
(The above sense of the term "intuition" is almost Kant's, said Peirce. It differs from the current looser sense that encompasses instinctive or anyway half-conscious inference.)

Peirce argued that those incapacities imply the reality of the general and of the continuous, the validity of the modes of reasoning, and the falsity of philosophical Cartesianism (see below).

Peirce rejected the conception (usually ascribed to Kant) of the unknowable thing-in-itself and later said that to "dismiss make-believes" is a prerequisite for pragmatism.

Peirce sought, through his wide-ranging studies through the decades, formal philosophical ways to articulate thought's processes, and also to explain the workings of science. These inextricably entangled questions of a dynamics of inquiry rooted in nature and nurture led him to develop his semiotic with very broadened conceptions of signs and inference, and, as its culmination, a theory of inquiry for the task of saying 'how science works' and devising research methods. This would be logic by the medieval definition taught for centuries: art of arts, science of sciences, having the way to the principles of all methods. Influences radiate from points on parallel lines of inquiry in Aristotle's work, in such "loci" as: the basic terminology of psychology in "On the Soul"; the founding description of sign relations in "On Interpretation"; and the differentiation of inference into three modes that are commonly translated into English as "abduction", "deduction", and "induction", in the "Prior Analytics", as well as inference by analogy (called "paradeigma" by Aristotle), which Peirce regarded as involving the other three modes.

Peirce began writing on semiotic in the 1860s, around the time when he devised his system of three categories. He called it both "semiotic" and "semeiotic". Both are current in singular and plural. He based it on the conception of a triadic sign relation, and defined "semiosis" as "action, or influence, which is, or involves, a cooperation of "three" subjects, such as a sign, its object, and its interpretant, this tri-relative influence not being in any way resolvable into actions between pairs". As to signs in thought, Peirce emphasized the reverse: "To say, therefore, that thought cannot happen in an instant, but requires a time, is but another way of saying that every thought must be interpreted in another, or that all thought is in signs."

Peirce held that all thought is in signs, issuing in and from interpretation, where "sign" is the word for the broadest variety of conceivable semblances, diagrams, metaphors, symptoms, signals, designations, symbols, texts, even mental concepts and ideas, all as determinations of a mind or "quasi-mind", that which at least functions like a mind, as in the work of crystals or bees—the focus is on sign action in general rather than on psychology, linguistics, or social studies (fields which he also pursued).

Inquiry is a kind of inference process, a manner of thinking and semiosis. Global divisions of ways for phenomena to stand as signs, and the subsumption of inquiry and thinking within inference as a sign process, enable the study of inquiry on semiotics' three levels:


Peirce uses examples often from common experience, but defines and discusses such things as assertion and interpretation in terms of philosophical logic. In a formal vein, Peirce said:

Peirce's theory of signs is known to be one of the most complex semiotic theories due to its generalistic claim. Anything is a sign—not absolutely as itself, but instead in some relation or other. The "sign relation" is the key. It defines three roles encompassing (1) the sign, (2) the sign's subject matter, called its "object", and (3) the sign's meaning or ramification as formed into a kind of effect called its "interpretant" (a further sign, for example a translation). It is an irreducible "triadic relation", according to Peirce. The roles are distinct even when the things that fill those roles are not. The roles are but three; a sign of an object leads to one or more interpretants, and, as signs, they lead to further interpretants.

"Extension × intension = information." Two traditional approaches to sign relation, necessary though insufficient, are the way of "extension" (a sign's objects, also called breadth, denotation, or application) and the way of "intension" (the objects' characteristics, qualities, attributes referenced by the sign, also called depth, comprehension, significance, or connotation). Peirce adds a third, the way of "information", including change of information, to integrate the other two approaches into a unified whole. For example, because of the equation above, if a term's total amount of information stays the same, then the more that the term 'intends' or signifies about objects, the fewer are the objects to which the term 'extends' or applies.

"Determination." A sign depends on its object in such a way as to represent its object—the object enables and, in a sense, determines the sign. A physically causal sense of this stands out when a sign consists in an indicative reaction. The interpretant depends likewise on both the sign and the object—an object determines a sign to determine an interpretant. But this determination is not a succession of dyadic events, like a row of toppling dominoes; sign determination is triadic. For example, an interpretant does not merely represent something which represented an object; instead an interpretant represents something "as" a sign representing the object. The object (be it a quality or fact or law or even fictional) determines the sign to an interpretant through one's collateral experience with the object, in which the object is found or from which it is recalled, as when a sign consists in a chance semblance of an absent object. Peirce used the word "determine" not in a strictly deterministic sense, but in a sense of "specializes", "bestimmt", involving variable amount, like an influence. Peirce came to define representation and interpretation in terms of (triadic) determination. The object determines the sign to determine another sign—the interpretant—to be related to the object "as the sign is related to the object", hence the interpretant, fulfilling its function as sign of the object, determines a further interpretant sign. The process is logically structured to perpetuate itself, and is definitive of sign, object, and interpretant in general.

Peirce held there are exactly three basic elements in semiosis (sign action):
Some of the understanding needed by the mind depends on familiarity with the object. To know what a given sign denotes, the mind needs some experience of that sign's object, experience outside of, and collateral to, that sign or sign system. In that context Peirce speaks of collateral experience, collateral observation, collateral acquaintance, all in much the same terms.

Among Peirce's many sign typologies, three stand out, interlocked. The first typology depends on the sign itself, the second on how the sign stands for its denoted object, and the third on how the sign stands for its object to its interpretant. Also, each of the three typologies is a three-way division, a trichotomy, via Peirce's three phenomenological categories: (1) quality of feeling, (2) reaction, resistance, and (3) representation, mediation.

I. "Qualisign, sinsign, legisign" (also called" tone, token, type," and also called "potisign, actisign, famisign"): This typology classifies every sign according to the sign's own phenomenological category—the qualisign is a quality, a possibility, a "First"; the sinsign is a reaction or resistance, a singular object, an actual event or fact, a "Second"; and the legisign is a habit, a rule, a representational relation, a "Third".

II. "Icon, index, symbol": This typology, the best known one, classifies every sign according to the category of the sign's way of denoting its object—the icon (also called semblance or likeness) by a quality of its own, the index by factual connection to its object, and the symbol by a habit or rule for its interpretant.

III. "Rheme, dicisign, argument" (also called "sumisign, dicisign, suadisign," also "seme, pheme, delome," and regarded as very broadened versions of the traditional "term, proposition, argument"): This typology classifies every sign according to the category which the interpretant attributes to the sign's way of denoting its object—the rheme, for example a term, is a sign interpreted to represent its object in respect of quality; the dicisign, for example a proposition, is a sign interpreted to represent its object in respect of fact; and the argument is a sign interpreted to represent its object in respect of habit or law. This is the culminating typology of the three, where the sign is understood as a structural element of inference.

Every sign belongs to one class or another within (I) "and" within (II) "and" within (III). Thus each of the three typologies is a three-valued parameter for every sign. The three parameters are not independent of each other; many co-classifications are absent, for reasons pertaining to the lack of either habit-taking or singular reaction in a quality, and the lack of habit-taking in a singular reaction. The result is not 27 but instead ten classes of signs fully specified at this level of analysis.

Borrowing a brace of concepts from Aristotle, Peirce examined three basic modes of inference—"abduction", "deduction", and "induction"—in his "critique of arguments" or "logic proper". Peirce also called abduction "retroduction", "presumption", and, earliest of all, "hypothesis". He characterized it as guessing and as inference to an explanatory hypothesis. He sometimes expounded the modes of inference by transformations of the categorical syllogism Barbara (AAA), for example in "Deduction, Induction, and Hypothesis" (1878). He does this by rearranging the "rule" (Barbara's major premise), the "case" (Barbara's minor premise), and the "result" (Barbara's conclusion):

Deduction.

"Rule:" All the beans from this bag are white. <br>
"Case:" These beans are beans from this bag. <br>
formula_1 "Result:" These beans are white.

Induction.

"Case:" These beans are [randomly selected] from this bag. <br>
"Result:" These beans are white. <br>
formula_1 "Rule:" All the beans from this bag are white.

Hypothesis (Abduction).

"Rule:" All the beans from this bag are white. <br>
"Result:" These beans [oddly] are white. <br>
formula_1 "Case:" These beans are from this bag.
Peirce 1883 in "A Theory of Probable Inference" ("Studies in Logic") equated hypothetical inference with the induction of characters of objects (as he had done in effect before). Eventually dissatisfied, by 1900 he distinguished them once and for all and also wrote that he now took the syllogistic forms and the doctrine of logical extension and comprehension as being less basic than he had thought. In 1903 he presented the following logical form for abductive inference:

The logical form does not also cover induction, since induction neither depends on surprise nor proposes a new idea for its conclusion. Induction seeks facts to test a hypothesis; abduction seeks a hypothesis to account for facts. "Deduction proves that something "must" be; Induction shows that something "actually is" operative; Abduction merely suggests that something "may be"." Peirce did not remain quite convinced that one logical form covers all abduction. In his methodeutic or theory of inquiry (see below), he portrayed abduction as an economic initiative to further inference and study, and portrayed all three modes as clarified by their coordination in essential roles in inquiry: hypothetical explanation, deductive prediction, inductive testing.

 Peirce's recipe for pragmatic thinking, which he called "pragmatism" and, later, "pragmaticism", is recapitulated in several versions of the so-called "pragmatic maxim". Here is one of his more emphatic reiterations of it:

As a movement, pragmatism began in the early 1870s in discussions among Peirce, William James, and others in the Metaphysical Club. James among others regarded some articles by Peirce such as "" (1877) and especially "" (1878) as foundational to pragmatism. Peirce (CP 5.11–12), like James ("", 1907), saw pragmatism as embodying familiar attitudes, in philosophy and elsewhere, elaborated into a new deliberate method for fruitful thinking about problems. Peirce differed from James and the early John Dewey, in some of their tangential enthusiasms, in being decidedly more rationalistic and realistic, in several senses of those terms, throughout the preponderance of his own philosophical moods.

In 1905 Peirce coined the new name pragmaticism "for the precise purpose of expressing the original definition", saying that "all went happily" with James's and F.C.S. Schiller's variant uses of the old name "pragmatism" and that he coined the new name because of the old name's growing use in "literary journals, where it gets abused". Yet he cited as causes, in a 1906 manuscript, his differences with James and Schiller and, in a 1908 publication, his differences with James as well as literary author Giovanni Papini's declaration of pragmatism's indefinability. Peirce in any case regarded his views that truth is immutable and infinity is real, as being opposed by the other pragmatists, but he remained allied with them on other issues.

Pragmatism begins with the idea that belief is that on which one is prepared to act. Peirce's pragmatism is a method of clarification of conceptions of objects. It equates any conception of an object to a conception of that object's effects to a general extent of the effects' conceivable implications for informed practice. It is a method of sorting out conceptual confusions occasioned, for example, by distinctions that make (sometimes needed) formal yet not practical differences. He formulated both pragmatism and statistical principles as aspects of scientific logic, in his "Illustrations of the Logic of Science" series of articles. In the second one, "", Peirce discussed three grades of clearness of conception:

By way of example of how to clarify conceptions, he addressed conceptions about truth and the real as questions of the presuppositions of reasoning in general. In clearness's second grade (the "nominal" grade), he defined truth as a sign's correspondence to its object, and the real as the object of such correspondence, such that truth and the real are independent of that which you or I or any actual, definite community of inquirers think. After that needful but confined step, next in clearness's third grade (the pragmatic, practice-oriented grade) he defined truth as that opinion which "would" be reached, sooner or later but still inevitably, by research taken far enough, such that the real does depend on that ideal final opinion—a dependence to which he appeals in theoretical arguments elsewhere, for instance for the long-run validity of the rule of induction. Peirce argued that even to argue against the independence and discoverability of truth and the real is to presuppose that there is, about that very question under argument, a truth with just such independence and discoverability.

Peirce said that a conception's meaning consists in "all general modes of rational conduct" implied by "acceptance" of the conception—that is, if one were to accept, first of all, the conception as true, then what could one conceive to be consequent general modes of rational conduct by all who accept the conception as true?—the whole of such consequent general modes is the whole meaning. His pragmatism does not equate a conception's meaning, its intellectual purport, with the conceived benefit or cost of the conception itself, like a meme (or, say, propaganda), outside the perspective of its being true, nor, since a conception is general, is its meaning equated with any definite set of actual consequences or upshots corroborating or undermining the conception or its worth. His pragmatism also bears no resemblance to "vulgar" pragmatism, which misleadingly connotes a ruthless and Machiavellian search for mercenary or political advantage. Instead the pragmatic maxim is the heart of his pragmatism as a method of experimentational mental reflection arriving at conceptions in terms of conceivable confirmatory and disconfirmatory circumstances—a method hospitable to the formation of explanatory hypotheses, and conducive to the use and improvement of verification.

Peirce's pragmatism, as method and theory of definitions and conceptual clearness, is part of his theory of inquiry, which he variously called speculative, general, formal or universal rhetoric or simply methodeutic. He applied his pragmatism as a method throughout his work.

Critical common-sensism, treated by Peirce as a consequence of his pragmatism, is his combination of Thomas Reid's common-sense philosophy with a fallibilism that recognizes that propositions of our more or less vague common sense now indubitable may later come into question, for example because of transformations of our world through science. It includes efforts to work up in tests genuine doubts for a core group of common indubitables that vary slowly if at all.

In "" (1877), Peirce described inquiry in general not as the pursuit of truth "per se" but as the struggle to move from irritating, inhibitory doubt born of surprise, disagreement, and the like, and to reach a secure belief, belief being that on which one is prepared to act. That let Peirce frame scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal, quarrelsome, or hyperbolic doubt, which he held to be fruitless. Peirce sketched four methods of settling opinion, ordered from least to most successful:

Peirce held that, in practical affairs, slow and stumbling ratiocination is often dangerously inferior to instinct and traditional sentiment, and that the scientific method is best suited to theoretical research, which in turn should not be trammeled by the other methods and practical ends; reason's "first rule" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry. Scientific method excels over the others finally by being deliberately designed to arrive—eventually—at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth "per se" but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential conduct correctly to its given goal, and wed themselves to the scientific method.

Insofar as clarification by pragmatic reflection suits explanatory hypotheses and fosters predictions and testing, pragmatism points beyond the usual duo of foundational alternatives: deduction from self-evident truths, or "rationalism"; and induction from experiential phenomena, or "empiricism".

Based on his critique of three modes of argument and different from either foundationalism or coherentism, Peirce's approach seeks to justify claims by a three-phase dynamic of inquiry:


Thereby, Peirce devised an approach to inquiry far more solid than the flatter image of inductive generalization "simpliciter", which is a mere re-labeling of phenomenological patterns. Peirce's pragmatism was the first time the scientific method was proposed as an epistemology for philosophical questions.

A theory that succeeds better than its rivals in predicting and controlling our world is said to be nearer the truth. This is an operational notion of truth used by scientists.

Peirce extracted the pragmatic model or theory of inquiry from its raw materials in classical logic and refined it in parallel with the early development of symbolic logic to address problems about the nature of scientific reasoning.

Abduction, deduction, and induction make incomplete sense in isolation from one another but comprise a cycle understandable as a whole insofar as they collaborate toward the common end of inquiry. In the pragmatic way of thinking about conceivable practical implications, every thing has a purpose, and, as possible, its purpose should first be denoted. Abduction hypothesizes an explanation for deduction to clarify into implications to be tested so that induction can evaluate the hypothesis, in the struggle to move from troublesome uncertainty to more secure belief. No matter how traditional and needful it is to study the modes of inference in abstraction from one another, the integrity of inquiry strongly limits the effective modularity of its principal components.

Peirce's outline of the scientific method in §III–IV of "A Neglected Argument" is summarized below (except as otherwise noted). There he also reviewed plausibility and inductive precision (issues of critique of arguments).

1. Abductive (or retroductive) phase. Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, on the basis of tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea so as to account in a simple, economical way for a surprising or complicated phenomenon. The modicum of success in our guesses far exceeds that of random luck, and seems born of attunement to nature by developed or inherent instincts, especially insofar as best guesses are optimally plausible and simple in the sense of the "facile and natural", as by Galileo's natural light of reason and as distinct from "logical simplicity". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and it has no substitute in expediting us toward new truths. In 1903, Peirce called pragmatism "the logic of abduction". Coordinative method leads from abducting a plausible hypothesis to judging it for its testability and for how its trial would economize inquiry itself. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if not costly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be selected for trial strategically, for their caution (for which Peirce gave as example the game of Twenty Questions), breadth, or incomplexity. One can discover only that which would be revealed through their sufficient experience anyway, and so the point is to expedite it; economy of research demands the leap, so to speak, of abduction and governs its art.

2. Deductive phase. Two stages:

3. Inductive phase. Evaluation of the hypothesis, inferring from observational or experimental tests of its deduced consequences. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning in general) that the real "is only the object of the final opinion to which sufficient investigation would lead"; in other words, anything excluding such a process would never be real. Induction involving the ongoing accumulation of evidence follows "a method which, sufficiently persisted in", will "diminish the error below any predesignate degree". Three stages:

Peirce drew on the methodological implications of the four incapacities—no genuine introspection, no intuition in the sense of non-inferential cognition, no thought but in signs, and no conception of the absolutely incognizable—to attack philosophical Cartesianism, of which he said that:

1. "It teaches that philosophy must begin in universal doubt" – when, instead, we start with preconceptions, "prejudices [...] which it does not occur to us "can" be questioned", though we may find reason to question them later. "Let us not pretend to doubt in philosophy what we do not doubt in our hearts."

2. "It teaches that the ultimate test of certainty is...in the individual consciousness" – when, instead, in science a theory stays on probation till agreement is reached, then it has no actual doubters left. No lone individual can reasonably hope to fulfill philosophy's multi-generational dream. When "candid and disciplined minds" continue to disagree on a theoretical issue, even the theory's author should feel doubts about it.

3. It trusts to "a single thread of inference depending often upon inconspicuous premisses" – when, instead, philosophy should, "like the successful sciences", proceed only from tangible, scrutinizable premisses and trust not to any one argument but instead to "the multitude and variety of its arguments" as forming, not a chain at least as weak as its weakest link, but "a cable whose fibers", soever "slender, are sufficiently numerous and intimately connected".

4. It renders many facts "absolutely inexplicable, unless to say that 'God makes them so' is to be regarded as an explanation" – when, instead, philosophy should avoid being "unidealistic", misbelieving that something real can defy or evade all possible ideas, and supposing, inevitably, "some absolutely inexplicable, unanalyzable ultimate", which explanatory surmise explains nothing and so is inadmissible.

Peirce divided metaphysics into (1) ontology or general metaphysics, (2) psychical or religious metaphysics, and (3) physical metaphysics.

Ontology. Peirce was a Scholastic Realist, declaring for the reality of generals as early as 1868. Regarding modalities (possibility, necessity, etc.), he came in later years to regard himself as having wavered earlier as to just how positively real the modalities are. In his 1897 "The Logic of Relatives" he wrote: Peirce retained, as useful for some purposes, the definitions in terms of information states, but insisted that the pragmaticist is committed to a strong modal realism by conceiving of objects in terms of predictive general conditional propositions about how they "would" behave under certain circumstances.

Psychical or religious metaphysics. Peirce believed in God, and characterized such belief as founded in an instinct explorable in musing over the worlds of ideas, brute facts, and evolving habits—and it is a belief in God not as an "actual" or "existent" being (in Peirce's sense of those words), but all the same as a "real" being. In "" (1908), Peirce sketches, for God's reality, an argument to a hypothesis of God as the Necessary Being, a hypothesis which he describes in terms of how it would tend to develop and become compelling in musement and inquiry by a normal person who is led, by the hypothesis, to consider as being purposed the features of the worlds of ideas, brute facts, and evolving habits (for example scientific progress), such that the thought of such purposefulness will "stand or fall with the hypothesis"; meanwhile, according to Peirce, the hypothesis, in supposing an "infinitely incomprehensible" being, starts off at odds with its own nature as a purportively true conception, and so, no matter how much the hypothesis grows, it both (A) inevitably regards itself as partly true, partly vague, and as continuing to define itself without limit, and (B) inevitably has God appearing likewise vague but growing, though God as the Necessary Being is not vague or growing; but the hypothesis will hold it to be "more" false to say the opposite, that God is purposeless. Peirce also argued that the will is free and (see Synechism) that there is at least an attenuated kind of immortality.

Physical metaphysics. Peirce held the view, which he called objective idealism, that "matter is effete mind, inveterate habits becoming physical laws". Peirce asserted the reality of (1) absolute chance (his tychist view), (2) mechanical necessity (anancist view), and (3) that which he called the law of love (agapist view), echoing his categories Firstness, Secondness, and Thirdness, respectively. He held that fortuitous variation (which he also called "sporting"), mechanical necessity, and creative love are the three modes of evolution (modes called "tychasm", "anancasm", and "agapasm") of the cosmos and its parts. He found his conception of agapasm embodied in Lamarckian evolution; the overall idea in any case is that of evolution tending toward an end or goal, and it could also be the evolution of a mind or a society; it is the kind of evolution which manifests workings of mind in some general sense. He said that overall he was a synechist, holding with reality of continuity, especially of space, time, and law.

Peirce outlined two fields, "Cenoscopy" and "Science of Review", both of which he called philosophy. Both included philosophy about science. In 1903 he arranged them, from more to less theoretically basic, thus:


Peirce placed, within Science of Review, the work and theory of classifying the sciences (including mathematics and philosophy). His classifications, on which he worked for many years, draw on argument and wide knowledge, and are of interest both as a map for navigating his philosophy and as an accomplished polymath's survey of research in his time.

Contemporaries associated with Peirce



</doc>
<doc id="6118" url="https://en.wikipedia.org/wiki?curid=6118" title="Carnot heat engine">
Carnot heat engine

A Carnot heat engine is a theoretical engine that operates on the reversible Carnot cycle. The basic model for this engine was developed by Nicolas Léonard Sadi Carnot in 1824. The Carnot engine model was graphically expanded upon by Benoît Paul Émile Clapeyron in 1834 and mathematically explored by Rudolf Clausius in 1857 from which the concept of entropy emerged.

Every thermodynamic system exists in a particular state. A thermodynamic cycle occurs when a system is taken through a series of different states, and finally returned to its initial state. In the process of going through this cycle, the system may perform work on its surroundings, thereby acting as a heat engine.

A heat engine acts by transferring energy from a warm region to a cool region of space and, in the process, converting some of that energy to mechanical work. The cycle may also be reversed. The system may be worked upon by an external force, and in the process, it can transfer thermal energy from a cooler system to a warmer one, thereby acting as a refrigerator or heat pump rather than a heat engine.

In the adjacent diagram, from Carnot's 1824 work, "Reflections on the Motive Power of Fire", there are "two bodies "A" and "B", kept each at a constant temperature, that of "A" being higher than that of "B". These two bodies to which we can give, or from which we can remove the heat without causing their temperatures to vary, exercise the functions of two unlimited reservoirs of caloric. We will call the first the furnace and the second the refrigerator.” Carnot then explains how we can obtain motive power, i.e., “work”, by carrying a certain quantity of heat from body "A" to body "B".
It also acts as a cooler and hence can also act as a Refrigerator.

The previous image shows the original piston-and-cylinder diagram used by Carnot in discussing his ideal engines. The figure at right shows a block diagram of a generic heat engine, such as the Carnot engine. In the diagram, the “working body” (system), a term introduced by Clausius in 1850, can be any fluid or vapor body through which heat "Q" can be introduced or transmitted to produce work. Carnot had postulated that the fluid body could be any substance capable of expansion, such as vapor of water, vapor of alcohol, vapor of mercury, a permanent gas, or air, etc. Although, in these early years, engines came in a number of configurations, typically "Q" was supplied by a boiler, wherein water was boiled over a furnace; "Q" was typically supplied by a stream of cold flowing water in the form of a condenser located on a separate part of the engine. The output work, "W", represents the movement of the piston as it is used to turn a crank-arm, which in turn was typically used to power a pulley so as to lift water out of flooded salt mines. Carnot defined work as “weight lifted through a height”.

The Carnot cycle when acting as a heat engine consists of the following steps:


Carnot's theorem is a formal statement of this fact: "No engine operating between two heat reservoirs can be more efficient than a Carnot engine operating between the same reservoirs."

Explanation 
This maximum efficiency formula_3 is defined as above:

A corollary to Carnot's theorem states that: All reversible engines operating between the same heat reservoirs are equally efficient.

It is easily shown that the efficiency is maximum when the entire cyclic process is a reversible process. This means the total entropy of the net system (the entropies of the hot furnace, the "working fluid" of the Heat engine, and the cold sink) remains constant when the "working fluid" completes one cycle and returns to its original state. (In the general case, the total entropy of this combined system would increase in a general irreversible process).

Since the "working fluid" comes back to the same state after one cycle, and entropy of the system is a state function; the change in entropy of the "working fluid" system is 0. Thus, it implies that the total entropy change of the furnace and sink is zero, for the process to be reversible and the efficiency of the engine to be maximum. This derivation is carried out in the next section.

The coefficient of performance (COP) of the heat engine is the reciprocal of its efficiency.

For a real heat engine, the total thermodynamic process is generally irreversible. The working fluid is brought back to its initial state after one cycle, and thus the change of entropy of the fluid system is 0, but the sum of the entropy changes in the hot and cold reservoir in this one cyclical process is greater than 0.

The internal energy of the fluid is also a state variable, so its total change in one cycle is 0. So the total work done by the system , is equal to the heat put into the system formula_4 minus the heat taken out formula_8.

For real engines, sections 1 and 3 of the Carnot Cycle; in which heat is absorbed by the "working fluid" from the hot reservoir, and released by it to the cold reservoir, respectively; no longer remain ideally reversible, and there is a temperature differential between the temperature of the reservoir and the temperature of the fluid while heat exchange takes place.

During heat transfer from the hot reservoir at formula_9 to the fluid, the fluid would have a slightly lower temperature than formula_9, and the process for the fluid may not necessarily remain isothermal. 
Let formula_11 be the total entropy change of the fluid in the process of intake of heat. 

where the temperature of the fluid is always slightly lesser than formula_9, in this process.

So, one would get 

Similarly, at the time of heat injection from the fluid to the cold reservoir one would have, for the magnitude of total entropy change formula_13 of the fluid in the process of expelling heat:

where, during this process of transfer of heat to the cold reservoir, the temperature of the fluid is always slightly greater than formula_14.

We have only considered the magnitude of the entropy change here. Since the total change of entropy of the fluid system for the cyclic process is 0, we must have

The previous three equations combine to give:

Equations () and () combine to give 

Hence,

where formula_15 is the efficiency of the real engine, and formula_3 is the efficiency of the Carnot engine working between the same two reservoirs at the temperatures formula_9 and formula_14. For the Carnot engine, the entire process is 'reversible', and Equation () is an equality.

Hence, the efficiency of the real engine is always less than the ideal Carnot engine.

Equation () signifies that the total entropy of the total system(the two reservoirs + fluid) increases for the real engine, because the entropy gain of the cold reservoir as formula_19 flows into it at the fixed temperature formula_14, is greater than the entropy loss of the hot reservoir as formula_21 leaves it at its fixed temperature formula_9. The inequality in Equation () is essentially the statement of the Clausius theorem.

According to the second theorem, "The efficiency of the Carnot engine is independent of the nature of the working substance".



</doc>
<doc id="6119" url="https://en.wikipedia.org/wiki?curid=6119" title="Context-sensitive">
Context-sensitive

Context-sensitive is an adjective meaning "depending on context" or "depending on circumstances". It may refer to:



</doc>
<doc id="6121" url="https://en.wikipedia.org/wiki?curid=6121" title="Central America">
Central America

Central America (, , "Centroamérica" ) is a region found in the southern tip of North America and is sometimes defined as a subcontinent of the Americas. This region is bordered by Mexico to the north, Colombia to the southeast, the Caribbean Sea to the east and the Pacific Ocean to the west and south. Central America consists of seven countries: Belize, Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua and Panama. The combined population of Central America is estimated to be between 41,739,000 (2009 estimate) and 42,688,190 (2012 estimate).

Central America is a part of the Mesoamerican biodiversity hotspot, which extends from northern Guatemala to central Panama. Due to the presence of several active geologic faults and the Central America Volcanic Arc, there is a great deal of seismic activity in the region, such as volcanic eruptions and earthquakes, which has resulted in death, injury and property damage.

In the Pre-Columbian era, Central America was inhabited by the indigenous peoples of Mesoamerica to the north and west and the Isthmo-Colombian peoples to the south and east. Following the Spanish expedition of Christopher Columbus' voyages to the Americas, Spain began to colonize the Americas. From 1609 to 1821, the majority of Central American territories (except for what would become Belize and Panama) were governed by the viceroyalty of New Spain from Mexico City as the Captaincy General of Guatemala. On 24 August 1821, Spanish Viceroy Juan de O’Donojú signed the Treaty of Córdoba, which established New Spain’s independence from Spain. On 15 September 1821, the Act of Independence of Central America was enacted to announce Central America’s separation from the Spanish Empire and provide for the establishment of a new Central American state. Some of New Spain’s provinces in the Central American region (i.e. what would become Guatemala, Honduras, El Salvador, Nicaragua and Costa Rica) were annexed to the First Mexican Empire; however, in 1823 they seceded from Mexico to form the Federal Republic of Central America until 1838. 

In 1838, Nicaragua, Honduras, Costa Rica and Guatemala became the first of Central America’s seven states to become independent autonomous countries, followed by El Salvador in 1841, Panama in 1903 and Belize in 1981. Despite the dissolution of the Federal Republic of Central America, there is anecdotal evidence that demonstrates that Nicaraguans, Hondurans, Costa Ricans, Guatemalans, Salvadorans, Panamanians and Belizeans continue to maintain a Central American identity. For instance, Central Americans sometimes refer to their nations as if they were provinces of a Central American state. It is not unusual to write "C.A." after the country’s name in formal and informal contexts. Governments in the region sometimes reinforce this sense of belonging to Central America in its citizens. For example, automobile licence plates in many of the region’s countries include the moniker, "Centroamerica," alongside the country’s name.

"Central America" may mean different things to various people, based upon different contexts:

In the Pre-Columbian era, the northern areas of Central America were inhabited by the indigenous peoples of Mesoamerica. Most notable among these were the Mayans, who had built numerous cities throughout the region, and the Aztecs, who had created a vast empire. The pre-Columbian cultures of eastern El Salvador, eastern Honduras, Caribbean Nicaragua, most of Costa Rica and Panama were predominantly speakers of the Chibchan languages at the time of European contact and are considered by some culturally different and grouped in the Isthmo-Colombian Area.

Following the Spanish expedition of Christopher Columbus's voyages to the Americas, the Spanish sent many expeditions to the region, and they began their conquest of Maya territory in 1523. Soon after the conquest of the Aztec Empire, Spanish conquistador Pedro de Alvarado commenced the conquest of northern Central America for the Spanish Empire. Beginning with his arrival in Soconusco in 1523, Alvarado's forces systematically conquered and subjugated most of the major Maya kingdoms, including the K'iche', Tz'utujil, Pipil, and the Kaqchikel. By 1528, the conquest of Guatemala was nearly complete, with only the Petén Basin remaining outside the Spanish sphere of influence. The last independent Maya kingdoms – the Kowoj and the Itza people – were finally defeated in 1697, as part of the Spanish conquest of Petén.

In 1538, Spain established the Real Audiencia of Panama, which had jurisdiction over all land from the Strait of Magellan to the Gulf of Fonseca. This entity was dissolved in 1543, and most of the territory within Central America then fell under the jurisdiction of the "Audiencia Real de Guatemala". This area included the current territories of Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, and the Mexican state of Chiapas, but excluded the lands that would become Belize and Panama. The president of the Audiencia, which had its seat in Antigua Guatemala, was the governor of the entire area. In 1609 the area became a captaincy general and the governor was also granted the title of captain general. The Captaincy General of Guatemala encompassed most of Central America, with the exception of present-day Belize and Panama.

The Captaincy General of Guatemala lasted for more than two centuries, but began to fray after a rebellion in 1811 which began in the intendancy of San Salvador. The Captaincy General formally ended on 15 September 1821, with the signing of the Act of Independence of Central America. Mexican independence was achieved at virtually the same time with the signing of the Treaty of Córdoba and the Declaration of Independence of the Mexican Empire, and the entire region was finally independent from Spanish authority by 28 September 1821.

From its independence from Spain in 1821 until 1823, the former Captaincy General remained intact as part of the short-lived First Mexican Empire. When the Emperor of Mexico abdicated on 19 March 1823, Central America again became independent. On 1 July 1823, the Congress of Central America peacefully seceded from Mexico and declared absolute independence from all foreign nations, and the region formed the Federal Republic of Central America.

The Federal Republic of Central America was a representative democracy with its capital at Guatemala City. This union consisted of the provinces of Costa Rica, El Salvador, Guatemala, Honduras, Los Altos, Mosquito Coast, and Nicaragua. The lowlands of southwest Chiapas, including Soconusco, initially belonged to the Republic until 1824, when Mexico annexed most of Chiapas and began its claims to Soconusco. The Republic lasted from 1823 to 1838, when it disintegrated as a result of civil wars.

The territory that now makes up Belize was heavily contested in a dispute that continued for decades after Guatemala achieved independence (see History of Belize (1506–1862). Spain, and later Guatemala, considered this land a Guatemalan department. In 1862, Britain formally declared it a British colony and named it British Honduras. It became independent as Belize in 1981.

Panama, situated in the southernmost part of Central America on the Isthmus of Panama, has for most of its history been culturally linked to South America. Panama was part of the Province of Tierra Firme from 1510 until 1538 when it came under the jurisdiction of the newly formed "Audiencia Real de Panama". Beginning in 1543, Panama was administered as part of the Viceroyalty of Peru, along with all other Spanish possessions in South America. Panama remained as part of the Viceroyalty of Peru until 1739, when it was transferred to the Viceroyalty of New Granada, the capital of which was located at Santa Fé de Bogotá. Panama remained as part of the Viceroyalty of New Granada until the disestablishment of that viceroyalty in 1819. A series of military and political struggles took place from that time until 1822, the result of which produced the republic of Gran Colombia. After the dissolution of Gran Colombia in 1830, Panama became part of a successor state, the Republic of New Granada. From 1855 until 1886, Panama existed as Panama State, first within the Republic of New Granada, then within the Granadine Confederation, and finally within the United States of Colombia. The United States of Colombia was replaced by the Republic of Colombia in 1886. As part of the Republic of Colombia, Panama State was abolished and it became the Isthmus Department. Despite the many political reorganizations, Colombia was still deeply plagued by conflict, which eventually led to the secession of Panama on 3 November 1903. Only after that time did some begin to regard Panama as a North or Central American entity.

By the 1930s the United Fruit Company owned 3.5 million acres of land in Central America and the Caribbean and was the single largest land owner in Guatemala. Such holdings gave it great power over the governments of small countries. That was one of the factors that led to the coining of the phrase Banana republic.

After more than two hundred years of social unrest, violent conflict and revolution, Central America today remains in a period of political transformation. Poverty, social injustice and violence are still widespread. Nicaragua is the second poorest country in the western hemisphere (only Haiti is poorer).

Central America is the tapering isthmus of southern North America, with unique and varied geographic features. The Pacific Ocean lies to the southwest, the Caribbean Sea lies to the northeast, and the Gulf of Mexico lies to the north. Some physiographists define the Isthmus of Tehuantepec as the northern geographic border of Central America, while others use the northwestern borders of Belize and Guatemala. From there, the Central American land mass extends southeastward to the Atrato River, where it connects to the Pacific Lowlands in northwestern South America.

Of the many mountain ranges within Central America, the longest are the Sierra Madre de Chiapas, the Cordillera Isabelia and the Cordillera de Talamanca. At , Volcán Tajumulco is the highest peak in Central America. Other high points of Central America are as listed in the table below:
High points in Central America

Between the mountain ranges lie fertile valleys that are suitable for the raising of livestock and for the production of coffee, tobacco, beans and other crops. Most of the population of Honduras, Costa Rica and Guatemala lives in valleys.

Trade winds have a significant effect upon the climate of Central America. Temperatures in Central America are highest just prior to the summer wet season, and are lowest during the winter dry season, when trade winds contribute to a cooler climate. The highest temperatures occur in April, due to higher levels of sunlight, lower cloud cover and a decrease in trade winds.

Central America is part of the Mesoamerican biodiversity hotspot, boasting 7% of the world's biodiversity. The Pacific Flyway is a major north-south flyway for migratory birds in the Americas, extending from Alaska to Tierra del Fuego. Due to the funnel-like shape of its land mass, migratory birds can be seen in very high concentrations in Central America, especially in the spring and autumn. As a bridge between North America and South America, Central America has many species from the Nearctic and the Neotropic ecozones. However the southern countries (Costa Rica and Panama) of the region have more biodiversity than the northern countries (Guatemala and Belize), meanwhile the central countries (Honduras, Nicaragua and El Salvador) have the least biodiversity. The table below shows recent statistics:
Biodiversity in Central America (number of different species of terrestrial vertebrate animals and vascular plants)

Over 300 species of the region's flora and fauna are threatened, 107 of which are classified as critically endangered. The underlying problems are deforestation, which is estimated by FAO at 1.2% per year in Central America and Mexico combined, fragmentation of rainforests and the fact that 80% of the vegetation in Central America has already been converted to agriculture.

Efforts to protect fauna and flora in the region are made by creating ecoregions and nature reserves. 36% of Belize's land territory falls under some form of official protected status, giving Belize one of the most extensive systems of terrestrial protected areas in the Americas. In addition, 13% of Belize's marine territory are also protected. A large coral reef extends from Mexico to Honduras: the Mesoamerican Barrier Reef System. The Belize Barrier Reef is part of this. The Belize Barrier Reef is home to a large diversity of plants and animals, and is one of the most diverse ecosystems of the world. It is home to 70 hard coral species, 36 soft coral species, 500 species of fish and hundreds of invertebrate species.
So far only about 10% of the species in the Belize barrier reef have been discovered.

From 2001 to 2010, of forest were lost in the region. In 2010 Belize had 63% of remaining forest cover, Costa Rica 46%, Panama 45%, Honduras 41%, Guatemala 37%, Nicaragua 29%, and El Salvador 21%. Most of the loss occurred in the moist forest biome, with . Woody vegetation loss was partially set off by a gain in the coniferous forest biome with , and a gain in the dry forest biome at . Mangroves and deserts contributed only 1% to the loss in forest vegetation. The bulk of the deforestation was located at the Caribbean slopes of Nicaragua with a loss of of forest in the period from 2001 to 2010. The most significant regrowth of of forest was seen in the coniferous woody vegetation of Honduras.

The Central American pine-oak forests ecoregion, in the tropical and subtropical coniferous forests biome, is found in Central America and southern Mexico. The Central American pine-oak forests occupy an area of , extending along the mountainous spine of Central America, extending from the Sierra Madre de Chiapas in Mexico's Chiapas state through the highlands of Guatemala, El Salvador, and Honduras to central Nicaragua. The pine-oak forests lie between elevation, and are surrounded at lower elevations by tropical moist forests and tropical dry forests. Higher elevations above are usually covered with Central American montane forests. The Central American pine-oak forests are composed of many species characteristic of temperate North America including oak, pine, fir, and cypress.

Laurel forest is the most common type of Central American temperate evergreen cloud forest, found in almost all Central American countries, normally more than above sea level. Tree species include evergreen oaks, members of the laurel family, and species of "Weinmannia", "Drimys", and "Magnolia". The cloud forest of Sierra de las Minas, Guatemala, is the largest in Central America. In some areas of southeastern Honduras there are cloud forests, the largest located near the border with Nicaragua. In Nicaragua, cloud forests are situated near the border with Honduras, but many were cleared to grow coffee. There are still some temperate evergreen hills in the north. The only cloud forest in the Pacific coastal zone of Central America is on the Mombacho volcano in Nicaragua. In Costa Rica, there are laurel forests in the Cordillera de Tilarán and Volcán Arenal, called Monteverde, also in the Cordillera de Talamanca.

The Central American montane forests are an ecoregion of the tropical and subtropical moist broadleaf forests biome, as defined by the World Wildlife Fund. These forests are of the moist deciduous and the semi-evergreen seasonal subtype of tropical and subtropical moist broadleaf forests and receive high overall rainfall with a warm summer wet season and a cooler winter dry season. Central American montane forests consist of forest patches located at altitudes ranging from , on the summits and slopes of the highest mountains in Central America ranging from Southern Mexico, through Guatemala, El Salvador, and Honduras, to northern Nicaragua. The entire ecoregion covers an area of and has a temperate climate with relatively high precipitation levels.

Ecoregions are not only established to protect the forests themselves but also because they are habitats for an incomparably rich and often endemic fauna. Almost half of the bird population of the Talamancan montane forests in Costa Rica and Panama are endemic to this region. Several birds are listed as threatened, most notably the resplendent quetzal (Pharomacrus mocinno), three-wattled bellbird (Procnias tricarunculata), bare-necked umbrellabird (Cephalopterus glabricollis), and black guan (Chamaepetes unicolor). Many of the amphibians are endemic and depend on the existence of forest. The golden toad that once inhabited a small region in the Monteverde Reserve, which is part of the Talamancan montane forests, has not been seen alive since 1989 and is listed as extinct by IUCN. The exact causes for its extinction are unknown. Global warming may have played a role, because the development of that frog is typical for this area may have been compromised. Seven small mammals are endemic to the Costa Rica-Chiriqui highlands within the Talamancan montane forest region. Jaguars, cougars, spider monkeys, as well as tapirs, and anteaters live in the woods of Central America. The Central American red brocket is a brocket deer found in Central America's tropical forest.

Central America is geologically very active, with volcanic eruptions and earthquakes occurring frequently, and tsunamis occurring occasionally. Many thousands of people have died as a result of these natural disasters.

Most of Central America rests atop the Caribbean Plate. This tectonic plate converges with the Cocos, Nazca, and North American plates to form the Middle America Trench, a major subduction zone. The Middle America Trench is situated some off the Pacific coast of Central America and runs roughly parallel to it. Many large earthquakes have occurred as a result of seismic activity at the Middle America Trench. For example, subduction of the Cocos Plate beneath the North American Plate at the Middle America Trench is believed to have caused the 1985 Mexico City earthquake that killed as many as 40,000 people. Seismic activity at the Middle America Trench is also responsible for earthquakes in 1902, 1942, 1956, 1982, 1992, 2001, 2007, 2012, 2014, and many other earthquakes throughout Central America.

The Middle America Trench is not the only source of seismic activity in Central America. The Motagua Fault is an onshore continuation of the Cayman Trough which forms part of the tectonic boundary between the North American Plate and the Caribbean Plate. This transform fault cuts right across Guatemala and then continues offshore until it merges with the Middle America Trench along the Pacific coast of Mexico, near Acapulco. Seismic activity at the Motagua Fault has been responsible for earthquakes in 1717, 1773, 1902, 1976, 1980, and 2009.

Another onshore continuation of the Cayman Trough is the Chixoy-Polochic Fault, which runs parallel to, and roughly to the north, of the Motagua Fault. Though less active than the Motagua Fault, seismic activity at the Chixoy-Polochic Fault is still thought to be capable of producing very large earthquakes, such as the 1816 earthquake of Guatemala.

Managua, the capital of Nicaragua, was devastated by earthquakes in 1931 and 1972.

Volcanic eruptions are also common in Central America. In 1968 the Arenal Volcano, in Costa Rica, erupted killing 87 people as the 3 villages of Tabacon, Pueblo Nuevo and San Luis were buried under pyroclastic flows and debris. Fertile soils from weathered volcanic lava have made it possible to sustain dense populations in the agriculturally productive highland areas.

The population of Central America is estimated at 47,448,333 as of . With an area of , it has a population density of . Human Development Index values are from the estimates for 2017.

The official language majority in all Central American countries is Spanish, except in Belize, where the official language is English. However, the majority of Belizeans speak Spanish at home. Mayan languages constitute a language family consisting of about 26 related languages. Guatemala formally recognized 21 of these in 1996. Xinca and Garifuna are also present in Central America.

This region of the continent is very rich in terms of ethnic groups. The majority of the population is mestizo, with sizable Mayan and White populations present, including Xinca and Garifuna minorities. The immigration of Arabs, Jews, Chinese, Europeans and others brought additional groups to the area.

The predominant religion in Central America is Christianity (95.6%). Beginning with the Spanish colonization of Central America in the 16th century, Roman Catholicism became the most popular religion in the region until the first half of the 20th century. Since the 1960s, there has been an increase in other Christian groups, particularly Protestantism, as well as other religious organizations, and individuals identifying themselves as having no religion.



Central America is currently undergoing a process of political, economic and cultural transformation that started in 1907 with the creation of the Central American Court of Justice.

In 1951 the integration process continued with the signature of the San Salvador Treaty, which created the ODECA, the Organization of Central American States. However, the unity of the ODECA was limited by conflicts between several member states.

In 1991, the integration agenda was further advanced by the creation of the Central American Integration System ("Sistema para la Integración Centroamericana", or SICA). SICA provides a clear legal basis to avoid disputes between the member states. SICA membership includes the 7 nations of Central America plus the Dominican Republic, a state that is traditionally considered part of the Caribbean.

On 6 December 2008, SICA announced an agreement to pursue a common currency and common passport for the member nations. No timeline for implementation was discussed.

Central America already has several supranational institutions such as the Central American Parliament, the Central American Bank for Economic Integration and the Central American Common Market.

On 22 July 2011, President Mauricio Funes of El Salvador became the first president "pro tempore" to SICA. El Salvador also became the headquarters of SICA with the inauguration of a new building.

Until recently, all Central American countries have maintained diplomatic relations with Taiwan instead of China. President Óscar Arias of Costa Rica, however, established diplomatic relations with China in 2007, severing formal diplomatic ties with Taiwan. After breaking off relations with the Republic of China in 2017, Panama established diplomatic relations with the People's Republic of China. In August 2018, El Salvador also severed ties with Taiwan to formally start recognizing the People's Republic of China as the sole China, a move many considered lacked transparency due to its abruptness and reports of the Chinese government's desires to invest in the department of La Union while also promising to fund the ruling party's reelection campaign.

The Central American Parliament (also known as PARLACEN) is a political and parliamentary body of SICA. The parliament started around 1980, and its primary goal was to resolve conflicts in Nicaragua, Guatemala, and El Salvador. Although the group was disbanded in 1986, ideas of unity of Central Americans still remained, so a treaty was signed in 1987 to create the Central American Parliament and other political bodies. Its original members were Guatemala, El Salvador, Nicaragua and Honduras. The parliament is the political organ of Central America, and is part of SICA. New members have since then joined including Panama and the Dominican Republic.

Costa Rica is not a member State of the Central American Parliament and its adhesion remains as a very unpopular topic at all levels of the Costa Rican society due to existing strong political criticism towards the regional parliament, since it is regarded by Costa Ricans as a menace to democratic accountability and effectiveness of integration efforts. Excessively high salaries for its members, legal immunity of jurisdiction from any member State, corruption, lack of a binding nature and effectiveness of the regional parliament's decisions, high operative costs and immediate membership of Central American Presidents once they leave their office and presidential terms, are the most common reasons invoked by Costa Ricans against the Central American Parliament.
Signed in 2004, the Central American Free Trade Agreement (CAFTA) is an agreement between the United States, Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, and the Dominican Republic. The treaty is aimed at promoting free trade among its members.

Guatemala has the largest economy in the region. Its main exports are coffee, sugar, bananas, petroleum, clothing, and cardamom. Of its 10.29 billion dollar annual exports, 40.2% go to the United States, 11.1% to neighboring El Salvador, 8% to Honduras, 5.5% to Mexico, 4.7% to Nicaragua, and 4.3% to Costa Rica.

The region is particularly attractive for companies (especially clothing companies) because of its geographical proximity to the[United States], very low wages and considerable tax advantages. In addition, the decline in the prices of coffee and other export products and the structural adjustment measures promoted by the international financial institutions have partly ruined agriculture, favouring the emergence of maquiladoras. This sector accounts for 42 per cent of total exports fromEl Salvador, 55 per cent fromGuatemala, and 65 per cent from Honduras. However, its contribution to the economies of these countries is disputed; raw materials are imported, jobs are precarious and low-paid, and tax exemptions weaken public finances.

They are also criticised for the working conditions of employees: insults and physical violence, abusive dismissals (especially of pregnant workers), working hours, non-payment of overtime. According to Lucrecia Bautista, coordinator of the"maquilas" sector of the audit firm Coverco,"labour law regulations are regularly violated in maquilas and there is no political will to enforce their application. In the case of infringements, the labour inspectorate shows remarkable leniency. It is a question of not discouraging investors. "Trade unionists are subject to pressure, and sometimes to kidnapping or murder. In some cases, business leaders have used the services of the maras. Finally, black lists containing the names of trade unionists or political activists are circulating in employers' circles.

Economic growth in Central America is projected to slow slightly in 2014–15, as country-specific domestic factors offset the positive effects from stronger economic activity in the United States.

Tourism in Belize has grown considerably in more recent times, and it is now the second largest industry in the nation. Belizean Prime Minister Dean Barrow has stated his intention to use tourism to combat poverty throughout the country. The growth in tourism has positively affected the agricultural, commercial, and finance industries, as well as the construction industry. The results for Belize's tourism-driven economy have been significant, with the nation welcoming almost one million tourists in a calendar year for the first time in its history in 2012. Belize is also the only country in Central America with English as its official language, making this country a comfortable destination for English-speaking tourists.

Costa Rica is the most visited nation in Central America. Tourism in Costa Rica is one of the fastest growing economic sectors of the country, having become the largest source of foreign revenue by 1995. Since 1999, tourism has earned more foreign exchange than bananas, pineapples and coffee exports combined. The tourism boom began in 1987, with the number of visitors up from 329,000 in 1988, through 1.03 million in 1999, to a historical record of 2.43 million foreign visitors and $1.92-billion in revenue in 2013. In 2012 tourism contributed with 12.5% of the country's GDP and it was responsible for 11.7% of direct and indirect employment.

Tourism in Nicaragua has grown considerably recently, and it is now the second largest industry in the nation. Nicaraguan President Daniel Ortega has stated his intention to use tourism to combat poverty throughout the country. The growth in tourism has positively affected the agricultural, commercial, and finance industries, as well as the construction industry. The results for Nicaragua's tourism-driven economy have been significant, with the nation welcoming one million tourists in a calendar year for the first time in its history in 2010.

The Inter-American Highway is the Central American section of the Pan-American Highway, and spans between Nuevo Laredo, Mexico, and Panama City, Panama. Because of the break in the highway known as the Darién Gap, it is not possible to cross between Central America and South America in an automobile.




</doc>
<doc id="6122" url="https://en.wikipedia.org/wiki?curid=6122" title="Continuous function">
Continuous function

In mathematics, a continuous function is a function for which sufficiently small changes in the input result in arbitrarily small changes in the output. Otherwise, a function is said to be a "discontinuous" function. A continuous function with a continuous inverse function is called a homeomorphism.

Continuity of functions is one of the core concepts of topology, which is treated in full generality below. The introductory portion of this article focuses on the special case where the inputs and outputs of functions are real numbers. A stronger form of continuity is uniform continuity. In addition, this article discusses the definition for the more general case of functions between two metric spaces. In order theory, especially in domain theory, one considers a notion of continuity known as Scott continuity. Other forms of continuity do exist but they are not discussed in this article.

As an example, consider the function "h"("t"), which describes the height of a growing flower at time "t". This function is continuous. By contrast, if "M"("t") denotes the amount of money in a bank account at time "t", then the function jumps at each point in time when money is deposited or withdrawn, so the function "M"("t") is discontinuous.

A form of the epsilon–delta definition of continuity was first given by Bernard Bolzano in 1817. Augustin-Louis Cauchy defined continuity of formula_1 as follows: an infinitely small increment formula_2 of the independent variable "x" always produces an infinitely small change formula_3 of the dependent variable "y" (see e.g. "Cours d'Analyse", p. 34). Cauchy defined infinitely small quantities in terms of variable quantities, and his definition of continuity closely parallels the infinitesimal definition used today (see microcontinuity). The formal definition and the distinction between pointwise continuity and uniform continuity were first given by Bolzano in the 1830s but the work wasn't published until the 1930s. Like Bolzano, Karl Weierstrass denied continuity of a function at a point "c" unless it was defined at and on both sides of "c", but Édouard Goursat allowed the function to be defined only at and on one side of "c", and Camille Jordan allowed it even if the function was defined only at "c". All three of those nonequivalent definitions of pointwise continuity are still in use. Eduard Heine provided the first published definition of uniform continuity in 1872, but based these ideas on lectures given by Peter Gustav Lejeune Dirichlet in 1854.

A real function, that is a function from real numbers to real numbers can be represented by a graph in the Cartesian plane; such a function is continuous if, roughly speaking, the graph is a single unbroken curve whose domain is the entire real line. A more mathematically rigorous definition is given below.

A rigorous definition of continuity of real functions is usually given in a first course in calculus in terms of the idea of a limit. First, a function with variable is said to be continuous "at the point" on the real line, if the limit of , as approaches that point , is equal to the value ; and second, the "function (as a whole)" is said to be "continuous", if it is continuous at every point. A function is said to be "discontinuous" (or to have a "discontinuity") at some point when it is not continuous there. These points themselves are also addressed as "discontinuities".

There are several different definitions of continuity of a function. Sometimes a function is said to be continuous if it is continuous at every point in its domain. In this case, the function , with the domain of all real ,  any integer, is continuous. Sometimes an exception is made for boundaries of the domain. For example, the graph of the function , with the domain of all non-negative reals, has a "left-hand" endpoint. In this case only the limit from the "right" is required to equal the value of the function. Under this definition "f" is continuous at the boundary and so for all non-negative arguments. The most common and restrictive definition is that a function is continuous if it is continuous at all real numbers. In this case, the previous two examples are not continuous, but every polynomial function is continuous, as are the sine, cosine, and exponential functions. Care should be exercised in using the word "continuous", so that it is clear from the context which meaning of the word is intended.

Using mathematical notation, there are several ways to define continuous functions in each of the three senses mentioned above.

Let

This subset formula_5 is the domain of "f". Some possible choices include 

In case of the domain formula_14 being defined as an open interval, formula_15 and formula_16 are no boundaries in the above sense and the values of formula_17 and formula_18 do not matter for continuity on formula_14.

The function "f" is "continuous at some point" "c" of its domain if the limit of "f"("x"), as "x" approaches "c" through the domain of "f", exists and is equal to "f"("c"). In mathematical notation, this is written as
In detail this means three conditions: first, "f" has to be defined at "c" (guaranteed by the requirement that "c" is in the domain of "f"). Second, the limit on the left hand side of that equation has to exist. Third, the value of this limit must equal "f"("c").

A neighborhood of a point "c" is a set that contains all points of the domain within some fixed distance of "c". Intuitively, a function is continuous at a point "c" if the range of the restriction of "f" to a neighborhood of "c" shrinks to a single point "f"("c") as the width of the neighborhood around "c" shrinks to zero. More precisely, a function "f" is continuous at a point "c" of its domain if, for any neighborhood formula_21 there is a neighborhood formula_22 such that formula_23 whenever formula_24.

This definition only requires that the domain and the codomain are topological spaces and is thus the most general definition. It follows from this definition that a function "f" is automatically continuous at every isolated point of its domain. As a specific example, every real valued function on the set of integers is continuous.

One can instead require that for any sequence formula_25 of points in the domain which converges to "c", the corresponding sequence formula_26 converges to "f"("c"). In mathematical notation, formula_27

Explicitly including the definition of the limit of a function, we obtain a self-contained definition:
Given a function "f" : "D" → "R" as above and an element "x" of the domain "D", "f" is said to be continuous at the point "x" when the following holds: For any number "ε" > 0, however small, there exists some number "δ" > 0 such that for all "x" in the domain of "f" with "x" − "δ" < "x" < "x" + "δ", the value of "f"("x") satisfies

Alternatively written, continuity of "f" : "D" → "R" at "x" ∈ "D" means that for every "ε" > 0 there exists a "δ" > 0 such that for all "x" ∈ "D" :

More intuitively, we can say that if we want to get all the "f"("x") values to stay in some small neighborhood around "f"("x"), we simply need to choose a small enough neighborhood for the "x" values around "x". If we can do that no matter how small the "f"("x") neighborhood is, then "f" is continuous at "x".

In modern terms, this is generalized by the definition of continuity of a function with respect to a basis for the topology, here the metric topology.

Weierstrass had required that the interval "x" − "δ" < "x" < "x" + "δ" be entirely within the domain "D", but Jordan removed that restriction.

In proofs and numerical analysis we often need to know how fast limits are converging, or in other words, control of the remainder. We can formalise this to a definition of continuity. 
A function formula_30 is called a control function if

A function "f" : "D" → "R" is "C"-continuous at "x" if 

A function is continuous in "x" if it is "C"-continuous for some control function "C".

This approach leads naturally to refining the notion of continuity by restricting the set of admissible control functions. For a given set of control functions formula_34 a function is formula_34-continuous if it is formula_36-continuous for some formula_37. For example, the Lipschitz and Hölder continuous functions of exponent α below are defined by the set of control functions 
respectively 

Continuity can also be defined in terms of oscillation: a function "f" is continuous at a point "x" if and only if its oscillation at that point is zero; in symbols, formula_41 A benefit of this definition is that it "quantifies" discontinuity: the oscillation gives how "much" the function is discontinuous at a point.

This definition is useful in descriptive set theory to study the set of discontinuities and continuous points – the continuous points are the intersection of the sets where the oscillation is less than "ε" (hence a G set) – and gives a very quick proof of one direction of the Lebesgue integrability condition.

The oscillation is equivalent to the "ε"-"δ" definition by a simple re-arrangement, and by using a limit (lim sup, lim inf) to define oscillation: if (at a given point) for a given "ε" there is no "δ" that satisfies the "ε"-"δ" definition, then the oscillation is at least "ε", and conversely if for every "ε" there is a desired "δ," the oscillation is 0. The oscillation definition can be naturally generalized to maps from a topological space to a metric space.

Cauchy defined continuity of a function in the following intuitive terms: an infinitesimal change in the independent variable corresponds to an infinitesimal change of the dependent variable (see "Cours d'analyse", page 34). Non-standard analysis is a way of making this mathematically rigorous. The real line is augmented by the addition of infinite and infinitesimal numbers to form the hyperreal numbers. In nonstandard analysis, continuity can be defined as follows.

(see microcontinuity). In other words, an infinitesimal increment of the independent variable always produces to an infinitesimal change of the dependent variable, giving a modern expression to Augustin-Louis Cauchy's definition of continuity.

Checking the continuity of a given function can be simplified by checking one of the above defining properties for the building blocks of the given function. It is straightforward to show that the sum of two functions, continuous on some domain, is also continuous on this domain. Given

then the "sum of continuous functions"
is continuous in formula_14.

The same holds for the "product of continuous functions,"
is continuous in formula_14.

Combining the above preservations of continuity and the continuity of constant functions and of the identity function formula_51 one arrives at the continuity of all polynomial functions such as
(pictured on the right).

In the same way it can be shown that the "reciprocal of a continuous function" 
is continuous in formula_56.

This implies that, excluding the roots of formula_57, the "quotient of continuous functions" 
is also continuous on formula_62.

For example, the function (pictured)

is defined for all real numbers and is continuous at every such point. Thus it is a continuous function. The question of continuity at does not arise, since is not in the domain of "y". There is no continuous function "F": R → R that agrees with "y"("x") for all .

Since the function sine is continuous on all reals, the sinc function "G"("x") = sin "x"/"x", is defined and continuous for all real "x" ≠ 0. However, unlike the previous example, "G" "can" be extended to a continuous function on "all" real numbers, by "defining" the value "G"(0) to be 1, which is the limit of "G"("x"), when "x" approaches 0, i.e.,

Thus, by setting

the sinc-function becomes a continuous function on all real numbers. The term "removable singularity" is used in such cases, when (re)defining values of a function to coincide with the appropriate limits make a function continuous at specific points.

A more involved construction of continuous functions is the function composition. Given two continuous functions
their composition, denoted as
formula_67, and defined by formula_68 is continuous.

This construction allows stating, for example, that

An example of a discontinuous function is the Heaviside step function formula_71, defined by

Pick for instance formula_73. Then there is no around formula_74, i.e. no open interval formula_75 with formula_76 that will force all the formula_77 values to be within the of formula_78, i.e. within formula_79. Intuitively we can think of this type of discontinuity as a sudden jump in function values.

Similarly, the signum or sign function
is discontinuous at formula_74 but continuous everywhere else. Yet another example: the function
is continuous everywhere apart from formula_74.
Besides plausible continuities and discontinuities like above, there are also functions with a behavior, often coined pathological, for example, Thomae's function,
is continuous at all irrational numbers and discontinuous at all rational numbers. In a similar vein, Dirichlet's function, the indicator function for the set of rational numbers,
is nowhere continuous.

The intermediate value theorem is an existence theorem, based on the real number property of completeness, and states:

For example, if a child grows from 1 m to 1.5 m between the ages of two and six years, then, at some time between two and six years of age, the child's height must have been 1.25 m.

As a consequence, if "f" is continuous on ["a", "b"] and "f"("a") and "f"("b") differ in sign, then, at some point "c" in ["a", "b"], "f"("c") must equal zero.

The extreme value theorem states that if a function "f" is defined on a closed interval ["a","b"] (or any closed and bounded set) and is continuous there, then the function attains its maximum, i.e. there exists "c" ∈ ["a","b"] with "f"("c") ≥ "f"("x") for all "x" ∈ ["a","b"]. The same is true of the minimum of "f". These statements are not, in general, true if the function is defined on an open interval ("a","b") (or any set that is not both closed and bounded), as, for example, the continuous function "f"("x") = 1/"x", defined on the open interval (0,1), does not attain a maximum, being unbounded above.

Every differentiable function
is continuous, as can be shown. The converse does not hold: for example, the absolute value function
is everywhere continuous. However, it is not differentiable at "x" = 0 (but is so everywhere else). Weierstrass's function is also everywhere continuous but nowhere differentiable.

The derivative "f′"("x") of a differentiable function "f"("x") need not be continuous. If "f′"("x") is continuous, "f"("x") is said to be continuously differentiable. The set of such functions is denoted "C"(). More generally, the set of functions
(from an open interval (or open subset of R) Ω to the reals) such that "f" is "n" times differentiable and such that the "n"-th derivative of "f" is continuous is denoted "C"(Ω). See differentiability class. In the field of computer graphics, properties related (but not identical) to "C", "C", "C" are sometimes called "G" (continuity of position), "G" (continuity of tangency), and "G" (continuity of curvature); see Smoothness of curves and surfaces.

Every continuous function
is integrable (for example in the sense of the Riemann integral). The converse does not hold, as the (integrable, but discontinuous) sign function shows.

Given a sequence
of functions such that the limit
exists for all "x" in "D", the resulting function "f"("x") is referred to as the pointwise limit of the sequence of functions ("f"). The pointwise limit function need not be continuous, even if all functions "f" are continuous, as the animation at the right shows. However, "f" is continuous if all functions "f" are continuous and the sequence converges uniformly, by the uniform convergence theorem. This theorem can be used to show that the exponential functions, logarithms, square root function, trigonometric functions are continuous.

Discontinuous functions may be discontinuous in a restricted way, giving rise to the concept of directional continuity (or right and left continuous functions) and semi-continuity. Roughly speaking, a function is "right-continuous" if no jump occurs when the limit point is approached from the right. Formally, "f" is said to be right-continuous at the point "c" if the following holds: For any number "ε" > 0 however small, there exists some number "δ" > 0 such that for all "x" in the domain with , the value of "f"("x") will satisfy

This is the same condition as for continuous functions, except that it is required to hold for "x" strictly larger than "c" only. Requiring it instead for all "x" with yields the notion of "left-continuous" functions. A function is continuous if and only if it is both right-continuous and left-continuous.

A function "f" is "lower semi-continuous" if, roughly, any jumps that might occur only go down, but not up. That is, for any "ε" > 0, there exists some number "δ" > 0 such that for all "x" in the domain with , the value of "f"("x") satisfies
The reverse condition is "upper semi-continuity".

The concept of continuous real-valued functions can be generalized to functions between metric spaces. A metric space is a set "X" equipped with a function (called metric) "d", that can be thought of as a measurement of the distance of any two elements in "X". Formally, the metric is a function
that satisfies a number of requirements, notably the triangle inequality. Given two metric spaces ("X", d) and ("Y", d) and a function
then "f" is continuous at the point "c" in "X" (with respect to the given metrics) if for any positive real number ε, there exists a positive real number δ such that all "x" in "X" satisfying d("x", "c") < δ will also satisfy d("f"("x"), "f"("c")) < ε. As in the case of real functions above, this is equivalent to the condition that for every sequence ("x") in "X" with limit lim "x" = "c", we have lim "f"("x") = "f"("c"). The latter condition can be weakened as follows: "f" is continuous at the point "c" if and only if for every convergent sequence ("x") in "X" with limit "c", the sequence ("f"("x")) is a Cauchy sequence, and "c" is in the domain of "f".

The set of points at which a function between metric spaces is continuous is a G set – this follows from the ε-δ definition of continuity.

This notion of continuity is applied, for example, in functional analysis. A key statement in this area says that a linear operator
between normed vector spaces "V" and "W" (which are vector spaces equipped with a compatible norm, denoted ||"x"||)
is continuous if and only if it is bounded, that is, there is a constant "K" such that
for all "x" in "V".

The concept of continuity for functions between metric spaces can be strengthened in various ways by limiting the way δ depends on ε and "c" in the definition above. Intuitively, a function "f" as above is uniformly continuous if the δ does
not depend on the point "c". More precisely, it is required that for every real number "ε" > 0 there exists "δ" > 0 such that for every "c", "b" ∈ "X" with "d"("b", "c") < "δ", we have that "d"("f"("b"), "f"("c")) < "ε". Thus, any uniformly continuous function is continuous. The converse does not hold in general, but holds when the domain space "X" is compact. Uniformly continuous maps can be defined in the more general situation of uniform spaces.

A function is Hölder continuous with exponent α (a real number) if there is a constant "K" such that for all "b" and "c" in "X", the inequality
holds. Any Hölder continuous function is uniformly continuous. The particular case is referred to as Lipschitz continuity. That is, a function is Lipschitz continuous if there is a constant "K" such that the inequality
holds for any "b", "c" in "X". The Lipschitz condition occurs, for example, in the Picard–Lindelöf theorem concerning the solutions of ordinary differential equations.

Another, more abstract, notion of continuity is continuity of functions between topological spaces in which there generally is no formal notion of distance, as there is in the case of metric spaces. A topological space is a set "X" together with a topology on "X", which is a set of subsets of "X" satisfying a few requirements with respect to their unions and intersections that generalize the properties of the open balls in metric spaces while still allowing to talk about the neighbourhoods of a given point. The elements of a topology are called open subsets of "X" (with respect to the topology).

A function
between two topological spaces "X" and "Y" is continuous if for every open set "V" ⊆ "Y", the inverse image
is an open subset of "X". That is, "f" is a function between the sets "X" and "Y" (not on the elements of the topology "T"), but the continuity of "f" depends on the topologies used on "X" and "Y".

This is equivalent to the condition that the preimages of the closed sets (which are the complements of the open subsets) in "Y" are closed in "X".

An extreme example: if a set "X" is given the discrete topology (in which every subset is open), all functions
to any topological space "T" are continuous. On the other hand, if "X" is equipped with the indiscrete topology (in which the only open subsets are the empty set and "X") and the space "T" set is at least T, then the only continuous functions are the constant functions. Conversely, any function whose range is indiscrete is continuous.

The translation in the language of neighborhoods of the (ε, δ)-definition of continuity leads to the following definition of the continuity at a point:
This definition is equivalent to the same statement with neighborhoods restricted to open neighborhoods and can be restated in several ways by using preimages rather than images.

Also, as every set that contains a neighborhood is also a neighborhood, and formula_103 is the largest subset of such that , this definition may be simplified into:
As an open set is a set that is a neighborhood of all its points, a function formula_104 is continuous at every point of if and only if it is a continuous function.

If "X" and "Y" are metric spaces, it is equivalent to consider the neighborhood system of open balls centered at "x" and "f"("x") instead of all neighborhoods. This gives back the above δ-ε definition of continuity in the context of metric spaces. In general topological spaces, there is no notion of nearness or distance. If however the target space is a Hausdorff space, it is still true that "f" is continuous at "a" if and only if the limit of "f" as "x" approaches "a" is "f"("a"). At an isolated point, every function is continuous.

Several equivalent definitions for a topological structure exist and thus there are several equivalent ways to define a continuous function.

In several contexts, the topology of a space is conveniently specified in terms of limit points. In many instances, this is accomplished by specifying when a point is the limit of a sequence, but for some spaces that are too large in some sense, one specifies also when a point is the limit of more general sets of points indexed by a directed set, known as nets. A function is (Heine-)continuous only if it takes limits of sequences to limits of sequences. In the former case, preservation of limits is also sufficient; in the latter, a function may preserve all limits of sequences yet still fail to be continuous, and preservation of nets is a necessary and sufficient condition.

In detail, a function "f": "X" → "Y" is sequentially continuous if whenever a sequence ("x") in "X" converges to a limit "x", the sequence ("f"("x")) converges to "f"("x"). Thus sequentially continuous functions "preserve sequential limits". Every continuous function is sequentially continuous. If "X" is a first-countable space and countable choice holds, then the converse also holds: any function preserving sequential limits is continuous. In particular, if "X" is a metric space, sequential continuity and continuity are equivalent. For non first-countable spaces, sequential continuity might be strictly weaker than continuity. (The spaces for which the two properties are equivalent are called sequential spaces.) This motivates the consideration of nets instead of sequences in general topological spaces. Continuous functions preserve limits of nets, and in fact this property characterizes continuous functions.

Instead of specifying the open subsets of a topological space, the topology can also be determined by a closure operator (denoted cl) which assigns to any subset "A" ⊆ "X" its closure, or an interior operator (denoted int), which assigns to any subset "A" of "X" its interior. In these terms, a function
between topological spaces is continuous in the sense above if and only if for all subsets "A" of "X"
That is to say, given any element "x" of "X" that is in the closure of any subset "A", "f"("x") belongs to the closure of "f"("A"). This is equivalent to the requirement that for all subsets "A"<nowiki>'</nowiki> of "X"<nowiki>'</nowiki>
Moreover,
is continuous if and only if
for any subset "A"' of "Y".

If "f": "X" → "Y" and "g": "Y" → "Z" are continuous, then so is the composition "g" ∘ "f": "X" → "Z". If "f": "X" → "Y" is continuous and

The possible topologies on a fixed set "X" are partially ordered: a topology τ is said to be coarser than another topology τ (notation: τ ⊆ τ) if every open subset with respect to τ is also open with respect to τ. Then, the identity map
is continuous if and only if τ ⊆ τ (see also comparison of topologies). More generally, a continuous function
stays continuous if the topology τ is replaced by a coarser topology and/or τ is replaced by a finer topology.

Symmetric to the concept of a continuous map is an open map, for which "images" of open sets are open. In fact, if an open map "f" has an inverse function, that inverse is continuous, and if a continuous map "g" has an inverse, that inverse is open. Given a bijective function "f" between two topological spaces, the inverse function "f" need not be continuous. A bijective continuous function with continuous inverse function is called a "homeomorphism".

If a continuous bijection has as its domain a compact space and its codomain is Hausdorff, then it is a homeomorphism.

Given a function
where "X" is a topological space and "S" is a set (without a specified topology), the final topology on "S" is defined by letting the open sets of "S" be those subsets "A" of "S" for which "f"("A") is open in "X". If "S" has an existing topology, "f" is continuous with respect to this topology if and only if the existing topology is coarser than the final topology on "S". Thus the final topology can be characterized as the finest topology on "S" that makes "f" continuous. If "f" is surjective, this topology is canonically identified with the quotient topology under the equivalence relation defined by "f".

Dually, for a function "f" from a set "S" to a topological space "X", the initial topology on "S" is defined by designating as an open set every subset "A" of "S" such that formula_112 for some open subset "U" of "X". If "S" has an existing topology, "f" is continuous with respect to this topology if and only if the existing topology is finer than the initial topology on "S". Thus the initial topology can be characterized as the coarsest topology on "S" that makes "f" continuous. If "f" is injective, this topology is canonically identified with the subspace topology of "S", viewed as a subset of "X".

A topology on a set "S" is uniquely determined by the class of all continuous functions formula_113 into all topological spaces "X". Dually, a similar idea can be applied to maps formula_114

Various other mathematical domains use the concept of continuity in different, but related meanings. For example, in order theory, an order-preserving function "f": "X" → "Y" between particular types of partially ordered sets "X" and "Y" is continuous if for each directed subset "A" of "X", we have sup("f"("A")) = "f"(sup("A")). Here sup is the supremum with respect to the orderings in "X" and "Y", respectively. This notion of continuity is the same as topological continuity when the partially ordered sets are given the Scott topology.

In category theory, a functor
between two categories is called "continuous", if it commutes with small limits. That is to say,
for any small (i.e., indexed by a set "I", as opposed to a class) diagram of objects in formula_117.

A "continuity space" is a generalization of metric spaces and posets, which uses the concept of quantales, and that can be used to unify the notions of metric spaces and domains.


</doc>
<doc id="6123" url="https://en.wikipedia.org/wiki?curid=6123" title="Curl (mathematics)">
Curl (mathematics)

In vector calculus, the curl is a vector operator that describes the infinitesimal rotation of a vector field in three-dimensional Euclidean space. At every point in the field, the curl of that point is represented by a vector. The attributes of this vector (length and direction) characterize the rotation at that point.

The direction of the curl is the axis of rotation, as determined by the right-hand rule, and the magnitude of the curl is the magnitude of rotation. If the vector field represents the flow velocity of a moving fluid, then the curl is the circulation density of the fluid. A vector field whose curl is zero is called irrotational. The curl is a form of differentiation for vector fields. The corresponding form of the fundamental theorem of calculus is Stokes' theorem, which relates the surface integral of the curl of a vector field to the line integral of the vector field around the boundary curve.

The alternative terminology "rotation" or "rotational" and alternative notations and are often used (the former especially in many European countries, the latter, using the del (or nabla) operator and the cross product, is more used in other countries) for .

Unlike the gradient and divergence, curl does not generalize as simply to other dimensions; some generalizations are possible, but only in three dimensions is the geometrically defined curl of a vector field again a vector field. This is a phenomenon similar to the 3-dimensional cross product, and the connection is reflected in the notation for the curl.

The name "curl" was first suggested by James Clerk Maxwell in 1871 but the concept was apparently first used in the construction of an optical field theory by James MacCullagh in 1839.

The curl of a vector field , denoted by , or , or , at a point is defined in terms of its projection onto various lines through the point. If is any unit vector, the projection of the curl of onto is defined to be the limiting value of a closed line integral in a plane orthogonal to divided by the area enclosed, as the path of integration is contracted around the point.

The curl operator maps continuously differentiable functions to continuous functions , and more generally, it maps functions in to functions in . 

Implicitly, curl is defined:

where is a line integral along the boundary of the area in question, and is the magnitude of the area. This equation defines the projection of the curl of onto , where is the normal vector to the surface bounded by ; and is defined via the right-hand rule (see diagram)..

The above formula means that the curl of a vector field is defined as the infinitesimal area density of the "circulation" of that field. To this definition fit naturally 

The equation for each component can be obtained by exchanging each occurrence of a subscript 1, 2, 3 in cyclic permutation: 1 → 2, 2 → 3, and 3 → 1 (where the subscripts represent the relevant indices).

If are the Cartesian coordinates and are the orthogonal coordinates, then 
is the length of the coordinate vector corresponding to . The remaining two components of curl result from cyclic permutation of indices: 3,1,2 → 1,2,3 → 2,3,1.

Suppose the vector field describes the velocity field of a fluid flow (such as a large tank of liquid or gas) and a small ball is located within the fluid or gas (the centre of the ball being fixed at a certain point). If the ball has a rough surface, the fluid flowing past it will make it rotate. The rotation axis (oriented according to the right hand rule) points in the direction of the curl of the field at the centre of the ball, and the angular speed of the rotation is half the magnitude of the curl at this point.

The curl of the vector at any point is given by the rotation of an infinitesimal area in the "xy"-plane (for "z"-axis component of the curl), "zx"-plane (for "y"-axis component of the curl) and "yz"-plane (for "x"-axis component of the curl vector). This can be clearly seen in the examples below.

The inverse curl of a three-dimensional vector field can be obtained up to an integration constant and an unknown irrotational field with the Biot-Savart law.

In practice, the above definition is rarely used because in virtually all cases, the curl operator can be applied using some set of curvilinear coordinates, for which simpler representations have been derived.

The notation has its origins in the similarities to the 3-dimensional cross product, and it is useful as a mnemonic in Cartesian coordinates if is taken as a vector differential operator del. Such notation involving operators is common in physics and algebra.

Expanded in 3-dimensional Cartesian coordinates (see "Del in cylindrical and spherical coordinates" for spherical and cylindrical coordinate representations), is, for composed of :

where , , and are the unit vectors for the -, -, and -axes, respectively. This expands as follows:

Although expressed in terms of coordinates, the result is invariant under proper rotations of the coordinate axes but the result inverts under reflection.

In a general coordinate system, the curl is given by

where denotes the Levi-Civita tensor and the covariant derivative, the metric tensor is used to lower the index on , and the Einstein summation convention implies that repeated indices are summed over. Equivalently,

where are the coordinate vector fields. Equivalently, using the exterior derivative, the curl can be expressed as:

Here and are the musical isomorphisms, and is the Hodge star operator. This formula shows how to calculate the curl of in any coordinate system, and how to extend the curl to any oriented three-dimensional Riemannian manifold. Since this depends on a choice of orientation, curl is a chiral operation. In other words, if the orientation is reversed, then the direction of the curl is also reversed.

Take the vector field:

For clarity, this can be decomposed as follows:

Its corresponding plot:

Upon visual inspection, the field can be described as "rotating". If a stationary object were to be placed in the field with the vectors representing a linear force, the object would rotate clockwise.

Calculating the curl:

The resulting vector field describing the curl would be uniformly going in the negative direction. The results of this equation align with what could have been predicted using the right-hand rule using a right-handed coordinate system. Being a uniform vector field, the object described before would have the same rotational intensity regardless of where it was placed.

The plot describing the curl of :

Take the vector field:

Its corresponding plot:

Upon initial inspection, curl existing in this graph would not be obvious. However, taking the object in the previous example, and placing it anywhere on the line , the force exerted on the right side would be slightly greater than the force exerted on the left, causing it to rotate clockwise. Using the right-hand rule, it can be predicted that the resulting curl would be straight in the negative direction. Inversely, if placed on , the object would rotate counterclockwise and the right-hand rule would result in a positive direction.

Calculating the curl:

As predicted, the curl points in the negative direction when is positive and vice versa. In this field, the intensity of rotation would be greater as the object moves away from the plane .

The plot describing the curl of :

In general curvilinear coordinates (not only in Cartesian coordinates), the curl of a cross product of vector fields and can be shown to be

Interchanging the vector field and operator, we arrive at the cross product of a vector field with curl of a vector field:

where is the Feynman subscript notation, which considers only the variation due to the vector field (i.e., in this case, is treated as being constant in space).

Another example is the curl of a curl of a vector field. It can be shown that in general coordinates

and this identity defines the vector Laplacian of , symbolized as .

The curl of the gradient of "any" scalar field is always the zero vector field

which follows from the antisymmetry in the definition of the curl, and the symmetry of second derivatives.

If is a scalar valued function and is a vector field, then


The vector calculus operations of grad, curl, and div are most easily generalized and understood in the context of differential forms, which involves a number of steps. In a nutshell, they correspond to the derivatives of 0-forms, 1-forms, and 2-forms, respectively. The geometric interpretation of curl as rotation corresponds to identifying bivectors (2-vectors) in 3 dimensions with the special orthogonal Lie algebra of infinitesimal rotations (in coordinates, skew-symmetric 3 × 3 matrices), while representing rotations by vectors corresponds to identifying 1-vectors (equivalently, 2-vectors) and , these all being 3-dimensional spaces.

In 3 dimensions, a differential 0-form is simply a function ; a differential 1-form is the following expression:
a differential 2-form is the formal sum:
and a differential 3-form is defined by a single term:

The exterior derivative of a -form in is defined as the -form from above—and in if, e.g.,

then the exterior derivative leads to

The exterior derivative of a 1-form is therefore a 2-form, and that of a 2-form is a 3-form. On the other hand, because of the interchangeability of mixed derivatives, e.g. because of

the twofold application of the exterior derivative leads to 0.

Thus, denoting the space of -forms by and the exterior derivative by one gets a sequence:

Here is the space of sections of the exterior algebra vector bundle over ℝ, whose dimension is the binomial coefficient ; note that for or . Writing only dimensions, one obtains a row of Pascal's triangle:

the 1-dimensional fibers correspond to scalar fields, and the 3-dimensional fibers to vector fields, as described below. Modulo suitable identifications, the three nontrivial occurrences of the exterior derivative correspond to grad, curl, and div.

Differential forms and the differential can be defined on any Euclidean space, or indeed any manifold, without any notion of a Riemannian metric. On a Riemannian manifold, or more generally pseudo-Riemannian manifold, -forms can be identified with -vector fields (-forms are -covector fields, and a pseudo-Riemannian metric gives an isomorphism between vectors and covectors), and on an "oriented" vector space with a nondegenerate form (an isomorphism between vectors and covectors), there is an isomorphism between -vectors and -vectors; in particular on (the tangent space of) an oriented pseudo-Riemannian manifold. Thus on an oriented pseudo-Riemannian manifold, one can interchange -forms, -vector fields, -forms, and -vector fields; this is known as Hodge duality. Concretely, on this is given by:

Thus, identifying 0-forms and 3-forms with scalar fields, and 1-forms and 2-forms with vector fields:

On the other hand, the fact that corresponds to the identities
for any scalar field , and
for any vector field .

Grad and div generalize to all oriented pseudo-Riemannian manifolds, with the same geometric interpretation, because the spaces of 0-forms and -forms is always (fiberwise) 1-dimensional and can be identified with scalar fields, while the spaces of 1-forms and -forms are always fiberwise -dimensional and can be identified with vector fields.

Curl does not generalize in this way to 4 or more dimensions (or down to 2 or fewer dimensions); in 4 dimensions the dimensions are

so the curl of a 1-vector field (fiberwise 4-dimensional) is a "2-vector field", which is fiberwise 6-dimensional, one has

which yields a sum of six independent terms, and cannot be identified with a 1-vector field. Nor can one meaningfully go from a 1-vector field to a 2-vector field to a 3-vector field (4 → 6 → 4), as taking the differential twice yields zero (). Thus there is no curl function from vector fields to vector fields in other dimensions arising in this way.

However, one can define a curl of a vector field as a "2-vector field" in general, as described below.

2-vectors correspond to the exterior power ; in the presence of an inner product, in coordinates these are the skew-symmetric matrices, which are geometrically considered as the special orthogonal Lie algebra of infinitesimal rotations. This has dimensions, and allows one to interpret the differential of a 1-vector field as its infinitesimal rotations. Only in 3 dimensions (or trivially in 0 dimensions) does , which is the most elegant and common case. In 2 dimensions the curl of a vector field is not a vector field but a function, as 2-dimensional rotations are given by an angle (a scalar – an orientation is required to choose whether one counts clockwise or counterclockwise rotations as positive); this is not the div, but is rather perpendicular to it. In 3 dimensions the curl of a vector field is a vector field as is familiar (in 1 and 0 dimensions the curl of a vector field is 0, because there are no non-trivial 2-vectors), while in 4 dimensions the curl of a vector field is, geometrically, at each point an element of the 6-dimensional Lie algebra .

Note also that the curl of a 3-dimensional vector field which only depends on 2 coordinates (say and ) is simply a vertical vector field (in the direction) whose magnitude is the curl of the 2-dimensional vector field, as in the examples on this page.

Considering curl as a 2-vector field (an antisymmetric 2-tensor) has been used to generalize vector calculus and associated physics to higher dimensions.





</doc>
<doc id="6125" url="https://en.wikipedia.org/wiki?curid=6125" title="Carl Friedrich Gauss">
Carl Friedrich Gauss

Johann Carl Friedrich Gauss (; ; ; (30 April 177723 February 1855) was a German mathematician and physicist who made significant contributions to many fields in mathematics and sciences. Sometimes referred to as the "Princeps mathematicorum" () and "the greatest mathematician since antiquity", Gauss had an exceptional influence in many fields of mathematics and science, and is ranked among history's most influential mathematicians.

Johann Carl Friedrich Gauss was born on 30 April 1777 in Brunswick (Braunschweig), in the Duchy of Brunswick-Wolfenbüttel (now part of Lower Saxony, Germany), to poor, working-class parents. His mother was illiterate and never recorded the date of his birth, remembering only that he had been born on a Wednesday, eight days before the Feast of the Ascension (which occurs 39 days after Easter). Gauss later solved this puzzle about his birthdate in the context of finding the date of Easter, deriving methods to compute the date in both past and future years. He was christened and confirmed in a church near the school he attended as a child.

Gauss was a child prodigy. In his memorial on Gauss, Wolfgang Sartorius von Waltershausen says that when Gauss was barely three years old he corrected a math error his father made; and that when he was seven, he confidently solved an arithmetic series problem faster than anyone else in his class of 100 students. Many versions of this story have been retold since that time with various details regarding what the series was – the most frequent being the classical problem of adding all the integers from 1 to 100. There are many other anecdotes about his precocity while a toddler, and he made his first groundbreaking mathematical discoveries while still a teenager. He completed his magnum opus, "Disquisitiones Arithmeticae", in 1798, at the age of 21—though it was not published until 1801. This work was fundamental in consolidating number theory as a discipline and has shaped the field to the present day.

Gauss's intellectual abilities attracted the attention of the Duke of Brunswick, who sent him to the Collegium Carolinum (now Braunschweig University of Technology), which he attended from 1792 to 1795, and to the University of Göttingen from 1795 to 1798.
While at university, Gauss independently rediscovered several important theorems. His breakthrough occurred in 1796 when he showed that a regular polygon can be constructed by compass and straightedge if the number of its sides is the product of distinct Fermat primes and a power of 2. This was a major discovery in an important field of mathematics; construction problems had occupied mathematicians since the days of the Ancient Greeks, and the discovery ultimately led Gauss to choose mathematics instead of philology as a career.
Gauss was so pleased with this result that he requested that a regular heptadecagon be inscribed on his tombstone. The stonemason declined, stating that the difficult construction would essentially look like a circle.

The year 1796 was productive for both Gauss and number theory. He discovered a construction of the heptadecagon on 30 March. He further advanced modular arithmetic, greatly simplifying manipulations in number theory. On 8 April he became the first to prove the quadratic reciprocity law. This remarkably general law allows mathematicians to determine the solvability of any quadratic equation in modular arithmetic. The prime number theorem, conjectured on 31 May, gives a good understanding of how the prime numbers are distributed among the integers.

Gauss also discovered that every positive integer is representable as a sum of at most three triangular numbers on 10 July and then jotted down in his diary the note: "ΕΥΡΗΚΑ! . On 1 October he published a result on the number of solutions of polynomials with coefficients in finite fields, which 150 years later led to the Weil conjectures.

Gauss remained mentally active into his old age, even while suffering from gout and general unhappiness. For example, at the age of 62, he taught himself Russian.

In 1840, Gauss published his influential "Dioptrische Untersuchungen", in which he gave the first systematic analysis on the formation of images under a paraxial approximation (Gaussian optics). Among his results, Gauss showed that under a paraxial approximation an optical system can be characterized by its cardinal points and he derived the Gaussian lens formula.

In 1845, he became an associated member of the Royal Institute of the Netherlands; when that became the Royal Netherlands Academy of Arts and Sciences in 1851, he joined as a foreign member.

In 1854, Gauss selected the topic for Bernhard Riemann's inaugural lecture "Über die Hypothesen, welche der Geometrie zu Grunde liegen" ("About the hypotheses that underlie Geometry"). On the way home from Riemann's lecture, Weber reported that Gauss was full of praise and excitement.

On 23 February 1855, Gauss died of a heart attack in Göttingen (then Kingdom of Hanover and now Lower Saxony); he is interred in the Albani Cemetery there. Two people gave eulogies at his funeral: Gauss's son-in-law Heinrich Ewald, and Wolfgang Sartorius von Waltershausen, who was Gauss's close friend and biographer. Gauss's brain was preserved and was studied by Rudolf Wagner, who found its mass to be slightly above average, at 1,492 grams, and the cerebral area equal to 219,588 square millimeters (340.362 square inches). Highly developed convolutions were also found, which in the early 20th century were suggested as the explanation of his genius.

Gauss was a Lutheran Protestant, a member of the St. Albans Evangelical Lutheran church in Göttingen. Potential evidence that Gauss believed in God comes from his response after solving a problem that had previously defeated him: "Finally, two days ago, I succeeded—not on account of my hard efforts, but by the grace of the Lord." One of his biographers, G. Waldo Dunnington, described Gauss's religious views as follows:
For him science was the means of exposing the immortal nucleus of the human soul. In the days of his full strength, it furnished him recreation and, by the prospects which it opened up to him, gave consolation. Toward the end of his life, it brought him confidence. Gauss's God was not a cold and distant figment of metaphysics, nor a distorted caricature of embittered theology. To man is not vouchsafed that fullness of knowledge which would warrant his arrogantly holding that his blurred vision is the full light and that there can be none other which might report the truth as does his. For Gauss, not he who mumbles his creed, but he who lives it, is accepted. He believed that a life worthily spent here on earth is the best, the only, preparation for heaven. Religion is not a question of literature, but of life. God's revelation is continuous, not contained in tablets of stone or sacred parchment. A book is inspired when it inspires. The unshakeable idea of personal continuance after death, the firm belief in a last regulator of things, in an eternal, just, omniscient, omnipotent God, formed the basis of his religious life, which harmonized completely with his scientific research.

Apart from his correspondence, there are not many known details about Gauss's personal creed. Many biographers of Gauss disagree about his religious stance, with Bühler and others considering him a deist with very unorthodox views, while Dunnington (though admitting that Gauss did not believe literally in all Christian dogmas and that it is unknown what he believed on most doctrinal and confessional questions) points out that he was, at least, a nominal Lutheran.

In connection to this, there is a record of a conversation between Rudolf Wagner and Gauss, in which they discussed William Whewell's book "Of the Plurality of Worlds". In this work, Whewell had discarded the possibility of existing life in other planets, on the basis of theological arguments, but this was a position with which both Wagner and Gauss disagreed. Later Wagner explained that he did not fully believe in the Bible, though he confessed that he "envied" those who were able to easily believe. This later led them to discuss the topic of faith, and in some other religious remarks, Gauss said that he had been more influenced by theologians like Lutheran minister Paul Gerhardt than by Moses. Other religious influences included Wilhelm Braubach, Johann Peter Süssmilch, and the New Testament.

Dunnington further elaborates on Gauss's religious views by writing: Gauss's religious consciousness was based on an insatiable thirst for truth and a deep feeling of justice extending to intellectual as well as material goods. He conceived spiritual life in the whole universe as a great system of law penetrated by eternal truth, and from this source he gained the firm confidence that death does not end all.

Gauss declared he firmly believed in the afterlife, and saw spirituality as something essentially important for human beings. He was quoted stating: ""The world would be nonsense, the whole creation an absurdity without immortality,"" and for this statement he was severely criticized by the atheist Eugen Dühring who judged him as a narrow superstitious man.

Though he was not a church-goer, Gauss strongly upheld religious tolerance, believing "that one is not justified in disturbing another's religious belief, in which they find consolation for earthly sorrows in time of trouble." When his son Eugene announced that he wanted to become a Christian missionary, Gauss approved of this, saying that regardless of the problems within religious organizations, missionary work was "a highly honorable" task.

On 9 October 1805, Gauss married Johanna Osthoff (1780–1809), and had two sons and a daughter with her. Johanna died on 11 October 1809, and her most recent child, Louis, died the following year. Gauss plunged into a depression from which he never fully recovered. He then married Minna Waldeck (1788–1831) on 4 August 1810, and had three more children. Gauss was never quite the same without his first wife, and he, just like his father, grew to dominate his children. Minna Waldeck died on 12 September 1831.

Gauss had six children. With Johanna (1780–1809), his children were Joseph (1806–1873), Wilhelmina (1808–1846) and Louis (1809–1810). With Minna Waldeck he also had three children: Eugene (1811–1896), Wilhelm (1813–1879) and Therese (1816–1864). Eugene shared a good measure of Gauss's talent in languages and computation. After his second wife's death in 1831 Therese took over the household and cared for Gauss for the rest of his life. His mother lived in his house from 1817 until her death in 1839.

Gauss eventually had conflicts with his sons. He did not want any of his sons to enter mathematics or science for "fear of lowering the family name", as he believed none of them would surpass his own achievements. Gauss wanted Eugene to become a lawyer, but Eugene wanted to study languages. They had an argument over a party Eugene held, which Gauss refused to pay for. The son left in anger and, in about 1832, emigrated to the United States, where he was quite successful. While working for the American Fur Company in the Midwest, he learned the Sioux language. Later, he moved to Missouri and became a successful businessman. Wilhelm also moved to America in 1837 and settled in Missouri, starting as a farmer and later becoming wealthy in the shoe business in St. Louis. It took many years for Eugene's success to counteract his reputation among Gauss's friends and colleagues. See also on 3 September 1912.

Carl Gauss was an ardent perfectionist and a hard worker. He was never a prolific writer, refusing to publish work which he did not consider complete and above criticism. This was in keeping with his personal motto "pauca sed matura" ("few, but ripe"). His personal diaries indicate that he had made several important mathematical discoveries years or decades before his contemporaries published them. Scottish-American mathematician and writer Eric Temple Bell said that if Gauss had published all of his discoveries in a timely manner, he would have advanced mathematics by fifty years.

Though he did take in a few students, Gauss was known to dislike teaching. It is said that he attended only a single scientific conference, which was in Berlin in 1828. However, several of his students became influential mathematicians, among them Richard Dedekind and Bernhard Riemann.

On Gauss's recommendation, Friedrich Bessel was awarded an honorary doctor degree from Göttingen in March 1811. Around that time, the two men engaged in an epistolary correspondence. However, when they met in person in 1825, they quarrelled; the details are unknown.

Before she died, Sophie Germain was recommended by Gauss to receive her honorary degree; she never received it.

Gauss usually declined to present the intuition behind his often very elegant proofs—he preferred them to appear "out of thin air" and erased all traces of how he discovered them. This is justified, if unsatisfactorily, by Gauss in his "Disquisitiones Arithmeticae", where he states that all analysis (i.e., the paths one traveled to reach the solution of a problem) must be suppressed for sake of brevity.

Gauss supported the monarchy and opposed Napoleon, whom he saw as an outgrowth of revolution.

Gauss summarized his views on the pursuit of knowledge in a letter to Farkas Bolyai dated 2 September 1808 as follows:It is not knowledge, but the act of learning, not possession but the act of getting there, which grants the greatest enjoyment. When I have clarified and exhausted a subject, then I turn away from it, in order to go into darkness again. The never-satisfied man is so strange; if he has completed a structure, then it is not in order to dwell in it peacefully, but in order to begin another. I imagine the world conqueror must feel thus, who, after one kingdom is scarcely conquered, stretches out his arms for others.

In his 1799 doctorate in absentia, "A new proof of the theorem that every integral rational algebraic function of one variable can be resolved into real factors of the first or second degree", Gauss proved the fundamental theorem of algebra which states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. Mathematicians including Jean le Rond d'Alembert had produced false proofs before him, and Gauss's dissertation contains a critique of d'Alembert's work. Ironically, by today's standard, Gauss's own attempt is not acceptable, owing to the implicit use of the Jordan curve theorem. However, he subsequently produced three other proofs, the last one in 1849 being generally rigorous. His attempts clarified the concept of complex numbers considerably along the way.

Gauss also made important contributions to number theory with his 1801 book "Disquisitiones Arithmeticae" (Latin, Arithmetical Investigations), which, among other things, introduced the symbol for congruence and used it in a clean presentation of modular arithmetic, contained the first two proofs of the law of quadratic reciprocity, developed the theories of binary and ternary quadratic forms, stated the class number problem for them, and showed that a regular heptadecagon (17-sided polygon) can be constructed with straightedge and compass. It appears that Gauss already knew the class number formula in 1801.

In addition, he proved the following conjectured theorems:

He also

“If this subject has hitherto been considered from the wrong viewpoint and thus enveloped in mystery and surrounded by darkness, it is largely an unsuitable terminology which should be blamed. Had +1, -1 and √−1, instead of being called positive, negative and imaginary (or worse still, impossible) unity, been given the names say,of direct, inverse and lateral unity, there would hardly have been any scope for such obscurity.” - Gauss

In the same year, Italian astronomer Giuseppe Piazzi discovered the dwarf planet Ceres. Piazzi could only track Ceres for somewhat more than a month, following it for three degrees across the night sky. Then it disappeared temporarily behind the glare of the Sun. Several months later, when Ceres should have reappeared, Piazzi could not locate it: the mathematical tools of the time were not able to extrapolate a position from such a scant amount of data—three degrees represent less than 1% of the total orbit. Gauss heard about the problem and tackled it. After three months of intense work, he predicted a position for Ceres in December 1801—just about a year after its first sighting—and this turned out to be accurate within a half-degree when it was rediscovered by Franz Xaver von Zach on 31 December at Gotha, and one day later by Heinrich Olbers in Bremen.

Gauss's method involved determining a conic section in space, given one focus (the Sun) and the conic's intersection with three given lines (lines of sight from the Earth, which is itself moving on an ellipse, to the planet) and given the time it takes the planet to traverse the arcs determined by these lines (from which the lengths of the arcs can be calculated by Kepler's Second Law). This problem leads to an equation of the eighth degree, of which one solution, the Earth's orbit, is known. The solution sought is then separated from the remaining six based on physical conditions. In this work, Gauss used comprehensive approximation methods which he created for that purpose.

One such method was the fast Fourier transform. While this method is traditionally attributed to a 1965 paper by J.W. Cooley and J.W. Tukey, Gauss developed it as a trigonometric interpolation method. His paper, "Theoria Interpolationis Methodo Nova Tractata", was only published posthumously in Volume 3 of his collected works. This paper predates the first presentation by Joseph Fourier on the subject in 1807.

Zach noted that "without the intelligent work and calculations of Doctor Gauss we might not have found Ceres again". Though Gauss had up to that point been financially supported by his stipend from the Duke, he doubted the security of this arrangement, and also did not believe pure mathematics to be important enough to deserve support. Thus he sought a position in astronomy, and in 1807 was appointed Professor of Astronomy and Director of the astronomical observatory in Göttingen, a post he held for the remainder of his life.
The discovery of Ceres led Gauss to his work on a theory of the motion of planetoids disturbed by large planets, eventually published in 1809 as "Theoria motus corporum coelestium in sectionibus conicis solem ambientum" (Theory of motion of the celestial bodies moving in conic sections around the Sun). In the process, he so streamlined the cumbersome mathematics of 18th-century orbital prediction that his work remains a cornerstone of astronomical computation. It introduced the Gaussian gravitational constant, and contained an influential treatment of the method of least squares, a procedure used in all sciences to this day to minimize the impact of measurement error.

Gauss proved the method under the assumption of normally distributed errors (see Gauss–Markov theorem; see also Gaussian). The method had been described earlier by Adrien-Marie Legendre in 1805, but Gauss claimed that he had been using it since 1794 or 1795. In the history of statistics, this disagreement is called the "priority dispute over the discovery of the method of least squares."

In 1818 Gauss, putting his calculation skills to practical use, carried out a geodetic survey of the Kingdom of Hanover, linking up with previous Danish surveys. To aid the survey, Gauss invented the heliotrope, an instrument that uses a mirror to reflect sunlight over great distances, to measure positions.

Gauss also claimed to have discovered the possibility of non-Euclidean geometries but never published it. This discovery was a major paradigm shift in mathematics, as it freed mathematicians from the mistaken belief that Euclid's axioms were the only way to make geometry consistent and non-contradictory.

Research on these geometries led to, among other things, Einstein's theory of general relativity, which describes the universe as non-Euclidean. His friend Farkas Wolfgang Bolyai with whom Gauss had sworn "brotherhood and the banner of truth" as a student, had tried in vain for many years to prove the parallel postulate from Euclid's other axioms of geometry.

Bolyai's son, János Bolyai, discovered non-Euclidean geometry in 1829; his work was published in 1832. After seeing it, Gauss wrote to Farkas Bolyai: "To praise it would amount to praising myself. For the entire content of the work ... coincides almost exactly with my own meditations which have occupied my mind for the past thirty or thirty-five years."

This unproved statement put a strain on his relationship with Bolyai who thought that Gauss was "stealing" his idea.

Letters from Gauss years before 1829 reveal him obscurely discussing the problem of parallel lines. Waldo Dunnington, a biographer of Gauss, argues in "Gauss, Titan of Science" that Gauss was in fact in full possession of non-Euclidean geometry long before it was published by Bolyai, but that he refused to publish any of it because of his fear of controversy.

The geodetic survey of Hanover, which required Gauss to spend summers traveling on horseback for a decade, fueled Gauss's interest in differential geometry and topology, fields of mathematics dealing with curves and surfaces. Among other things, he came up with the notion of Gaussian curvature.
This led in 1828 to an important theorem, the Theorema Egregium ("remarkable theorem"), establishing an important property of the notion of curvature. Informally, the theorem says that the curvature of a surface can be determined entirely by measuring angles and distances on the surface.

That is, curvature does not depend on how the surface might be embedded in 3-dimensional space or 2-dimensional space.

In 1821, he was made a foreign member of the Royal Swedish Academy of Sciences. Gauss was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1822.

In 1831, Gauss developed a fruitful collaboration with the physics professor Wilhelm Weber, leading to new knowledge in magnetism (including finding a representation for the unit of magnetism in terms of mass, charge, and time) and the discovery of Kirchhoff's circuit laws in electricity. It was during this time that he formulated his namesake law. They constructed the first electromechanical telegraph in 1833, which connected the observatory with the institute for physics in Göttingen. Gauss ordered a magnetic observatory to be built in the garden of the observatory, and with Weber founded the "Magnetischer Verein" ("magnetic association"), which supported measurements of Earth's magnetic field in many regions of the world. He developed a method of measuring the horizontal intensity of the magnetic field which was in use well into the second half of the 20th century, and worked out the mathematical theory for separating the inner and outer (magnetospheric) sources of Earth's magnetic field.

The British mathematician Henry John Stephen Smith (1826–1883) gave the following appraisal of Gauss:
There are several stories of his early genius. According to one, his gifts became very apparent at the age of three when he corrected, mentally and without fault in his calculations, an error his father had made on paper while calculating finances.

Another story has it that in primary school after the young Gauss misbehaved, his teacher, J.G. Büttner, gave him a task: add a list of integers in arithmetic progression; as the story is most often told, these were the numbers from 1 to 100. The young Gauss reputedly produced the correct answer within seconds, to the astonishment of his teacher and his assistant Martin Bartels.

Gauss's presumed method was to realize that pairwise addition of terms from opposite ends of the list yielded identical intermediate sums: 1 + 100 = 101, 2 + 99 = 101, 3 + 98 = 101, and so on, for a total sum of 50 × 101 = 5050.
However, the details of the story are at best uncertain (see for discussion of the original Wolfgang Sartorius von Waltershausen source and the changes in other versions); some authors, such as Joseph Rotman in his book "A first course in Abstract Algebra", question whether it ever happened.

He referred to mathematics as "the queen of sciences" and supposedly once espoused a belief in the necessity of immediately understanding Euler's identity as a benchmark pursuant to becoming a first-class mathematician.

From 1989 through 2001, Gauss's portrait, a normal distribution curve and some prominent Göttingen buildings were featured on the German ten-mark banknote. The reverse featured the approach for Hanover. Germany has also issued three postage stamps honoring Gauss. One (no. 725) appeared in 1955 on the hundredth anniversary of his death; two others, nos. 1246 and 1811, in 1977, the 200th anniversary of his birth.

Daniel Kehlmann's 2005 novel "Die Vermessung der Welt", translated into English as "Measuring the World" (2006), explores Gauss's life and work through a lens of historical fiction, contrasting them with those of the German explorer Alexander von Humboldt. A film version directed by Detlev Buck was released in 2012.

In 2007 a bust of Gauss was placed in the Walhalla temple.

The numerous things named in honor of Gauss include:

In 1929 the Polish mathematician Marian Rejewski, who helped to solve the German Enigma cipher machine in December 1932, began studying actuarial statistics at Göttingen. At the request of his Poznań University professor, Zdzisław Krygowski, on arriving at Göttingen Rejewski laid flowers on Gauss's grave.

On 30 April 2018, Google honoured Gauss in his would-be 241st birthday with a Google Doodle showcased in Europe, Russia, Israel, Japan, Taiwan, parts of Southern and Central America and the United States.

Carl Friedrich Gauss, who also introduced the so-called Gaussian logarithms, sometimes gets confused with (1829–1915), a German geologist, who also published some well-known logarithm tables used up into the early 1980s.






</doc>
<doc id="6130" url="https://en.wikipedia.org/wiki?curid=6130" title="Cornish language">
Cornish language

Cornish ("") is a revived language that became extinct as a first language in the late 18th century. It is a member of the Brittonic Southwestern branch of the Celtic languages of the Indo-European language family, that was native to Cornwall in south-west England. A revival began in the early 20th century. Some have expressed the opinion that the language is an important part of Cornish identity, culture and heritage. Cornish is currently a recognised minority language under the European Charter for Regional or Minority Languages. It has a growing number of second language speakers. A few parents are inspired to create new first language speakers, by teaching their children the language from birth.

Along with Welsh and Breton, Cornish is descended directly from the Common Brittonic language spoken throughout much of Britain before the English language came to dominate. It was the main language of Cornwall for centuries until it was pushed westwards by English, maintaining close links with its sister language Breton, with which it was mutually intelligible until well into the Middle Ages. Cornish continued to function as a common community language in parts of Cornwall until the late 18th century and continued to be spoken in the home by some families into the 19th and possibly 20th centuries, overlapping the beginning of revival efforts. 

A process to revive the language was begun in the early 20th century, with a number of orthographical systems still in use, although an attempt was made to impose a Standard Written Form in 2008. In 2010, UNESCO announced that its former classification of the language as "extinct" was "no longer accurate". Since the revival of the language, some Cornish textbooks and works of literature have been published, and an increasing number of people are studying the language. Recent developments include Cornish music, independent films and children's books. A small number of people in Cornwall have been brought up to be bilingual native speakers, and the language is taught in schools. The first Cornish language crèche opened in 2010.

Cornish is one of the Brittonic languages, which constitute a branch of the Insular Celtic section of the Celtic language family. Brittonic also includes Welsh, Breton and the Cumbric language; the last is extinct. Scottish Gaelic, Irish and Manx are part of the separate Goidelic branch of Insular Celtic.

Joseph Loth viewed Cornish and Breton as being two dialects of the same language, claiming that "Middle Cornish is without doubt closer to Breton as a whole than the modern Breton dialect of Quiberon ["Kiberen"] is to that of Saint-Pol-de-Léon ["Kastell-Paol"]."

Cornish evolved from the Common Brittonic spoken throughout Britain south of the Firth of Forth during the British Iron Age and Roman period. As a result of westward Anglo-Saxon expansion, the Britons of the southwest were separated from those in modern-day Wales and Cumbria. Some scholars have proposed that this split took place after the Battle of Deorham in about 577. The western dialects eventually evolved into modern Welsh and the now extinct Cumbric, while Southwestern Brittonic developed into Cornish and Breton, the latter as a result of emigration to parts of the continent, known as Brittany over the following centuries.

The area controlled by the southwestern Britons was progressively reduced by the expansion of Wessex over the next few centuries. During the Old Cornish period (800–1200), the Cornish-speaking area was largely coterminous with modern-day Cornwall; the region of Devon was isolated by Wessex in 936 AD and many inhabitants fled to Cornwall or Brittany. The earliest written record of the Cornish language comes from this period; a 9th-century gloss in a Latin manuscript of ' by Boethius, which used the words '. The phrase means "it [the mind] hated the gloomy places". A much more substantial survival from Old Cornish is a Cornish-Latin glossary (the Vocabularium Cornicum or Cottonian Vocabulary) containing translations of around 300 words. The manuscript was widely thought to be in Old Welsh until the 1700s when it was identified as Cornish. At this time there was still little difference between Welsh and Cornish, and even fewer differences between Cornish and Breton, with some scholars arguing that the terms "Old Cornish" and "Old Breton" are merely geographical terms for the same language.

The Cornish language continued to flourish well through the Middle Cornish period (1200–1600), reaching a peak of about 39,000 speakers in the 13th century, after which the number started to decline. This period provided the bulk of traditional Cornish literature, which was used to reconstruct the language during its revival. Most important is the "Ordinalia", a cycle of three mystery plays, "Origo Mundi", "Passio Christi" and "Resurrexio Domini". Together these provide about 20,000 lines of text. Various plays were written by the canons of Glasney College, intended to educate the Cornish people about the Bible and the Celtic saints. From this period also is ' and the recently discovered '.

In the reign of Henry VIII, an account was given by Andrew Boorde in his 1542 "Boke of the Introduction of Knowledge". He states, ""

When Parliament passed the Act of Uniformity 1549, people in many areas of Cornwall did not speak or understand English. The intention of the Act was to replace worship in Latin with worship in English, which was known by the lawmakers not to be universally spoken throughout England. Instead of merely banning Latin, the Act was framed so as to enforce English. The Prayer Book Rebellion, which may also have been influenced by the retaliation of the English after the failed Cornish Rebellion of 1497, broke out, and was ruthlessly suppressed: over 4,000 people who protested against the imposition of an English prayer book were massacred by Edward VI's army. Their leaders were executed and the people suffered numerous reprisals.

The rebels' document claimed they wanted a return to the old religious services and ended, "We the Cornishmen (whereof certain of us understand no English) utterly refuse this new English [altered spelling]." Edward Seymour, Duke of Somerset, replied to the Cornishmen, inquiring as to why they should be offended by services in English when they had them in Latin, which they also did not understand.
Through many factors, including loss of life and the spread of English, the Prayer Book Rebellion proved a turning-point for the Cornish language. Peter Berresford-Ellis cites the years 1550–1650 as a century of immense damage for the language, and its decline can be traced to this period. In 1680, William Scawen wrote an essay describing 16 reasons for the decline of Cornish, among them the lack of a distinctive Cornish alphabet, the loss of contact between Cornwall and Brittany, the cessation of the miracle plays, loss of records in the Civil War, lack of a Cornish bible, and immigration to Cornwall.

By the middle of the 17th century, the language had retreated to Penwith and Kerrier, and transmission of the language to new generations had almost entirely ceased. In his "Survey of Cornwall", published in 1602, Richard Carew writes:[M]ost of the inhabitants can speak no word of Cornish, but very few are ignorant of the English; and yet some so affect their own, as to a stranger they will not speak it; for if meeting them by chance, you inquire the way, or any such matter, your answer shall be, "," "I [will] speak no Saxonage."
The Late Cornish period from 1578 to about 1800 has fewer sources of information on the language but they are more varied in nature. Written sources from this period are often spelled following English spelling conventions since the majority of writers of the time had had no exposure to Middle Cornish texts or the Cornish orthography within them, although after 1700 some writers began to adopt the orthography used by Edward Lhuyd in his Archaeologia Britannica, for example using the circumflex to denote long vowels. In 1776, William Bodinar, who had learnt Cornish from fishermen, wrote a letter in Cornish which was probably the last prose in the language. However, the last verse was the "Cranken Rhyme", written in the late 19th century by John Davey of Boswednack.

The last native speakers of Cornish are thought to have died by the end of the 18th century.

In the 18th and 19th centuries, there was academic interest in the language and in attempting to find the last speaker of Cornish. This academic interest, along with the beginning of the Celtic Revival in the late 19th century, provided the groundwork for a Cornish language revival movement.

In 1904, the Celtic language scholar and Cornish cultural activist Henry Jenner published "A Handbook of the Cornish Language". The publication of this book is often considered to be the point at which the revival movement started.

The revival focused on reconstructing and standardising the language, including coining new words for modern concepts, and creating educational material in order to teach Cornish to others. In 1929 Robert Morton Nance published his Unified Cornish system, based on the Middle Cornish literature while extending the attested vocabulary with forms based on Celtic roots also found in Breton and Welsh, publishing a dictionary in 1938. Nance's work became the basis of revived Cornish for most of the 20th century. However, as the revival grew in strength and focus shifted from written to spoken Cornish, Nance's stiff, archaic formulation of the language seemed less suitable for a spoken revival, and academic research into the traditional literature proved that the Unified system lacked some phonological distinctions.

In the 1980s, in response to dissatisfaction with Unified Cornish, Ken George published a new system, ("Common Cornish"). Like Unified Cornish, it retained a Middle Cornish base but implemented an orthography that aspired to be as phonemic as possible. It was subsequently adopted by the Cornish Language Board as well as by many Cornish speakers, but came under fierce criticism by academic linguists for its phonological base, as well as those who found its orthography too different from traditional Cornish spelling conventions. Also during this period, Richard Gendall created his Modern Cornish system (also known as "Revived Late Cornish"), which used Late Cornish as a basis, and Nicholas Williams published a revised version of Unified; however neither of these systems gained the popularity of Unified or Kemmyn.

The revival entered a period of factionalism and public disputes, with each orthography attempting to push the others aside. By the time that Cornish was recognised by the UK government under the European Charter for Regional or Minority Languages in 2002, it had become recognised that the existence of multiple orthographies was unsustainable with regards to using the language in education and public life, as none had achieved a wide consensus. A process of unification was set about which resulted in the creation of the public-body Cornish Language Partnership in 2005 and agreement on a Standard Written Form in 2008. In 2010 UNESCO altered its classification of Cornish, stating that its previous label of "extinct" was no longer accurate. This was seen by Cornish speakers as a milestone, turning the language from a state of undergoing revival, to having been revived.

Speakers of Cornish reside primarily in Cornwall, which has a population of 563,600 (2017 estimate). There are also some speakers living outside Cornwall, particularly in the countries of the Cornish diaspora, as well as other Celtic nations. Estimates of the number of Cornish speakers vary according to the definition of being a speaker, and is difficult to accurately determine due to the individualised nature of language take-up. Nevertheless, there is recognition that the number of Cornish speakers is growing. From before the 1980s to the end of the 20th century there was a sixfold increased in the number of speakers to around 300. One figure for the mean number of people who know a few basic words, such as knowing that "Kernow" means "Cornwall", was 300,000; the same survey gave the figure of people able to have simple conversations at 3,000.

The Cornish Language Strategy project commissioned research to provide quantitative and qualitative evidence for the number of Cornish speakers: due to the success of the revival project it was estimated that 2,000 people were fluent (surveyed in spring 2008), an increase from the estimated 300 people who spoke Cornish fluently suggested in a study by Kenneth MacKinnon in 2000.<ref name="BBC BBC/British Council"></ref>

Jenefer Lowe of the Cornish Language Partnership said in an interview with the BBC in 2010 that there were around 300 fluent speakers. Cornwall Council estimated in 2015 that there were 300–400 fluent speakers who used the language regularly, with 5,000 people having a basic conversational ability in the language.

A report on the 2011 Census published in 2013 by the Office for National Statistics placed the number of speakers at somewhere from 325 to 625 speakers. In 2017 the ONS released a freedom of information request based on the 2011 Census which placed the number of speakers at 557 people in England and Wales declared Cornish to be their main language, 464 of whom lived in Cornwall.

The Institute of Cornish Studies at the University of Exeter is working with the Cornish Language Partnership to study the Cornish language revival of the 20th Century, including the growth in number of speakers.

Cornish has no official status anywhere but, since 2002, it has been recognised as a minority language under the European Charter for Regional or Minority Languages. The Cornish Language Partnership promotes and develops the language in Cornwall.

Cornwall Council's policy is to support the language, in line with the European Charter. A motion was passed in November 2009 in which the council promoted the inclusion of Cornish, as appropriate and where possible, in council publications and on signs. This plan has drawn some criticism.

In October 2015, Cornwall Council announced that staff would be encouraged to use "basic words and phrases" in Cornish when dealing with the public.
In 2014 the Cornish people were recognised by the UK Government as a national minority under the Framework Convention for the Protection of National Minorities. The FCNM provides certain rights and protections to a national minority with regard to their minority language.

UNESCO's "Atlas of World Languages" classifies Cornish as "critically endangered". UNESCO has said that a previous classification of "extinct", which came under fierce criticism from Cornish speakers, "does not reflect the current situation for Cornish".

In 2016, British government funding for the Cornish language ceased, and responsibility transferred to Cornwall council.

The phonology of modern Cornish is based on a number of sources. The work of the linguist Edward Lhuyd who visited Cornwall in 1700 to record the language, as well as the modern Cornish dialect and accent of English, which got much of its intonation and sounds from the Cornish language, have provided a major source of input. Analysis of the traditional literature has also been used, as the Middle Cornish plays were often written in rhyming verse, and Late Cornish texts were written phonetically following English spelling conventions.

The grammar of Cornish shares with other Celtic languages a number of features which, while not unique, are unusual in an Indo-European context. The grammatical features most unfamiliar to English speakers of the language are the initial consonant mutations, the verb–subject–object word order, inflected prepositions, fronting of emphasised syntactic elements, and the use of two different forms for "to be". Cornish nouns belong to one of two grammatical genders, masculine and feminine, but are not inflected for case. Cornish has a variety of different endings to indicate the plural, and some nouns have a third collective form. Verbs are conjugated for tense and mood, which can be indicated either by inflection of the main verb, or by the use of auxiliary verbs. In Cornish vocabulary, a large number of the lexical items are language and culture specific. Examples of these include the Cornish word ', which means "mine waste" and the word ', which means "to mend fishing nets". ' and ' are different types of pastry cakes. ' is culture specific when referring to "a traditional Cornish dance get-together", while ' is a specific kind of ceremonial dance that takes place in Cornwall.

In contrast, Cornish translates the English noun, "book", as ' (= Welsh '), but "" can actually be translated into English as "book" or "volume" because it can be considered one in a set of books.

As in other Celtic languages, Cornish lacks a number of verbs that are commonly found in other languages. This includes modals and psych-verbs; examples "have", "like", "hate", "prefer", "must"/"have to", "make"/"compel to". These functions are instead fulfilled by periphrastic constructions involving a verb and various prepositional phrases.


The Celtic Congress and Celtic League are groups that advocate cooperation amongst the Celtic Nations in order to protect and promote Celtic languages and cultures, thus working in the interests of the Cornish language.

There have been films such as "", some televised, made entirely, or significantly, in the language. Some businesses use Cornish names.

According to sociolinguist Kenneth MacKinnon, Jenner wrote "There has never been a time when there has been no person in Cornwall without a knowledge of the Cornish language."

Cornish has significantly and durably affected Cornwall's place-names, as well as in Cornish surnames, and knowledge of the language helps the understanding of these ancient meanings. Cornish names are adopted for children, pets, houses and boats.

There is Cornish literature, in which poetry is the most important genre, particularly in oral form or as song or as traditional Cornish chants historically performed in marketplaces during religious holidays and public festivals and gatherings.

There are periodicals solely in the language such as the monthly ', ', and "". BBC Radio Cornwall has a news broadcast in Cornish, and sometimes has other programmes and features for learners and enthusiasts. Local newspapers such as the "Western Morning News" have articles in Cornish, and newspapers such as "The Packet", "The West Briton" and "The Cornishman" have also been known to have Cornish features. There is an online radio service in Cornish called , publishing a one-hour podcast each week, based on a magazine format. It includes music in Cornish as well as interviews and features.

The language has financial sponsorship from sources, including the Millennium Commission. A number of language organisations exist in Cornwall: (Our Language), the Cornish sub-group of the European Bureau for Lesser-Used Languages, , (the Cornish Language Board) and (the Cornish Language Fellowship)]. There are ceremonies, some ancient, some modern, which use the language or are entirely in the language.

Though estimations of the number of Cornish speakers vary, the speakers of Cornish today are thought to be around five hundred. Currently, Cornish is spoken by its speakers at home, outside the home, in the workplace, and at ritual ceremonies. Cornish is also being used in the arts. Revived Cornish is constructed on historical Cornish, so that the Cornish language develops. English language has had some effect in this development. Regardless of having "no concrete purpose during the twentieth century," the number of Cornish speakers has gradually increased.

The Celtic Congress and Celtic League are groups that advocate cooperation amongst the Celtic Nations in order to protect and promote Celtic languages and cultures, thus working in the interests of the Cornish language. Cornish has significantly and durably affected Cornwall's place-names, as well as in Cornish surnames, and knowledge of the language helps the understanding of these ancient meanings. Cornish names are adopted for children, pets, houses and boats. There are periodicals solely in the language such as the monthly "An Gannas", "An Gowsva", and "An Garrick". BBC Radio Cornwall has a news broadcast in Cornish, and sometimes has other programmes and features for learners and enthusiasts. Local newspapers such as the "Western Morning News" have articles in Cornish, and newspapers such as "The Packet", "The West Briton" and "The Cornishman" have also been known to have Cornish features. The language has financial sponsorship from sources, including the Millennium Commission. A number of language organisations exist in Cornwall: Agan Tavas (Our Language), the Cornish sub-group of the European Bureau for Lesser-Used Languages, Gorsedh Kernow, Kesva an Taves Kernewek (the Cornish Language Board) and Kowethas an Yeth Kernewek (the Cornish Language Fellowship).

Cornwall has had cultural events associated with the language, including the international Celtic Media Festival, hosted in St Ives in 1997. The Old Cornwall Society has promoted the use of the language at events and meetings. Two examples of ceremonies that are performed in both the English and Cornish languages are Crying the Neck and the annual mid-summer bonfires.

Cornish is taught in some schools; it was previously taught at degree level at the University of Wales, though the only existing course in the language at University level is as part of a course in Cornish Studies at the University of Exeter. In March 2008, a course in the language was started as part of the Celtic Studies curriculum at the University of Vienna, Austria.
The University of Cambridge offers courses in Cornish through its John Trim Resources Centre, which is part of its Language Centre. In addition, the Department of Anglo-Saxon, Norse and Celtic (which is part of the faculty of English), also carries out research into the Cornish language.

In 2015 a university level course aiming to encourage and support practitioners working with young children to introduce the Cornish language into their settings was launched. The "Cornish Language Practice Project (Early Years)" is a level 4 course approved by Plymouth University and run at Cornwall College. The course is not a Cornish language course, but students will be assessed on their ability to use the Cornish language constructively in their work with young children. The course will cover such topics as "Understanding Bilingualism", "Creating Resources" and "Integrating Language and Play", but the focus of the language provision will be on Cornish. A non-accredited specialist Cornish language course has been developed to run alongside the level 4 course for those who prefer tutor support to learn the language or develop their skills further for use with young children.

Cornwall's first Cornish language crèche, "", was established in 2010 at Cornwall College, Camborne. The nursery teaches children aged between two and five years alongside their parents to ensure the language is also spoken in the home.

A number of dictionaries are available in the different orthographies (a dictionary in the Standard Written Form has yet to be published), including ' by Ken George, ' by Nicholas Williams and "A Practical Dictionary of Modern Cornish" by Richard Gendall. Course books include the three-part ' series, ', ' and ', as well as the more recent ' and '.

Classes and conversation groups for adults are available at several locations in Cornwall, as well as in London, Cardiff and Bristol.

William Scawen produced a manuscript on the declining Cornish language that continually evolved until he died in 1689, aged 89. He was the first person to realise the language was dying out and wrote detailed manuscripts which he started working on when he was 78. The only version that was ever published was a short first draft, but the final version, which he worked on until his death, is a few hundred pages long. At the same time a group of scholars, led by John Keigwin (nephew of William Scawen), of Mousehole, tried to preserve and further the Cornish language. They left behind a large number of translations of parts of the Bible, proverbs and songs. This group was contacted by the Welsh linguist Edward Lhuyd who came to Cornwall to study the language.

Early Modern Cornish was the subject of a study published by Lhuyd in 1707, and differs from the medieval language in having a considerably simpler structure and grammar. Such differences included the wide use of certain modal affixes that, although out of use by Lhuyd's time, had a considerable effect on the word-order of medieval Cornish. The medieval language also possessed two additional tenses for expressing past events and an extended set of possessive suffixes.

John Whitaker, the Manchester-born rector of Ruan Lanihorne, studied the decline of the Cornish language. In his 1804 work "the Ancient Cathedral of Cornwall" he concluded that: "[T]he English Liturgy, was not desired by the Cornish, but forced upon them by the tyranny of England, at a time when the English language was yet unknown in Cornwall. This act of tyranny was at once gross barbarity to the Cornish people, and a death blow to the Cornish language."

Robert Williams published the first comprehensive Cornish dictionary in 1865, the "Lexicon Cornu-Britannicum". As a result of the discovery of additional ancient Cornish manuscripts, 2000 new words were added to the vocabulary by Whitley Stokes in "A Cornish Glossary". William C. Borlase published "Proverbs and Rhymes in Cornish" in 1866 while "A Glossary of Cornish Names" was produced by John Bannister in the same year. Frederick Jago published his "English–Cornish Dictionary" in 1882.

In 2002, the Cornish language gained new recognition because of the European Charter for Regional and Minority Languages. Conversely, along with government provision was the governmental basis of "New Public Management," measuring quantifiable results as means of determining effectiveness. This put enormous pressure on finding a single orthography that could be used in unison. The revival of Cornish required extensive rebuilding. The Cornish orthographies that were reconstructed may be considered versions of Cornish because they are not traditional sociolinguistic variations. In the middle-to-late twentieth century, the debate over Cornish orthographies angered more people because several language groups received public funding. This caused other groups to sense favouritism as playing a role in the debate.

A governmental policymaking structure called New Public Management (NPM) has helped the Cornish language by managing public life of the Cornish language and people. In 2007, the Cornish Language Partnership MAGA represents separate divisions of government and their purpose is to further enhance the Cornish Language Developmental Plan. MAGA established an "Ad-Hoc Group," which resulted in three orthographies being presented. The relations for the Ad-Hoc Group were to obtain consensus among the three orthographies, and then develop a "single written form." The end result was creating a new form of Cornish, which had to be natural for both new learners and skilled speakers.

In 1981, the Breton library ' edited ' (Passion of our lord), a 15th-century Cornish poem. The first complete translation of the Bible into Cornish, translated from English, was published in 2011. Another Bible translation project translating from original languages is underway. The New Testament and Psalms were posted on-line on YouVersion (Bible.com) and Bibles.org in July 2014 by the Bible Society.

A few small publishers produce books in Cornish which are stocked in some local bookshops, as well as in Cornish branches of Waterstones and WH Smiths, although newer publications are becoming increasingly available on the Internet. The Truro Waterstones hosts the annual "" literary awards, established by to recognise publications relating to Cornwall or in the Cornish language. In recent years, a number of Cornish translations of literature have been published, including "Alice's Adventures in Wonderland" (2009), "Around the World in Eighty Days" (2009), "Treasure Island" (2010), "The Railway Children" (2012), "Hound of the Baskervilles" (2012), "The War of the Worlds" (2012), "The Wind in the Willows" (2013), "Three Men in a Boat" (2013), "Alice in Wonderland and Through the Looking-Glass" (2014), and "A Christmas Carol" (which won the 2012 award for Cornish Language books), as well as original Cornish literature such as ' ("The Lyonesse Stone") by Craig Weatherhill. Literature aimed at children is also available, such as ' ("Where's Spot?"), ' ("The Beast of Bodmin Moor"), three "Topsy and Tim" titles, two "Tintin" titles and ' ("Briallen and the Alien"), which won the 2015 award for Cornish Language books for children. In 2014 "", Nicholas Williams' translation of J.R.R. Tolkien's "The Hobbit" was published.

An Gannas is a monthly magazine published entirely in the Cornish language. Members contribute articles on various subjects. The magazine is produced by Graham Sandercock who has been its editor since 1976.

In 1983 BBC Radio Cornwall started broadcasting around two minutes of Cornish every week. In 1987, however, they gave over 15 minutes of airtime on Sunday mornings for a programme called ' ("Holdall"), presented by John King, running until the early 1990s. It was eventually replaced with a five-minute news bulletin called ' ("The News"). The bulletin was presented every Sunday evening for many years by Rod Lyon, then Elizabeth Stewart, and currently a team presents in rotation. Pirate FM ran short bulletins on Saturday lunchtimes from 1998 to 1999. In 2006, Matthew Clarke who had presented the Pirate FM bulletin, launched a web-streamed news bulletin called ' ("Weekly News"), which in 2008 was merged into a new weekly magazine podcast ' (RanG).

Cornish television shows have included a 1982 series by Westward Television each episode containing a three-minute lesson in Cornish. ', an eight episode series produced by Television South West and broadcast between June and July 1984, later on S4C from May to July 1985, and as a schools programme in 1986. Also by Television South West were two bilingual programmes on Cornish Culture called '
In 2016 Kelly's Ice Cream of Bodmin introduced a light hearted television commercial in the Cornish language and this has been repeated in 2017.

The first episode from the third season of the US television program Deadwood features a conversation between miners in the Cornish language, including a toast of shots using "sláinte", a term also common in the Irish language. One of the miners is then shot by thugs working for businessman George Hearst and justifies the murder by saying, "He come at me with his foreign gibberish."

English composer Peter Warlock wrote a Christmas carol in Cornish (setting words by Henry Jenner). Cornish musician Jory Bennett has composed "Six Songs of Cornwall" for bass and piano, a Cornish song-cycle, settings of Cornish language poems by Nicholas Williams /trans. E. G. Retallack Hooper (f.p. Keele University, 7 May 1986). The Cornish electronic musician Aphex Twin has used Cornish names for track titles, most notably on his "DrukQs" album. 

Attempts have been made to recreate, in order to preserve, some Cornish folk songs, including "An Awhesyth", "Bro Goth agan Tasow", and "Delkiow Sivy".

In 2018, the singer Gwenno Saunders released an album in Cornish, entitled "Le Kov".

The Cornish language has influenced the toponomy of Cornwall, and has historically been used in surnames for the Cornish people. Long before the agreement of the Standard Written Form of Cornish in the 21st century, Late Cornish orthography in the Early Modern period usually followed Welsh to English transliteration so phonetically rendering C for K, I for Y, U for W, and Z for S, caused place-names such as Porthcurno and Penzance to be adopted into English instead of their Standard Written Form "Porth Kernow" and "Pen Sans". Likewise, words such as Enys ("island") can be found spelled as "Ince" as at Ince Castle. These apparent mistranslations can however reveal an insight into how names and places were actually pronounced, explaining, for example, how anglicised Launceston is still pronounced "Lann-zan" from Cornish "Lann Stefan" (though with emphasis on the first element, the result of accent shift in English, and a consonant change s > z which again would have occurred in English in this alternative English form of the name).

The following tables present some examples of Cornish place-names and surnames, and their anglicised versions:

From the Universal Declaration of Human Rights:

From ", the Cornish anthem:






</doc>
<doc id="6132" url="https://en.wikipedia.org/wiki?curid=6132" title="Complexity theory">
Complexity theory

Complexity theory (or complexity science) is the study of complexity and of complex systems. It may also refer to:



</doc>
<doc id="6134" url="https://en.wikipedia.org/wiki?curid=6134" title="Charybdis">
Charybdis

Charybdis (; Ancient Greek: Χάρυβδις, , "Kharubdis") was a sea monster in the Greek Mythology, which was later rationalized as a whirlpool and considered a shipping hazard in the Strait of Messina.

The sea monster Charybdis was believed to live under a small rock on one side of a narrow channel. Opposite her was Scylla, another sea monster, that lived inside a much larger rock. The sides of the strait were within an arrow-shot of each other, and sailors attempting to avoid one of them would come in reach of the other. To be "between Scylla and Charybdis" therefore means to be presented with two opposite dangers, the task being to find a route that avoids both. Three times a day, Charybdis swallowed a huge amount of water, before belching it back out again, creating large whirlpools capable of dragging a ship underwater. In some variations of the story, Charybdis was simply a large whirlpool instead of a sea monster.

The theoretical size of Charybdis remains unknown, yet in order to consume Greek ships the whirlpool can be estimated to about 23 meters (75 ft) across. Charybdis has been associated with the Strait of Messina, off the coast of Sicily and opposite a rock on the mainland identified with Scylla. Were Charybdis to be located in the Strait of Messina, it would in fact have the size to accommodate the whirlpool. A whirlpool does exist there, caused by currents meeting, but it is dangerous only to small craft in extreme conditions.

A later myth makes Charybdis the daughter of Poseidon and Gaia and living as a loyal servant to her father.

Charybdis aided her father Poseidon in his feud with her paternal uncle Zeus and, as such, helped him engulf lands and islands in water. Zeus, angry over the land she stole from him, captured and chained her to the sea-bed. Charybdis was then cursed by the god into a hideous bladder of a monster, with flippers for arms and legs, and an uncontrollable thirst for the sea. As such, she drank the water from the sea thrice a day to quench it, which created whirlpools. She lingered on a rock with Scylla facing her directly on another rock, making a strait.

 Odysseus faced both Charybdis and Scylla while rowing through a narrow channel. He ordered his men to avoid Charybdis, thus forcing them to pass near Scylla, which resulted in the deaths of six of his men. Later, stranded on a raft, Odysseus was swept back through the strait and passed near Charybdis. His raft was sucked into her maw, but he survived by clinging to a fig tree growing on a rock over her lair. On the next outflow of water, when his raft was expelled, Odysseus recovered it and paddled away safely.

The Argonauts were able to avoid both dangers because Hera ordered the Nereid nymph Thetis, Achilles' mother, to guide them through the perilous passage.

Aristotle mentions in his "Meteorologica" that Aesop once teased a ferryman by telling him a myth concerning Charybdis. With one gulp of the sea, she brought the mountains to view; islands appeared after the next. The third is yet to come and will dry the sea altogether, thus depriving the ferryman of his livelihood.



</doc>
<doc id="6136" url="https://en.wikipedia.org/wiki?curid=6136" title="Carbon monoxide">
Carbon monoxide

Carbon monoxide (CO) is a colorless, odorless, and tasteless flammable gas that is slightly less dense than air. It is toxic to animals that use hemoglobin as an oxygen carrier (both invertebrate and vertebrate) when encountered in concentrations above about 35 ppm, although it is also produced in normal animal metabolism in low quantities, and is thought to have some normal biological functions. In the atmosphere, it is spatially variable and short lived, having a role in the formation of ground-level ozone.
Carbon monoxide consists of one carbon atom and one oxygen atom, connected by a triple bond that consists of two covalent bonds as well as one dative covalent bond. It is the simplest oxocarbon and is isoelectronic with other triply-bonded diatomic molecules having ten valence electrons, including the cyanide anion, the nitrosonium cation and molecular nitrogen. In coordination complexes the carbon monoxide ligand is called carbonyl.

Aristotle (384–322 BC) first recorded that burning coals produced toxic fumes. An ancient method of execution was to shut the criminal in a bathing room with smoldering coals. What was not known was the mechanism of death. Greek physician Galen (129–199 AD) speculated that there was a change in the composition of the air that caused harm when inhaled. In 1776, the French chemist produced CO by heating zinc oxide with coke, but mistakenly concluded that the gaseous product was hydrogen, as it burned with a blue flame. The gas was identified as a compound containing carbon and oxygen by the Scottish chemist William Cruikshank in 1800. Its toxic properties on dogs were thoroughly investigated by Claude Bernard around 1846.

During World War II, a gas mixture including carbon monoxide was used to keep motor vehicles running in parts of the world where gasoline and diesel fuel were scarce. External (with a few exceptions) charcoal or wood gas generators were fitted, and the mixture of atmospheric nitrogen, hydrogen, carbon monoxide, and small amounts of other gases produced by gasification was piped to a gas mixer. The gas mixture produced by this process is known as wood gas. Carbon monoxide was also used on a large scale during the Holocaust at some Nazi German extermination camps, the most notable by gas vans in Chełmno, and in the Action T4 "euthanasia" program.

Carbon monoxide is produced from the partial oxidation of carbon-containing compounds; it forms when there is not enough oxygen to produce carbon dioxide (CO), such as when operating a stove or an internal combustion engine in an enclosed space. In the presence of oxygen, including atmospheric concentrations, carbon monoxide burns with a blue flame, producing carbon dioxide. Coal gas, which was widely used before the 1960s for domestic lighting, cooking, and heating, had carbon monoxide as a significant fuel constituent. Some processes in modern technology, such as iron smelting, still produce carbon monoxide as a byproduct. A large quantity of CO byproduct is formed during the oxidative processes for the production of chemicals. For this reason, the process off-gases have to be purified. On the other hand, considerable research efforts are made in order to optimize the process conditions, develop catalyst with improved selectivity and to understand the reaction pathways leading to the target product and side products.

Worldwide, the largest source of carbon monoxide is natural in origin, due to photochemical reactions in the troposphere that generate about 5 kilograms per year. Other natural sources of CO include volcanoes, forest fires, other forms of combustion, and carbon monoxide-releasing molecules.

In biology, carbon monoxide is naturally produced by the action of heme oxygenase 1 and 2 on the heme from hemoglobin breakdown. This process produces a certain amount of carboxyhemoglobin in normal persons, even if they do not breathe any carbon monoxide. Following the first report that carbon monoxide is a normal neurotransmitter in 1993, as well as one of three gases that naturally modulate inflammatory responses in the body (the other two being nitric oxide and hydrogen sulfide), carbon monoxide has received a great deal of clinical attention as a biological regulator. In many tissues, all three gases are known to act as anti-inflammatories, vasodilators, and promoters of neovascular growth. Clinical trials of small amounts of carbon monoxide as a drug are ongoing. Too much carbon monoxide causes carbon monoxide poisoning.

Carbon monoxide has a molar mass of 28.0, which, according to the ideal gas law, makes it slightly less dense than air, whose average molar mass is 28.8.

The bond length between the carbon atom and the oxygen atom is 112.8 pm. This bond length is consistent with a triple bond, as in molecular nitrogen (N), which has a similar bond length (109.76 pm) and nearly the same molecular mass. Carbon–oxygen double bonds are significantly longer, 120.8 pm in formaldehyde, for example. The boiling point (82 K) and melting point (68 K) are very similar to those of N (77 K and 63 K, respectively). The bond-dissociation energy of 1072 kJ/mol is stronger than that of N (942 kJ/mol) and represents the strongest chemical bond known.

The ground electronic state of carbon monoxide is a singlet state since there are no unpaired electrons.

The carbon monoxide has a very high bond-dissociation energy, the strongest of any neutral molecule, 11.65 eV. Carbon and oxygen together have a total of 10 electrons in the valence shell. Following the octet rule for both carbon and oxygen, the two atoms form a triple bond, with six shared electrons in three bonding molecular orbitals, rather than the usual double bond found in organic carbonyl compounds. Since four of the shared electrons come from the oxygen atom and only two from carbon, one bonding orbital is occupied by two electrons from oxygen, forming a dative or dipolar bond. This causes a C←O polarization of the molecule, with a small negative charge on carbon and a small positive charge on oxygen. The other two bonding orbitals are each occupied by one electron from carbon and one from oxygen, forming (polar) covalent bonds with a reverse C→O polarization, since oxygen is more electronegative than carbon. In the free carbon monoxide, a net negative charge δ remains at the carbon end and the molecule has a small dipole moment of 0.122 D.

The molecule is therefore asymmetric: oxygen has more electron density than carbon, and is also slightly positively charged compared to carbon being negative. By contrast, the isoelectronic dinitrogen molecule has no dipole moment.

Carbon monoxide has a computed fractional bond order of 2.6, indicating that the "third" bond is important but constitutes somewhat less than a full bond. Thus, in valence bond terms, C≡O is the most important structure, while :C=O is non-octet, but has a neutral formal charge on each atom and represents the second most important resonance contributor. Because of the lone pair and divalence of carbon in this resonance structure, carbon monoxide is often considered to be an extraordinarily stabilized carbene. Isocyanides are compounds in which the O is replaced by an NR (R = alkyl or aryl) group and have a similar bonding scheme.

If carbon monoxide acts as a ligand, the polarity of the dipole may reverse with a net negative charge on the oxygen end, depending on the structure of the coordination complex.
See also the section ""Coordination chemistry"" below.

Theoretical and experimental studies show that, despite the greater electronegativity of oxygen, the dipole moment points from the more-negative carbon end to the more-positive oxygen end. The three bonds are in fact polar covalent bonds that are strongly polarized. The calculated polarization toward the oxygen atom is 71% for the σ-bond and 77% for both π-bonds.

The oxidation state of carbon in carbon monoxide is +2 in each of these structures. It is calculated by counting all the bonding electrons as belonging to the more electronegative oxygen. Only the two non-bonding electrons on carbon are assigned to carbon. In this count, carbon then has only two valence electrons in the molecule compared to four in the free atom.

Carbon monoxide poisoning is the most common type of fatal air poisoning in many countries. Carbon monoxide is colorless, odorless, and tasteless, but highly toxic. It combines with hemoglobin to produce carboxyhemoglobin, which usurps the space in hemoglobin that normally carries oxygen, but is ineffective for delivering oxygen to bodily tissues. Concentrations as low as 667 ppm may cause up to 50% of the body's hemoglobin to convert to carboxyhemoglobin. A level of 50% carboxyhemoglobin may result in seizure, coma, and fatality. In the United States, the OSHA limits long-term workplace exposure levels above 50 ppm.

The most common symptoms of carbon monoxide poisoning may resemble other types of poisonings and infections, including symptoms such as headache, nausea, vomiting, dizziness, fatigue, and a feeling of weakness. Affected families often believe they are victims of food poisoning. Infants may be irritable and feed poorly. Neurological signs include confusion, disorientation, visual disturbance, syncope (fainting), and seizures.

Some descriptions of carbon monoxide poisoning include retinal hemorrhages, and an abnormal cherry-red blood hue. In most clinical diagnoses these signs are seldom noticed. One difficulty with the usefulness of this cherry-red effect is that it corrects, or masks, what would otherwise be an unhealthy appearance, since the chief effect of removing deoxygenated hemoglobin is to make an asphyxiated person appear more normal, or a dead person appear more lifelike, similar to the effect of red colorants in embalming fluid. The "false" or unphysiologic red-coloring effect in anoxic CO-poisoned tissue is related to the meat-coloring commercial use of carbon monoxide, discussed below.

Carbon monoxide also binds to other molecules such as myoglobin and mitochondrial cytochrome oxidase. Exposures to carbon monoxide may cause significant damage to the heart and central nervous system, especially to the globus pallidus, often with long-term chronic pathological conditions. Carbon monoxide may have severe adverse effects on the fetus of a pregnant woman.

Carbon monoxide is produced naturally by the human body as a signaling molecule. Thus, carbon monoxide may have a physiological role in the body, such as a neurotransmitter or a blood vessel relaxant. Because of carbon monoxide's role in the body, abnormalities in its metabolism have been linked to a variety of diseases, including neurodegenerations, hypertension, heart failure, and pathological inflammation. Relative to inflammation, carbon monoxide has been shown to inhibit the movement of leukocytes to inflamed tissues, stimulate leukocyte phagocytosis of bacteria, and reduce the production of pro-inflammatory cytokines by leukocytes. In animal model studies, furthermore, carbon monoxide reduced the severity of experimentally induced bacterial sepsis, pancreatitis, hepatic ischemia/reperfusion injury, colitis, osteoarthritis, lung injury, lung transplantation rejection, and neuropathic pain while promoting skin wound healing. These actions are similar to those of Specialized pro-resolving mediators which act to dampen, reverse, and repair the tissue damage due to diverse inflammation responses. Indeed, carbon monoxide can act additively with one of these mediators (Resolvin D1) to limit inflammatory responses. The studies implicate carbon monoxide as a physiological contributor to limiting inflammation and suggest that its delivery by inhalation or carbon monoxide-forming drugs may be therapeutically useful for controlling pathological inflammatory responses. 
CO functions as an endogenous signaling molecule, modulates functions of the cardiovascular system, inhibits blood platelet aggregation and adhesion, suppresses, reverses, and repairs the damage caused by inflammatory responses.
It may play a role as potential therapeutic agent.

Carbon monoxide is a nutrient for methanogenic archaea, which reduce it to methane using hydrogen. This is the theme for the emerging field of bioorganometallic chemistry. Extremophile micro-organisms can, thus, utilize carbon monoxide in such locations as the thermal vents of volcanoes.

Some microbes can convert carbon monoxide to carbon dioxide to yield energy.

In bacteria, carbon monoxide is produced via the reduction of carbon dioxide by the enzyme carbon monoxide dehydrogenase, an Fe-Ni-S-containing protein.

CooA is a carbon monoxide sensor protein. The scope of its biological role is still unknown; it may be part of a signaling pathway in bacteria and archaea. Its occurrence in mammals is not established.

Carbon monoxide occurs in various natural and artificial environments. Typical concentrations in parts per million are as follows:

Carbon monoxide (CO) is present in small amounts (about 80 ppb) in the Earth's atmosphere. About half of the carbon monoxide in Earth's atmosphere is from the burning of fossil fuels and biomass (such as forest and bushfires). Most of the rest of carbon monoxide comes from chemical reactions with organic compounds emitted by human activities and plants. Small amounts are also emitted from the ocean, and from geological activity because carbon monoxide occurs dissolved in molten volcanic rock at high pressures in the Earth's mantle. Because natural sources of carbon monoxide are so variable from year to year, it is difficult to accurately measure natural emissions of the gas.

Carbon monoxide has an indirect effect on radiative forcing by elevating concentrations of direct greenhouse gases, including methane and tropospheric ozone. CO can react chemically with other atmospheric constituents (primarily the hydroxyl radical, OH) that would otherwise destroy methane. Through natural processes in the atmosphere, it is eventually oxidized to carbon dioxide and ozone. Carbon monoxide is both short-lived in the atmosphere (with an average lifetime of about one to two months) and spatially variable in concentration.

In the atmosphere of Venus carbon monoxide occurs as a result of the photodissociation of carbon dioxide by electromagnetic radiation of wavelengths shorter than 169 nm.

Due to its long lifetime in the mid-troposphere, carbon monoxide is also used as tracer of transport for pollutant plumes.

Carbon monoxide is a temporary atmospheric pollutant in some urban areas, chiefly from the exhaust of internal combustion engines (including vehicles, portable and back-up generators, lawn mowers, power washers, etc.), but also from incomplete combustion of various other fuels (including wood, coal, charcoal, oil, paraffin, propane, natural gas, and trash).

Large CO pollution events can be observed from space over cities.

Carbon monoxide is, along with aldehydes, part of the series of cycles of chemical reactions that form photochemical smog. It reacts with hydroxyl radical (OH) to produce a radical intermediate HOCO, which transfers rapidly its radical hydrogen to O to form peroxy radical (HO) and carbon dioxide (CO). Peroxy radical subsequently reacts with nitrogen oxide (NO) to form nitrogen dioxide (NO) and hydroxyl radical. NO gives O(P) via photolysis, thereby forming O following reaction with O.
Since hydroxyl radical is formed during the formation of NO, the balance of the sequence of chemical reactions starting with carbon monoxide and leading to the formation of ozone is:

Although the creation of NO is the critical step leading to low level ozone formation, it also increases this ozone in another, somewhat mutually exclusive way, by reducing the quantity of NO that is available to react with ozone.

In closed environments, the concentration of carbon monoxide can easily rise to lethal levels. On average, 170 people in the United States die every year from carbon monoxide produced by non-automotive consumer products. However, according to the Florida Department of Health, "every year more than 500 Americans die from accidental exposure to carbon monoxide and thousands more across the U.S. require emergency medical care for non-fatal carbon monoxide poisoning" These products include malfunctioning fuel-burning appliances such as furnaces, ranges, water heaters, and gas and kerosene room heaters; engine-powered equipment such as portable generators; fireplaces; and charcoal that is burned in homes and other enclosed areas. The American Association of Poison Control Centers (AAPCC) reported 15,769 cases of carbon monoxide poisoning resulting in 39 deaths in 2007. In 2005, the CPSC reported 94 generator-related carbon monoxide poisoning deaths. Forty-seven of these deaths were known to have occurred during power outages due to severe weather, including Hurricane Katrina. Still others die from carbon monoxide produced by non-consumer products, such as cars left running in attached garages. The Centers for Disease Control and Prevention estimates that several thousand people go to hospital emergency rooms every year to be treated for carbon monoxide poisoning.

Carbon monoxide is absorbed through breathing and enters the blood stream through gas exchange in the lungs. It is also produced in heme catabolism and enters the blood from the tissues, and thus is present in all normal tissues, even if not inhaled.

Normal circulating levels in the blood are 0% to 3% saturation, i.e. the ratio of the amount of carboxyhaemoglobin present to the total circulating haemoglobin, and are higher in smokers. Carbon monoxide levels cannot be assessed through a physical exam. Laboratory testing requires a blood sample (arterial or venous) and laboratory analysis on a CO-Oximeter. Additionally, a noninvasive carboxyhemoglobin (SpCO) test method from Pulse CO-Oximetry exists and has been validated compared to invasive methods.

Outside of Earth, carbon monoxide is the second-most common molecule in the interstellar medium, after molecular hydrogen. Because of its asymmetry, this polar molecule produces far brighter spectral lines than the hydrogen molecule, making CO much easier to detect. Interstellar CO was first detected with radio telescopes in 1970. It is now the most commonly used tracer of molecular gas in general in the interstellar medium of galaxies, as molecular hydrogen can only be detected using ultraviolet light, which requires space telescopes. Carbon monoxide observations provide much of the information about the molecular clouds in which most stars form.

Beta Pictoris, the second brightest star in the constellation Pictor, shows an excess of infrared emission compared to normal stars of its type, which is caused by large quantities of dust and gas (including carbon monoxide) near the star.

Solid carbon monoxide is a component of comets. Halley's Comet is about 15% carbon monoxide. It has also been identified spectroscopy on the surface of Neptune's moon Triton. At room temperature and at atmospheric pressure carbon monoxide is actually only metastable (see Boudouard reaction) and the same is true at low temperatures where CO and are solid, but nevertheless it can exist for billions of years in comets. However, there is very little CO in the atmosphere of Pluto, which seems to have been formed from comets. This may be because there is (or was) liquid water inside Pluto. Carbon monoxide can react with water to form carbon dioxide and hydrogen:

This is called the water-gas shift reaction when occurring in the gas phase, but it can also take place (very slowly) in aqueous solution.
If the hydrogen partial pressure is high enough (for instance in an underground sea), formic acid will be formed:

These reactions can take place in only a few million years even at temperatures such as found at Pluto.

Miners refer to carbon monoxide as "white damp" or the "silent killer". It can be found in confined areas of poor ventilation in both surface mines and underground mines. The most common sources of carbon monoxide in mining operations are the internal combustion engine and explosives, however in coal mines carbon monoxide can also be found due to the low temperature oxidation of coal.

Many methods have been developed for carbon monoxide's production.

A major industrial source of CO is producer gas, a mixture containing mostly carbon monoxide and nitrogen, formed by combustion of carbon in air at high temperature when there is an excess of carbon. In an oven, air is passed through a bed of coke. The initially produced CO equilibrates with the remaining hot carbon to give CO. The reaction of CO with carbon to give CO is described as the Boudouard reaction. Above 800 °C, CO is the predominant product:

Another source is "water gas", a mixture of hydrogen and carbon monoxide produced via the endothermic reaction of steam and carbon:

Other similar "synthesis gases" can be obtained from natural gas and other fuels.

Carbon monoxide can also be produced by high-temperature electrolysis of carbon dioxide with solid oxide electrolyzer cells:

Carbon monoxide is also a byproduct of the reduction of metal oxide ores with carbon, shown in a simplified form as follows:

Carbon monoxide is also produced by the direct oxidation of carbon in a limited supply of oxygen or air.

Since CO is a gas, the reduction process can be driven by heating, exploiting the positive (favorable) entropy of reaction. The Ellingham diagram shows that CO formation is favored over CO in high temperatures.

Carbon monoxide is conveniently produced in the laboratory by the dehydration of formic acid or oxalic acid, for example with concentrated sulfuric acid. Another method is heating an intimate mixture of powdered zinc metal and calcium carbonate, which releases CO and leaves behind zinc oxide and calcium oxide:

Silver nitrate and iodoform also afford carbon monoxide:

Finally, metal oxalate salts release CO upon heating, leaving a carbonate as byproduct:

Most metals form coordination complexes containing covalently attached carbon monoxide. Only metals in lower oxidation states will complex with carbon monoxide ligands. This is because there must be sufficient electron density to facilitate back-donation from the metal d-orbital, to the π* molecular orbital from CO. The lone pair on the carbon atom in CO, also donates electron density to the d on the metal to form a sigma bond. This electron donation is also exhibited with the cis effect, or the labilization of CO ligands in the cis position. Nickel carbonyl, for example, forms by the direct combination of carbon monoxide and nickel metal:
For this reason, nickel in any tubing or part must not come into prolonged contact with carbon monoxide. Nickel carbonyl decomposes readily back to Ni and CO upon contact with hot surfaces, and this method is used for the industrial purification of nickel in the Mond process.

In nickel carbonyl and other carbonyls, the electron pair on the carbon interacts with the metal; the carbon monoxide donates the electron pair to the metal. In these situations, carbon monoxide is called the carbonyl ligand. One of the most important metal carbonyls is iron pentacarbonyl, Fe(CO):

Many metal-CO complexes are prepared by decarbonylation of organic solvents, not from CO. For instance, iridium trichloride and triphenylphosphine react in boiling 2-methoxyethanol or DMF to afford IrCl(CO)(PPh).

Metal carbonyls in coordination chemistry are usually studied using infrared spectroscopy.

In the presence of strong acids and water, carbon monoxide reacts with alkenes to form carboxylic acids in a process known as the Koch–Haaf reaction. In the Gattermann–Koch reaction, arenes are converted to benzaldehyde derivatives in the presence of AlCl and HCl. Organolithium compounds (e.g. butyl lithium) react with carbon monoxide, but these reactions have little scientific use.

Although CO reacts with carbocations and carbanions, it is relatively nonreactive toward organic compounds without the intervention of metal catalysts.

With main group reagents, CO undergoes several noteworthy reactions. Chlorination of CO is the industrial route to the important compound phosgene. With borane CO forms the adduct HBCO, which is isoelectronic with the acetylium cation [HCCO]. CO reacts with sodium to give products resulting from C-C coupling such as sodium acetylenediolate 2·. It reacts with molten potassium to give a mixture of an organometallic compound, potassium acetylenediolate 2·, potassium benzenehexolate 6 , and potassium rhodizonate 2·.

The compounds cyclohexanehexone or triquinoyl (CO) and cyclopentanepentone or leuconic acid (CO), which so far have been obtained only in trace amounts, can be regarded as polymers of carbon monoxide.

At pressures of over 5 gigapascals, carbon monoxide converts into a solid polymer of carbon and oxygen. This is metastable at atmospheric pressure but is a powerful explosive.

Carbon monoxide is an industrial gas that has many applications in bulk chemicals manufacturing. Large quantities of aldehydes are produced by the hydroformylation reaction of alkenes, carbon monoxide, and H. Hydroformylation is coupled to the Shell higher olefin process to give precursors to detergents.

Phosgene, useful for preparing isocyanates, polycarbonates, and polyurethanes, is produced by passing purified carbon monoxide and chlorine gas through a bed of porous activated carbon, which serves as a catalyst. World production of this compound was estimated to be 2.74 million tonnes in 1989.
Methanol is produced by the hydrogenation of carbon monoxide. In a related reaction, the hydrogenation of carbon monoxide is coupled to C-C bond formation, as in the Fischer-Tropsch process where carbon monoxide is hydrogenated to liquid hydrocarbon fuels. This technology allows coal or biomass to be converted to diesel.

In the Cativa process, carbon monoxide and methanol react in the presence of a homogeneous Iridium catalyst and hydroiodic acid to give acetic acid. This process is responsible for most of the industrial production of acetic acid.

An industrial scale use for pure carbon monoxide is purifying nickel in the Mond process.

Carbon monoxide can also be used in the water-gas shift reaction to produce hydrogen.

Carbon monoxide is used in modified atmosphere packaging systems in the US, mainly with fresh meat products such as beef, pork, and fish to keep them looking fresh. The carbon monoxide combines with myoglobin to form carboxymyoglobin, a bright-cherry-red pigment. Carboxymyoglobin is more stable than the oxygenated form of myoglobin, oxymyoglobin, which can become oxidized to the brown pigment metmyoglobin. This stable red color can persist much longer than in normally packaged meat. Typical levels of carbon monoxide used in the facilities that use this process are between 0.4% to 0.5%.

The technology was first given "generally recognized as safe" (GRAS) status by the U.S. Food and Drug Administration (FDA) in 2002 for use as a secondary packaging system, and does not require labeling. In 2004, the FDA approved CO as primary packaging method, declaring that CO does not mask spoilage odor. Despite this ruling, the process remains controversial for fears that it masks spoilage. In 2007, a bill was introduced to the United States House of Representatives to label modified atmosphere carbon monoxide packaging as a color additive, but the bill died in subcommittee. The process is banned in many other countries, including Japan, Singapore, and the European Union.

In biology, carbon monoxide is naturally produced by the action of heme oxygenase 1 and 2 on the heme from hemoglobin breakdown. This process produces a certain amount of carboxyhemoglobin in normal persons, even if they do not breathe any carbon monoxide.

Following the first report that carbon monoxide is a normal neurotransmitter in 1993, as well as one of three gases that naturally modulate inflammatory responses in the body (the other two being nitric oxide and hydrogen sulfide), carbon monoxide has received a great deal of clinical attention as a biological regulator. In many tissues, all three gases are known to act as anti-inflammatories, vasodilators, and encouragers of neovascular growth. However, the issues are complex, as neovascular growth is not always beneficial, since it plays a role in tumor growth, and also the damage from "wet" macular degeneration, a disease for which smoking (a major source of carbon monoxide in the blood, several times more than natural production) increases the risk from 4 to 6 times.

There is a theory that, in some nerve cell synapses, when long-term memories are being laid down, the receiving cell makes carbon monoxide, which back-transmits to the transmitting cell, telling it to transmit more readily in future. Some such nerve cells have been shown to contain guanylate cyclase, an enzyme that is activated by carbon monoxide.

Studies involving carbon monoxide have been conducted in many laboratories throughout the world for its anti-inflammatory and cytoprotective properties. These properties have potential to be used to prevent the development of a series of pathological conditions including ischemia reperfusion injury, transplant rejection, atherosclerosis, severe sepsis, severe malaria, or autoimmunity. Clinical tests involving humans have been performed, however the results have not yet been released.

Carbon monoxide is a strong reductive agent, and whilst not known, it has been used in pyrometallurgy to reduce metals from ores since ancient times. Carbon monoxide strips oxygen off metal oxides, reducing them to pure metal in high temperatures, forming carbon dioxide in the process. Carbon monoxide is not usually supplied as is, in gaseous phase, in the reactor, but rather it is formed in high temperature in presence of oxygen-carrying ore, carboniferous agent such as coke and high temperature. The blast furnace process is a typical example of a process of reduction of metal from ore with carbon monoxide.

Carbon monoxide has also been used as a lasing medium in high-powered infrared lasers.

Carbon monoxide has been proposed for use as a fuel on Mars. Carbon monoxide/oxygen engines have been suggested for early surface transportation use as both carbon monoxide and oxygen can be straightforwardly produced from the atmosphere of Mars by zirconia electrolysis, without using any Martian water resources to obtain hydrogen, which would be needed to make methane or any hydrogen-based fuel. Likewise, blast furnace gas collected at the top of blast furnace, still contains some 10% to 30% of carbon monoxide, and is used as fuel on Cowper stoves and on Siemens-Martin furnaces on open hearth steelmaking.




</doc>
<doc id="6138" url="https://en.wikipedia.org/wiki?curid=6138" title="Conjecture">
Conjecture

In mathematics, a conjecture is a conclusion or proposition based on incomplete information, for which no proof or disproof has yet been found. Conjectures such as the Riemann hypothesis (still a conjecture) or Fermat's Last Theorem (which was a conjecture until proven in 1995 by Andrew Wiles) have shaped much of mathematical history as new areas of mathematics are developed in order to prove them.

In number theory, Fermat's Last Theorem (sometimes called Fermat's conjecture, especially in older texts) states that no three positive integers "a", "b", and "c" can satisfy the equation "a" + "b" = "c" for any integer value of "n" greater than two.

This theorem was first conjectured by Pierre de Fermat in 1637 in the margin of a copy of "Arithmetica" where he claimed he had a proof that was too large to fit in the margin. The first successful proof was released in 1994 by Andrew Wiles, and formally published in 1995, after 358 years of effort by mathematicians. The unsolved problem stimulated the development of algebraic number theory in the 19th century and the proof of the modularity theorem in the 20th century. It is among the most notable theorems in the history of mathematics and prior to its proof it was in the "Guinness Book of World Records" for "most difficult mathematical problems".

In mathematics, the four color theorem, or the four color map theorem, states that, given any separation of a plane into contiguous regions, producing a figure called a "map", no more than four colors are required to color the regions of the map so that no two adjacent regions have the same color. Two regions are called "adjacent" if they share a common boundary that is not a corner, where corners are the points shared by three or more regions. For example, in the map of the United States of America, Utah and Arizona are adjacent, but Utah and New Mexico, which only share a point that also belongs to Arizona and Colorado, are not.

Möbius mentioned the problem in his lectures as early as 1840. The conjecture was first proposed on October 23, 1852 when Francis Guthrie, while trying to color the map of countries of England, noticed that only four different colors were needed. The five color theorem, which has a short elementary proof, states that five colors suffice to color a map and was proven in the late 19th century ; however, proving that four colors suffice turned out to be significantly harder. A number of false proofs and false counterexamples have appeared since the first statement of the four color theorem in 1852.

The four color theorem was proven in 1976 by Kenneth Appel and Wolfgang Haken. It was the first major theorem to be proved using a computer. Appel and Haken's approach started by showing that there is a particular set of 1,936 maps, each of which cannot be part of a smallest-sized counterexample to the four color theorem. (If they did appear, you could make a smaller counter-example.) Appel and Haken used a special-purpose computer program to confirm that each of these maps had this property. Additionally, any map that could potentially be a counterexample must have a portion that looks like one of these 1,936 maps. Showing this required hundreds of pages of hand analysis, Appel and Haken concluded that no smallest counterexamples exists because any must contain, yet do not contain, one of these 1,936 maps. This contradiction means there are no counterexamples at all and that the theorem is therefore true. Initially, their proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand . Since then the proof has gained wider acceptance, although doubts remain .

The Hauptvermutung (German for main conjecture) of geometric topology is the conjecture that any two triangulations of a triangulable space have a common refinement, a single triangulation that is a subdivision of both of them. It was originally formulated in 1908, by Steinitz and Tietze.

This conjecture is now known to be false. The non-manifold version was disproved by John Milnor in 1961 using Reidemeister torsion.

The manifold version is true in dimensions . The cases were proved by Tibor Radó and Edwin E. Moise in the 1920s and 1950s, respectively.

In mathematics, the Weil conjectures were some highly influential proposals by on the generating functions (known as local zeta-functions) derived from counting the number of points on algebraic varieties over finite fields.

A variety "V" over a finite field with "q" elements has a finite number of rational points, as well as points over every finite field with "q" elements containing that field. The generating function has coefficients derived from the numbers "N" of points over the (essentially unique) field with "q" elements.

Weil conjectured that such "zeta-functions" should be rational functions, should satisfy a form of functional equation, and should have their zeroes in restricted places. The last two parts were quite consciously modeled on the Riemann zeta function and Riemann hypothesis. The rationality was proved by , the functional equation by , and the analogue of the Riemann hypothesis was proved by 

In mathematics, the Poincaré conjecture is a theorem about the characterization of the 3-sphere, which is the hypersphere that bounds the unit ball in four-dimensional space. The conjecture states: An equivalent form of the conjecture involves a coarser form of equivalence than homeomorphism called homotopy equivalence: if a 3-manifold is "homotopy equivalent" to the 3-sphere, then it is necessarily "homeomorphic" to it.

Originally conjectured by Henri Poincaré, the theorem concerns a space that locally looks like ordinary three-dimensional space but is connected, finite in size, and lacks any boundary (a closed 3-manifold). The Poincaré conjecture claims that if such a space has the additional property that each loop in the space can be continuously tightened to a point, then it is necessarily a three-dimensional sphere. An analogous result has been known in higher dimensions for some time.

After nearly a century of effort by mathematicians, Grigori Perelman presented a proof of the conjecture in three papers made available in 2002 and 2003 on arXiv. The proof followed on from the program of Richard S. Hamilton to use the Ricci flow to attempt to solve the problem. Hamilton later introduced a modification of the standard Ricci flow, called "Ricci flow with surgery" to systematically excise singular regions as they develop, in a controlled way, but was unable to prove this method "converged" in three dimensions. Perelman completed this portion of the proof. Several teams of mathematicians have verified that Perelman's proof is correct.

The Poincaré conjecture, before being proven, was one of the most important open questions in topology.

In mathematics, the Riemann hypothesis, proposed by , is a conjecture that the non-trivial zeros of the Riemann zeta function all have real part 1/2. The name is also used for some closely related analogues, such as the Riemann hypothesis for curves over finite fields.

The Riemann hypothesis implies results about the distribution of prime numbers. Along with suitable generalizations, some mathematicians consider it the most important unresolved problem in pure mathematics . The Riemann hypothesis, along with the Goldbach conjecture, is part of Hilbert's eighth problem in David Hilbert's list of 23 unsolved problems; it is also one of the Clay Mathematics Institute Millennium Prize Problems.

The P versus NP problem is a major unsolved problem in computer science. Informally, it asks whether every problem whose solution can be quickly verified by a computer can also be quickly solved by a computer; it is widely conjectured that the answer is no. It was essentially first mentioned in a 1956 letter written by Kurt Gödel to John von Neumann. Gödel asked whether a certain NP-complete problem could be solved in quadratic or linear time. The precise statement of the P=NP problem was introduced in 1971 by Stephen Cook in his seminal paper "The complexity of theorem proving procedures" and is considered by many to be the most important open problem in the field. It is one of the seven Millennium Prize Problems selected by the Clay Mathematics Institute to carry a US$1,000,000 prize for the first correct solution.


Formal mathematics is based on "provable" truth. In mathematics, any number of cases supporting a conjecture, no matter how large, is insufficient for establishing the conjecture's veracity, since a single counterexample would immediately bring down the conjecture. Mathematical journals sometimes publish the minor results of research teams having extended the search for a counterexample farther than previously done. For instance, the Collatz conjecture, which concerns whether or not certain sequences of integers terminate, has been tested for all integers up to 1.2 × 10 (over a trillion). However, the failure to find a counterexample after extensive search does not constitute a proof that no counterexample exists nor that the conjecture is true, because the conjecture might be false but with a very large minimal counterexample.

Instead, a conjecture is considered proven only when it has been shown that it is logically impossible for it to be false. There are various methods of doing so; see Mathematical proof#Methods for details.

One method of proof, usable when there are only a finite number of cases that could lead to counterexamples, is known as "brute force": in this approach, all possible cases are considered and shown not to give counterexamples. Sometimes the number of cases is quite large, in which situation a brute-force proof may require as a practical matter the use of a computer algorithm to check all the cases: the validity of the 1976 and 1997 brute-force proofs of the four color theorem by computer was initially doubted, but was eventually confirmed in 2005 by theorem-proving software.

When a conjecture has been proven, it is no longer a conjecture but a theorem. Many important theorems were once conjectures, such as the Geometrization theorem (which resolved the Poincaré conjecture), Fermat's Last Theorem, and others.

Conjectures disproven through counterexample are sometimes referred to as "false conjectures" (cf. the Pólya conjecture and Euler's sum of powers conjecture). In the case of the latter, the first counterexample found for the n=4 case involved numbers in the millions, although subsequently it has been found that the minimal counterexample is smaller than that.

Not every conjecture ends up being proven true or false. The continuum hypothesis, which tries to ascertain the relative cardinality of certain infinite sets, was eventually shown to be undecidable (or independent) from the generally accepted set of axioms of set theory. It is therefore possible to adopt this statement, or its negation, as a new axiom in a consistent manner (much as we can take Euclid's parallel postulate as either true or false).

In this case, if a proof uses this statement, researchers will often look for a new proof that "doesn't" require the hypothesis (in the same way that it is desirable that statements in Euclidean geometry be proved using only the axioms of neutral geometry, i.e. no parallel postulate.) The one major exception to this in practice is the axiom of choice—unless studying this axiom in particular, the majority of researchers do not usually worry whether a result requires the axiom of choice.

Sometimes a conjecture is called a "hypothesis" when it is used frequently and repeatedly as an assumption in proofs of other results. For example, the Riemann hypothesis is a conjecture from number theory that (amongst other things) makes predictions about the distribution of prime numbers. Few number theorists doubt that the Riemann hypothesis is true. In anticipation of its eventual proof, some have proceeded to develop further proofs which are contingent on the truth of this conjecture. These are called "conditional proofs": the conjectures assumed appear in the hypotheses of the theorem, for the time being.

These "proofs", however, would fall apart if it turned out that the hypothesis was false, so there is considerable interest in verifying the truth or falsity of conjectures of this type.

Karl Popper pioneered the use of the term "conjecture" in scientific philosophy. Conjecture is related to hypothesis, which in science refers to a testable conjecture.




</doc>
<doc id="6139" url="https://en.wikipedia.org/wiki?curid=6139" title="Christoph Ludwig Agricola">
Christoph Ludwig Agricola

Christoph Ludwig Agricola (November 5, 1667 – August 8, 1719) was a German landscape painter and etcher. He was born and died at Regensburg (Ratisbon).

Christoph Ludwig Agricola was born on 5 November 1667 at Regensburg in Germany. He trained, as many painters of the period did, by studying nature. 

He spent a great part of his life in travel, visiting England, the Netherlands and France, and residing for a considerable period at Naples, where he may have been influenced by Poussin. He also stayed for some years circa 1712 in Venice, where he painted many works for the patron Zaccaria Sagredo. 

He died in Regensburg in 1719. 

Although he primarily worked in gouache and oils, documentary sources reveal that he also produced a small number of etchings. He was a good draughtsman, used warm lighting and exhibited a warm, masterly brushstroke. 

His numerous landscapes, chiefly cabinet pictures, are remarkable for fidelity to nature, and especially for their skilful representation of varied phases of climate, especially nocturnal scenes and weather anomalies such as thunderstorms. In composition his style shows the influence of Nicolas Poussin, while in light and colour he imitates Claude Lorrain. His work often displays the idealistic scenes associated with his former mentor, Poussin. His compositions include ruins of ancient buildings in the foreground, but his favourite figure for the foreground was men dressed in Oriental attire. He also produced a series of etchings of birds. 

His pictures can be found in Dresden, Braunschweig, Vienna, Florence, Naples and many other towns of both Germany and Italy.

He probably tutored the artist, Johann Theile, and had an enormous influence on him. Art historians have also noted that the work of the landscape painter, Christian Johann Bendeler (1699-1728), was also influenced by Agricola. 


</doc>
